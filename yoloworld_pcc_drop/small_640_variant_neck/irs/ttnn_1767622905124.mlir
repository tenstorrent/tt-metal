#dram = #ttnn.buffer_type<dram>
#loc = loc(unknown)
#loc42 = loc("p0.1")
#loc43 = loc("p1.5")
#loc44 = loc("p2.9")
#loc45 = loc("p3.13")
#loc46 = loc("p4.17")
#loc47 = loc("p5.21")
#loc48 = loc("p6.31")
#loc49 = loc("p7.35")
#loc50 = loc("p8.40")
#loc51 = loc("p9.50")
#loc52 = loc("p10.54")
#loc53 = loc("p11.58")
#loc54 = loc("p12.62")
#loc55 = loc("p13.66")
#loc56 = loc("p14.68")
#loc57 = loc("p15.72")
#loc58 = loc("p16.76")
#loc59 = loc("p17.80")
#loc60 = loc("p18.84")
#loc61 = loc("p19.86")
#loc62 = loc("p20.90")
#loc63 = loc("p21.94")
#loc64 = loc("p22.98")
#loc65 = loc("p23.102")
#loc66 = loc("p24.104")
#loc67 = loc("p25.136")
#loc68 = loc("p26.140")
#loc69 = loc("p27.144")
#loc70 = loc("p28.148")
#loc71 = loc("p29.152")
#loc72 = loc("p30.156")
#loc73 = loc("p31.166")
#loc74 = loc("p32.170")
#loc75 = loc("p33.183")
#loc76 = loc("p34.187")
#loc77 = loc("p35.191")
#loc78 = loc("p36.195")
#loc79 = loc("p37.199")
#loc80 = loc("p38.201")
#loc81 = loc("p39.205")
#loc82 = loc("p40.209")
#loc83 = loc("p41.213")
#loc84 = loc("p42.217")
#loc85 = loc("p43.219")
#loc86 = loc("p44.223")
#loc87 = loc("p45.227")
#loc88 = loc("p46.231")
#loc89 = loc("p47.235")
#loc90 = loc("p48.237")
#loc91 = loc("p49.269")
#loc92 = loc("p50.354")
#loc93 = loc("p51.358")
#loc94 = loc("p52.362")
#loc95 = loc("p53.366")
#loc96 = loc("p54.370")
#loc97 = loc("p55.476")
#loc98 = loc("p56.480")
#loc99 = loc("p57.484")
#loc100 = loc("p58.488")
#loc101 = loc("p59.492")
#loc102 = loc("p60.515")
#loc103 = loc("p61.519")
#loc104 = loc("p62.523")
#loc105 = loc("p63.527")
#loc106 = loc("p64.531")
#loc107 = loc("p65.535")
#loc108 = loc("p66.545")
#loc109 = loc("p67.549")
#loc110 = loc("p68.562")
#loc111 = loc("p69.566")
#loc112 = loc("p70.570")
#loc113 = loc("p71.574")
#loc114 = loc("p72.578")
#loc115 = loc("p73.580")
#loc116 = loc("p74.584")
#loc117 = loc("p75.588")
#loc118 = loc("p76.592")
#loc119 = loc("p77.596")
#loc120 = loc("p78.598")
#loc121 = loc("p79.602")
#loc122 = loc("p80.606")
#loc123 = loc("p81.610")
#loc124 = loc("p82.614")
#loc125 = loc("p83.616")
#loc126 = loc("p84.620")
#loc127 = loc("p85.624")
#loc128 = loc("p86.628")
#loc129 = loc("p87.632")
#loc130 = loc("p88.719")
#loc131 = loc("p89.723")
#loc132 = loc("p90.727")
#loc133 = loc("p91.731")
#loc134 = loc("p92.735")
#loc135 = loc("p93.758")
#loc136 = loc("p94.762")
#loc137 = loc("p95.766")
#loc138 = loc("p96.770")
#loc139 = loc("p97.774")
#loc140 = loc("p98.778")
#loc141 = loc("p99.788")
#loc142 = loc("p100.792")
#loc143 = loc("p101.805")
#loc144 = loc("p102.809")
#loc145 = loc("p103.813")
#loc146 = loc("p104.817")
#loc147 = loc("p105.821")
#loc148 = loc("p106.823")
#loc149 = loc("p107.827")
#loc150 = loc("p108.831")
#loc151 = loc("p109.835")
#loc152 = loc("p110.839")
#loc153 = loc("p111.841")
#loc154 = loc("p112.845")
#loc155 = loc("p113.849")
#loc156 = loc("p114.853")
#loc157 = loc("p115.857")
#loc158 = loc("p116.859")
#loc159 = loc("p117.863")
#loc160 = loc("p118.867")
#loc161 = loc("p119.871")
#loc162 = loc("p120.875")
#loc163 = loc("p121.962")
#loc164 = loc("p122.966")
#loc165 = loc("p123.970")
#loc166 = loc("p124.974")
#loc167 = loc("p125.978")
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 102656, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073125888, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [1 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2x1x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x2x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64xbf16, #system_memory>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64xbf16, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128xbf16, #system_memory>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x128xbf16, #dram>, <interleaved>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #system_memory>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x256xbf16, #dram>, <interleaved>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 32 + d2, d3), <1x1>, memref<256x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4xbf16, #system_memory>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 * 32 + d2, d3), <1x1>, memref<4x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4xbf16, #dram>, <interleaved>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2xbf16, #system_memory>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 64 + d1 * 32 + d2, d3), <1x1>, memref<2x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2xbf16, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 64 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8xbf16, #system_memory>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8xbf16, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xbf16, #system_memory>>
#ttnn_layout36 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout37 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x512xbf16, #dram>, <interleaved>>
#ttnn_layout38 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout39 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout40 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 192 + d1 * 96 + d2, d3), <1x1>, memref<6x3x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout41 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2x3x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout42 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x3x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout43 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x3x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout44 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<3x1x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout45 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<3x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout46 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x3x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout47 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x3x!ttcore.tile<32x32, si32>, #dram>, <interleaved>>
#ttnn_layout48 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 + d2, d3), <1x1>, memref<32768x1xbf16, #system_memory>>
#ttnn_layout49 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout50 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 3 + d1, d2), <1x1>, memref<3x512xbf16, #dram>, <interleaved>>
#ttnn_layout51 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 192 + d1 * 3 + d2, d3), <1x1>, memref<12288x3xbf16, #system_memory>>
#ttnn_layout52 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 + d2, d3), <1x1>, memref<49152x1xbf16, #system_memory>>
#ttnn_layout53 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 10240 + d1 * 80 + d2, d3), <1x1>, memref<10240x80xbf16, #dram>, <interleaved>>
#ttnn_layout54 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 + d2, d3), <1x1>, memref<131072x1xbf16, #system_memory>>
#ttnn_layout55 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout56 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 3 + d2, d3), <1x1>, memref<49152x3xbf16, #system_memory>>
#ttnn_layout57 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 + d2, d3), <1x1>, memref<196608x1xbf16, #system_memory>>
#ttnn_layout58 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 10240 + d1 * 40 + d2, d3), <1x1>, memref<10240x40xbf16, #dram>, <interleaved>>
#ttnn_layout59 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 10240 + d1 * 20 + d2, d3), <1x1>, memref<10240x20xbf16, #dram>, <interleaved>>
#ttnn_layout60 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 + d2, d3), <1x1>, memref<98304x1xbf16, #system_memory>>
#ttnn_layout61 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 + d2, d3), <1x1>, memref<524288x1xbf16, #system_memory>>
#ttnn_layout62 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<8x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout63 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 3 + d2, d3), <1x1>, memref<196608x3xbf16, #system_memory>>
#ttnn_layout64 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 + d2, d3), <1x1>, memref<393216x1xbf16, #system_memory>>
#ttnn_layout65 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12288 + d1 * 96 + d2, d3), <1x1>, memref<384x3x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout66 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 64 + d2, d3), <1x1>, memref<512x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout67 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<320x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout68 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<320x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout69 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout70 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32768 + d1 * 64 + d2, d3), <1x1>, memref<1024x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout71 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<640x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout72 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<640x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout73 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32768 + d1 * 64 + d2, d3), <1x1>, memref<1024x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout74 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2560 + d1 * 64 + d2, d3), <1x1>, memref<80x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout75 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2560 + d1 * 64 + d2, d3), <1x1>, memref<80x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout76 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 1600 + d2, d3), <1x1>, memref<50x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout77 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 1600 + d2, d3), <1x1>, memref<50x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout78 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 1600 + d2, d3), <1x1>, memref<50x24x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout79 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 1600 + d2, d3), <1x1>, memref<1600x768xbf16, #dram>, <interleaved>>
#ttnn_layout80 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 1600 + d2, d3), <1x1>, memref<50x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout81 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 1600 + d2, d3), <1x1>, memref<1600x128xbf16, #dram>, <interleaved>>
#ttnn_layout82 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2560 + d1 * 64 + d2, d3), <1x1>, memref<80x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout83 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 64 + d2, d3), <1x1>, memref<256x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout84 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 8192 + d1 * 2048 + d2 * 64 + d3, d4), <1x1>, memref<256x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout85 = #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout86 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout87 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout88 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 96 + d1 * 32 + d2, d3), <1x1>, memref<3x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout89 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 10240 + d1 * 2560 + d2 * 64 + d3, d4), <1x1>, memref<320x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout90 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 6400 + d1 * 1600 + d2, d3), <1x1>, memref<200x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout91 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 256 + d1 * 64 + d2 * 64 + d3, d4), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout92 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 1600 + d2, d3), <1x1>, memref<1600x512xbf16, #dram>, <interleaved>>
#ttnn_layout93 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<320x3x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout94 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 64 + d2, d3), <1x1>, memref<512x3x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout95 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 24576 + d1 * 96 + d2, d3), <1x1>, memref<768x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout96 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<640x3x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout97 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 24576 + d1 * 96 + d2, d3), <1x1>, memref<768x3x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout98 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 7680 + d1 * 96 + d2, d3), <1x1>, memref<240x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout99 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 7680 + d1 * 96 + d2, d3), <1x1>, memref<240x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout100 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 6400 + d1 * 6400 + d2, d3), <1x1>, memref<200x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout101 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 6400 + d1 * 6400 + d2, d3), <1x1>, memref<200x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout102 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 6400 + d1 * 6400 + d2, d3), <1x1>, memref<200x12x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout103 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 6400 + d1 * 6400 + d2, d3), <1x1>, memref<6400x384xbf16, #dram>, <interleaved>>
#ttnn_layout104 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 6400 + d1 * 6400 + d2, d3), <1x1>, memref<200x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout105 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 6400 + d1 * 6400 + d2, d3), <1x1>, memref<6400x64xbf16, #dram>, <interleaved>>
#ttnn_layout106 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 7680 + d1 * 96 + d2, d3), <1x1>, memref<240x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout107 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 6144 + d1 * 96 + d2, d3), <1x1>, memref<192x3x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout108 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 6144 + d1 * 3072 + d2 * 96 + d3, d4), <1x1>, memref<192x3x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout109 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 15360 + d1 * 7680 + d2 * 96 + d3, d4), <1x1>, memref<480x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout110 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12800 + d1 * 6400 + d2, d3), <1x1>, memref<400x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout111 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 192 + d1 * 96 + d2 * 96 + d3, d4), <1x1>, memref<6x3x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout112 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 6400 + d1 * 6400 + d2, d3), <1x1>, memref<6400x256xbf16, #dram>, <interleaved>>
#ttnn_layout113 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 6400 + d1 * 6400 + d2, d3), <1x1>, memref<6400x128xbf16, #dram>, <interleaved>>
#ttnn_layout114 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 1600 + d2, d3), <1x1>, memref<50x12x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout115 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 1600 + d2, d3), <1x1>, memref<1600x384xbf16, #dram>, <interleaved>>
#ttnn_layout116 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 1600 + d2, d3), <1x1>, memref<1600x256xbf16, #dram>, <interleaved>>
#ttnn_layout117 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 416 + d1 * 416 + d2, d3), <1x1>, memref<13x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout118 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 640 + d1 * 32 + d2, d3), <1x1>, memref<20x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout119 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 640 + d1 * 32 + d2, d3), <1x1>, memref<20x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout120 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 416 + d1 * 416 + d2, d3), <1x1>, memref<13x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout121 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 416 + d1 * 416 + d2, d3), <1x1>, memref<13x24x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout122 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 400 + d1 * 400 + d2, d3), <1x1>, memref<400x768xbf16, #dram>, <interleaved>>
#ttnn_layout123 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 400 + d1 * 400 + d2, d3), <1x1>, memref<400x256xbf16, #dram>, <interleaved>>
#ttnn_layout124 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 8192 + d1 * 1024 + d2 * 32 + d3, d4), <1x1>, memref<256x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout125 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout126 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 5120 + d1 * 640 + d2 * 32 + d3, d4), <1x1>, memref<160x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout127 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3328 + d1 * 416 + d2, d3), <1x1>, memref<104x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout128 = #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 256 + d1 * 32 + d2 * 32 + d3, d4), <1x1>, memref<8x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout129 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 416 + d1 * 416 + d2, d3), <1x1>, memref<13x32x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout130 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 400 + d1 * 400 + d2, d3), <1x1>, memref<400x1024xbf16, #dram>, <interleaved>>
module @SyncTensorsGraph.1002 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.1002 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x1, chipIds = [0]> loc(#loc)
      func.func private @main_const_eval_0() -> tensor<20x40xbf16, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc1)
        %1 = "ttnn.constant"(%0) <{dtype = #ttcore.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]> : tensor<20xsi32>}> : (!ttnn.device) -> tensor<20xsi32, #ttnn_layout1> loc(#loc)
        %2 = "ttnn.constant"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, value = dense<[0.000000e+00, 5.000000e-01, 1.000000e+00, 1.500000e+00, 2.000000e+00, 2.500000e+00, 3.000000e+00, 3.500000e+00, 4.000000e+00, 4.500000e+00, 5.000000e+00, 5.500000e+00, 6.000000e+00, 6.500000e+00, 7.000000e+00, 7.500000e+00, 8.000000e+00, 8.500000e+00, 9.000000e+00, 9.500000e+00, 1.000000e+01, 1.050000e+01, 1.100000e+01, 1.150000e+01, 1.200000e+01, 1.250000e+01, 1.300000e+01, 1.350000e+01, 1.400000e+01, 1.450000e+01, 1.500000e+01, 1.550000e+01, 1.600000e+01, 1.650000e+01, 1.700000e+01, 1.750000e+01, 1.800000e+01, 1.850000e+01, 1.900000e+01, 1.950000e+01]> : tensor<40xf32>}> : (!ttnn.device) -> tensor<40xf32, #ttnn_layout2> loc(#loc)
        %3 = "ttnn.floor"(%2) : (tensor<40xf32, #ttnn_layout2>) -> tensor<40xf32, #ttnn_layout2> loc(#loc2)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<40xf32, #ttnn_layout2>) -> () loc(#loc2)
        %4 = "ttnn.typecast"(%3) <{dtype = #ttcore.supportedDataTypes<si32>}> : (tensor<40xf32, #ttnn_layout2>) -> tensor<40xsi32, #ttnn_layout3> loc(#loc3)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<40xf32, #ttnn_layout2>) -> () loc(#loc3)
        %5 = "ttnn.reshape"(%4) <{shape = [40 : i32, 1 : i32]}> : (tensor<40xsi32, #ttnn_layout3>) -> tensor<40x1xsi32, #ttnn_layout4> loc(#loc3)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<40xsi32, #ttnn_layout3>) -> () loc(#loc3)
        %6 = "ttnn.typecast"(%5) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<40x1xsi32, #ttnn_layout4>) -> tensor<40x1xf32, #ttnn_layout5> loc(#loc281)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<40x1xsi32, #ttnn_layout4>) -> () loc(#loc281)
        %7 = "ttnn.permute"(%6) <{permutation = array<i64: 1, 0>}> : (tensor<40x1xf32, #ttnn_layout5>) -> tensor<1x40xf32, #ttnn_layout6> loc(#loc3)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<40x1xf32, #ttnn_layout5>) -> () loc(#loc3)
        %8 = "ttnn.typecast"(%7) <{dtype = #ttcore.supportedDataTypes<si32>}> : (tensor<1x40xf32, #ttnn_layout6>) -> tensor<1x40xsi32, #ttnn_layout7> loc(#loc281)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x40xf32, #ttnn_layout6>) -> () loc(#loc281)
        %9 = "ttnn.reshape"(%1) <{shape = [1 : i32, 20 : i32]}> : (tensor<20xsi32, #ttnn_layout1>) -> tensor<1x20xsi32, #ttnn_layout8> loc(#loc4)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<20xsi32, #ttnn_layout1>) -> () loc(#loc4)
        %10 = "ttnn.typecast"(%9) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x20xsi32, #ttnn_layout8>) -> tensor<1x20xf32, #ttnn_layout9> loc(#loc379)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x20xsi32, #ttnn_layout8>) -> () loc(#loc379)
        %11 = "ttnn.permute"(%10) <{permutation = array<i64: 1, 0>}> : (tensor<1x20xf32, #ttnn_layout9>) -> tensor<20x1xf32, #ttnn_layout9> loc(#loc371)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x20xf32, #ttnn_layout9>) -> () loc(#loc371)
        %12 = "ttnn.typecast"(%11) <{dtype = #ttcore.supportedDataTypes<si32>}> : (tensor<20x1xf32, #ttnn_layout9>) -> tensor<20x1xsi32, #ttnn_layout8> loc(#loc379)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<20x1xf32, #ttnn_layout9>) -> () loc(#loc379)
        %13 = "ttnn.eq"(%8, %12) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x40xsi32, #ttnn_layout7>, tensor<20x1xsi32, #ttnn_layout8>) -> tensor<20x40xbf16, #ttnn_layout> loc(#loc6)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<20x1xsi32, #ttnn_layout8>) -> () loc(#loc6)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x40xsi32, #ttnn_layout7>) -> () loc(#loc6)
        return %13 : tensor<20x40xbf16, #ttnn_layout> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_1(%arg0: tensor<64xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x64x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<64xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<64xbf16, #ttnn_layout12> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<64xbf16, #ttnn_layout12>) -> tensor<64xbf16, #ttnn_layout13> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<64xbf16, #ttnn_layout12>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc7)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc7)
        return %3 : tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_2() -> tensor<1x8x20x20xbf16, #ttnn_layout14> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc1)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 5.656250e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x8x20x20>}> : (!ttnn.device) -> tensor<1x8x20x20xbf16, #ttnn_layout14> loc(#loc)
        return %1 : tensor<1x8x20x20xbf16, #ttnn_layout14> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_3(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc8)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc8)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_4(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x3x256xbf16, #ttnn_layout20> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x1x256xbf16, #ttnn_layout20> loc(#loc9)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc9)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x3x1>}> : (tensor<1x1x256xbf16, #ttnn_layout20>) -> tensor<1x3x256xbf16, #ttnn_layout20> loc(#loc9)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x256xbf16, #ttnn_layout20>) -> () loc(#loc9)
        return %4 : tensor<1x3x256xbf16, #ttnn_layout20> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_5(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc10)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc10)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_6(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc11)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc11)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_7(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc12)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc12)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_8(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc13)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc13)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_9(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc14)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc14)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_10(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc10)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc10)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_11(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc15)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc15)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_12(%arg0: tensor<64xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x3x64xbf16, #ttnn_layout24> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<64xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<64xbf16, #ttnn_layout12> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<64xbf16, #ttnn_layout12>) -> tensor<64xbf16, #ttnn_layout13> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<64xbf16, #ttnn_layout12>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x1x64xbf16, #ttnn_layout24> loc(#loc16)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc16)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x3x1>}> : (tensor<1x1x64xbf16, #ttnn_layout24>) -> tensor<1x3x64xbf16, #ttnn_layout24> loc(#loc16)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x64xbf16, #ttnn_layout24>) -> () loc(#loc16)
        return %4 : tensor<1x3x64xbf16, #ttnn_layout24> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_13(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc15)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc15)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_14(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc17)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc17)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_15(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc13)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc13)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_16(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc12)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc12)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_17(%arg0: tensor<64xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x64x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<64xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<64xbf16, #ttnn_layout12> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<64xbf16, #ttnn_layout12>) -> tensor<64xbf16, #ttnn_layout13> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<64xbf16, #ttnn_layout12>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc7)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc7)
        return %3 : tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_18(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc18)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc18)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_19(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc19)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc19)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_20(%arg0: tensor<4xbf16, #ttnn_layout25> loc(unknown)) -> tensor<1x4x1x1xbf16, #ttnn_layout26> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<4xbf16, #ttnn_layout25>, !ttnn.device) -> tensor<4xbf16, #ttnn_layout27> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<4xbf16, #ttnn_layout27>) -> tensor<4xbf16, #ttnn_layout28> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<4xbf16, #ttnn_layout27>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 4 : i32, 1 : i32, 1 : i32]}> : (tensor<4xbf16, #ttnn_layout28>) -> tensor<1x4x1x1xbf16, #ttnn_layout26> loc(#loc20)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<4xbf16, #ttnn_layout28>) -> () loc(#loc20)
        return %3 : tensor<1x4x1x1xbf16, #ttnn_layout26> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_21(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc21)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc21)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_22(%arg0: tensor<64xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x64x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<64xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<64xbf16, #ttnn_layout12> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<64xbf16, #ttnn_layout12>) -> tensor<64xbf16, #ttnn_layout13> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<64xbf16, #ttnn_layout12>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc7)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc7)
        return %3 : tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_23(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc13)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc13)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_24(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc22)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc22)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_25(%arg0: tensor<2xbf16, #ttnn_layout29> loc(unknown)) -> tensor<1x2x1x1xbf16, #ttnn_layout30> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<2xbf16, #ttnn_layout29>, !ttnn.device) -> tensor<2xbf16, #ttnn_layout31> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<2xbf16, #ttnn_layout31>) -> tensor<2xbf16, #ttnn_layout28> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<2xbf16, #ttnn_layout31>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 2 : i32, 1 : i32, 1 : i32]}> : (tensor<2xbf16, #ttnn_layout28>) -> tensor<1x2x1x1xbf16, #ttnn_layout30> loc(#loc23)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<2xbf16, #ttnn_layout28>) -> () loc(#loc23)
        return %3 : tensor<1x2x1x1xbf16, #ttnn_layout30> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_26(%arg0: tensor<64xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x64x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<64xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<64xbf16, #ttnn_layout12> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<64xbf16, #ttnn_layout12>) -> tensor<64xbf16, #ttnn_layout13> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<64xbf16, #ttnn_layout12>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc7)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc7)
        return %3 : tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_27(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc24)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc24)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_28(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc13)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc13)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_29(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc25)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc25)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_30(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc18)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc18)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_31(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc26)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc26)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_32(%arg0: tensor<64xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x64x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<64xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<64xbf16, #ttnn_layout12> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<64xbf16, #ttnn_layout12>) -> tensor<64xbf16, #ttnn_layout13> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<64xbf16, #ttnn_layout12>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc27)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc27)
        return %3 : tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_33(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc28)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc28)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_34(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc14)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc14)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_35(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc22)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc22)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_36(%arg0: tensor<64xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x64x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<64xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<64xbf16, #ttnn_layout12> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<64xbf16, #ttnn_layout12>) -> tensor<64xbf16, #ttnn_layout13> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<64xbf16, #ttnn_layout12>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc29)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc29)
        return %3 : tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_37() -> tensor<1x4x40x40xbf16, #ttnn_layout32> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc1)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 5.656250e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x4x40x40>}> : (!ttnn.device) -> tensor<1x4x40x40xbf16, #ttnn_layout32> loc(#loc)
        return %1 : tensor<1x4x40x40xbf16, #ttnn_layout32> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_38(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc25)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc25)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_39(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc25)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc25)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_40(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc14)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc14)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_41(%arg0: tensor<64xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x64x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<64xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<64xbf16, #ttnn_layout12> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<64xbf16, #ttnn_layout12>) -> tensor<64xbf16, #ttnn_layout13> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<64xbf16, #ttnn_layout12>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc29)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc29)
        return %3 : tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_42(%arg0: tensor<8xbf16, #ttnn_layout33> loc(unknown)) -> tensor<1x8x1x1xbf16, #ttnn_layout14> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<8xbf16, #ttnn_layout33>, !ttnn.device) -> tensor<8xbf16, #ttnn_layout34> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<8xbf16, #ttnn_layout34>) -> tensor<8xbf16, #ttnn_layout28> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<8xbf16, #ttnn_layout34>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 8 : i32, 1 : i32, 1 : i32]}> : (tensor<8xbf16, #ttnn_layout28>) -> tensor<1x8x1x1xbf16, #ttnn_layout14> loc(#loc30)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<8xbf16, #ttnn_layout28>) -> () loc(#loc30)
        return %3 : tensor<1x8x1x1xbf16, #ttnn_layout14> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_43(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc11)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc11)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_44(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc28)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc28)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_45(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc21)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc21)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_46(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc8)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc8)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_47(%arg0: tensor<512xbf16, #ttnn_layout35> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout36> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<512xbf16, #ttnn_layout35>, !ttnn.device) -> tensor<512xbf16, #ttnn_layout37> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<512xbf16, #ttnn_layout37>) -> tensor<512xbf16, #ttnn_layout38> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<512xbf16, #ttnn_layout37>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout38>) -> tensor<1x512x1x1xbf16, #ttnn_layout36> loc(#loc31)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<512xbf16, #ttnn_layout38>) -> () loc(#loc31)
        return %3 : tensor<1x512x1x1xbf16, #ttnn_layout36> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_48(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc26)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc26)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_49(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc8)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc8)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_50(%arg0: tensor<64xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x64x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<64xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<64xbf16, #ttnn_layout12> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<64xbf16, #ttnn_layout12>) -> tensor<64xbf16, #ttnn_layout13> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<64xbf16, #ttnn_layout12>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc29)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc29)
        return %3 : tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_51(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc24)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc24)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_52(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc12)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc12)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_53(%arg0: tensor<512xbf16, #ttnn_layout35> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout36> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<512xbf16, #ttnn_layout35>, !ttnn.device) -> tensor<512xbf16, #ttnn_layout37> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<512xbf16, #ttnn_layout37>) -> tensor<512xbf16, #ttnn_layout38> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<512xbf16, #ttnn_layout37>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout38>) -> tensor<1x512x1x1xbf16, #ttnn_layout36> loc(#loc32)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<512xbf16, #ttnn_layout38>) -> () loc(#loc32)
        return %3 : tensor<1x512x1x1xbf16, #ttnn_layout36> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_54(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc18)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc18)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_55(%arg0: tensor<64xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x64x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<64xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<64xbf16, #ttnn_layout12> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<64xbf16, #ttnn_layout12>) -> tensor<64xbf16, #ttnn_layout13> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<64xbf16, #ttnn_layout12>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc27)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc27)
        return %3 : tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_56(%arg0: tensor<512xbf16, #ttnn_layout35> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout36> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<512xbf16, #ttnn_layout35>, !ttnn.device) -> tensor<512xbf16, #ttnn_layout37> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<512xbf16, #ttnn_layout37>) -> tensor<512xbf16, #ttnn_layout38> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<512xbf16, #ttnn_layout37>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout38>) -> tensor<1x512x1x1xbf16, #ttnn_layout36> loc(#loc32)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<512xbf16, #ttnn_layout38>) -> () loc(#loc32)
        return %3 : tensor<1x512x1x1xbf16, #ttnn_layout36> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_57(%arg0: tensor<512xbf16, #ttnn_layout35> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout36> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<512xbf16, #ttnn_layout35>, !ttnn.device) -> tensor<512xbf16, #ttnn_layout37> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<512xbf16, #ttnn_layout37>) -> tensor<512xbf16, #ttnn_layout38> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<512xbf16, #ttnn_layout37>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout38>) -> tensor<1x512x1x1xbf16, #ttnn_layout36> loc(#loc32)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<512xbf16, #ttnn_layout38>) -> () loc(#loc32)
        return %3 : tensor<1x512x1x1xbf16, #ttnn_layout36> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_58(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc18)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc18)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_59(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc28)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc28)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_60(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc24)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc24)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_61(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc21)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc21)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_62(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc14)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc14)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_63(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc19)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc19)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_64(%arg0: tensor<512xbf16, #ttnn_layout35> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout36> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<512xbf16, #ttnn_layout35>, !ttnn.device) -> tensor<512xbf16, #ttnn_layout37> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<512xbf16, #ttnn_layout37>) -> tensor<512xbf16, #ttnn_layout38> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<512xbf16, #ttnn_layout37>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout38>) -> tensor<1x512x1x1xbf16, #ttnn_layout36> loc(#loc31)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<512xbf16, #ttnn_layout38>) -> () loc(#loc31)
        return %3 : tensor<1x512x1x1xbf16, #ttnn_layout36> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_65(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc33)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc33)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_66(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc19)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc19)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_67(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x3x128xbf16, #ttnn_layout39> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x1x128xbf16, #ttnn_layout39> loc(#loc34)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc34)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x3x1>}> : (tensor<1x1x128xbf16, #ttnn_layout39>) -> tensor<1x3x128xbf16, #ttnn_layout39> loc(#loc34)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x128xbf16, #ttnn_layout39>) -> () loc(#loc34)
        return %4 : tensor<1x3x128xbf16, #ttnn_layout39> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_68(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc11)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc11)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_69(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc19)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc19)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_70(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc33)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc33)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_71(%arg0: tensor<64xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x64x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<64xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<64xbf16, #ttnn_layout12> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<64xbf16, #ttnn_layout12>) -> tensor<64xbf16, #ttnn_layout13> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<64xbf16, #ttnn_layout12>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc27)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc27)
        return %3 : tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_72(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc8)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc8)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_73(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc10)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc10)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_74(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc12)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc12)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_75(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc26)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc26)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_76(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc10)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc10)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_77(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc15)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc15)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_78(%arg0: tensor<512xbf16, #ttnn_layout35> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout36> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<512xbf16, #ttnn_layout35>, !ttnn.device) -> tensor<512xbf16, #ttnn_layout37> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<512xbf16, #ttnn_layout37>) -> tensor<512xbf16, #ttnn_layout38> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<512xbf16, #ttnn_layout37>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout38>) -> tensor<1x512x1x1xbf16, #ttnn_layout36> loc(#loc32)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<512xbf16, #ttnn_layout38>) -> () loc(#loc32)
        return %3 : tensor<1x512x1x1xbf16, #ttnn_layout36> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_79(%arg0: tensor<64xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x64x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<64xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<64xbf16, #ttnn_layout12> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<64xbf16, #ttnn_layout12>) -> tensor<64xbf16, #ttnn_layout13> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<64xbf16, #ttnn_layout12>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc29)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc29)
        return %3 : tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_80(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x3x128xbf16, #ttnn_layout39> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x1x128xbf16, #ttnn_layout39> loc(#loc35)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc35)
        %4 = "ttnn.repeat"(%3) <{repeat_dims = #ttnn.shape<1x3x1>}> : (tensor<1x1x128xbf16, #ttnn_layout39>) -> tensor<1x3x128xbf16, #ttnn_layout39> loc(#loc35)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x1x128xbf16, #ttnn_layout39>) -> () loc(#loc35)
        return %4 : tensor<1x3x128xbf16, #ttnn_layout39> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_81(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc17)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc17)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_82(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc22)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc22)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_83(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc17)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc17)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_84(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc24)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc24)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_85(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc17)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc17)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_86(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc22)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc22)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_87(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc28)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc28)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_88(%arg0: tensor<4xbf16, #ttnn_layout25> loc(unknown)) -> tensor<1x4x1x1xbf16, #ttnn_layout26> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<4xbf16, #ttnn_layout25>, !ttnn.device) -> tensor<4xbf16, #ttnn_layout27> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<4xbf16, #ttnn_layout27>) -> tensor<4xbf16, #ttnn_layout28> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<4xbf16, #ttnn_layout27>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 4 : i32, 1 : i32, 1 : i32]}> : (tensor<4xbf16, #ttnn_layout28>) -> tensor<1x4x1x1xbf16, #ttnn_layout26> loc(#loc36)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<4xbf16, #ttnn_layout28>) -> () loc(#loc36)
        return %3 : tensor<1x4x1x1xbf16, #ttnn_layout26> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_89(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc11)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc11)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_90(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc33)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc33)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_91(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc21)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc21)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_92(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc26)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc26)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_93(%arg0: tensor<64xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x64x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<64xbf16, #ttnn_layout10>, !ttnn.device) -> tensor<64xbf16, #ttnn_layout12> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<64xbf16, #ttnn_layout12>) -> tensor<64xbf16, #ttnn_layout13> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<64xbf16, #ttnn_layout12>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout13>) -> tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc27)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<64xbf16, #ttnn_layout13>) -> () loc(#loc27)
        return %3 : tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_94() -> tensor<1x2x80x80xbf16, #ttnn_layout40> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc1)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 5.656250e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x2x80x80>}> : (!ttnn.device) -> tensor<1x2x80x80xbf16, #ttnn_layout40> loc(#loc)
        return %1 : tensor<1x2x80x80xbf16, #ttnn_layout40> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_95() -> tensor<40x80xbf16, #ttnn_layout41> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc1)
        %1 = "ttnn.constant"(%0) <{dtype = #ttcore.supportedDataTypes<si32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]> : tensor<40xsi32>}> : (!ttnn.device) -> tensor<40xsi32, #ttnn_layout3> loc(#loc)
        %2 = "ttnn.constant"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<#dram, <interleaved>>, value = dense<[0.000000e+00, 5.000000e-01, 1.000000e+00, 1.500000e+00, 2.000000e+00, 2.500000e+00, 3.000000e+00, 3.500000e+00, 4.000000e+00, 4.500000e+00, 5.000000e+00, 5.500000e+00, 6.000000e+00, 6.500000e+00, 7.000000e+00, 7.500000e+00, 8.000000e+00, 8.500000e+00, 9.000000e+00, 9.500000e+00, 1.000000e+01, 1.050000e+01, 1.100000e+01, 1.150000e+01, 1.200000e+01, 1.250000e+01, 1.300000e+01, 1.350000e+01, 1.400000e+01, 1.450000e+01, 1.500000e+01, 1.550000e+01, 1.600000e+01, 1.650000e+01, 1.700000e+01, 1.750000e+01, 1.800000e+01, 1.850000e+01, 1.900000e+01, 1.950000e+01, 2.000000e+01, 2.050000e+01, 2.100000e+01, 2.150000e+01, 2.200000e+01, 2.250000e+01, 2.300000e+01, 2.350000e+01, 2.400000e+01, 2.450000e+01, 2.500000e+01, 2.550000e+01, 2.600000e+01, 2.650000e+01, 2.700000e+01, 2.750000e+01, 2.800000e+01, 2.850000e+01, 2.900000e+01, 2.950000e+01, 3.000000e+01, 3.050000e+01, 3.100000e+01, 3.150000e+01, 3.200000e+01, 3.250000e+01, 3.300000e+01, 3.350000e+01, 3.400000e+01, 3.450000e+01, 3.500000e+01, 3.550000e+01, 3.600000e+01, 3.650000e+01, 3.700000e+01, 3.750000e+01, 3.800000e+01, 3.850000e+01, 3.900000e+01, 3.950000e+01]> : tensor<80xf32>}> : (!ttnn.device) -> tensor<80xf32, #ttnn_layout42> loc(#loc)
        %3 = "ttnn.floor"(%2) : (tensor<80xf32, #ttnn_layout42>) -> tensor<80xf32, #ttnn_layout42> loc(#loc37)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<80xf32, #ttnn_layout42>) -> () loc(#loc37)
        %4 = "ttnn.typecast"(%3) <{dtype = #ttcore.supportedDataTypes<si32>}> : (tensor<80xf32, #ttnn_layout42>) -> tensor<80xsi32, #ttnn_layout43> loc(#loc38)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<80xf32, #ttnn_layout42>) -> () loc(#loc38)
        %5 = "ttnn.reshape"(%4) <{shape = [80 : i32, 1 : i32]}> : (tensor<80xsi32, #ttnn_layout43>) -> tensor<80x1xsi32, #ttnn_layout44> loc(#loc38)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<80xsi32, #ttnn_layout43>) -> () loc(#loc38)
        %6 = "ttnn.typecast"(%5) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<80x1xsi32, #ttnn_layout44>) -> tensor<80x1xf32, #ttnn_layout45> loc(#loc283)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<80x1xsi32, #ttnn_layout44>) -> () loc(#loc283)
        %7 = "ttnn.permute"(%6) <{permutation = array<i64: 1, 0>}> : (tensor<80x1xf32, #ttnn_layout45>) -> tensor<1x80xf32, #ttnn_layout46> loc(#loc38)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<80x1xf32, #ttnn_layout45>) -> () loc(#loc38)
        %8 = "ttnn.typecast"(%7) <{dtype = #ttcore.supportedDataTypes<si32>}> : (tensor<1x80xf32, #ttnn_layout46>) -> tensor<1x80xsi32, #ttnn_layout47> loc(#loc283)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x80xf32, #ttnn_layout46>) -> () loc(#loc283)
        %9 = "ttnn.reshape"(%1) <{shape = [1 : i32, 40 : i32]}> : (tensor<40xsi32, #ttnn_layout3>) -> tensor<1x40xsi32, #ttnn_layout7> loc(#loc39)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<40xsi32, #ttnn_layout3>) -> () loc(#loc39)
        %10 = "ttnn.typecast"(%9) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x40xsi32, #ttnn_layout7>) -> tensor<1x40xf32, #ttnn_layout6> loc(#loc380)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x40xsi32, #ttnn_layout7>) -> () loc(#loc380)
        %11 = "ttnn.permute"(%10) <{permutation = array<i64: 1, 0>}> : (tensor<1x40xf32, #ttnn_layout6>) -> tensor<40x1xf32, #ttnn_layout5> loc(#loc372)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x40xf32, #ttnn_layout6>) -> () loc(#loc372)
        %12 = "ttnn.typecast"(%11) <{dtype = #ttcore.supportedDataTypes<si32>}> : (tensor<40x1xf32, #ttnn_layout5>) -> tensor<40x1xsi32, #ttnn_layout4> loc(#loc380)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<40x1xf32, #ttnn_layout5>) -> () loc(#loc380)
        %13 = "ttnn.eq"(%8, %12) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x80xsi32, #ttnn_layout47>, tensor<40x1xsi32, #ttnn_layout4>) -> tensor<40x80xbf16, #ttnn_layout41> loc(#loc41)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<40x1xsi32, #ttnn_layout4>) -> () loc(#loc41)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x80xsi32, #ttnn_layout47>) -> () loc(#loc41)
        return %13 : tensor<40x80xbf16, #ttnn_layout41> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_96(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc15)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc15)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_97(%arg0: tensor<128xbf16, #ttnn_layout15> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout16> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<128xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<128xbf16, #ttnn_layout17> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<128xbf16, #ttnn_layout17>) -> tensor<128xbf16, #ttnn_layout18> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<128xbf16, #ttnn_layout17>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout18>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc25)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<128xbf16, #ttnn_layout18>) -> () loc(#loc25)
        return %3 : tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_98(%arg0: tensor<256xbf16, #ttnn_layout19> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout23> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<256xbf16, #ttnn_layout19>, !ttnn.device) -> tensor<256xbf16, #ttnn_layout21> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<256xbf16, #ttnn_layout21>) -> tensor<256xbf16, #ttnn_layout22> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<256xbf16, #ttnn_layout21>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout22>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc33)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<256xbf16, #ttnn_layout22>) -> () loc(#loc33)
        return %3 : tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_99(%arg0: tensor<512xbf16, #ttnn_layout35> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout36> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<512xbf16, #ttnn_layout35>, !ttnn.device) -> tensor<512xbf16, #ttnn_layout37> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<512xbf16, #ttnn_layout37>) -> tensor<512xbf16, #ttnn_layout38> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<512xbf16, #ttnn_layout37>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout38>) -> tensor<1x512x1x1xbf16, #ttnn_layout36> loc(#loc31)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<512xbf16, #ttnn_layout38>) -> () loc(#loc31)
        return %3 : tensor<1x512x1x1xbf16, #ttnn_layout36> loc(#loc)
      } loc(#loc)
      func.func private @main_const_eval_100(%arg0: tensor<512xbf16, #ttnn_layout35> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout36> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.to_device"(%arg0, %0) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<512xbf16, #ttnn_layout35>, !ttnn.device) -> tensor<512xbf16, #ttnn_layout37> loc(#loc)
        %2 = "ttnn.to_layout"(%1) <{layout = #ttnn.layout<tile>}> : (tensor<512xbf16, #ttnn_layout37>) -> tensor<512xbf16, #ttnn_layout38> loc(#loc)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<512xbf16, #ttnn_layout37>) -> () loc(#loc)
        %3 = "ttnn.reshape"(%2) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout38>) -> tensor<1x512x1x1xbf16, #ttnn_layout36> loc(#loc31)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<512xbf16, #ttnn_layout38>) -> () loc(#loc31)
        return %3 : tensor<1x512x1x1xbf16, #ttnn_layout36> loc(#loc)
      } loc(#loc)
      func.func @main(%arg0: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_final_conv_batch_norm2d_running_var"} loc("p0.1"), %arg1: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_final_conv_batch_norm2d_running_mean"} loc("p1.5"), %arg2: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_final_conv_batch_norm2d_bias"} loc("p2.9"), %arg3: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_final_conv_batch_norm2d_weight"} loc("p3.13"), %arg4: tensor<128x256x1x1xbf16, #ttnn_layout48> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___neck_top_down_layers_1_final_conv_conv_weight"} loc("p4.17"), %arg5: tensor<2xbf16, #ttnn_layout29> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_attn_block_bias"} loc("p5.21"), %arg6: tensor<64xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_attn_block_guide_fc_bias"} loc("p6.31"), %arg7: tensor<64x512xbf16, #ttnn_layout49> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_attn_block_guide_fc_weight"} loc("p7.35"), %arg8: tensor<1x3x512xbf16, #ttnn_layout50> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_3"} loc("p8.40"), %arg9: tensor<64xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv2_batch_norm2d_running_var"} loc("p9.50"), %arg10: tensor<64xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv2_batch_norm2d_running_mean"} loc("p10.54"), %arg11: tensor<64xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv2_batch_norm2d_bias"} loc("p11.58"), %arg12: tensor<64xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv2_batch_norm2d_weight"} loc("p12.62"), %arg13: tensor<64x64x3x3xbf16, #ttnn_layout51> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv2_conv_weight"} loc("p13.66"), %arg14: tensor<64xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv1_batch_norm2d_running_var"} loc("p14.68"), %arg15: tensor<64xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv1_batch_norm2d_running_mean"} loc("p15.72"), %arg16: tensor<64xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv1_batch_norm2d_bias"} loc("p16.76"), %arg17: tensor<64xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv1_batch_norm2d_weight"} loc("p17.80"), %arg18: tensor<64x64x3x3xbf16, #ttnn_layout51> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv1_conv_weight"} loc("p18.84"), %arg19: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_main_conv_batch_norm2d_running_var"} loc("p19.86"), %arg20: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_main_conv_batch_norm2d_running_mean"} loc("p20.90"), %arg21: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_main_conv_batch_norm2d_bias"} loc("p21.94"), %arg22: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_main_conv_batch_norm2d_weight"} loc("p22.98"), %arg23: tensor<128x384x1x1xbf16, #ttnn_layout52> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___neck_top_down_layers_1_main_conv_conv_weight"} loc("p23.102"), %arg24: tensor<1x128x80x80xbf16, #ttnn_layout53> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("p24.104"), %arg25: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_final_conv_batch_norm2d_running_var"} loc("p25.136"), %arg26: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_final_conv_batch_norm2d_running_mean"} loc("p26.140"), %arg27: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_final_conv_batch_norm2d_bias"} loc("p27.144"), %arg28: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_final_conv_batch_norm2d_weight"} loc("p28.148"), %arg29: tensor<256x512x1x1xbf16, #ttnn_layout54> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___neck_top_down_layers_0_final_conv_conv_weight"} loc("p29.152"), %arg30: tensor<4xbf16, #ttnn_layout25> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_attn_block_bias"} loc("p30.156"), %arg31: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_attn_block_guide_fc_bias"} loc("p31.166"), %arg32: tensor<128x512xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_attn_block_guide_fc_weight"} loc("p32.170"), %arg33: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv2_batch_norm2d_running_var"} loc("p33.183"), %arg34: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv2_batch_norm2d_running_mean"} loc("p34.187"), %arg35: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv2_batch_norm2d_bias"} loc("p35.191"), %arg36: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv2_batch_norm2d_weight"} loc("p36.195"), %arg37: tensor<128x128x3x3xbf16, #ttnn_layout56> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv2_conv_weight"} loc("p37.199"), %arg38: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv1_batch_norm2d_running_var"} loc("p38.201"), %arg39: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv1_batch_norm2d_running_mean"} loc("p39.205"), %arg40: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv1_batch_norm2d_bias"} loc("p40.209"), %arg41: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv1_batch_norm2d_weight"} loc("p41.213"), %arg42: tensor<128x128x3x3xbf16, #ttnn_layout56> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv1_conv_weight"} loc("p42.217"), %arg43: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_main_conv_batch_norm2d_running_var"} loc("p43.219"), %arg44: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_main_conv_batch_norm2d_running_mean"} loc("p44.223"), %arg45: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_main_conv_batch_norm2d_bias"} loc("p45.227"), %arg46: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_main_conv_batch_norm2d_weight"} loc("p46.231"), %arg47: tensor<256x768x1x1xbf16, #ttnn_layout57> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___neck_top_down_layers_0_main_conv_conv_weight"} loc("p47.235"), %arg48: tensor<1x256x40x40xbf16, #ttnn_layout58> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_1"} loc("p48.237"), %arg49: tensor<1x512x20x20xbf16, #ttnn_layout59> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_2"} loc("p49.269"), %arg50: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_attn_block_project_conv_batch_norm2d_running_var"} loc("p50.354"), %arg51: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_attn_block_project_conv_batch_norm2d_running_mean"} loc("p51.358"), %arg52: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_attn_block_project_conv_batch_norm2d_bias"} loc("p52.362"), %arg53: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_attn_block_project_conv_batch_norm2d_weight"} loc("p53.366"), %arg54: tensor<128x128x3x3xbf16, #ttnn_layout56> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___neck_top_down_layers_0_attn_block_project_conv_conv_weight"} loc("p54.370"), %arg55: tensor<64xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_attn_block_project_conv_batch_norm2d_running_var"} loc("p55.476"), %arg56: tensor<64xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_attn_block_project_conv_batch_norm2d_running_mean"} loc("p56.480"), %arg57: tensor<64xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_attn_block_project_conv_batch_norm2d_bias"} loc("p57.484"), %arg58: tensor<64xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_attn_block_project_conv_batch_norm2d_weight"} loc("p58.488"), %arg59: tensor<64x64x3x3xbf16, #ttnn_layout51> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___neck_top_down_layers_1_attn_block_project_conv_conv_weight"} loc("p59.492"), %arg60: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_final_conv_batch_norm2d_running_var"} loc("p60.515"), %arg61: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_final_conv_batch_norm2d_running_mean"} loc("p61.519"), %arg62: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_final_conv_batch_norm2d_bias"} loc("p62.523"), %arg63: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_final_conv_batch_norm2d_weight"} loc("p63.527"), %arg64: tensor<256x512x1x1xbf16, #ttnn_layout54> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___neck_bottom_up_layers_0_final_conv_conv_weight"} loc("p64.531"), %arg65: tensor<4xbf16, #ttnn_layout25> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_attn_block_bias"} loc("p65.535"), %arg66: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_attn_block_guide_fc_bias"} loc("p66.545"), %arg67: tensor<128x512xbf16, #ttnn_layout55> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_attn_block_guide_fc_weight"} loc("p67.549"), %arg68: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv2_batch_norm2d_running_var"} loc("p68.562"), %arg69: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv2_batch_norm2d_running_mean"} loc("p69.566"), %arg70: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv2_batch_norm2d_bias"} loc("p70.570"), %arg71: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv2_batch_norm2d_weight"} loc("p71.574"), %arg72: tensor<128x128x3x3xbf16, #ttnn_layout56> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv2_conv_weight"} loc("p72.578"), %arg73: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv1_batch_norm2d_running_var"} loc("p73.580"), %arg74: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv1_batch_norm2d_running_mean"} loc("p74.584"), %arg75: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv1_batch_norm2d_bias"} loc("p75.588"), %arg76: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv1_batch_norm2d_weight"} loc("p76.592"), %arg77: tensor<128x128x3x3xbf16, #ttnn_layout56> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv1_conv_weight"} loc("p77.596"), %arg78: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_main_conv_batch_norm2d_running_var"} loc("p78.598"), %arg79: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_main_conv_batch_norm2d_running_mean"} loc("p79.602"), %arg80: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_main_conv_batch_norm2d_bias"} loc("p80.606"), %arg81: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_main_conv_batch_norm2d_weight"} loc("p81.610"), %arg82: tensor<256x384x1x1xbf16, #ttnn_layout60> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___neck_bottom_up_layers_0_main_conv_conv_weight"} loc("p82.614"), %arg83: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_downsample_layers_0_batch_norm2d_running_var"} loc("p83.616"), %arg84: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_downsample_layers_0_batch_norm2d_running_mean"} loc("p84.620"), %arg85: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_downsample_layers_0_batch_norm2d_bias"} loc("p85.624"), %arg86: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_downsample_layers_0_batch_norm2d_weight"} loc("p86.628"), %arg87: tensor<128x128x3x3xbf16, #ttnn_layout56> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___neck_downsample_layers_0_conv_weight"} loc("p87.632"), %arg88: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_attn_block_project_conv_batch_norm2d_running_var"} loc("p88.719"), %arg89: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_attn_block_project_conv_batch_norm2d_running_mean"} loc("p89.723"), %arg90: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_attn_block_project_conv_batch_norm2d_bias"} loc("p90.727"), %arg91: tensor<128xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_attn_block_project_conv_batch_norm2d_weight"} loc("p91.731"), %arg92: tensor<128x128x3x3xbf16, #ttnn_layout56> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___neck_bottom_up_layers_0_attn_block_project_conv_conv_weight"} loc("p92.735"), %arg93: tensor<512xbf16, #ttnn_layout35> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_final_conv_batch_norm2d_running_var"} loc("p93.758"), %arg94: tensor<512xbf16, #ttnn_layout35> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_final_conv_batch_norm2d_running_mean"} loc("p94.762"), %arg95: tensor<512xbf16, #ttnn_layout35> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_final_conv_batch_norm2d_bias"} loc("p95.766"), %arg96: tensor<512xbf16, #ttnn_layout35> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_final_conv_batch_norm2d_weight"} loc("p96.770"), %arg97: tensor<512x1024x1x1xbf16, #ttnn_layout61> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___neck_bottom_up_layers_1_final_conv_conv_weight"} loc("p97.774"), %arg98: tensor<8xbf16, #ttnn_layout33> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_attn_block_bias"} loc("p98.778"), %arg99: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_attn_block_guide_fc_bias"} loc("p99.788"), %arg100: tensor<256x512xbf16, #ttnn_layout62> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_attn_block_guide_fc_weight"} loc("p100.792"), %arg101: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv2_batch_norm2d_running_var"} loc("p101.805"), %arg102: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv2_batch_norm2d_running_mean"} loc("p102.809"), %arg103: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv2_batch_norm2d_bias"} loc("p103.813"), %arg104: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv2_batch_norm2d_weight"} loc("p104.817"), %arg105: tensor<256x256x3x3xbf16, #ttnn_layout63> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv2_conv_weight"} loc("p105.821"), %arg106: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv1_batch_norm2d_running_var"} loc("p106.823"), %arg107: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv1_batch_norm2d_running_mean"} loc("p107.827"), %arg108: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv1_batch_norm2d_bias"} loc("p108.831"), %arg109: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv1_batch_norm2d_weight"} loc("p109.835"), %arg110: tensor<256x256x3x3xbf16, #ttnn_layout63> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv1_conv_weight"} loc("p110.839"), %arg111: tensor<512xbf16, #ttnn_layout35> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_main_conv_batch_norm2d_running_var"} loc("p111.841"), %arg112: tensor<512xbf16, #ttnn_layout35> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_main_conv_batch_norm2d_running_mean"} loc("p112.845"), %arg113: tensor<512xbf16, #ttnn_layout35> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_main_conv_batch_norm2d_bias"} loc("p113.849"), %arg114: tensor<512xbf16, #ttnn_layout35> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_main_conv_batch_norm2d_weight"} loc("p114.853"), %arg115: tensor<512x768x1x1xbf16, #ttnn_layout64> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___neck_bottom_up_layers_1_main_conv_conv_weight"} loc("p115.857"), %arg116: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_downsample_layers_1_batch_norm2d_running_var"} loc("p116.859"), %arg117: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_downsample_layers_1_batch_norm2d_running_mean"} loc("p117.863"), %arg118: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_downsample_layers_1_batch_norm2d_bias"} loc("p118.867"), %arg119: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_downsample_layers_1_batch_norm2d_weight"} loc("p119.871"), %arg120: tensor<256x256x3x3xbf16, #ttnn_layout63> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___neck_downsample_layers_1_conv_weight"} loc("p120.875"), %arg121: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_attn_block_project_conv_batch_norm2d_running_var"} loc("p121.962"), %arg122: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_attn_block_project_conv_batch_norm2d_running_mean"} loc("p122.966"), %arg123: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_attn_block_project_conv_batch_norm2d_bias"} loc("p123.970"), %arg124: tensor<256xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_attn_block_project_conv_batch_norm2d_weight"} loc("p124.974"), %arg125: tensor<256x256x3x3xbf16, #ttnn_layout63> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___neck_bottom_up_layers_1_attn_block_project_conv_conv_weight"} loc("p125.978")) -> (tensor<1x128x80x80xbf16, #ttnn_layout65> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x256x40x40xbf16, #ttnn_layout66> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x512x20x20xbf16, #ttnn_layout36> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<20x40xbf16, #ttnn_layout> loc(#loc)
        %1 = ttcore.load_cached(@main_const_eval_1, [%arg12]) : (tensor<64xbf16, #ttnn_layout10>) -> tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<64xbf16, #ttnn_layout10>) -> () loc(#loc)
        %2 = ttcore.load_cached(@main_const_eval_2, []) : () -> tensor<1x8x20x20xbf16, #ttnn_layout14> loc(#loc)
        %3 = ttcore.load_cached(@main_const_eval_3, [%arg34]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg34) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %4 = ttcore.load_cached(@main_const_eval_4, [%arg99]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x3x256xbf16, #ttnn_layout20> loc(#loc)
        "ttnn.deallocate"(%arg99) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %5 = ttcore.load_cached(@main_const_eval_5, [%arg103]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg103) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %6 = ttcore.load_cached(@main_const_eval_6, [%arg50]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg50) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %7 = ttcore.load_cached(@main_const_eval_7, [%arg69]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg69) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %8 = ttcore.load_cached(@main_const_eval_8, [%arg79]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg79) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %9 = ttcore.load_cached(@main_const_eval_9, [%arg28]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg28) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %10 = ttcore.load_cached(@main_const_eval_10, [%arg101]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg101) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %11 = ttcore.load_cached(@main_const_eval_11, [%arg117]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg117) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %12 = ttcore.load_cached(@main_const_eval_12, [%arg6]) : (tensor<64xbf16, #ttnn_layout10>) -> tensor<1x3x64xbf16, #ttnn_layout24> loc(#loc)
        "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<64xbf16, #ttnn_layout10>) -> () loc(#loc)
        %13 = ttcore.load_cached(@main_const_eval_13, [%arg116]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg116) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %14 = ttcore.load_cached(@main_const_eval_14, [%arg85]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg85) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %15 = ttcore.load_cached(@main_const_eval_15, [%arg80]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg80) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %16 = ttcore.load_cached(@main_const_eval_16, [%arg71]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg71) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %17 = ttcore.load_cached(@main_const_eval_17, [%arg11]) : (tensor<64xbf16, #ttnn_layout10>) -> tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<64xbf16, #ttnn_layout10>) -> () loc(#loc)
        %18 = ttcore.load_cached(@main_const_eval_18, [%arg107]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg107) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %19 = ttcore.load_cached(@main_const_eval_19, [%arg1]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %20 = ttcore.load_cached(@main_const_eval_20, [%arg30]) : (tensor<4xbf16, #ttnn_layout25>) -> tensor<1x4x1x1xbf16, #ttnn_layout26> loc(#loc)
        "ttnn.deallocate"(%arg30) <{force = false}> : (tensor<4xbf16, #ttnn_layout25>) -> () loc(#loc)
        %21 = ttcore.load_cached(@main_const_eval_21, [%arg39]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg39) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %22 = ttcore.load_cached(@main_const_eval_22, [%arg10]) : (tensor<64xbf16, #ttnn_layout10>) -> tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<64xbf16, #ttnn_layout10>) -> () loc(#loc)
        %23 = ttcore.load_cached(@main_const_eval_23, [%arg81]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg81) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %24 = ttcore.load_cached(@main_const_eval_24, [%arg91]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg91) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %25 = ttcore.load_cached(@main_const_eval_25, [%arg5]) : (tensor<2xbf16, #ttnn_layout29>) -> tensor<1x2x1x1xbf16, #ttnn_layout30> loc(#loc)
        "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<2xbf16, #ttnn_layout29>) -> () loc(#loc)
        %26 = ttcore.load_cached(@main_const_eval_26, [%arg9]) : (tensor<64xbf16, #ttnn_layout10>) -> tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<64xbf16, #ttnn_layout10>) -> () loc(#loc)
        %27 = ttcore.load_cached(@main_const_eval_27, [%arg61]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg61) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %28 = ttcore.load_cached(@main_const_eval_28, [%arg78]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg78) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %29 = ttcore.load_cached(@main_const_eval_29, [%arg22]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg22) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %30 = ttcore.load_cached(@main_const_eval_30, [%arg108]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg108) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %31 = ttcore.load_cached(@main_const_eval_31, [%arg76]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg76) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %32 = ttcore.load_cached(@main_const_eval_32, [%arg14]) : (tensor<64xbf16, #ttnn_layout10>) -> tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg14) <{force = false}> : (tensor<64xbf16, #ttnn_layout10>) -> () loc(#loc)
        %33 = ttcore.load_cached(@main_const_eval_33, [%arg46]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg46) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %34 = ttcore.load_cached(@main_const_eval_34, [%arg25]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg25) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %35 = ttcore.load_cached(@main_const_eval_35, [%arg88]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg88) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %36 = ttcore.load_cached(@main_const_eval_36, [%arg56]) : (tensor<64xbf16, #ttnn_layout10>) -> tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg56) <{force = false}> : (tensor<64xbf16, #ttnn_layout10>) -> () loc(#loc)
        %37 = ttcore.load_cached(@main_const_eval_37, []) : () -> tensor<1x4x40x40xbf16, #ttnn_layout32> loc(#loc)
        %38 = ttcore.load_cached(@main_const_eval_38, [%arg21]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg21) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %39 = ttcore.load_cached(@main_const_eval_39, [%arg20]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg20) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %40 = ttcore.load_cached(@main_const_eval_40, [%arg26]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg26) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %41 = ttcore.load_cached(@main_const_eval_41, [%arg55]) : (tensor<64xbf16, #ttnn_layout10>) -> tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg55) <{force = false}> : (tensor<64xbf16, #ttnn_layout10>) -> () loc(#loc)
        %42 = ttcore.load_cached(@main_const_eval_42, [%arg98]) : (tensor<8xbf16, #ttnn_layout33>) -> tensor<1x8x1x1xbf16, #ttnn_layout14> loc(#loc)
        "ttnn.deallocate"(%arg98) <{force = false}> : (tensor<8xbf16, #ttnn_layout33>) -> () loc(#loc)
        %43 = ttcore.load_cached(@main_const_eval_43, [%arg51]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg51) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %44 = ttcore.load_cached(@main_const_eval_44, [%arg44]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg44) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %45 = ttcore.load_cached(@main_const_eval_45, [%arg38]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg38) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %46 = ttcore.load_cached(@main_const_eval_46, [%arg36]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg36) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %47 = ttcore.load_cached(@main_const_eval_47, [%arg96]) : (tensor<512xbf16, #ttnn_layout35>) -> tensor<1x512x1x1xbf16, #ttnn_layout36> loc(#loc)
        "ttnn.deallocate"(%arg96) <{force = false}> : (tensor<512xbf16, #ttnn_layout35>) -> () loc(#loc)
        %48 = ttcore.load_cached(@main_const_eval_48, [%arg74]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg74) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %49 = ttcore.load_cached(@main_const_eval_49, [%arg35]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg35) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %50 = ttcore.load_cached(@main_const_eval_50, [%arg58]) : (tensor<64xbf16, #ttnn_layout10>) -> tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg58) <{force = false}> : (tensor<64xbf16, #ttnn_layout10>) -> () loc(#loc)
        %51 = ttcore.load_cached(@main_const_eval_51, [%arg63]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg63) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %52 = ttcore.load_cached(@main_const_eval_52, [%arg70]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg70) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %53 = ttcore.load_cached(@main_const_eval_53, [%arg112]) : (tensor<512xbf16, #ttnn_layout35>) -> tensor<1x512x1x1xbf16, #ttnn_layout36> loc(#loc)
        "ttnn.deallocate"(%arg112) <{force = false}> : (tensor<512xbf16, #ttnn_layout35>) -> () loc(#loc)
        %54 = ttcore.load_cached(@main_const_eval_54, [%arg106]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg106) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %55 = ttcore.load_cached(@main_const_eval_55, [%arg15]) : (tensor<64xbf16, #ttnn_layout10>) -> tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg15) <{force = false}> : (tensor<64xbf16, #ttnn_layout10>) -> () loc(#loc)
        %56 = ttcore.load_cached(@main_const_eval_56, [%arg111]) : (tensor<512xbf16, #ttnn_layout35>) -> tensor<1x512x1x1xbf16, #ttnn_layout36> loc(#loc)
        "ttnn.deallocate"(%arg111) <{force = false}> : (tensor<512xbf16, #ttnn_layout35>) -> () loc(#loc)
        %57 = ttcore.load_cached(@main_const_eval_57, [%arg113]) : (tensor<512xbf16, #ttnn_layout35>) -> tensor<1x512x1x1xbf16, #ttnn_layout36> loc(#loc)
        "ttnn.deallocate"(%arg113) <{force = false}> : (tensor<512xbf16, #ttnn_layout35>) -> () loc(#loc)
        %58 = ttcore.load_cached(@main_const_eval_58, [%arg109]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg109) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %59 = ttcore.load_cached(@main_const_eval_59, [%arg45]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg45) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %60 = ttcore.load_cached(@main_const_eval_60, [%arg60]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg60) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %61 = ttcore.load_cached(@main_const_eval_61, [%arg41]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg41) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %62 = ttcore.load_cached(@main_const_eval_62, [%arg27]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg27) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %63 = ttcore.load_cached(@main_const_eval_63, [%arg2]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %64 = ttcore.load_cached(@main_const_eval_64, [%arg95]) : (tensor<512xbf16, #ttnn_layout35>) -> tensor<1x512x1x1xbf16, #ttnn_layout36> loc(#loc)
        "ttnn.deallocate"(%arg95) <{force = false}> : (tensor<512xbf16, #ttnn_layout35>) -> () loc(#loc)
        %65 = ttcore.load_cached(@main_const_eval_65, [%arg124]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg124) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %66 = ttcore.load_cached(@main_const_eval_66, [%arg0]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %67 = ttcore.load_cached(@main_const_eval_67, [%arg66]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x3x128xbf16, #ttnn_layout39> loc(#loc)
        "ttnn.deallocate"(%arg66) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %68 = ttcore.load_cached(@main_const_eval_68, [%arg53]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg53) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %69 = ttcore.load_cached(@main_const_eval_69, [%arg3]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %70 = ttcore.load_cached(@main_const_eval_70, [%arg123]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg123) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %71 = ttcore.load_cached(@main_const_eval_71, [%arg16]) : (tensor<64xbf16, #ttnn_layout10>) -> tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg16) <{force = false}> : (tensor<64xbf16, #ttnn_layout10>) -> () loc(#loc)
        %72 = ttcore.load_cached(@main_const_eval_72, [%arg33]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg33) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %73 = ttcore.load_cached(@main_const_eval_73, [%arg102]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg102) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %74 = ttcore.load_cached(@main_const_eval_74, [%arg68]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg68) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %75 = ttcore.load_cached(@main_const_eval_75, [%arg75]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg75) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %76 = ttcore.load_cached(@main_const_eval_76, [%arg104]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg104) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %77 = ttcore.load_cached(@main_const_eval_77, [%arg119]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg119) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %78 = ttcore.load_cached(@main_const_eval_78, [%arg114]) : (tensor<512xbf16, #ttnn_layout35>) -> tensor<1x512x1x1xbf16, #ttnn_layout36> loc(#loc)
        "ttnn.deallocate"(%arg114) <{force = false}> : (tensor<512xbf16, #ttnn_layout35>) -> () loc(#loc)
        %79 = ttcore.load_cached(@main_const_eval_79, [%arg57]) : (tensor<64xbf16, #ttnn_layout10>) -> tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg57) <{force = false}> : (tensor<64xbf16, #ttnn_layout10>) -> () loc(#loc)
        %80 = ttcore.load_cached(@main_const_eval_80, [%arg31]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x3x128xbf16, #ttnn_layout39> loc(#loc)
        "ttnn.deallocate"(%arg31) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %81 = ttcore.load_cached(@main_const_eval_81, [%arg86]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg86) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %82 = ttcore.load_cached(@main_const_eval_82, [%arg89]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg89) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %83 = ttcore.load_cached(@main_const_eval_83, [%arg84]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg84) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %84 = ttcore.load_cached(@main_const_eval_84, [%arg62]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg62) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %85 = ttcore.load_cached(@main_const_eval_85, [%arg83]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg83) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %86 = ttcore.load_cached(@main_const_eval_86, [%arg90]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg90) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %87 = ttcore.load_cached(@main_const_eval_87, [%arg43]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg43) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %88 = ttcore.load_cached(@main_const_eval_88, [%arg65]) : (tensor<4xbf16, #ttnn_layout25>) -> tensor<1x4x1x1xbf16, #ttnn_layout26> loc(#loc)
        "ttnn.deallocate"(%arg65) <{force = false}> : (tensor<4xbf16, #ttnn_layout25>) -> () loc(#loc)
        %89 = ttcore.load_cached(@main_const_eval_89, [%arg52]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg52) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %90 = ttcore.load_cached(@main_const_eval_90, [%arg121]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg121) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %91 = ttcore.load_cached(@main_const_eval_91, [%arg40]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg40) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %92 = ttcore.load_cached(@main_const_eval_92, [%arg73]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg73) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %93 = ttcore.load_cached(@main_const_eval_93, [%arg17]) : (tensor<64xbf16, #ttnn_layout10>) -> tensor<1x64x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg17) <{force = false}> : (tensor<64xbf16, #ttnn_layout10>) -> () loc(#loc)
        %94 = ttcore.load_cached(@main_const_eval_94, []) : () -> tensor<1x2x80x80xbf16, #ttnn_layout40> loc(#loc)
        %95 = ttcore.load_cached(@main_const_eval_95, []) : () -> tensor<40x80xbf16, #ttnn_layout41> loc(#loc)
        %96 = ttcore.load_cached(@main_const_eval_96, [%arg118]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg118) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %97 = ttcore.load_cached(@main_const_eval_97, [%arg19]) : (tensor<128xbf16, #ttnn_layout15>) -> tensor<1x128x1x1xbf16, #ttnn_layout16> loc(#loc)
        "ttnn.deallocate"(%arg19) <{force = false}> : (tensor<128xbf16, #ttnn_layout15>) -> () loc(#loc)
        %98 = ttcore.load_cached(@main_const_eval_98, [%arg122]) : (tensor<256xbf16, #ttnn_layout19>) -> tensor<1x256x1x1xbf16, #ttnn_layout23> loc(#loc)
        "ttnn.deallocate"(%arg122) <{force = false}> : (tensor<256xbf16, #ttnn_layout19>) -> () loc(#loc)
        %99 = ttcore.load_cached(@main_const_eval_99, [%arg93]) : (tensor<512xbf16, #ttnn_layout35>) -> tensor<1x512x1x1xbf16, #ttnn_layout36> loc(#loc)
        "ttnn.deallocate"(%arg93) <{force = false}> : (tensor<512xbf16, #ttnn_layout35>) -> () loc(#loc)
        %100 = ttcore.load_cached(@main_const_eval_100, [%arg94]) : (tensor<512xbf16, #ttnn_layout35>) -> tensor<1x512x1x1xbf16, #ttnn_layout36> loc(#loc)
        "ttnn.deallocate"(%arg94) <{force = false}> : (tensor<512xbf16, #ttnn_layout35>) -> () loc(#loc)
        %101 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc1)
        %102 = "ttnn.to_layout"(%arg49) <{layout = #ttnn.layout<tile>}> : (tensor<1x512x20x20xbf16, #ttnn_layout59>) -> tensor<1x512x20x20xbf16, #ttnn_layout36> loc(#loc1)
        %103 = "ttnn.permute"(%102) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x512x20x20xbf16, #ttnn_layout36>) -> tensor<1x512x20x20xbf16, #ttnn_layout36> loc(#loc168)
        "ttnn.deallocate"(%102) <{force = false}> : (tensor<1x512x20x20xbf16, #ttnn_layout36>) -> () loc(#loc168)
        %104 = "ttnn.reshape"(%103) <{shape = [10240 : i32, 20 : i32]}> : (tensor<1x512x20x20xbf16, #ttnn_layout36>) -> tensor<10240x20xbf16, #ttnn_layout67> loc(#loc285)
        "ttnn.deallocate"(%103) <{force = false}> : (tensor<1x512x20x20xbf16, #ttnn_layout36>) -> () loc(#loc285)
        %105 = "ttnn.matmul"(%104, %0) <{transpose_a = false, transpose_b = false}> : (tensor<10240x20xbf16, #ttnn_layout67>, tensor<20x40xbf16, #ttnn_layout>) -> tensor<10240x40xbf16, #ttnn_layout68> loc(#loc169)
        "ttnn.deallocate"(%104) <{force = false}> : (tensor<10240x20xbf16, #ttnn_layout67>) -> () loc(#loc169)
        %106 = "ttnn.reshape"(%105) <{shape = [1 : i32, 512 : i32, 20 : i32, 40 : i32]}> : (tensor<10240x40xbf16, #ttnn_layout68>) -> tensor<1x512x20x40xbf16, #ttnn_layout69> loc(#loc286)
        "ttnn.deallocate"(%105) <{force = false}> : (tensor<10240x40xbf16, #ttnn_layout68>) -> () loc(#loc286)
        %107 = "ttnn.permute"(%106) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x512x20x40xbf16, #ttnn_layout69>) -> tensor<1x512x40x20xbf16, #ttnn_layout70> loc(#loc170)
        "ttnn.deallocate"(%106) <{force = false}> : (tensor<1x512x20x40xbf16, #ttnn_layout69>) -> () loc(#loc170)
        %108 = "ttnn.reshape"(%107) <{shape = [20480 : i32, 20 : i32]}> : (tensor<1x512x40x20xbf16, #ttnn_layout70>) -> tensor<20480x20xbf16, #ttnn_layout71> loc(#loc287)
        "ttnn.deallocate"(%107) <{force = false}> : (tensor<1x512x40x20xbf16, #ttnn_layout70>) -> () loc(#loc287)
        %109 = "ttnn.matmul"(%108, %0) <{transpose_a = false, transpose_b = false}> : (tensor<20480x20xbf16, #ttnn_layout71>, tensor<20x40xbf16, #ttnn_layout>) -> tensor<20480x40xbf16, #ttnn_layout72> loc(#loc171)
        "ttnn.deallocate"(%108) <{force = false}> : (tensor<20480x20xbf16, #ttnn_layout71>) -> () loc(#loc171)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<20x40xbf16, #ttnn_layout>) -> () loc(#loc171)
        %110 = "ttnn.reshape"(%109) <{shape = [1 : i32, 512 : i32, 40 : i32, 40 : i32]}> : (tensor<20480x40xbf16, #ttnn_layout72>) -> tensor<1x512x40x40xbf16, #ttnn_layout73> loc(#loc288)
        "ttnn.deallocate"(%109) <{force = false}> : (tensor<20480x40xbf16, #ttnn_layout72>) -> () loc(#loc288)
        %111 = "ttnn.permute"(%110) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x512x40x40xbf16, #ttnn_layout73>) -> tensor<1x40x40x512xbf16, #ttnn_layout74> loc(#loc172)
        "ttnn.deallocate"(%110) <{force = false}> : (tensor<1x512x40x40xbf16, #ttnn_layout73>) -> () loc(#loc172)
        %112 = "ttnn.to_layout"(%arg48) <{layout = #ttnn.layout<tile>}> : (tensor<1x256x40x40xbf16, #ttnn_layout58>) -> tensor<1x256x40x40xbf16, #ttnn_layout66> loc(#loc173)
        "ttnn.deallocate"(%arg48) <{force = false}> : (tensor<1x256x40x40xbf16, #ttnn_layout58>) -> () loc(#loc173)
        %113 = "ttnn.permute"(%112) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x40x40xbf16, #ttnn_layout66>) -> tensor<1x40x40x256xbf16, #ttnn_layout75> loc(#loc172)
        "ttnn.deallocate"(%112) <{force = false}> : (tensor<1x256x40x40xbf16, #ttnn_layout66>) -> () loc(#loc172)
        %114 = "ttnn.reshape"(%111) <{shape = [1 : i32, 1 : i32, 1600 : i32, 512 : i32]}> : (tensor<1x40x40x512xbf16, #ttnn_layout74>) -> tensor<1x1x1600x512xbf16, #ttnn_layout76> loc(#loc172)
        "ttnn.deallocate"(%111) <{force = false}> : (tensor<1x40x40x512xbf16, #ttnn_layout74>) -> () loc(#loc172)
        %115 = "ttnn.reshape"(%113) <{shape = [1 : i32, 1 : i32, 1600 : i32, 256 : i32]}> : (tensor<1x40x40x256xbf16, #ttnn_layout75>) -> tensor<1x1x1600x256xbf16, #ttnn_layout77> loc(#loc172)
        "ttnn.deallocate"(%113) <{force = false}> : (tensor<1x40x40x256xbf16, #ttnn_layout75>) -> () loc(#loc172)
        %116 = "ttnn.concat"(%114, %115) <{dim = 3 : si32}> : (tensor<1x1x1600x512xbf16, #ttnn_layout76>, tensor<1x1x1600x256xbf16, #ttnn_layout77>) -> tensor<1x1x1600x768xbf16, #ttnn_layout78> loc(#loc172)
        "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x1x1600x256xbf16, #ttnn_layout77>) -> () loc(#loc172)
        "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x1x1600x512xbf16, #ttnn_layout76>) -> () loc(#loc172)
        %117 = "ttnn.to_layout"(%116) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1600x768xbf16, #ttnn_layout78>) -> tensor<1x1x1600x768xbf16, #ttnn_layout79> loc(#loc289)
        "ttnn.deallocate"(%116) <{force = false}> : (tensor<1x1x1600x768xbf16, #ttnn_layout78>) -> () loc(#loc289)
        %118 = "ttnn.conv2d"(%117, %arg47, %101) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<enable_kernel_stride_folding = false, config_tensors_in_dram = true>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 768 : i32, input_height = 40 : i32, input_width = 40 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1600x768xbf16, #ttnn_layout79>, tensor<256x768x1x1xbf16, #ttnn_layout57>, !ttnn.device) -> tensor<1x1x1600x256xbf16, #ttnn_layout77> loc(#loc174)
        "ttnn.deallocate"(%117) <{force = false}> : (tensor<1x1x1600x768xbf16, #ttnn_layout79>) -> () loc(#loc174)
        "ttnn.deallocate"(%arg47) <{force = false}> : (tensor<256x768x1x1xbf16, #ttnn_layout57>) -> () loc(#loc174)
        %119 = "ttnn.reshape"(%118) <{shape = [1 : i32, 40 : i32, 40 : i32, 256 : i32]}> : (tensor<1x1x1600x256xbf16, #ttnn_layout77>) -> tensor<1x40x40x256xbf16, #ttnn_layout75> loc(#loc290)
        "ttnn.deallocate"(%118) <{force = false}> : (tensor<1x1x1600x256xbf16, #ttnn_layout77>) -> () loc(#loc290)
        %120 = "ttnn.permute"(%119) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x40x40x256xbf16, #ttnn_layout75>) -> tensor<1x256x40x40xbf16, #ttnn_layout66> loc(#loc174)
        "ttnn.deallocate"(%119) <{force = false}> : (tensor<1x40x40x256xbf16, #ttnn_layout75>) -> () loc(#loc174)
        %121 = "ttnn.batch_norm_inference"(%120, %44, %87, %33, %59) <{epsilon = 1.000000e-03 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x40x40xbf16, #ttnn_layout66>, tensor<1x256x1x1xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>) -> tensor<1x256x40x40xbf16, #ttnn_layout66> loc(#loc28)
        "ttnn.deallocate"(%120) <{force = false}> : (tensor<1x256x40x40xbf16, #ttnn_layout66>) -> () loc(#loc28)
        "ttnn.deallocate"(%87) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc28)
        "ttnn.deallocate"(%59) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc28)
        "ttnn.deallocate"(%44) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc28)
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc28)
        %122 = "ttnn.silu"(%121) : (tensor<1x256x40x40xbf16, #ttnn_layout66>) -> tensor<1x256x40x40xbf16, #ttnn_layout66> loc(#loc175)
        "ttnn.deallocate"(%121) <{force = false}> : (tensor<1x256x40x40xbf16, #ttnn_layout66>) -> () loc(#loc175)
        %123 = "ttnn.permute"(%122) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x40x40xbf16, #ttnn_layout66>) -> tensor<1x40x40x256xbf16, #ttnn_layout75> loc(#loc175)
        "ttnn.deallocate"(%122) <{force = false}> : (tensor<1x256x40x40xbf16, #ttnn_layout66>) -> () loc(#loc175)
        %124 = "ttnn.reshape"(%123) <{shape = [1 : i32, 1 : i32, 1600 : i32, 256 : i32]}> : (tensor<1x40x40x256xbf16, #ttnn_layout75>) -> tensor<1x1x1600x256xbf16, #ttnn_layout77> loc(#loc175)
        "ttnn.deallocate"(%123) <{force = false}> : (tensor<1x40x40x256xbf16, #ttnn_layout75>) -> () loc(#loc175)
        %125 = "ttnn.slice_static"(%124) <{begins = [0 : i32, 0 : i32, 0 : i32, 128 : i32], ends = [1 : i32, 1 : i32, 1600 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x1600x256xbf16, #ttnn_layout77>) -> tensor<1x1x1600x128xbf16, #ttnn_layout80> loc(#loc176)
        %126 = "ttnn.to_layout"(%125) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> tensor<1x1x1600x128xbf16, #ttnn_layout81> loc(#loc291)
        %127 = "ttnn.conv2d"(%126, %arg42, %101) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<enable_kernel_stride_folding = false, config_tensors_in_dram = true>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 128 : i32, input_height = 40 : i32, input_width = 40 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x1600x128xbf16, #ttnn_layout81>, tensor<128x128x3x3xbf16, #ttnn_layout56>, !ttnn.device) -> tensor<1x1x1600x128xbf16, #ttnn_layout80> loc(#loc177)
        "ttnn.deallocate"(%126) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout81>) -> () loc(#loc177)
        "ttnn.deallocate"(%arg42) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout56>) -> () loc(#loc177)
        %128 = "ttnn.reshape"(%127) <{shape = [1 : i32, 40 : i32, 40 : i32, 128 : i32]}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> tensor<1x40x40x128xbf16, #ttnn_layout82> loc(#loc292)
        "ttnn.deallocate"(%127) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> () loc(#loc292)
        %129 = "ttnn.permute"(%128) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> tensor<1x128x40x40xbf16, #ttnn_layout83> loc(#loc177)
        "ttnn.deallocate"(%128) <{force = false}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> () loc(#loc177)
        %130 = "ttnn.batch_norm_inference"(%129, %21, %45, %61, %91) <{epsilon = 1.000000e-03 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>) -> tensor<1x128x40x40xbf16, #ttnn_layout83> loc(#loc21)
        "ttnn.deallocate"(%129) <{force = false}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> () loc(#loc21)
        "ttnn.deallocate"(%91) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc21)
        "ttnn.deallocate"(%61) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc21)
        "ttnn.deallocate"(%45) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc21)
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc21)
        %131 = "ttnn.silu"(%130) : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> tensor<1x128x40x40xbf16, #ttnn_layout83> loc(#loc178)
        "ttnn.deallocate"(%130) <{force = false}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> () loc(#loc178)
        %132 = "ttnn.permute"(%131) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> tensor<1x40x40x128xbf16, #ttnn_layout82> loc(#loc178)
        "ttnn.deallocate"(%131) <{force = false}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> () loc(#loc178)
        %133 = "ttnn.reshape"(%132) <{shape = [1 : i32, 1 : i32, 1600 : i32, 128 : i32]}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> tensor<1x1x1600x128xbf16, #ttnn_layout80> loc(#loc178)
        "ttnn.deallocate"(%132) <{force = false}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> () loc(#loc178)
        %134 = "ttnn.to_layout"(%133) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> tensor<1x1x1600x128xbf16, #ttnn_layout81> loc(#loc293)
        "ttnn.deallocate"(%133) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> () loc(#loc293)
        %135 = "ttnn.conv2d"(%134, %arg37, %101) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<enable_kernel_stride_folding = false, config_tensors_in_dram = true>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 128 : i32, input_height = 40 : i32, input_width = 40 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x1600x128xbf16, #ttnn_layout81>, tensor<128x128x3x3xbf16, #ttnn_layout56>, !ttnn.device) -> tensor<1x1x1600x128xbf16, #ttnn_layout80> loc(#loc179)
        "ttnn.deallocate"(%134) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout81>) -> () loc(#loc179)
        "ttnn.deallocate"(%arg37) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout56>) -> () loc(#loc179)
        %136 = "ttnn.reshape"(%135) <{shape = [1 : i32, 40 : i32, 40 : i32, 128 : i32]}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> tensor<1x40x40x128xbf16, #ttnn_layout82> loc(#loc294)
        "ttnn.deallocate"(%135) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> () loc(#loc294)
        %137 = "ttnn.permute"(%136) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> tensor<1x128x40x40xbf16, #ttnn_layout83> loc(#loc179)
        "ttnn.deallocate"(%136) <{force = false}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> () loc(#loc179)
        %138 = "ttnn.batch_norm_inference"(%137, %3, %72, %46, %49) <{epsilon = 1.000000e-03 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>) -> tensor<1x128x40x40xbf16, #ttnn_layout83> loc(#loc8)
        "ttnn.deallocate"(%137) <{force = false}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> () loc(#loc8)
        "ttnn.deallocate"(%72) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc8)
        "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc8)
        "ttnn.deallocate"(%46) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc8)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc8)
        %139 = "ttnn.silu"(%138) : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> tensor<1x128x40x40xbf16, #ttnn_layout83> loc(#loc180)
        "ttnn.deallocate"(%138) <{force = false}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> () loc(#loc180)
        %140 = "ttnn.reshape"(%139) <{shape = [1 : i32, 4 : i32, 32 : i32, 40 : i32, 40 : i32]}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> tensor<1x4x32x40x40xbf16, #ttnn_layout84> loc(#loc181)
        %141 = "ttnn.to_layout"(%arg8) <{layout = #ttnn.layout<tile>}> : (tensor<1x3x512xbf16, #ttnn_layout50>) -> tensor<1x3x512xbf16, #ttnn_layout85> loc(#loc182)
        "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<1x3x512xbf16, #ttnn_layout50>) -> () loc(#loc182)
        %142 = "ttnn.reshape"(%141) <{shape = [3 : i32, 512 : i32]}> : (tensor<1x3x512xbf16, #ttnn_layout85>) -> tensor<3x512xbf16, #ttnn_layout86> loc(#loc183)
        "ttnn.deallocate"(%141) <{force = false}> : (tensor<1x3x512xbf16, #ttnn_layout85>) -> () loc(#loc183)
        %143 = "ttnn.matmul"(%142, %arg32) <{transpose_a = false, transpose_b = true}> : (tensor<3x512xbf16, #ttnn_layout86>, tensor<128x512xbf16, #ttnn_layout55>) -> tensor<3x128xbf16, #ttnn_layout87> loc(#loc295)
        "ttnn.deallocate"(%arg32) <{force = false}> : (tensor<128x512xbf16, #ttnn_layout55>) -> () loc(#loc295)
        %144 = "ttnn.add"(%143, %80) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<3x128xbf16, #ttnn_layout87>, tensor<1x3x128xbf16, #ttnn_layout39>) -> tensor<1x3x128xbf16, #ttnn_layout39> loc(#loc296)
        "ttnn.deallocate"(%143) <{force = false}> : (tensor<3x128xbf16, #ttnn_layout87>) -> () loc(#loc296)
        "ttnn.deallocate"(%80) <{force = false}> : (tensor<1x3x128xbf16, #ttnn_layout39>) -> () loc(#loc296)
        %145 = "ttnn.reshape"(%144) <{shape = [1 : i32, 3 : i32, 4 : i32, 32 : i32]}> : (tensor<1x3x128xbf16, #ttnn_layout39>) -> tensor<1x3x4x32xbf16, #ttnn_layout88> loc(#loc185)
        "ttnn.deallocate"(%144) <{force = false}> : (tensor<1x3x128xbf16, #ttnn_layout39>) -> () loc(#loc185)
        %146 = "ttnn.permute"(%140) <{permutation = array<i64: 0, 1, 3, 4, 2>}> : (tensor<1x4x32x40x40xbf16, #ttnn_layout84>) -> tensor<1x4x40x40x32xbf16, #ttnn_layout89> loc(#loc297)
        "ttnn.deallocate"(%140) <{force = false}> : (tensor<1x4x32x40x40xbf16, #ttnn_layout84>) -> () loc(#loc297)
        %147 = "ttnn.permute"(%145) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x3x4x32xbf16, #ttnn_layout88>) -> tensor<1x4x32x3xbf16, #ttnn_layout26> loc(#loc298)
        "ttnn.deallocate"(%145) <{force = false}> : (tensor<1x3x4x32xbf16, #ttnn_layout88>) -> () loc(#loc298)
        %148 = "ttnn.reshape"(%146) <{shape = [1 : i32, 4 : i32, 1600 : i32, 32 : i32]}> : (tensor<1x4x40x40x32xbf16, #ttnn_layout89>) -> tensor<1x4x1600x32xbf16, #ttnn_layout90> loc(#loc299)
        "ttnn.deallocate"(%146) <{force = false}> : (tensor<1x4x40x40x32xbf16, #ttnn_layout89>) -> () loc(#loc299)
        %149 = "ttnn.matmul"(%148, %147) <{transpose_a = false, transpose_b = false}> : (tensor<1x4x1600x32xbf16, #ttnn_layout90>, tensor<1x4x32x3xbf16, #ttnn_layout26>) -> tensor<1x4x1600x3xbf16, #ttnn_layout90> loc(#loc186)
        "ttnn.deallocate"(%148) <{force = false}> : (tensor<1x4x1600x32xbf16, #ttnn_layout90>) -> () loc(#loc186)
        "ttnn.deallocate"(%147) <{force = false}> : (tensor<1x4x32x3xbf16, #ttnn_layout26>) -> () loc(#loc186)
        %150 = "ttnn.reshape"(%149) <{shape = [1 : i32, 4 : i32, 40 : i32, 40 : i32, 3 : i32]}> : (tensor<1x4x1600x3xbf16, #ttnn_layout90>) -> tensor<1x4x40x40x3xbf16, #ttnn_layout89> loc(#loc300)
        "ttnn.deallocate"(%149) <{force = false}> : (tensor<1x4x1600x3xbf16, #ttnn_layout90>) -> () loc(#loc300)
        %151 = "ttnn.slice_static"(%124) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 1600 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x1600x256xbf16, #ttnn_layout77>) -> tensor<1x1x1600x128xbf16, #ttnn_layout80> loc(#loc187)
        "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x1x1600x256xbf16, #ttnn_layout77>) -> () loc(#loc187)
        %152 = "ttnn.permute"(%139) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> tensor<1x40x40x128xbf16, #ttnn_layout82> loc(#loc301)
        %153 = "ttnn.reshape"(%152) <{shape = [1 : i32, 1 : i32, 1600 : i32, 128 : i32]}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> tensor<1x1x1600x128xbf16, #ttnn_layout80> loc(#loc373)
        "ttnn.deallocate"(%152) <{force = false}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> () loc(#loc373)
        %154 = "ttnn.to_layout"(%153) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> tensor<1x1x1600x128xbf16, #ttnn_layout81> loc(#loc302)
        "ttnn.deallocate"(%153) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> () loc(#loc302)
        %155 = "ttnn.conv2d"(%154, %arg54, %101) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<enable_kernel_stride_folding = false, config_tensors_in_dram = true>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 128 : i32, input_height = 40 : i32, input_width = 40 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x1600x128xbf16, #ttnn_layout81>, tensor<128x128x3x3xbf16, #ttnn_layout56>, !ttnn.device) -> tensor<1x1x1600x128xbf16, #ttnn_layout80> loc(#loc188)
        "ttnn.deallocate"(%154) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout81>) -> () loc(#loc188)
        "ttnn.deallocate"(%arg54) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout56>) -> () loc(#loc188)
        %156 = "ttnn.reshape"(%155) <{shape = [1 : i32, 40 : i32, 40 : i32, 128 : i32]}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> tensor<1x40x40x128xbf16, #ttnn_layout82> loc(#loc303)
        "ttnn.deallocate"(%155) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> () loc(#loc303)
        %157 = "ttnn.permute"(%156) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> tensor<1x128x40x40xbf16, #ttnn_layout83> loc(#loc188)
        "ttnn.deallocate"(%156) <{force = false}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> () loc(#loc188)
        %158 = "ttnn.batch_norm_inference"(%157, %43, %6, %68, %89) <{epsilon = 1.000000e-03 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>) -> tensor<1x128x40x40xbf16, #ttnn_layout83> loc(#loc11)
        "ttnn.deallocate"(%157) <{force = false}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> () loc(#loc11)
        "ttnn.deallocate"(%89) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc11)
        "ttnn.deallocate"(%68) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc11)
        "ttnn.deallocate"(%43) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc11)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc11)
        %159 = "ttnn.max"(%150) <{dim_arg = [4 : i32], keep_dim = false}> : (tensor<1x4x40x40x3xbf16, #ttnn_layout89>) -> tensor<1x4x40x40xbf16, #ttnn_layout32> loc(#loc189)
        "ttnn.deallocate"(%150) <{force = false}> : (tensor<1x4x40x40x3xbf16, #ttnn_layout89>) -> () loc(#loc189)
        %160 = "ttnn.divide"(%159, %37) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x40x40xbf16, #ttnn_layout32>, tensor<1x4x40x40xbf16, #ttnn_layout32>) -> tensor<1x4x40x40xbf16, #ttnn_layout32> loc(#loc190)
        "ttnn.deallocate"(%159) <{force = false}> : (tensor<1x4x40x40xbf16, #ttnn_layout32>) -> () loc(#loc190)
        %161 = "ttnn.add"(%160, %20) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x40x40xbf16, #ttnn_layout32>, tensor<1x4x1x1xbf16, #ttnn_layout26>) -> tensor<1x4x40x40xbf16, #ttnn_layout32> loc(#loc191)
        "ttnn.deallocate"(%160) <{force = false}> : (tensor<1x4x40x40xbf16, #ttnn_layout32>) -> () loc(#loc191)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x4x1x1xbf16, #ttnn_layout26>) -> () loc(#loc191)
        %162 = "ttnn.sigmoid"(%161) : (tensor<1x4x40x40xbf16, #ttnn_layout32>) -> tensor<1x4x40x40xbf16, #ttnn_layout32> loc(#loc192)
        "ttnn.deallocate"(%161) <{force = false}> : (tensor<1x4x40x40xbf16, #ttnn_layout32>) -> () loc(#loc192)
        %163 = "ttnn.reshape"(%162) <{shape = [1 : i32, 4 : i32, 1 : i32, 40 : i32, 40 : i32]}> : (tensor<1x4x40x40xbf16, #ttnn_layout32>) -> tensor<1x4x1x40x40xbf16, #ttnn_layout91> loc(#loc192)
        "ttnn.deallocate"(%162) <{force = false}> : (tensor<1x4x40x40xbf16, #ttnn_layout32>) -> () loc(#loc192)
        %164 = "ttnn.repeat"(%163) <{repeat_dims = #ttnn.shape<1x1x32x1x1>}> : (tensor<1x4x1x40x40xbf16, #ttnn_layout91>) -> tensor<1x4x32x40x40xbf16, #ttnn_layout84> loc(#loc193)
        "ttnn.deallocate"(%163) <{force = false}> : (tensor<1x4x1x40x40xbf16, #ttnn_layout91>) -> () loc(#loc193)
        %165 = "ttnn.reshape"(%164) <{shape = [1 : i32, 128 : i32, 40 : i32, 40 : i32]}> : (tensor<1x4x32x40x40xbf16, #ttnn_layout84>) -> tensor<1x128x40x40xbf16, #ttnn_layout83> loc(#loc304)
        "ttnn.deallocate"(%164) <{force = false}> : (tensor<1x4x32x40x40xbf16, #ttnn_layout84>) -> () loc(#loc304)
        %166 = "ttnn.multiply"(%158, %165) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>, tensor<1x128x40x40xbf16, #ttnn_layout83>) -> tensor<1x128x40x40xbf16, #ttnn_layout83> loc(#loc195)
        "ttnn.deallocate"(%165) <{force = false}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> () loc(#loc195)
        "ttnn.deallocate"(%158) <{force = false}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> () loc(#loc195)
        %167 = "ttnn.permute"(%166) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> tensor<1x40x40x128xbf16, #ttnn_layout82> loc(#loc195)
        "ttnn.deallocate"(%166) <{force = false}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> () loc(#loc195)
        %168 = "ttnn.reshape"(%167) <{shape = [1 : i32, 1 : i32, 1600 : i32, 128 : i32]}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> tensor<1x1x1600x128xbf16, #ttnn_layout80> loc(#loc195)
        "ttnn.deallocate"(%167) <{force = false}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> () loc(#loc195)
        %169 = "ttnn.permute"(%139) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> tensor<1x40x40x128xbf16, #ttnn_layout82> loc(#loc196)
        "ttnn.deallocate"(%139) <{force = false}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> () loc(#loc196)
        %170 = "ttnn.reshape"(%169) <{shape = [1 : i32, 1 : i32, 1600 : i32, 128 : i32]}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> tensor<1x1x1600x128xbf16, #ttnn_layout80> loc(#loc196)
        "ttnn.deallocate"(%169) <{force = false}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> () loc(#loc196)
        %171 = "ttnn.concat"(%151, %125, %170, %168) <{dim = 3 : si32}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>, tensor<1x1x1600x128xbf16, #ttnn_layout80>, tensor<1x1x1600x128xbf16, #ttnn_layout80>, tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> tensor<1x1x1600x512xbf16, #ttnn_layout76> loc(#loc196)
        "ttnn.deallocate"(%170) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> () loc(#loc196)
        "ttnn.deallocate"(%168) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> () loc(#loc196)
        "ttnn.deallocate"(%151) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> () loc(#loc196)
        "ttnn.deallocate"(%125) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> () loc(#loc196)
        %172 = "ttnn.to_layout"(%171) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1600x512xbf16, #ttnn_layout76>) -> tensor<1x1x1600x512xbf16, #ttnn_layout92> loc(#loc305)
        "ttnn.deallocate"(%171) <{force = false}> : (tensor<1x1x1600x512xbf16, #ttnn_layout76>) -> () loc(#loc305)
        %173 = "ttnn.conv2d"(%172, %arg29, %101) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<enable_kernel_stride_folding = false, config_tensors_in_dram = true>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 40 : i32, input_width = 40 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1600x512xbf16, #ttnn_layout92>, tensor<256x512x1x1xbf16, #ttnn_layout54>, !ttnn.device) -> tensor<1x1x1600x256xbf16, #ttnn_layout77> loc(#loc197)
        "ttnn.deallocate"(%172) <{force = false}> : (tensor<1x1x1600x512xbf16, #ttnn_layout92>) -> () loc(#loc197)
        "ttnn.deallocate"(%arg29) <{force = false}> : (tensor<256x512x1x1xbf16, #ttnn_layout54>) -> () loc(#loc197)
        %174 = "ttnn.reshape"(%173) <{shape = [1 : i32, 40 : i32, 40 : i32, 256 : i32]}> : (tensor<1x1x1600x256xbf16, #ttnn_layout77>) -> tensor<1x40x40x256xbf16, #ttnn_layout75> loc(#loc306)
        "ttnn.deallocate"(%173) <{force = false}> : (tensor<1x1x1600x256xbf16, #ttnn_layout77>) -> () loc(#loc306)
        %175 = "ttnn.permute"(%174) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x40x40x256xbf16, #ttnn_layout75>) -> tensor<1x256x40x40xbf16, #ttnn_layout66> loc(#loc197)
        "ttnn.deallocate"(%174) <{force = false}> : (tensor<1x40x40x256xbf16, #ttnn_layout75>) -> () loc(#loc197)
        %176 = "ttnn.batch_norm_inference"(%175, %40, %34, %9, %62) <{epsilon = 1.000000e-03 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x40x40xbf16, #ttnn_layout66>, tensor<1x256x1x1xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>) -> tensor<1x256x40x40xbf16, #ttnn_layout66> loc(#loc14)
        "ttnn.deallocate"(%175) <{force = false}> : (tensor<1x256x40x40xbf16, #ttnn_layout66>) -> () loc(#loc14)
        "ttnn.deallocate"(%62) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc14)
        "ttnn.deallocate"(%40) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc14)
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc14)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc14)
        %177 = "ttnn.silu"(%176) : (tensor<1x256x40x40xbf16, #ttnn_layout66>) -> tensor<1x256x40x40xbf16, #ttnn_layout66> loc(#loc198)
        "ttnn.deallocate"(%176) <{force = false}> : (tensor<1x256x40x40xbf16, #ttnn_layout66>) -> () loc(#loc198)
        %178 = "ttnn.permute"(%177) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x256x40x40xbf16, #ttnn_layout66>) -> tensor<1x256x40x40xbf16, #ttnn_layout66> loc(#loc199)
        %179 = "ttnn.reshape"(%178) <{shape = [10240 : i32, 40 : i32]}> : (tensor<1x256x40x40xbf16, #ttnn_layout66>) -> tensor<10240x40xbf16, #ttnn_layout68> loc(#loc307)
        "ttnn.deallocate"(%178) <{force = false}> : (tensor<1x256x40x40xbf16, #ttnn_layout66>) -> () loc(#loc307)
        %180 = "ttnn.matmul"(%179, %95) <{transpose_a = false, transpose_b = false}> : (tensor<10240x40xbf16, #ttnn_layout68>, tensor<40x80xbf16, #ttnn_layout41>) -> tensor<10240x80xbf16, #ttnn_layout93> loc(#loc200)
        "ttnn.deallocate"(%179) <{force = false}> : (tensor<10240x40xbf16, #ttnn_layout68>) -> () loc(#loc200)
        %181 = "ttnn.reshape"(%180) <{shape = [1 : i32, 256 : i32, 40 : i32, 80 : i32]}> : (tensor<10240x80xbf16, #ttnn_layout93>) -> tensor<1x256x40x80xbf16, #ttnn_layout94> loc(#loc308)
        "ttnn.deallocate"(%180) <{force = false}> : (tensor<10240x80xbf16, #ttnn_layout93>) -> () loc(#loc308)
        %182 = "ttnn.permute"(%181) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x256x40x80xbf16, #ttnn_layout94>) -> tensor<1x256x80x40xbf16, #ttnn_layout95> loc(#loc201)
        "ttnn.deallocate"(%181) <{force = false}> : (tensor<1x256x40x80xbf16, #ttnn_layout94>) -> () loc(#loc201)
        %183 = "ttnn.reshape"(%182) <{shape = [20480 : i32, 40 : i32]}> : (tensor<1x256x80x40xbf16, #ttnn_layout95>) -> tensor<20480x40xbf16, #ttnn_layout72> loc(#loc309)
        "ttnn.deallocate"(%182) <{force = false}> : (tensor<1x256x80x40xbf16, #ttnn_layout95>) -> () loc(#loc309)
        %184 = "ttnn.matmul"(%183, %95) <{transpose_a = false, transpose_b = false}> : (tensor<20480x40xbf16, #ttnn_layout72>, tensor<40x80xbf16, #ttnn_layout41>) -> tensor<20480x80xbf16, #ttnn_layout96> loc(#loc202)
        "ttnn.deallocate"(%183) <{force = false}> : (tensor<20480x40xbf16, #ttnn_layout72>) -> () loc(#loc202)
        "ttnn.deallocate"(%95) <{force = false}> : (tensor<40x80xbf16, #ttnn_layout41>) -> () loc(#loc202)
        %185 = "ttnn.reshape"(%184) <{shape = [1 : i32, 256 : i32, 80 : i32, 80 : i32]}> : (tensor<20480x80xbf16, #ttnn_layout96>) -> tensor<1x256x80x80xbf16, #ttnn_layout97> loc(#loc310)
        "ttnn.deallocate"(%184) <{force = false}> : (tensor<20480x80xbf16, #ttnn_layout96>) -> () loc(#loc310)
        %186 = "ttnn.permute"(%185) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x80x80xbf16, #ttnn_layout97>) -> tensor<1x80x80x256xbf16, #ttnn_layout98> loc(#loc203)
        "ttnn.deallocate"(%185) <{force = false}> : (tensor<1x256x80x80xbf16, #ttnn_layout97>) -> () loc(#loc203)
        %187 = "ttnn.to_layout"(%arg24) <{layout = #ttnn.layout<tile>}> : (tensor<1x128x80x80xbf16, #ttnn_layout53>) -> tensor<1x128x80x80xbf16, #ttnn_layout65> loc(#loc204)
        "ttnn.deallocate"(%arg24) <{force = false}> : (tensor<1x128x80x80xbf16, #ttnn_layout53>) -> () loc(#loc204)
        %188 = "ttnn.permute"(%187) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x128x80x80xbf16, #ttnn_layout65>) -> tensor<1x80x80x128xbf16, #ttnn_layout99> loc(#loc203)
        "ttnn.deallocate"(%187) <{force = false}> : (tensor<1x128x80x80xbf16, #ttnn_layout65>) -> () loc(#loc203)
        %189 = "ttnn.reshape"(%186) <{shape = [1 : i32, 1 : i32, 6400 : i32, 256 : i32]}> : (tensor<1x80x80x256xbf16, #ttnn_layout98>) -> tensor<1x1x6400x256xbf16, #ttnn_layout100> loc(#loc203)
        "ttnn.deallocate"(%186) <{force = false}> : (tensor<1x80x80x256xbf16, #ttnn_layout98>) -> () loc(#loc203)
        %190 = "ttnn.reshape"(%188) <{shape = [1 : i32, 1 : i32, 6400 : i32, 128 : i32]}> : (tensor<1x80x80x128xbf16, #ttnn_layout99>) -> tensor<1x1x6400x128xbf16, #ttnn_layout101> loc(#loc203)
        "ttnn.deallocate"(%188) <{force = false}> : (tensor<1x80x80x128xbf16, #ttnn_layout99>) -> () loc(#loc203)
        %191 = "ttnn.concat"(%189, %190) <{dim = 3 : si32}> : (tensor<1x1x6400x256xbf16, #ttnn_layout100>, tensor<1x1x6400x128xbf16, #ttnn_layout101>) -> tensor<1x1x6400x384xbf16, #ttnn_layout102> loc(#loc203)
        "ttnn.deallocate"(%190) <{force = false}> : (tensor<1x1x6400x128xbf16, #ttnn_layout101>) -> () loc(#loc203)
        "ttnn.deallocate"(%189) <{force = false}> : (tensor<1x1x6400x256xbf16, #ttnn_layout100>) -> () loc(#loc203)
        %192 = "ttnn.to_layout"(%191) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x6400x384xbf16, #ttnn_layout102>) -> tensor<1x1x6400x384xbf16, #ttnn_layout103> loc(#loc311)
        "ttnn.deallocate"(%191) <{force = false}> : (tensor<1x1x6400x384xbf16, #ttnn_layout102>) -> () loc(#loc311)
        %193 = "ttnn.conv2d"(%192, %arg23, %101) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<enable_kernel_stride_folding = false, config_tensors_in_dram = true>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 384 : i32, input_height = 80 : i32, input_width = 80 : i32, kernel_size = array<i32: 1, 1>, out_channels = 128 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x6400x384xbf16, #ttnn_layout103>, tensor<128x384x1x1xbf16, #ttnn_layout52>, !ttnn.device) -> tensor<1x1x6400x128xbf16, #ttnn_layout101> loc(#loc205)
        "ttnn.deallocate"(%192) <{force = false}> : (tensor<1x1x6400x384xbf16, #ttnn_layout103>) -> () loc(#loc205)
        "ttnn.deallocate"(%arg23) <{force = false}> : (tensor<128x384x1x1xbf16, #ttnn_layout52>) -> () loc(#loc205)
        %194 = "ttnn.reshape"(%193) <{shape = [1 : i32, 80 : i32, 80 : i32, 128 : i32]}> : (tensor<1x1x6400x128xbf16, #ttnn_layout101>) -> tensor<1x80x80x128xbf16, #ttnn_layout99> loc(#loc312)
        "ttnn.deallocate"(%193) <{force = false}> : (tensor<1x1x6400x128xbf16, #ttnn_layout101>) -> () loc(#loc312)
        %195 = "ttnn.permute"(%194) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x80x80x128xbf16, #ttnn_layout99>) -> tensor<1x128x80x80xbf16, #ttnn_layout65> loc(#loc205)
        "ttnn.deallocate"(%194) <{force = false}> : (tensor<1x80x80x128xbf16, #ttnn_layout99>) -> () loc(#loc205)
        %196 = "ttnn.batch_norm_inference"(%195, %39, %97, %29, %38) <{epsilon = 1.000000e-03 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x128x80x80xbf16, #ttnn_layout65>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>) -> tensor<1x128x80x80xbf16, #ttnn_layout65> loc(#loc25)
        "ttnn.deallocate"(%195) <{force = false}> : (tensor<1x128x80x80xbf16, #ttnn_layout65>) -> () loc(#loc25)
        "ttnn.deallocate"(%97) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc25)
        "ttnn.deallocate"(%39) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc25)
        "ttnn.deallocate"(%38) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc25)
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc25)
        %197 = "ttnn.silu"(%196) : (tensor<1x128x80x80xbf16, #ttnn_layout65>) -> tensor<1x128x80x80xbf16, #ttnn_layout65> loc(#loc206)
        "ttnn.deallocate"(%196) <{force = false}> : (tensor<1x128x80x80xbf16, #ttnn_layout65>) -> () loc(#loc206)
        %198 = "ttnn.permute"(%197) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x128x80x80xbf16, #ttnn_layout65>) -> tensor<1x80x80x128xbf16, #ttnn_layout99> loc(#loc206)
        "ttnn.deallocate"(%197) <{force = false}> : (tensor<1x128x80x80xbf16, #ttnn_layout65>) -> () loc(#loc206)
        %199 = "ttnn.reshape"(%198) <{shape = [1 : i32, 1 : i32, 6400 : i32, 128 : i32]}> : (tensor<1x80x80x128xbf16, #ttnn_layout99>) -> tensor<1x1x6400x128xbf16, #ttnn_layout101> loc(#loc206)
        "ttnn.deallocate"(%198) <{force = false}> : (tensor<1x80x80x128xbf16, #ttnn_layout99>) -> () loc(#loc206)
        %200 = "ttnn.slice_static"(%199) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 1 : i32, 6400 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x6400x128xbf16, #ttnn_layout101>) -> tensor<1x1x6400x64xbf16, #ttnn_layout104> loc(#loc207)
        %201 = "ttnn.to_layout"(%200) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x6400x64xbf16, #ttnn_layout104>) -> tensor<1x1x6400x64xbf16, #ttnn_layout105> loc(#loc313)
        %202 = "ttnn.conv2d"(%201, %arg18, %101) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<enable_kernel_stride_folding = false, config_tensors_in_dram = true>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 64 : i32, input_height = 80 : i32, input_width = 80 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x6400x64xbf16, #ttnn_layout105>, tensor<64x64x3x3xbf16, #ttnn_layout51>, !ttnn.device) -> tensor<1x1x6400x64xbf16, #ttnn_layout104> loc(#loc208)
        "ttnn.deallocate"(%201) <{force = false}> : (tensor<1x1x6400x64xbf16, #ttnn_layout105>) -> () loc(#loc208)
        "ttnn.deallocate"(%arg18) <{force = false}> : (tensor<64x64x3x3xbf16, #ttnn_layout51>) -> () loc(#loc208)
        %203 = "ttnn.reshape"(%202) <{shape = [1 : i32, 80 : i32, 80 : i32, 64 : i32]}> : (tensor<1x1x6400x64xbf16, #ttnn_layout104>) -> tensor<1x80x80x64xbf16, #ttnn_layout106> loc(#loc314)
        "ttnn.deallocate"(%202) <{force = false}> : (tensor<1x1x6400x64xbf16, #ttnn_layout104>) -> () loc(#loc314)
        %204 = "ttnn.permute"(%203) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x80x80x64xbf16, #ttnn_layout106>) -> tensor<1x64x80x80xbf16, #ttnn_layout107> loc(#loc208)
        "ttnn.deallocate"(%203) <{force = false}> : (tensor<1x80x80x64xbf16, #ttnn_layout106>) -> () loc(#loc208)
        %205 = "ttnn.batch_norm_inference"(%204, %55, %32, %93, %71) <{epsilon = 1.000000e-03 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x64x80x80xbf16, #ttnn_layout107>, tensor<1x64x1x1xbf16, #ttnn_layout11>, tensor<1x64x1x1xbf16, #ttnn_layout11>, tensor<1x64x1x1xbf16, #ttnn_layout11>, tensor<1x64x1x1xbf16, #ttnn_layout11>) -> tensor<1x64x80x80xbf16, #ttnn_layout107> loc(#loc27)
        "ttnn.deallocate"(%204) <{force = false}> : (tensor<1x64x80x80xbf16, #ttnn_layout107>) -> () loc(#loc27)
        "ttnn.deallocate"(%93) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout11>) -> () loc(#loc27)
        "ttnn.deallocate"(%71) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout11>) -> () loc(#loc27)
        "ttnn.deallocate"(%55) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout11>) -> () loc(#loc27)
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout11>) -> () loc(#loc27)
        %206 = "ttnn.silu"(%205) : (tensor<1x64x80x80xbf16, #ttnn_layout107>) -> tensor<1x64x80x80xbf16, #ttnn_layout107> loc(#loc209)
        "ttnn.deallocate"(%205) <{force = false}> : (tensor<1x64x80x80xbf16, #ttnn_layout107>) -> () loc(#loc209)
        %207 = "ttnn.permute"(%206) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x80x80xbf16, #ttnn_layout107>) -> tensor<1x80x80x64xbf16, #ttnn_layout106> loc(#loc209)
        "ttnn.deallocate"(%206) <{force = false}> : (tensor<1x64x80x80xbf16, #ttnn_layout107>) -> () loc(#loc209)
        %208 = "ttnn.reshape"(%207) <{shape = [1 : i32, 1 : i32, 6400 : i32, 64 : i32]}> : (tensor<1x80x80x64xbf16, #ttnn_layout106>) -> tensor<1x1x6400x64xbf16, #ttnn_layout104> loc(#loc209)
        "ttnn.deallocate"(%207) <{force = false}> : (tensor<1x80x80x64xbf16, #ttnn_layout106>) -> () loc(#loc209)
        %209 = "ttnn.to_layout"(%208) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x6400x64xbf16, #ttnn_layout104>) -> tensor<1x1x6400x64xbf16, #ttnn_layout105> loc(#loc315)
        "ttnn.deallocate"(%208) <{force = false}> : (tensor<1x1x6400x64xbf16, #ttnn_layout104>) -> () loc(#loc315)
        %210 = "ttnn.conv2d"(%209, %arg13, %101) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<enable_kernel_stride_folding = false, config_tensors_in_dram = true>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 64 : i32, input_height = 80 : i32, input_width = 80 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x6400x64xbf16, #ttnn_layout105>, tensor<64x64x3x3xbf16, #ttnn_layout51>, !ttnn.device) -> tensor<1x1x6400x64xbf16, #ttnn_layout104> loc(#loc210)
        "ttnn.deallocate"(%209) <{force = false}> : (tensor<1x1x6400x64xbf16, #ttnn_layout105>) -> () loc(#loc210)
        "ttnn.deallocate"(%arg13) <{force = false}> : (tensor<64x64x3x3xbf16, #ttnn_layout51>) -> () loc(#loc210)
        %211 = "ttnn.reshape"(%210) <{shape = [1 : i32, 80 : i32, 80 : i32, 64 : i32]}> : (tensor<1x1x6400x64xbf16, #ttnn_layout104>) -> tensor<1x80x80x64xbf16, #ttnn_layout106> loc(#loc316)
        "ttnn.deallocate"(%210) <{force = false}> : (tensor<1x1x6400x64xbf16, #ttnn_layout104>) -> () loc(#loc316)
        %212 = "ttnn.permute"(%211) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x80x80x64xbf16, #ttnn_layout106>) -> tensor<1x64x80x80xbf16, #ttnn_layout107> loc(#loc210)
        "ttnn.deallocate"(%211) <{force = false}> : (tensor<1x80x80x64xbf16, #ttnn_layout106>) -> () loc(#loc210)
        %213 = "ttnn.batch_norm_inference"(%212, %22, %26, %1, %17) <{epsilon = 1.000000e-03 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x64x80x80xbf16, #ttnn_layout107>, tensor<1x64x1x1xbf16, #ttnn_layout11>, tensor<1x64x1x1xbf16, #ttnn_layout11>, tensor<1x64x1x1xbf16, #ttnn_layout11>, tensor<1x64x1x1xbf16, #ttnn_layout11>) -> tensor<1x64x80x80xbf16, #ttnn_layout107> loc(#loc7)
        "ttnn.deallocate"(%212) <{force = false}> : (tensor<1x64x80x80xbf16, #ttnn_layout107>) -> () loc(#loc7)
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout11>) -> () loc(#loc7)
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout11>) -> () loc(#loc7)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout11>) -> () loc(#loc7)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout11>) -> () loc(#loc7)
        %214 = "ttnn.silu"(%213) : (tensor<1x64x80x80xbf16, #ttnn_layout107>) -> tensor<1x64x80x80xbf16, #ttnn_layout107> loc(#loc211)
        "ttnn.deallocate"(%213) <{force = false}> : (tensor<1x64x80x80xbf16, #ttnn_layout107>) -> () loc(#loc211)
        %215 = "ttnn.reshape"(%214) <{shape = [1 : i32, 2 : i32, 32 : i32, 80 : i32, 80 : i32]}> : (tensor<1x64x80x80xbf16, #ttnn_layout107>) -> tensor<1x2x32x80x80xbf16, #ttnn_layout108> loc(#loc212)
        %216 = "ttnn.matmul"(%142, %arg7) <{transpose_a = false, transpose_b = true}> : (tensor<3x512xbf16, #ttnn_layout86>, tensor<64x512xbf16, #ttnn_layout49>) -> tensor<3x64xbf16, #ttnn_layout> loc(#loc317)
        "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<64x512xbf16, #ttnn_layout49>) -> () loc(#loc317)
        %217 = "ttnn.add"(%216, %12) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<3x64xbf16, #ttnn_layout>, tensor<1x3x64xbf16, #ttnn_layout24>) -> tensor<1x3x64xbf16, #ttnn_layout24> loc(#loc318)
        "ttnn.deallocate"(%216) <{force = false}> : (tensor<3x64xbf16, #ttnn_layout>) -> () loc(#loc318)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x3x64xbf16, #ttnn_layout24>) -> () loc(#loc318)
        %218 = "ttnn.reshape"(%217) <{shape = [1 : i32, 3 : i32, 2 : i32, 32 : i32]}> : (tensor<1x3x64xbf16, #ttnn_layout24>) -> tensor<1x3x2x32xbf16, #ttnn_layout88> loc(#loc214)
        "ttnn.deallocate"(%217) <{force = false}> : (tensor<1x3x64xbf16, #ttnn_layout24>) -> () loc(#loc214)
        %219 = "ttnn.permute"(%215) <{permutation = array<i64: 0, 1, 3, 4, 2>}> : (tensor<1x2x32x80x80xbf16, #ttnn_layout108>) -> tensor<1x2x80x80x32xbf16, #ttnn_layout109> loc(#loc319)
        "ttnn.deallocate"(%215) <{force = false}> : (tensor<1x2x32x80x80xbf16, #ttnn_layout108>) -> () loc(#loc319)
        %220 = "ttnn.permute"(%218) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x3x2x32xbf16, #ttnn_layout88>) -> tensor<1x2x32x3xbf16, #ttnn_layout30> loc(#loc320)
        "ttnn.deallocate"(%218) <{force = false}> : (tensor<1x3x2x32xbf16, #ttnn_layout88>) -> () loc(#loc320)
        %221 = "ttnn.reshape"(%219) <{shape = [1 : i32, 2 : i32, 6400 : i32, 32 : i32]}> : (tensor<1x2x80x80x32xbf16, #ttnn_layout109>) -> tensor<1x2x6400x32xbf16, #ttnn_layout110> loc(#loc321)
        "ttnn.deallocate"(%219) <{force = false}> : (tensor<1x2x80x80x32xbf16, #ttnn_layout109>) -> () loc(#loc321)
        %222 = "ttnn.matmul"(%221, %220) <{transpose_a = false, transpose_b = false}> : (tensor<1x2x6400x32xbf16, #ttnn_layout110>, tensor<1x2x32x3xbf16, #ttnn_layout30>) -> tensor<1x2x6400x3xbf16, #ttnn_layout110> loc(#loc215)
        "ttnn.deallocate"(%221) <{force = false}> : (tensor<1x2x6400x32xbf16, #ttnn_layout110>) -> () loc(#loc215)
        "ttnn.deallocate"(%220) <{force = false}> : (tensor<1x2x32x3xbf16, #ttnn_layout30>) -> () loc(#loc215)
        %223 = "ttnn.reshape"(%222) <{shape = [1 : i32, 2 : i32, 80 : i32, 80 : i32, 3 : i32]}> : (tensor<1x2x6400x3xbf16, #ttnn_layout110>) -> tensor<1x2x80x80x3xbf16, #ttnn_layout109> loc(#loc322)
        "ttnn.deallocate"(%222) <{force = false}> : (tensor<1x2x6400x3xbf16, #ttnn_layout110>) -> () loc(#loc322)
        %224 = "ttnn.slice_static"(%199) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 6400 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x6400x128xbf16, #ttnn_layout101>) -> tensor<1x1x6400x64xbf16, #ttnn_layout104> loc(#loc216)
        "ttnn.deallocate"(%199) <{force = false}> : (tensor<1x1x6400x128xbf16, #ttnn_layout101>) -> () loc(#loc216)
        %225 = "ttnn.permute"(%214) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x80x80xbf16, #ttnn_layout107>) -> tensor<1x80x80x64xbf16, #ttnn_layout106> loc(#loc323)
        %226 = "ttnn.reshape"(%225) <{shape = [1 : i32, 1 : i32, 6400 : i32, 64 : i32]}> : (tensor<1x80x80x64xbf16, #ttnn_layout106>) -> tensor<1x1x6400x64xbf16, #ttnn_layout104> loc(#loc374)
        "ttnn.deallocate"(%225) <{force = false}> : (tensor<1x80x80x64xbf16, #ttnn_layout106>) -> () loc(#loc374)
        %227 = "ttnn.to_layout"(%226) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x6400x64xbf16, #ttnn_layout104>) -> tensor<1x1x6400x64xbf16, #ttnn_layout105> loc(#loc324)
        "ttnn.deallocate"(%226) <{force = false}> : (tensor<1x1x6400x64xbf16, #ttnn_layout104>) -> () loc(#loc324)
        %228 = "ttnn.conv2d"(%227, %arg59, %101) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<enable_kernel_stride_folding = false, config_tensors_in_dram = true>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 64 : i32, input_height = 80 : i32, input_width = 80 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x6400x64xbf16, #ttnn_layout105>, tensor<64x64x3x3xbf16, #ttnn_layout51>, !ttnn.device) -> tensor<1x1x6400x64xbf16, #ttnn_layout104> loc(#loc217)
        "ttnn.deallocate"(%227) <{force = false}> : (tensor<1x1x6400x64xbf16, #ttnn_layout105>) -> () loc(#loc217)
        "ttnn.deallocate"(%arg59) <{force = false}> : (tensor<64x64x3x3xbf16, #ttnn_layout51>) -> () loc(#loc217)
        %229 = "ttnn.reshape"(%228) <{shape = [1 : i32, 80 : i32, 80 : i32, 64 : i32]}> : (tensor<1x1x6400x64xbf16, #ttnn_layout104>) -> tensor<1x80x80x64xbf16, #ttnn_layout106> loc(#loc325)
        "ttnn.deallocate"(%228) <{force = false}> : (tensor<1x1x6400x64xbf16, #ttnn_layout104>) -> () loc(#loc325)
        %230 = "ttnn.permute"(%229) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x80x80x64xbf16, #ttnn_layout106>) -> tensor<1x64x80x80xbf16, #ttnn_layout107> loc(#loc217)
        "ttnn.deallocate"(%229) <{force = false}> : (tensor<1x80x80x64xbf16, #ttnn_layout106>) -> () loc(#loc217)
        %231 = "ttnn.batch_norm_inference"(%230, %36, %41, %50, %79) <{epsilon = 1.000000e-03 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x64x80x80xbf16, #ttnn_layout107>, tensor<1x64x1x1xbf16, #ttnn_layout11>, tensor<1x64x1x1xbf16, #ttnn_layout11>, tensor<1x64x1x1xbf16, #ttnn_layout11>, tensor<1x64x1x1xbf16, #ttnn_layout11>) -> tensor<1x64x80x80xbf16, #ttnn_layout107> loc(#loc29)
        "ttnn.deallocate"(%230) <{force = false}> : (tensor<1x64x80x80xbf16, #ttnn_layout107>) -> () loc(#loc29)
        "ttnn.deallocate"(%79) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout11>) -> () loc(#loc29)
        "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout11>) -> () loc(#loc29)
        "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout11>) -> () loc(#loc29)
        "ttnn.deallocate"(%36) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout11>) -> () loc(#loc29)
        %232 = "ttnn.max"(%223) <{dim_arg = [4 : i32], keep_dim = false}> : (tensor<1x2x80x80x3xbf16, #ttnn_layout109>) -> tensor<1x2x80x80xbf16, #ttnn_layout40> loc(#loc218)
        "ttnn.deallocate"(%223) <{force = false}> : (tensor<1x2x80x80x3xbf16, #ttnn_layout109>) -> () loc(#loc218)
        %233 = "ttnn.divide"(%232, %94) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x2x80x80xbf16, #ttnn_layout40>, tensor<1x2x80x80xbf16, #ttnn_layout40>) -> tensor<1x2x80x80xbf16, #ttnn_layout40> loc(#loc219)
        "ttnn.deallocate"(%232) <{force = false}> : (tensor<1x2x80x80xbf16, #ttnn_layout40>) -> () loc(#loc219)
        "ttnn.deallocate"(%94) <{force = false}> : (tensor<1x2x80x80xbf16, #ttnn_layout40>) -> () loc(#loc219)
        %234 = "ttnn.add"(%233, %25) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x2x80x80xbf16, #ttnn_layout40>, tensor<1x2x1x1xbf16, #ttnn_layout30>) -> tensor<1x2x80x80xbf16, #ttnn_layout40> loc(#loc220)
        "ttnn.deallocate"(%233) <{force = false}> : (tensor<1x2x80x80xbf16, #ttnn_layout40>) -> () loc(#loc220)
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<1x2x1x1xbf16, #ttnn_layout30>) -> () loc(#loc220)
        %235 = "ttnn.sigmoid"(%234) : (tensor<1x2x80x80xbf16, #ttnn_layout40>) -> tensor<1x2x80x80xbf16, #ttnn_layout40> loc(#loc221)
        "ttnn.deallocate"(%234) <{force = false}> : (tensor<1x2x80x80xbf16, #ttnn_layout40>) -> () loc(#loc221)
        %236 = "ttnn.reshape"(%235) <{shape = [1 : i32, 2 : i32, 1 : i32, 80 : i32, 80 : i32]}> : (tensor<1x2x80x80xbf16, #ttnn_layout40>) -> tensor<1x2x1x80x80xbf16, #ttnn_layout111> loc(#loc221)
        "ttnn.deallocate"(%235) <{force = false}> : (tensor<1x2x80x80xbf16, #ttnn_layout40>) -> () loc(#loc221)
        %237 = "ttnn.repeat"(%236) <{repeat_dims = #ttnn.shape<1x1x32x1x1>}> : (tensor<1x2x1x80x80xbf16, #ttnn_layout111>) -> tensor<1x2x32x80x80xbf16, #ttnn_layout108> loc(#loc222)
        "ttnn.deallocate"(%236) <{force = false}> : (tensor<1x2x1x80x80xbf16, #ttnn_layout111>) -> () loc(#loc222)
        %238 = "ttnn.reshape"(%237) <{shape = [1 : i32, 64 : i32, 80 : i32, 80 : i32]}> : (tensor<1x2x32x80x80xbf16, #ttnn_layout108>) -> tensor<1x64x80x80xbf16, #ttnn_layout107> loc(#loc326)
        "ttnn.deallocate"(%237) <{force = false}> : (tensor<1x2x32x80x80xbf16, #ttnn_layout108>) -> () loc(#loc326)
        %239 = "ttnn.multiply"(%231, %238) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x64x80x80xbf16, #ttnn_layout107>, tensor<1x64x80x80xbf16, #ttnn_layout107>) -> tensor<1x64x80x80xbf16, #ttnn_layout107> loc(#loc224)
        "ttnn.deallocate"(%238) <{force = false}> : (tensor<1x64x80x80xbf16, #ttnn_layout107>) -> () loc(#loc224)
        "ttnn.deallocate"(%231) <{force = false}> : (tensor<1x64x80x80xbf16, #ttnn_layout107>) -> () loc(#loc224)
        %240 = "ttnn.permute"(%239) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x80x80xbf16, #ttnn_layout107>) -> tensor<1x80x80x64xbf16, #ttnn_layout106> loc(#loc224)
        "ttnn.deallocate"(%239) <{force = false}> : (tensor<1x64x80x80xbf16, #ttnn_layout107>) -> () loc(#loc224)
        %241 = "ttnn.reshape"(%240) <{shape = [1 : i32, 1 : i32, 6400 : i32, 64 : i32]}> : (tensor<1x80x80x64xbf16, #ttnn_layout106>) -> tensor<1x1x6400x64xbf16, #ttnn_layout104> loc(#loc224)
        "ttnn.deallocate"(%240) <{force = false}> : (tensor<1x80x80x64xbf16, #ttnn_layout106>) -> () loc(#loc224)
        %242 = "ttnn.permute"(%214) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x80x80xbf16, #ttnn_layout107>) -> tensor<1x80x80x64xbf16, #ttnn_layout106> loc(#loc225)
        "ttnn.deallocate"(%214) <{force = false}> : (tensor<1x64x80x80xbf16, #ttnn_layout107>) -> () loc(#loc225)
        %243 = "ttnn.reshape"(%242) <{shape = [1 : i32, 1 : i32, 6400 : i32, 64 : i32]}> : (tensor<1x80x80x64xbf16, #ttnn_layout106>) -> tensor<1x1x6400x64xbf16, #ttnn_layout104> loc(#loc225)
        "ttnn.deallocate"(%242) <{force = false}> : (tensor<1x80x80x64xbf16, #ttnn_layout106>) -> () loc(#loc225)
        %244 = "ttnn.concat"(%224, %200, %243, %241) <{dim = 3 : si32}> : (tensor<1x1x6400x64xbf16, #ttnn_layout104>, tensor<1x1x6400x64xbf16, #ttnn_layout104>, tensor<1x1x6400x64xbf16, #ttnn_layout104>, tensor<1x1x6400x64xbf16, #ttnn_layout104>) -> tensor<1x1x6400x256xbf16, #ttnn_layout100> loc(#loc225)
        "ttnn.deallocate"(%243) <{force = false}> : (tensor<1x1x6400x64xbf16, #ttnn_layout104>) -> () loc(#loc225)
        "ttnn.deallocate"(%241) <{force = false}> : (tensor<1x1x6400x64xbf16, #ttnn_layout104>) -> () loc(#loc225)
        "ttnn.deallocate"(%224) <{force = false}> : (tensor<1x1x6400x64xbf16, #ttnn_layout104>) -> () loc(#loc225)
        "ttnn.deallocate"(%200) <{force = false}> : (tensor<1x1x6400x64xbf16, #ttnn_layout104>) -> () loc(#loc225)
        %245 = "ttnn.to_layout"(%244) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x6400x256xbf16, #ttnn_layout100>) -> tensor<1x1x6400x256xbf16, #ttnn_layout112> loc(#loc327)
        "ttnn.deallocate"(%244) <{force = false}> : (tensor<1x1x6400x256xbf16, #ttnn_layout100>) -> () loc(#loc327)
        %246 = "ttnn.conv2d"(%245, %arg4, %101) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<enable_kernel_stride_folding = false, config_tensors_in_dram = true>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 80 : i32, input_width = 80 : i32, kernel_size = array<i32: 1, 1>, out_channels = 128 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x6400x256xbf16, #ttnn_layout112>, tensor<128x256x1x1xbf16, #ttnn_layout48>, !ttnn.device) -> tensor<1x1x6400x128xbf16, #ttnn_layout101> loc(#loc226)
        "ttnn.deallocate"(%245) <{force = false}> : (tensor<1x1x6400x256xbf16, #ttnn_layout112>) -> () loc(#loc226)
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<128x256x1x1xbf16, #ttnn_layout48>) -> () loc(#loc226)
        %247 = "ttnn.reshape"(%246) <{shape = [1 : i32, 80 : i32, 80 : i32, 128 : i32]}> : (tensor<1x1x6400x128xbf16, #ttnn_layout101>) -> tensor<1x80x80x128xbf16, #ttnn_layout99> loc(#loc328)
        "ttnn.deallocate"(%246) <{force = false}> : (tensor<1x1x6400x128xbf16, #ttnn_layout101>) -> () loc(#loc328)
        %248 = "ttnn.permute"(%247) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x80x80x128xbf16, #ttnn_layout99>) -> tensor<1x128x80x80xbf16, #ttnn_layout65> loc(#loc226)
        "ttnn.deallocate"(%247) <{force = false}> : (tensor<1x80x80x128xbf16, #ttnn_layout99>) -> () loc(#loc226)
        %249 = "ttnn.batch_norm_inference"(%248, %19, %66, %69, %63) <{epsilon = 1.000000e-03 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x128x80x80xbf16, #ttnn_layout65>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>) -> tensor<1x128x80x80xbf16, #ttnn_layout65> loc(#loc19)
        "ttnn.deallocate"(%248) <{force = false}> : (tensor<1x128x80x80xbf16, #ttnn_layout65>) -> () loc(#loc19)
        "ttnn.deallocate"(%69) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc19)
        "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc19)
        "ttnn.deallocate"(%63) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc19)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc19)
        %250 = "ttnn.silu"(%249) : (tensor<1x128x80x80xbf16, #ttnn_layout65>) -> tensor<1x128x80x80xbf16, #ttnn_layout65> loc(#loc227)
        "ttnn.deallocate"(%249) <{force = false}> : (tensor<1x128x80x80xbf16, #ttnn_layout65>) -> () loc(#loc227)
        %251 = "ttnn.permute"(%250) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x128x80x80xbf16, #ttnn_layout65>) -> tensor<1x80x80x128xbf16, #ttnn_layout99> loc(#loc329)
        %252 = "ttnn.reshape"(%251) <{shape = [1 : i32, 1 : i32, 6400 : i32, 128 : i32]}> : (tensor<1x80x80x128xbf16, #ttnn_layout99>) -> tensor<1x1x6400x128xbf16, #ttnn_layout101> loc(#loc375)
        "ttnn.deallocate"(%251) <{force = false}> : (tensor<1x80x80x128xbf16, #ttnn_layout99>) -> () loc(#loc375)
        %253 = "ttnn.to_layout"(%252) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x6400x128xbf16, #ttnn_layout101>) -> tensor<1x1x6400x128xbf16, #ttnn_layout113> loc(#loc330)
        "ttnn.deallocate"(%252) <{force = false}> : (tensor<1x1x6400x128xbf16, #ttnn_layout101>) -> () loc(#loc330)
        %254 = "ttnn.conv2d"(%253, %arg87, %101) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<enable_kernel_stride_folding = false, config_tensors_in_dram = true>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 128 : i32, input_height = 80 : i32, input_width = 80 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 2, 2>}> : (tensor<1x1x6400x128xbf16, #ttnn_layout113>, tensor<128x128x3x3xbf16, #ttnn_layout56>, !ttnn.device) -> tensor<1x1x1600x128xbf16, #ttnn_layout80> loc(#loc228)
        "ttnn.deallocate"(%253) <{force = false}> : (tensor<1x1x6400x128xbf16, #ttnn_layout113>) -> () loc(#loc228)
        "ttnn.deallocate"(%arg87) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout56>) -> () loc(#loc228)
        %255 = "ttnn.reshape"(%254) <{shape = [1 : i32, 40 : i32, 40 : i32, 128 : i32]}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> tensor<1x40x40x128xbf16, #ttnn_layout82> loc(#loc331)
        "ttnn.deallocate"(%254) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> () loc(#loc331)
        %256 = "ttnn.permute"(%255) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> tensor<1x128x40x40xbf16, #ttnn_layout83> loc(#loc228)
        "ttnn.deallocate"(%255) <{force = false}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> () loc(#loc228)
        %257 = "ttnn.batch_norm_inference"(%256, %83, %85, %81, %14) <{epsilon = 1.000000e-03 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>) -> tensor<1x128x40x40xbf16, #ttnn_layout83> loc(#loc17)
        "ttnn.deallocate"(%256) <{force = false}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> () loc(#loc17)
        "ttnn.deallocate"(%85) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc17)
        "ttnn.deallocate"(%83) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc17)
        "ttnn.deallocate"(%81) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc17)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc17)
        %258 = "ttnn.silu"(%257) : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> tensor<1x128x40x40xbf16, #ttnn_layout83> loc(#loc229)
        "ttnn.deallocate"(%257) <{force = false}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> () loc(#loc229)
        %259 = "ttnn.permute"(%258) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> tensor<1x40x40x128xbf16, #ttnn_layout82> loc(#loc229)
        "ttnn.deallocate"(%258) <{force = false}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> () loc(#loc229)
        %260 = "ttnn.reshape"(%259) <{shape = [1 : i32, 1 : i32, 1600 : i32, 128 : i32]}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> tensor<1x1x1600x128xbf16, #ttnn_layout80> loc(#loc229)
        "ttnn.deallocate"(%259) <{force = false}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> () loc(#loc229)
        %261 = "ttnn.permute"(%177) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x40x40xbf16, #ttnn_layout66>) -> tensor<1x40x40x256xbf16, #ttnn_layout75> loc(#loc230)
        "ttnn.deallocate"(%177) <{force = false}> : (tensor<1x256x40x40xbf16, #ttnn_layout66>) -> () loc(#loc230)
        %262 = "ttnn.reshape"(%261) <{shape = [1 : i32, 1 : i32, 1600 : i32, 256 : i32]}> : (tensor<1x40x40x256xbf16, #ttnn_layout75>) -> tensor<1x1x1600x256xbf16, #ttnn_layout77> loc(#loc230)
        "ttnn.deallocate"(%261) <{force = false}> : (tensor<1x40x40x256xbf16, #ttnn_layout75>) -> () loc(#loc230)
        %263 = "ttnn.concat"(%260, %262) <{dim = 3 : si32}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>, tensor<1x1x1600x256xbf16, #ttnn_layout77>) -> tensor<1x1x1600x384xbf16, #ttnn_layout114> loc(#loc230)
        "ttnn.deallocate"(%262) <{force = false}> : (tensor<1x1x1600x256xbf16, #ttnn_layout77>) -> () loc(#loc230)
        "ttnn.deallocate"(%260) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> () loc(#loc230)
        %264 = "ttnn.to_layout"(%263) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1600x384xbf16, #ttnn_layout114>) -> tensor<1x1x1600x384xbf16, #ttnn_layout115> loc(#loc332)
        "ttnn.deallocate"(%263) <{force = false}> : (tensor<1x1x1600x384xbf16, #ttnn_layout114>) -> () loc(#loc332)
        %265 = "ttnn.conv2d"(%264, %arg82, %101) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<enable_kernel_stride_folding = false, config_tensors_in_dram = true>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 384 : i32, input_height = 40 : i32, input_width = 40 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1600x384xbf16, #ttnn_layout115>, tensor<256x384x1x1xbf16, #ttnn_layout60>, !ttnn.device) -> tensor<1x1x1600x256xbf16, #ttnn_layout77> loc(#loc231)
        "ttnn.deallocate"(%264) <{force = false}> : (tensor<1x1x1600x384xbf16, #ttnn_layout115>) -> () loc(#loc231)
        "ttnn.deallocate"(%arg82) <{force = false}> : (tensor<256x384x1x1xbf16, #ttnn_layout60>) -> () loc(#loc231)
        %266 = "ttnn.reshape"(%265) <{shape = [1 : i32, 40 : i32, 40 : i32, 256 : i32]}> : (tensor<1x1x1600x256xbf16, #ttnn_layout77>) -> tensor<1x40x40x256xbf16, #ttnn_layout75> loc(#loc333)
        "ttnn.deallocate"(%265) <{force = false}> : (tensor<1x1x1600x256xbf16, #ttnn_layout77>) -> () loc(#loc333)
        %267 = "ttnn.permute"(%266) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x40x40x256xbf16, #ttnn_layout75>) -> tensor<1x256x40x40xbf16, #ttnn_layout66> loc(#loc231)
        "ttnn.deallocate"(%266) <{force = false}> : (tensor<1x40x40x256xbf16, #ttnn_layout75>) -> () loc(#loc231)
        %268 = "ttnn.batch_norm_inference"(%267, %8, %28, %23, %15) <{epsilon = 1.000000e-03 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x40x40xbf16, #ttnn_layout66>, tensor<1x256x1x1xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>) -> tensor<1x256x40x40xbf16, #ttnn_layout66> loc(#loc13)
        "ttnn.deallocate"(%267) <{force = false}> : (tensor<1x256x40x40xbf16, #ttnn_layout66>) -> () loc(#loc13)
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc13)
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc13)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc13)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc13)
        %269 = "ttnn.silu"(%268) : (tensor<1x256x40x40xbf16, #ttnn_layout66>) -> tensor<1x256x40x40xbf16, #ttnn_layout66> loc(#loc232)
        "ttnn.deallocate"(%268) <{force = false}> : (tensor<1x256x40x40xbf16, #ttnn_layout66>) -> () loc(#loc232)
        %270 = "ttnn.permute"(%269) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x40x40xbf16, #ttnn_layout66>) -> tensor<1x40x40x256xbf16, #ttnn_layout75> loc(#loc232)
        "ttnn.deallocate"(%269) <{force = false}> : (tensor<1x256x40x40xbf16, #ttnn_layout66>) -> () loc(#loc232)
        %271 = "ttnn.reshape"(%270) <{shape = [1 : i32, 1 : i32, 1600 : i32, 256 : i32]}> : (tensor<1x40x40x256xbf16, #ttnn_layout75>) -> tensor<1x1x1600x256xbf16, #ttnn_layout77> loc(#loc232)
        "ttnn.deallocate"(%270) <{force = false}> : (tensor<1x40x40x256xbf16, #ttnn_layout75>) -> () loc(#loc232)
        %272 = "ttnn.slice_static"(%271) <{begins = [0 : i32, 0 : i32, 0 : i32, 128 : i32], ends = [1 : i32, 1 : i32, 1600 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x1600x256xbf16, #ttnn_layout77>) -> tensor<1x1x1600x128xbf16, #ttnn_layout80> loc(#loc233)
        %273 = "ttnn.to_layout"(%272) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> tensor<1x1x1600x128xbf16, #ttnn_layout81> loc(#loc334)
        %274 = "ttnn.conv2d"(%273, %arg77, %101) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<enable_kernel_stride_folding = false, config_tensors_in_dram = true>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 128 : i32, input_height = 40 : i32, input_width = 40 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x1600x128xbf16, #ttnn_layout81>, tensor<128x128x3x3xbf16, #ttnn_layout56>, !ttnn.device) -> tensor<1x1x1600x128xbf16, #ttnn_layout80> loc(#loc234)
        "ttnn.deallocate"(%273) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout81>) -> () loc(#loc234)
        "ttnn.deallocate"(%arg77) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout56>) -> () loc(#loc234)
        %275 = "ttnn.reshape"(%274) <{shape = [1 : i32, 40 : i32, 40 : i32, 128 : i32]}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> tensor<1x40x40x128xbf16, #ttnn_layout82> loc(#loc335)
        "ttnn.deallocate"(%274) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> () loc(#loc335)
        %276 = "ttnn.permute"(%275) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> tensor<1x128x40x40xbf16, #ttnn_layout83> loc(#loc234)
        "ttnn.deallocate"(%275) <{force = false}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> () loc(#loc234)
        %277 = "ttnn.batch_norm_inference"(%276, %48, %92, %31, %75) <{epsilon = 1.000000e-03 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>) -> tensor<1x128x40x40xbf16, #ttnn_layout83> loc(#loc26)
        "ttnn.deallocate"(%276) <{force = false}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> () loc(#loc26)
        "ttnn.deallocate"(%92) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc26)
        "ttnn.deallocate"(%75) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc26)
        "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc26)
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc26)
        %278 = "ttnn.silu"(%277) : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> tensor<1x128x40x40xbf16, #ttnn_layout83> loc(#loc235)
        "ttnn.deallocate"(%277) <{force = false}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> () loc(#loc235)
        %279 = "ttnn.permute"(%278) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> tensor<1x40x40x128xbf16, #ttnn_layout82> loc(#loc235)
        "ttnn.deallocate"(%278) <{force = false}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> () loc(#loc235)
        %280 = "ttnn.reshape"(%279) <{shape = [1 : i32, 1 : i32, 1600 : i32, 128 : i32]}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> tensor<1x1x1600x128xbf16, #ttnn_layout80> loc(#loc235)
        "ttnn.deallocate"(%279) <{force = false}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> () loc(#loc235)
        %281 = "ttnn.to_layout"(%280) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> tensor<1x1x1600x128xbf16, #ttnn_layout81> loc(#loc336)
        "ttnn.deallocate"(%280) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> () loc(#loc336)
        %282 = "ttnn.conv2d"(%281, %arg72, %101) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<enable_kernel_stride_folding = false, config_tensors_in_dram = true>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 128 : i32, input_height = 40 : i32, input_width = 40 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x1600x128xbf16, #ttnn_layout81>, tensor<128x128x3x3xbf16, #ttnn_layout56>, !ttnn.device) -> tensor<1x1x1600x128xbf16, #ttnn_layout80> loc(#loc236)
        "ttnn.deallocate"(%281) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout81>) -> () loc(#loc236)
        "ttnn.deallocate"(%arg72) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout56>) -> () loc(#loc236)
        %283 = "ttnn.reshape"(%282) <{shape = [1 : i32, 40 : i32, 40 : i32, 128 : i32]}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> tensor<1x40x40x128xbf16, #ttnn_layout82> loc(#loc337)
        "ttnn.deallocate"(%282) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> () loc(#loc337)
        %284 = "ttnn.permute"(%283) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> tensor<1x128x40x40xbf16, #ttnn_layout83> loc(#loc236)
        "ttnn.deallocate"(%283) <{force = false}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> () loc(#loc236)
        %285 = "ttnn.batch_norm_inference"(%284, %7, %74, %16, %52) <{epsilon = 1.000000e-03 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>) -> tensor<1x128x40x40xbf16, #ttnn_layout83> loc(#loc12)
        "ttnn.deallocate"(%284) <{force = false}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> () loc(#loc12)
        "ttnn.deallocate"(%74) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc12)
        "ttnn.deallocate"(%52) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc12)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc12)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc12)
        %286 = "ttnn.silu"(%285) : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> tensor<1x128x40x40xbf16, #ttnn_layout83> loc(#loc237)
        "ttnn.deallocate"(%285) <{force = false}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> () loc(#loc237)
        %287 = "ttnn.reshape"(%286) <{shape = [1 : i32, 4 : i32, 32 : i32, 40 : i32, 40 : i32]}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> tensor<1x4x32x40x40xbf16, #ttnn_layout84> loc(#loc238)
        %288 = "ttnn.matmul"(%142, %arg67) <{transpose_a = false, transpose_b = true}> : (tensor<3x512xbf16, #ttnn_layout86>, tensor<128x512xbf16, #ttnn_layout55>) -> tensor<3x128xbf16, #ttnn_layout87> loc(#loc338)
        "ttnn.deallocate"(%arg67) <{force = false}> : (tensor<128x512xbf16, #ttnn_layout55>) -> () loc(#loc338)
        %289 = "ttnn.add"(%288, %67) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<3x128xbf16, #ttnn_layout87>, tensor<1x3x128xbf16, #ttnn_layout39>) -> tensor<1x3x128xbf16, #ttnn_layout39> loc(#loc339)
        "ttnn.deallocate"(%288) <{force = false}> : (tensor<3x128xbf16, #ttnn_layout87>) -> () loc(#loc339)
        "ttnn.deallocate"(%67) <{force = false}> : (tensor<1x3x128xbf16, #ttnn_layout39>) -> () loc(#loc339)
        %290 = "ttnn.reshape"(%289) <{shape = [1 : i32, 3 : i32, 4 : i32, 32 : i32]}> : (tensor<1x3x128xbf16, #ttnn_layout39>) -> tensor<1x3x4x32xbf16, #ttnn_layout88> loc(#loc240)
        "ttnn.deallocate"(%289) <{force = false}> : (tensor<1x3x128xbf16, #ttnn_layout39>) -> () loc(#loc240)
        %291 = "ttnn.permute"(%287) <{permutation = array<i64: 0, 1, 3, 4, 2>}> : (tensor<1x4x32x40x40xbf16, #ttnn_layout84>) -> tensor<1x4x40x40x32xbf16, #ttnn_layout89> loc(#loc340)
        "ttnn.deallocate"(%287) <{force = false}> : (tensor<1x4x32x40x40xbf16, #ttnn_layout84>) -> () loc(#loc340)
        %292 = "ttnn.permute"(%290) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x3x4x32xbf16, #ttnn_layout88>) -> tensor<1x4x32x3xbf16, #ttnn_layout26> loc(#loc341)
        "ttnn.deallocate"(%290) <{force = false}> : (tensor<1x3x4x32xbf16, #ttnn_layout88>) -> () loc(#loc341)
        %293 = "ttnn.reshape"(%291) <{shape = [1 : i32, 4 : i32, 1600 : i32, 32 : i32]}> : (tensor<1x4x40x40x32xbf16, #ttnn_layout89>) -> tensor<1x4x1600x32xbf16, #ttnn_layout90> loc(#loc342)
        "ttnn.deallocate"(%291) <{force = false}> : (tensor<1x4x40x40x32xbf16, #ttnn_layout89>) -> () loc(#loc342)
        %294 = "ttnn.matmul"(%293, %292) <{transpose_a = false, transpose_b = false}> : (tensor<1x4x1600x32xbf16, #ttnn_layout90>, tensor<1x4x32x3xbf16, #ttnn_layout26>) -> tensor<1x4x1600x3xbf16, #ttnn_layout90> loc(#loc241)
        "ttnn.deallocate"(%293) <{force = false}> : (tensor<1x4x1600x32xbf16, #ttnn_layout90>) -> () loc(#loc241)
        "ttnn.deallocate"(%292) <{force = false}> : (tensor<1x4x32x3xbf16, #ttnn_layout26>) -> () loc(#loc241)
        %295 = "ttnn.reshape"(%294) <{shape = [1 : i32, 4 : i32, 40 : i32, 40 : i32, 3 : i32]}> : (tensor<1x4x1600x3xbf16, #ttnn_layout90>) -> tensor<1x4x40x40x3xbf16, #ttnn_layout89> loc(#loc343)
        "ttnn.deallocate"(%294) <{force = false}> : (tensor<1x4x1600x3xbf16, #ttnn_layout90>) -> () loc(#loc343)
        %296 = "ttnn.slice_static"(%271) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 1600 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x1600x256xbf16, #ttnn_layout77>) -> tensor<1x1x1600x128xbf16, #ttnn_layout80> loc(#loc242)
        "ttnn.deallocate"(%271) <{force = false}> : (tensor<1x1x1600x256xbf16, #ttnn_layout77>) -> () loc(#loc242)
        %297 = "ttnn.permute"(%286) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> tensor<1x40x40x128xbf16, #ttnn_layout82> loc(#loc344)
        %298 = "ttnn.reshape"(%297) <{shape = [1 : i32, 1 : i32, 1600 : i32, 128 : i32]}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> tensor<1x1x1600x128xbf16, #ttnn_layout80> loc(#loc376)
        "ttnn.deallocate"(%297) <{force = false}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> () loc(#loc376)
        %299 = "ttnn.to_layout"(%298) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> tensor<1x1x1600x128xbf16, #ttnn_layout81> loc(#loc345)
        "ttnn.deallocate"(%298) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> () loc(#loc345)
        %300 = "ttnn.conv2d"(%299, %arg92, %101) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<enable_kernel_stride_folding = false, config_tensors_in_dram = true>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 128 : i32, input_height = 40 : i32, input_width = 40 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x1600x128xbf16, #ttnn_layout81>, tensor<128x128x3x3xbf16, #ttnn_layout56>, !ttnn.device) -> tensor<1x1x1600x128xbf16, #ttnn_layout80> loc(#loc243)
        "ttnn.deallocate"(%299) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout81>) -> () loc(#loc243)
        "ttnn.deallocate"(%arg92) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout56>) -> () loc(#loc243)
        %301 = "ttnn.reshape"(%300) <{shape = [1 : i32, 40 : i32, 40 : i32, 128 : i32]}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> tensor<1x40x40x128xbf16, #ttnn_layout82> loc(#loc346)
        "ttnn.deallocate"(%300) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> () loc(#loc346)
        %302 = "ttnn.permute"(%301) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> tensor<1x128x40x40xbf16, #ttnn_layout83> loc(#loc243)
        "ttnn.deallocate"(%301) <{force = false}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> () loc(#loc243)
        %303 = "ttnn.batch_norm_inference"(%302, %82, %35, %24, %86) <{epsilon = 1.000000e-03 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>, tensor<1x128x1x1xbf16, #ttnn_layout16>) -> tensor<1x128x40x40xbf16, #ttnn_layout83> loc(#loc22)
        "ttnn.deallocate"(%302) <{force = false}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> () loc(#loc22)
        "ttnn.deallocate"(%86) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc22)
        "ttnn.deallocate"(%82) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc22)
        "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc22)
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout16>) -> () loc(#loc22)
        %304 = "ttnn.max"(%295) <{dim_arg = [4 : i32], keep_dim = false}> : (tensor<1x4x40x40x3xbf16, #ttnn_layout89>) -> tensor<1x4x40x40xbf16, #ttnn_layout32> loc(#loc244)
        "ttnn.deallocate"(%295) <{force = false}> : (tensor<1x4x40x40x3xbf16, #ttnn_layout89>) -> () loc(#loc244)
        %305 = "ttnn.divide"(%304, %37) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x40x40xbf16, #ttnn_layout32>, tensor<1x4x40x40xbf16, #ttnn_layout32>) -> tensor<1x4x40x40xbf16, #ttnn_layout32> loc(#loc245)
        "ttnn.deallocate"(%304) <{force = false}> : (tensor<1x4x40x40xbf16, #ttnn_layout32>) -> () loc(#loc245)
        "ttnn.deallocate"(%37) <{force = false}> : (tensor<1x4x40x40xbf16, #ttnn_layout32>) -> () loc(#loc245)
        %306 = "ttnn.add"(%305, %88) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x4x40x40xbf16, #ttnn_layout32>, tensor<1x4x1x1xbf16, #ttnn_layout26>) -> tensor<1x4x40x40xbf16, #ttnn_layout32> loc(#loc246)
        "ttnn.deallocate"(%305) <{force = false}> : (tensor<1x4x40x40xbf16, #ttnn_layout32>) -> () loc(#loc246)
        "ttnn.deallocate"(%88) <{force = false}> : (tensor<1x4x1x1xbf16, #ttnn_layout26>) -> () loc(#loc246)
        %307 = "ttnn.sigmoid"(%306) : (tensor<1x4x40x40xbf16, #ttnn_layout32>) -> tensor<1x4x40x40xbf16, #ttnn_layout32> loc(#loc247)
        "ttnn.deallocate"(%306) <{force = false}> : (tensor<1x4x40x40xbf16, #ttnn_layout32>) -> () loc(#loc247)
        %308 = "ttnn.reshape"(%307) <{shape = [1 : i32, 4 : i32, 1 : i32, 40 : i32, 40 : i32]}> : (tensor<1x4x40x40xbf16, #ttnn_layout32>) -> tensor<1x4x1x40x40xbf16, #ttnn_layout91> loc(#loc247)
        "ttnn.deallocate"(%307) <{force = false}> : (tensor<1x4x40x40xbf16, #ttnn_layout32>) -> () loc(#loc247)
        %309 = "ttnn.repeat"(%308) <{repeat_dims = #ttnn.shape<1x1x32x1x1>}> : (tensor<1x4x1x40x40xbf16, #ttnn_layout91>) -> tensor<1x4x32x40x40xbf16, #ttnn_layout84> loc(#loc248)
        "ttnn.deallocate"(%308) <{force = false}> : (tensor<1x4x1x40x40xbf16, #ttnn_layout91>) -> () loc(#loc248)
        %310 = "ttnn.reshape"(%309) <{shape = [1 : i32, 128 : i32, 40 : i32, 40 : i32]}> : (tensor<1x4x32x40x40xbf16, #ttnn_layout84>) -> tensor<1x128x40x40xbf16, #ttnn_layout83> loc(#loc347)
        "ttnn.deallocate"(%309) <{force = false}> : (tensor<1x4x32x40x40xbf16, #ttnn_layout84>) -> () loc(#loc347)
        %311 = "ttnn.multiply"(%303, %310) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>, tensor<1x128x40x40xbf16, #ttnn_layout83>) -> tensor<1x128x40x40xbf16, #ttnn_layout83> loc(#loc250)
        "ttnn.deallocate"(%310) <{force = false}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> () loc(#loc250)
        "ttnn.deallocate"(%303) <{force = false}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> () loc(#loc250)
        %312 = "ttnn.permute"(%311) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> tensor<1x40x40x128xbf16, #ttnn_layout82> loc(#loc250)
        "ttnn.deallocate"(%311) <{force = false}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> () loc(#loc250)
        %313 = "ttnn.reshape"(%312) <{shape = [1 : i32, 1 : i32, 1600 : i32, 128 : i32]}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> tensor<1x1x1600x128xbf16, #ttnn_layout80> loc(#loc250)
        "ttnn.deallocate"(%312) <{force = false}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> () loc(#loc250)
        %314 = "ttnn.permute"(%286) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> tensor<1x40x40x128xbf16, #ttnn_layout82> loc(#loc251)
        "ttnn.deallocate"(%286) <{force = false}> : (tensor<1x128x40x40xbf16, #ttnn_layout83>) -> () loc(#loc251)
        %315 = "ttnn.reshape"(%314) <{shape = [1 : i32, 1 : i32, 1600 : i32, 128 : i32]}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> tensor<1x1x1600x128xbf16, #ttnn_layout80> loc(#loc251)
        "ttnn.deallocate"(%314) <{force = false}> : (tensor<1x40x40x128xbf16, #ttnn_layout82>) -> () loc(#loc251)
        %316 = "ttnn.concat"(%296, %272, %315, %313) <{dim = 3 : si32}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>, tensor<1x1x1600x128xbf16, #ttnn_layout80>, tensor<1x1x1600x128xbf16, #ttnn_layout80>, tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> tensor<1x1x1600x512xbf16, #ttnn_layout76> loc(#loc251)
        "ttnn.deallocate"(%315) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> () loc(#loc251)
        "ttnn.deallocate"(%313) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> () loc(#loc251)
        "ttnn.deallocate"(%296) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> () loc(#loc251)
        "ttnn.deallocate"(%272) <{force = false}> : (tensor<1x1x1600x128xbf16, #ttnn_layout80>) -> () loc(#loc251)
        %317 = "ttnn.to_layout"(%316) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1600x512xbf16, #ttnn_layout76>) -> tensor<1x1x1600x512xbf16, #ttnn_layout92> loc(#loc348)
        "ttnn.deallocate"(%316) <{force = false}> : (tensor<1x1x1600x512xbf16, #ttnn_layout76>) -> () loc(#loc348)
        %318 = "ttnn.conv2d"(%317, %arg64, %101) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<enable_kernel_stride_folding = false, config_tensors_in_dram = true>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 40 : i32, input_width = 40 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x1600x512xbf16, #ttnn_layout92>, tensor<256x512x1x1xbf16, #ttnn_layout54>, !ttnn.device) -> tensor<1x1x1600x256xbf16, #ttnn_layout77> loc(#loc252)
        "ttnn.deallocate"(%317) <{force = false}> : (tensor<1x1x1600x512xbf16, #ttnn_layout92>) -> () loc(#loc252)
        "ttnn.deallocate"(%arg64) <{force = false}> : (tensor<256x512x1x1xbf16, #ttnn_layout54>) -> () loc(#loc252)
        %319 = "ttnn.reshape"(%318) <{shape = [1 : i32, 40 : i32, 40 : i32, 256 : i32]}> : (tensor<1x1x1600x256xbf16, #ttnn_layout77>) -> tensor<1x40x40x256xbf16, #ttnn_layout75> loc(#loc349)
        "ttnn.deallocate"(%318) <{force = false}> : (tensor<1x1x1600x256xbf16, #ttnn_layout77>) -> () loc(#loc349)
        %320 = "ttnn.permute"(%319) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x40x40x256xbf16, #ttnn_layout75>) -> tensor<1x256x40x40xbf16, #ttnn_layout66> loc(#loc252)
        "ttnn.deallocate"(%319) <{force = false}> : (tensor<1x40x40x256xbf16, #ttnn_layout75>) -> () loc(#loc252)
        %321 = "ttnn.batch_norm_inference"(%320, %27, %60, %51, %84) <{epsilon = 1.000000e-03 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x40x40xbf16, #ttnn_layout66>, tensor<1x256x1x1xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>) -> tensor<1x256x40x40xbf16, #ttnn_layout66> loc(#loc24)
        "ttnn.deallocate"(%320) <{force = false}> : (tensor<1x256x40x40xbf16, #ttnn_layout66>) -> () loc(#loc24)
        "ttnn.deallocate"(%84) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc24)
        "ttnn.deallocate"(%60) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc24)
        "ttnn.deallocate"(%51) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc24)
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc24)
        %322 = "ttnn.silu"(%321) : (tensor<1x256x40x40xbf16, #ttnn_layout66>) -> tensor<1x256x40x40xbf16, #ttnn_layout66> loc(#loc253)
        "ttnn.deallocate"(%321) <{force = false}> : (tensor<1x256x40x40xbf16, #ttnn_layout66>) -> () loc(#loc253)
        %323 = "ttnn.permute"(%322) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x40x40xbf16, #ttnn_layout66>) -> tensor<1x40x40x256xbf16, #ttnn_layout75> loc(#loc350)
        %324 = "ttnn.reshape"(%323) <{shape = [1 : i32, 1 : i32, 1600 : i32, 256 : i32]}> : (tensor<1x40x40x256xbf16, #ttnn_layout75>) -> tensor<1x1x1600x256xbf16, #ttnn_layout77> loc(#loc377)
        "ttnn.deallocate"(%323) <{force = false}> : (tensor<1x40x40x256xbf16, #ttnn_layout75>) -> () loc(#loc377)
        %325 = "ttnn.to_layout"(%324) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x1600x256xbf16, #ttnn_layout77>) -> tensor<1x1x1600x256xbf16, #ttnn_layout116> loc(#loc351)
        "ttnn.deallocate"(%324) <{force = false}> : (tensor<1x1x1600x256xbf16, #ttnn_layout77>) -> () loc(#loc351)
        %326 = "ttnn.conv2d"(%325, %arg120, %101) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<enable_kernel_stride_folding = false, config_tensors_in_dram = true>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 40 : i32, input_width = 40 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 2, 2>}> : (tensor<1x1x1600x256xbf16, #ttnn_layout116>, tensor<256x256x3x3xbf16, #ttnn_layout63>, !ttnn.device) -> tensor<1x1x400x256xbf16, #ttnn_layout117> loc(#loc254)
        "ttnn.deallocate"(%325) <{force = false}> : (tensor<1x1x1600x256xbf16, #ttnn_layout116>) -> () loc(#loc254)
        "ttnn.deallocate"(%arg120) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout63>) -> () loc(#loc254)
        %327 = "ttnn.reshape"(%326) <{shape = [1 : i32, 20 : i32, 20 : i32, 256 : i32]}> : (tensor<1x1x400x256xbf16, #ttnn_layout117>) -> tensor<1x20x20x256xbf16, #ttnn_layout118> loc(#loc352)
        "ttnn.deallocate"(%326) <{force = false}> : (tensor<1x1x400x256xbf16, #ttnn_layout117>) -> () loc(#loc352)
        %328 = "ttnn.permute"(%327) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x20x20x256xbf16, #ttnn_layout118>) -> tensor<1x256x20x20xbf16, #ttnn_layout23> loc(#loc254)
        "ttnn.deallocate"(%327) <{force = false}> : (tensor<1x20x20x256xbf16, #ttnn_layout118>) -> () loc(#loc254)
        %329 = "ttnn.batch_norm_inference"(%328, %11, %13, %77, %96) <{epsilon = 1.000000e-03 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x20x20xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>) -> tensor<1x256x20x20xbf16, #ttnn_layout23> loc(#loc15)
        "ttnn.deallocate"(%328) <{force = false}> : (tensor<1x256x20x20xbf16, #ttnn_layout23>) -> () loc(#loc15)
        "ttnn.deallocate"(%96) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc15)
        "ttnn.deallocate"(%77) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc15)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc15)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc15)
        %330 = "ttnn.silu"(%329) : (tensor<1x256x20x20xbf16, #ttnn_layout23>) -> tensor<1x256x20x20xbf16, #ttnn_layout23> loc(#loc255)
        "ttnn.deallocate"(%329) <{force = false}> : (tensor<1x256x20x20xbf16, #ttnn_layout23>) -> () loc(#loc255)
        %331 = "ttnn.permute"(%330) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x20x20xbf16, #ttnn_layout23>) -> tensor<1x20x20x256xbf16, #ttnn_layout118> loc(#loc255)
        "ttnn.deallocate"(%330) <{force = false}> : (tensor<1x256x20x20xbf16, #ttnn_layout23>) -> () loc(#loc255)
        %332 = "ttnn.reshape"(%331) <{shape = [1 : i32, 1 : i32, 400 : i32, 256 : i32]}> : (tensor<1x20x20x256xbf16, #ttnn_layout118>) -> tensor<1x1x400x256xbf16, #ttnn_layout117> loc(#loc255)
        "ttnn.deallocate"(%331) <{force = false}> : (tensor<1x20x20x256xbf16, #ttnn_layout118>) -> () loc(#loc255)
        %333 = "ttnn.to_layout"(%arg49) <{layout = #ttnn.layout<tile>}> : (tensor<1x512x20x20xbf16, #ttnn_layout59>) -> tensor<1x512x20x20xbf16, #ttnn_layout36> loc(#loc256)
        "ttnn.deallocate"(%arg49) <{force = false}> : (tensor<1x512x20x20xbf16, #ttnn_layout59>) -> () loc(#loc256)
        %334 = "ttnn.permute"(%333) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x512x20x20xbf16, #ttnn_layout36>) -> tensor<1x20x20x512xbf16, #ttnn_layout119> loc(#loc257)
        "ttnn.deallocate"(%333) <{force = false}> : (tensor<1x512x20x20xbf16, #ttnn_layout36>) -> () loc(#loc257)
        %335 = "ttnn.reshape"(%334) <{shape = [1 : i32, 1 : i32, 400 : i32, 512 : i32]}> : (tensor<1x20x20x512xbf16, #ttnn_layout119>) -> tensor<1x1x400x512xbf16, #ttnn_layout120> loc(#loc257)
        "ttnn.deallocate"(%334) <{force = false}> : (tensor<1x20x20x512xbf16, #ttnn_layout119>) -> () loc(#loc257)
        %336 = "ttnn.concat"(%332, %335) <{dim = 3 : si32}> : (tensor<1x1x400x256xbf16, #ttnn_layout117>, tensor<1x1x400x512xbf16, #ttnn_layout120>) -> tensor<1x1x400x768xbf16, #ttnn_layout121> loc(#loc257)
        "ttnn.deallocate"(%335) <{force = false}> : (tensor<1x1x400x512xbf16, #ttnn_layout120>) -> () loc(#loc257)
        "ttnn.deallocate"(%332) <{force = false}> : (tensor<1x1x400x256xbf16, #ttnn_layout117>) -> () loc(#loc257)
        %337 = "ttnn.to_layout"(%336) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x400x768xbf16, #ttnn_layout121>) -> tensor<1x1x400x768xbf16, #ttnn_layout122> loc(#loc353)
        "ttnn.deallocate"(%336) <{force = false}> : (tensor<1x1x400x768xbf16, #ttnn_layout121>) -> () loc(#loc353)
        %338 = "ttnn.conv2d"(%337, %arg115, %101) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<enable_kernel_stride_folding = false, config_tensors_in_dram = true>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 768 : i32, input_height = 20 : i32, input_width = 20 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x400x768xbf16, #ttnn_layout122>, tensor<512x768x1x1xbf16, #ttnn_layout64>, !ttnn.device) -> tensor<1x1x400x512xbf16, #ttnn_layout120> loc(#loc258)
        "ttnn.deallocate"(%337) <{force = false}> : (tensor<1x1x400x768xbf16, #ttnn_layout122>) -> () loc(#loc258)
        "ttnn.deallocate"(%arg115) <{force = false}> : (tensor<512x768x1x1xbf16, #ttnn_layout64>) -> () loc(#loc258)
        %339 = "ttnn.reshape"(%338) <{shape = [1 : i32, 20 : i32, 20 : i32, 512 : i32]}> : (tensor<1x1x400x512xbf16, #ttnn_layout120>) -> tensor<1x20x20x512xbf16, #ttnn_layout119> loc(#loc354)
        "ttnn.deallocate"(%338) <{force = false}> : (tensor<1x1x400x512xbf16, #ttnn_layout120>) -> () loc(#loc354)
        %340 = "ttnn.permute"(%339) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x20x20x512xbf16, #ttnn_layout119>) -> tensor<1x512x20x20xbf16, #ttnn_layout36> loc(#loc258)
        "ttnn.deallocate"(%339) <{force = false}> : (tensor<1x20x20x512xbf16, #ttnn_layout119>) -> () loc(#loc258)
        %341 = "ttnn.batch_norm_inference"(%340, %53, %56, %78, %57) <{epsilon = 1.000000e-03 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x512x20x20xbf16, #ttnn_layout36>, tensor<1x512x1x1xbf16, #ttnn_layout36>, tensor<1x512x1x1xbf16, #ttnn_layout36>, tensor<1x512x1x1xbf16, #ttnn_layout36>, tensor<1x512x1x1xbf16, #ttnn_layout36>) -> tensor<1x512x20x20xbf16, #ttnn_layout36> loc(#loc32)
        "ttnn.deallocate"(%340) <{force = false}> : (tensor<1x512x20x20xbf16, #ttnn_layout36>) -> () loc(#loc32)
        "ttnn.deallocate"(%78) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout36>) -> () loc(#loc32)
        "ttnn.deallocate"(%57) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout36>) -> () loc(#loc32)
        "ttnn.deallocate"(%56) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout36>) -> () loc(#loc32)
        "ttnn.deallocate"(%53) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout36>) -> () loc(#loc32)
        %342 = "ttnn.silu"(%341) : (tensor<1x512x20x20xbf16, #ttnn_layout36>) -> tensor<1x512x20x20xbf16, #ttnn_layout36> loc(#loc259)
        "ttnn.deallocate"(%341) <{force = false}> : (tensor<1x512x20x20xbf16, #ttnn_layout36>) -> () loc(#loc259)
        %343 = "ttnn.permute"(%342) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x512x20x20xbf16, #ttnn_layout36>) -> tensor<1x20x20x512xbf16, #ttnn_layout119> loc(#loc259)
        "ttnn.deallocate"(%342) <{force = false}> : (tensor<1x512x20x20xbf16, #ttnn_layout36>) -> () loc(#loc259)
        %344 = "ttnn.reshape"(%343) <{shape = [1 : i32, 1 : i32, 400 : i32, 512 : i32]}> : (tensor<1x20x20x512xbf16, #ttnn_layout119>) -> tensor<1x1x400x512xbf16, #ttnn_layout120> loc(#loc259)
        "ttnn.deallocate"(%343) <{force = false}> : (tensor<1x20x20x512xbf16, #ttnn_layout119>) -> () loc(#loc259)
        %345 = "ttnn.slice_static"(%344) <{begins = [0 : i32, 0 : i32, 0 : i32, 256 : i32], ends = [1 : i32, 1 : i32, 400 : i32, 512 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x400x512xbf16, #ttnn_layout120>) -> tensor<1x1x400x256xbf16, #ttnn_layout117> loc(#loc260)
        %346 = "ttnn.to_layout"(%345) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x400x256xbf16, #ttnn_layout117>) -> tensor<1x1x400x256xbf16, #ttnn_layout123> loc(#loc355)
        %347 = "ttnn.conv2d"(%346, %arg110, %101) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<enable_kernel_stride_folding = false, config_tensors_in_dram = true>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 20 : i32, input_width = 20 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x400x256xbf16, #ttnn_layout123>, tensor<256x256x3x3xbf16, #ttnn_layout63>, !ttnn.device) -> tensor<1x1x400x256xbf16, #ttnn_layout117> loc(#loc261)
        "ttnn.deallocate"(%346) <{force = false}> : (tensor<1x1x400x256xbf16, #ttnn_layout123>) -> () loc(#loc261)
        "ttnn.deallocate"(%arg110) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout63>) -> () loc(#loc261)
        %348 = "ttnn.reshape"(%347) <{shape = [1 : i32, 20 : i32, 20 : i32, 256 : i32]}> : (tensor<1x1x400x256xbf16, #ttnn_layout117>) -> tensor<1x20x20x256xbf16, #ttnn_layout118> loc(#loc356)
        "ttnn.deallocate"(%347) <{force = false}> : (tensor<1x1x400x256xbf16, #ttnn_layout117>) -> () loc(#loc356)
        %349 = "ttnn.permute"(%348) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x20x20x256xbf16, #ttnn_layout118>) -> tensor<1x256x20x20xbf16, #ttnn_layout23> loc(#loc261)
        "ttnn.deallocate"(%348) <{force = false}> : (tensor<1x20x20x256xbf16, #ttnn_layout118>) -> () loc(#loc261)
        %350 = "ttnn.batch_norm_inference"(%349, %18, %54, %58, %30) <{epsilon = 1.000000e-03 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x20x20xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>) -> tensor<1x256x20x20xbf16, #ttnn_layout23> loc(#loc18)
        "ttnn.deallocate"(%349) <{force = false}> : (tensor<1x256x20x20xbf16, #ttnn_layout23>) -> () loc(#loc18)
        "ttnn.deallocate"(%58) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc18)
        "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc18)
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc18)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc18)
        %351 = "ttnn.silu"(%350) : (tensor<1x256x20x20xbf16, #ttnn_layout23>) -> tensor<1x256x20x20xbf16, #ttnn_layout23> loc(#loc262)
        "ttnn.deallocate"(%350) <{force = false}> : (tensor<1x256x20x20xbf16, #ttnn_layout23>) -> () loc(#loc262)
        %352 = "ttnn.permute"(%351) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x20x20xbf16, #ttnn_layout23>) -> tensor<1x20x20x256xbf16, #ttnn_layout118> loc(#loc262)
        "ttnn.deallocate"(%351) <{force = false}> : (tensor<1x256x20x20xbf16, #ttnn_layout23>) -> () loc(#loc262)
        %353 = "ttnn.reshape"(%352) <{shape = [1 : i32, 1 : i32, 400 : i32, 256 : i32]}> : (tensor<1x20x20x256xbf16, #ttnn_layout118>) -> tensor<1x1x400x256xbf16, #ttnn_layout117> loc(#loc262)
        "ttnn.deallocate"(%352) <{force = false}> : (tensor<1x20x20x256xbf16, #ttnn_layout118>) -> () loc(#loc262)
        %354 = "ttnn.to_layout"(%353) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x400x256xbf16, #ttnn_layout117>) -> tensor<1x1x400x256xbf16, #ttnn_layout123> loc(#loc357)
        "ttnn.deallocate"(%353) <{force = false}> : (tensor<1x1x400x256xbf16, #ttnn_layout117>) -> () loc(#loc357)
        %355 = "ttnn.conv2d"(%354, %arg105, %101) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<enable_kernel_stride_folding = false, config_tensors_in_dram = true>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 20 : i32, input_width = 20 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x400x256xbf16, #ttnn_layout123>, tensor<256x256x3x3xbf16, #ttnn_layout63>, !ttnn.device) -> tensor<1x1x400x256xbf16, #ttnn_layout117> loc(#loc263)
        "ttnn.deallocate"(%354) <{force = false}> : (tensor<1x1x400x256xbf16, #ttnn_layout123>) -> () loc(#loc263)
        "ttnn.deallocate"(%arg105) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout63>) -> () loc(#loc263)
        %356 = "ttnn.reshape"(%355) <{shape = [1 : i32, 20 : i32, 20 : i32, 256 : i32]}> : (tensor<1x1x400x256xbf16, #ttnn_layout117>) -> tensor<1x20x20x256xbf16, #ttnn_layout118> loc(#loc358)
        "ttnn.deallocate"(%355) <{force = false}> : (tensor<1x1x400x256xbf16, #ttnn_layout117>) -> () loc(#loc358)
        %357 = "ttnn.permute"(%356) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x20x20x256xbf16, #ttnn_layout118>) -> tensor<1x256x20x20xbf16, #ttnn_layout23> loc(#loc263)
        "ttnn.deallocate"(%356) <{force = false}> : (tensor<1x20x20x256xbf16, #ttnn_layout118>) -> () loc(#loc263)
        %358 = "ttnn.batch_norm_inference"(%357, %73, %10, %76, %5) <{epsilon = 1.000000e-03 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x20x20xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>) -> tensor<1x256x20x20xbf16, #ttnn_layout23> loc(#loc10)
        "ttnn.deallocate"(%357) <{force = false}> : (tensor<1x256x20x20xbf16, #ttnn_layout23>) -> () loc(#loc10)
        "ttnn.deallocate"(%76) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc10)
        "ttnn.deallocate"(%73) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc10)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc10)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc10)
        %359 = "ttnn.silu"(%358) : (tensor<1x256x20x20xbf16, #ttnn_layout23>) -> tensor<1x256x20x20xbf16, #ttnn_layout23> loc(#loc264)
        "ttnn.deallocate"(%358) <{force = false}> : (tensor<1x256x20x20xbf16, #ttnn_layout23>) -> () loc(#loc264)
        %360 = "ttnn.reshape"(%359) <{shape = [1 : i32, 8 : i32, 32 : i32, 20 : i32, 20 : i32]}> : (tensor<1x256x20x20xbf16, #ttnn_layout23>) -> tensor<1x8x32x20x20xbf16, #ttnn_layout124> loc(#loc265)
        %361 = "ttnn.matmul"(%142, %arg100) <{transpose_a = false, transpose_b = true}> : (tensor<3x512xbf16, #ttnn_layout86>, tensor<256x512xbf16, #ttnn_layout62>) -> tensor<3x256xbf16, #ttnn_layout125> loc(#loc359)
        "ttnn.deallocate"(%142) <{force = false}> : (tensor<3x512xbf16, #ttnn_layout86>) -> () loc(#loc359)
        "ttnn.deallocate"(%arg100) <{force = false}> : (tensor<256x512xbf16, #ttnn_layout62>) -> () loc(#loc359)
        %362 = "ttnn.add"(%361, %4) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<3x256xbf16, #ttnn_layout125>, tensor<1x3x256xbf16, #ttnn_layout20>) -> tensor<1x3x256xbf16, #ttnn_layout20> loc(#loc360)
        "ttnn.deallocate"(%361) <{force = false}> : (tensor<3x256xbf16, #ttnn_layout125>) -> () loc(#loc360)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x3x256xbf16, #ttnn_layout20>) -> () loc(#loc360)
        %363 = "ttnn.reshape"(%362) <{shape = [1 : i32, 3 : i32, 8 : i32, 32 : i32]}> : (tensor<1x3x256xbf16, #ttnn_layout20>) -> tensor<1x3x8x32xbf16, #ttnn_layout88> loc(#loc267)
        "ttnn.deallocate"(%362) <{force = false}> : (tensor<1x3x256xbf16, #ttnn_layout20>) -> () loc(#loc267)
        %364 = "ttnn.permute"(%360) <{permutation = array<i64: 0, 1, 3, 4, 2>}> : (tensor<1x8x32x20x20xbf16, #ttnn_layout124>) -> tensor<1x8x20x20x32xbf16, #ttnn_layout126> loc(#loc361)
        "ttnn.deallocate"(%360) <{force = false}> : (tensor<1x8x32x20x20xbf16, #ttnn_layout124>) -> () loc(#loc361)
        %365 = "ttnn.permute"(%363) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x3x8x32xbf16, #ttnn_layout88>) -> tensor<1x8x32x3xbf16, #ttnn_layout14> loc(#loc362)
        "ttnn.deallocate"(%363) <{force = false}> : (tensor<1x3x8x32xbf16, #ttnn_layout88>) -> () loc(#loc362)
        %366 = "ttnn.reshape"(%364) <{shape = [1 : i32, 8 : i32, 400 : i32, 32 : i32]}> : (tensor<1x8x20x20x32xbf16, #ttnn_layout126>) -> tensor<1x8x400x32xbf16, #ttnn_layout127> loc(#loc363)
        "ttnn.deallocate"(%364) <{force = false}> : (tensor<1x8x20x20x32xbf16, #ttnn_layout126>) -> () loc(#loc363)
        %367 = "ttnn.matmul"(%366, %365) <{transpose_a = false, transpose_b = false}> : (tensor<1x8x400x32xbf16, #ttnn_layout127>, tensor<1x8x32x3xbf16, #ttnn_layout14>) -> tensor<1x8x400x3xbf16, #ttnn_layout127> loc(#loc268)
        "ttnn.deallocate"(%366) <{force = false}> : (tensor<1x8x400x32xbf16, #ttnn_layout127>) -> () loc(#loc268)
        "ttnn.deallocate"(%365) <{force = false}> : (tensor<1x8x32x3xbf16, #ttnn_layout14>) -> () loc(#loc268)
        %368 = "ttnn.reshape"(%367) <{shape = [1 : i32, 8 : i32, 20 : i32, 20 : i32, 3 : i32]}> : (tensor<1x8x400x3xbf16, #ttnn_layout127>) -> tensor<1x8x20x20x3xbf16, #ttnn_layout126> loc(#loc364)
        "ttnn.deallocate"(%367) <{force = false}> : (tensor<1x8x400x3xbf16, #ttnn_layout127>) -> () loc(#loc364)
        %369 = "ttnn.slice_static"(%344) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 1 : i32, 400 : i32, 256 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x400x512xbf16, #ttnn_layout120>) -> tensor<1x1x400x256xbf16, #ttnn_layout117> loc(#loc269)
        "ttnn.deallocate"(%344) <{force = false}> : (tensor<1x1x400x512xbf16, #ttnn_layout120>) -> () loc(#loc269)
        %370 = "ttnn.permute"(%359) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x20x20xbf16, #ttnn_layout23>) -> tensor<1x20x20x256xbf16, #ttnn_layout118> loc(#loc365)
        %371 = "ttnn.reshape"(%370) <{shape = [1 : i32, 1 : i32, 400 : i32, 256 : i32]}> : (tensor<1x20x20x256xbf16, #ttnn_layout118>) -> tensor<1x1x400x256xbf16, #ttnn_layout117> loc(#loc378)
        "ttnn.deallocate"(%370) <{force = false}> : (tensor<1x20x20x256xbf16, #ttnn_layout118>) -> () loc(#loc378)
        %372 = "ttnn.to_layout"(%371) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x400x256xbf16, #ttnn_layout117>) -> tensor<1x1x400x256xbf16, #ttnn_layout123> loc(#loc366)
        "ttnn.deallocate"(%371) <{force = false}> : (tensor<1x1x400x256xbf16, #ttnn_layout117>) -> () loc(#loc366)
        %373 = "ttnn.conv2d"(%372, %arg125, %101) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<enable_kernel_stride_folding = false, config_tensors_in_dram = true>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 20 : i32, input_width = 20 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x400x256xbf16, #ttnn_layout123>, tensor<256x256x3x3xbf16, #ttnn_layout63>, !ttnn.device) -> tensor<1x1x400x256xbf16, #ttnn_layout117> loc(#loc270)
        "ttnn.deallocate"(%372) <{force = false}> : (tensor<1x1x400x256xbf16, #ttnn_layout123>) -> () loc(#loc270)
        "ttnn.deallocate"(%arg125) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout63>) -> () loc(#loc270)
        %374 = "ttnn.reshape"(%373) <{shape = [1 : i32, 20 : i32, 20 : i32, 256 : i32]}> : (tensor<1x1x400x256xbf16, #ttnn_layout117>) -> tensor<1x20x20x256xbf16, #ttnn_layout118> loc(#loc367)
        "ttnn.deallocate"(%373) <{force = false}> : (tensor<1x1x400x256xbf16, #ttnn_layout117>) -> () loc(#loc367)
        %375 = "ttnn.permute"(%374) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x20x20x256xbf16, #ttnn_layout118>) -> tensor<1x256x20x20xbf16, #ttnn_layout23> loc(#loc270)
        "ttnn.deallocate"(%374) <{force = false}> : (tensor<1x20x20x256xbf16, #ttnn_layout118>) -> () loc(#loc270)
        %376 = "ttnn.batch_norm_inference"(%375, %98, %90, %65, %70) <{epsilon = 1.000000e-03 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x20x20xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>, tensor<1x256x1x1xbf16, #ttnn_layout23>) -> tensor<1x256x20x20xbf16, #ttnn_layout23> loc(#loc33)
        "ttnn.deallocate"(%375) <{force = false}> : (tensor<1x256x20x20xbf16, #ttnn_layout23>) -> () loc(#loc33)
        "ttnn.deallocate"(%98) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc33)
        "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc33)
        "ttnn.deallocate"(%70) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc33)
        "ttnn.deallocate"(%65) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc33)
        %377 = "ttnn.max"(%368) <{dim_arg = [4 : i32], keep_dim = false}> : (tensor<1x8x20x20x3xbf16, #ttnn_layout126>) -> tensor<1x8x20x20xbf16, #ttnn_layout14> loc(#loc271)
        "ttnn.deallocate"(%368) <{force = false}> : (tensor<1x8x20x20x3xbf16, #ttnn_layout126>) -> () loc(#loc271)
        %378 = "ttnn.divide"(%377, %2) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x8x20x20xbf16, #ttnn_layout14>, tensor<1x8x20x20xbf16, #ttnn_layout14>) -> tensor<1x8x20x20xbf16, #ttnn_layout14> loc(#loc272)
        "ttnn.deallocate"(%377) <{force = false}> : (tensor<1x8x20x20xbf16, #ttnn_layout14>) -> () loc(#loc272)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x8x20x20xbf16, #ttnn_layout14>) -> () loc(#loc272)
        %379 = "ttnn.add"(%378, %42) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x8x20x20xbf16, #ttnn_layout14>, tensor<1x8x1x1xbf16, #ttnn_layout14>) -> tensor<1x8x20x20xbf16, #ttnn_layout14> loc(#loc273)
        "ttnn.deallocate"(%378) <{force = false}> : (tensor<1x8x20x20xbf16, #ttnn_layout14>) -> () loc(#loc273)
        "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x8x1x1xbf16, #ttnn_layout14>) -> () loc(#loc273)
        %380 = "ttnn.sigmoid"(%379) : (tensor<1x8x20x20xbf16, #ttnn_layout14>) -> tensor<1x8x20x20xbf16, #ttnn_layout14> loc(#loc274)
        "ttnn.deallocate"(%379) <{force = false}> : (tensor<1x8x20x20xbf16, #ttnn_layout14>) -> () loc(#loc274)
        %381 = "ttnn.reshape"(%380) <{shape = [1 : i32, 8 : i32, 1 : i32, 20 : i32, 20 : i32]}> : (tensor<1x8x20x20xbf16, #ttnn_layout14>) -> tensor<1x8x1x20x20xbf16, #ttnn_layout128> loc(#loc274)
        "ttnn.deallocate"(%380) <{force = false}> : (tensor<1x8x20x20xbf16, #ttnn_layout14>) -> () loc(#loc274)
        %382 = "ttnn.repeat"(%381) <{repeat_dims = #ttnn.shape<1x1x32x1x1>}> : (tensor<1x8x1x20x20xbf16, #ttnn_layout128>) -> tensor<1x8x32x20x20xbf16, #ttnn_layout124> loc(#loc275)
        "ttnn.deallocate"(%381) <{force = false}> : (tensor<1x8x1x20x20xbf16, #ttnn_layout128>) -> () loc(#loc275)
        %383 = "ttnn.reshape"(%382) <{shape = [1 : i32, 256 : i32, 20 : i32, 20 : i32]}> : (tensor<1x8x32x20x20xbf16, #ttnn_layout124>) -> tensor<1x256x20x20xbf16, #ttnn_layout23> loc(#loc368)
        "ttnn.deallocate"(%382) <{force = false}> : (tensor<1x8x32x20x20xbf16, #ttnn_layout124>) -> () loc(#loc368)
        %384 = "ttnn.multiply"(%376, %383) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x20x20xbf16, #ttnn_layout23>, tensor<1x256x20x20xbf16, #ttnn_layout23>) -> tensor<1x256x20x20xbf16, #ttnn_layout23> loc(#loc277)
        "ttnn.deallocate"(%383) <{force = false}> : (tensor<1x256x20x20xbf16, #ttnn_layout23>) -> () loc(#loc277)
        "ttnn.deallocate"(%376) <{force = false}> : (tensor<1x256x20x20xbf16, #ttnn_layout23>) -> () loc(#loc277)
        %385 = "ttnn.permute"(%384) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x20x20xbf16, #ttnn_layout23>) -> tensor<1x20x20x256xbf16, #ttnn_layout118> loc(#loc277)
        "ttnn.deallocate"(%384) <{force = false}> : (tensor<1x256x20x20xbf16, #ttnn_layout23>) -> () loc(#loc277)
        %386 = "ttnn.reshape"(%385) <{shape = [1 : i32, 1 : i32, 400 : i32, 256 : i32]}> : (tensor<1x20x20x256xbf16, #ttnn_layout118>) -> tensor<1x1x400x256xbf16, #ttnn_layout117> loc(#loc277)
        "ttnn.deallocate"(%385) <{force = false}> : (tensor<1x20x20x256xbf16, #ttnn_layout118>) -> () loc(#loc277)
        %387 = "ttnn.permute"(%359) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x20x20xbf16, #ttnn_layout23>) -> tensor<1x20x20x256xbf16, #ttnn_layout118> loc(#loc278)
        "ttnn.deallocate"(%359) <{force = false}> : (tensor<1x256x20x20xbf16, #ttnn_layout23>) -> () loc(#loc278)
        %388 = "ttnn.reshape"(%387) <{shape = [1 : i32, 1 : i32, 400 : i32, 256 : i32]}> : (tensor<1x20x20x256xbf16, #ttnn_layout118>) -> tensor<1x1x400x256xbf16, #ttnn_layout117> loc(#loc278)
        "ttnn.deallocate"(%387) <{force = false}> : (tensor<1x20x20x256xbf16, #ttnn_layout118>) -> () loc(#loc278)
        %389 = "ttnn.concat"(%369, %345, %388, %386) <{dim = 3 : si32}> : (tensor<1x1x400x256xbf16, #ttnn_layout117>, tensor<1x1x400x256xbf16, #ttnn_layout117>, tensor<1x1x400x256xbf16, #ttnn_layout117>, tensor<1x1x400x256xbf16, #ttnn_layout117>) -> tensor<1x1x400x1024xbf16, #ttnn_layout129> loc(#loc278)
        "ttnn.deallocate"(%388) <{force = false}> : (tensor<1x1x400x256xbf16, #ttnn_layout117>) -> () loc(#loc278)
        "ttnn.deallocate"(%386) <{force = false}> : (tensor<1x1x400x256xbf16, #ttnn_layout117>) -> () loc(#loc278)
        "ttnn.deallocate"(%369) <{force = false}> : (tensor<1x1x400x256xbf16, #ttnn_layout117>) -> () loc(#loc278)
        "ttnn.deallocate"(%345) <{force = false}> : (tensor<1x1x400x256xbf16, #ttnn_layout117>) -> () loc(#loc278)
        %390 = "ttnn.to_layout"(%389) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x400x1024xbf16, #ttnn_layout129>) -> tensor<1x1x400x1024xbf16, #ttnn_layout130> loc(#loc369)
        "ttnn.deallocate"(%389) <{force = false}> : (tensor<1x1x400x1024xbf16, #ttnn_layout129>) -> () loc(#loc369)
        %391 = "ttnn.conv2d"(%390, %arg97, %101) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<enable_kernel_stride_folding = false, config_tensors_in_dram = true>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 1024 : i32, input_height = 20 : i32, input_width = 20 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x400x1024xbf16, #ttnn_layout130>, tensor<512x1024x1x1xbf16, #ttnn_layout61>, !ttnn.device) -> tensor<1x1x400x512xbf16, #ttnn_layout120> loc(#loc279)
        "ttnn.deallocate"(%390) <{force = false}> : (tensor<1x1x400x1024xbf16, #ttnn_layout130>) -> () loc(#loc279)
        "ttnn.deallocate"(%arg97) <{force = false}> : (tensor<512x1024x1x1xbf16, #ttnn_layout61>) -> () loc(#loc279)
        %392 = "ttnn.reshape"(%391) <{shape = [1 : i32, 20 : i32, 20 : i32, 512 : i32]}> : (tensor<1x1x400x512xbf16, #ttnn_layout120>) -> tensor<1x20x20x512xbf16, #ttnn_layout119> loc(#loc370)
        "ttnn.deallocate"(%391) <{force = false}> : (tensor<1x1x400x512xbf16, #ttnn_layout120>) -> () loc(#loc370)
        %393 = "ttnn.permute"(%392) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x20x20x512xbf16, #ttnn_layout119>) -> tensor<1x512x20x20xbf16, #ttnn_layout36> loc(#loc279)
        "ttnn.deallocate"(%392) <{force = false}> : (tensor<1x20x20x512xbf16, #ttnn_layout119>) -> () loc(#loc279)
        %394 = "ttnn.batch_norm_inference"(%393, %100, %99, %47, %64) <{epsilon = 1.000000e-03 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x512x20x20xbf16, #ttnn_layout36>, tensor<1x512x1x1xbf16, #ttnn_layout36>, tensor<1x512x1x1xbf16, #ttnn_layout36>, tensor<1x512x1x1xbf16, #ttnn_layout36>, tensor<1x512x1x1xbf16, #ttnn_layout36>) -> tensor<1x512x20x20xbf16, #ttnn_layout36> loc(#loc31)
        "ttnn.deallocate"(%393) <{force = false}> : (tensor<1x512x20x20xbf16, #ttnn_layout36>) -> () loc(#loc31)
        "ttnn.deallocate"(%100) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout36>) -> () loc(#loc31)
        "ttnn.deallocate"(%99) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout36>) -> () loc(#loc31)
        "ttnn.deallocate"(%64) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout36>) -> () loc(#loc31)
        "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout36>) -> () loc(#loc31)
        %395 = "ttnn.silu"(%394) : (tensor<1x512x20x20xbf16, #ttnn_layout36>) -> tensor<1x512x20x20xbf16, #ttnn_layout36> loc(#loc280)
        "ttnn.deallocate"(%394) <{force = false}> : (tensor<1x512x20x20xbf16, #ttnn_layout36>) -> () loc(#loc280)
        return %250, %322, %395 : tensor<1x128x80x80xbf16, #ttnn_layout65>, tensor<1x256x40x40xbf16, #ttnn_layout66>, tensor<1x512x20x20xbf16, #ttnn_layout36> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc1 = loc("transpose.271_in_0_layout")
#loc2 = loc("floor.259")
#loc3 = loc("convert.260")
#loc4 = loc("broadcast.262")
#loc5 = loc("transpose.267")
#loc6 = loc("compare.265")
#loc7 = loc("batch-norm-inference.418")
#loc8 = loc("batch-norm-inference.296")
#loc9 = loc("broadcast.802")
#loc10 = loc("batch-norm-inference.904")
#loc11 = loc("batch-norm-inference.373")
#loc12 = loc("batch-norm-inference.661")
#loc13 = loc("batch-norm-inference.644")
#loc14 = loc("batch-norm-inference.386")
#loc15 = loc("batch-norm-inference.878")
#loc16 = loc("broadcast.47")
#loc17 = loc("batch-norm-inference.635")
#loc18 = loc("batch-norm-inference.896")
#loc19 = loc("batch-norm-inference.508")
#loc20 = loc("broadcast.348")
#loc21 = loc("batch-norm-inference.288")
#loc22 = loc("batch-norm-inference.738")
#loc23 = loc("broadcast.470")
#loc24 = loc("batch-norm-inference.751")
#loc25 = loc("batch-norm-inference.401")
#loc26 = loc("batch-norm-inference.653")
#loc27 = loc("batch-norm-inference.410")
#loc28 = loc("batch-norm-inference.279")
#loc29 = loc("batch-norm-inference.495")
#loc30 = loc("broadcast.956")
#loc31 = loc("batch-norm-inference.994")
#loc32 = loc("batch-norm-inference.887")
#loc33 = loc("batch-norm-inference.981")
#loc34 = loc("broadcast.559")
#loc35 = loc("broadcast.180")
#loc36 = loc("broadcast.713")
#loc37 = loc("floor.126")
#loc38 = loc("convert.127")
#loc39 = loc("broadcast.129")
#loc40 = loc("transpose.134")
#loc41 = loc("compare.132")
#loc168 = loc("transpose.271")
#loc169 = loc("dot.272")
#loc170 = loc("transpose.274")
#loc171 = loc("dot.275")
#loc172 = loc("concatenate.277")
#loc173 = loc("concatenate.277_in_0_layout")
#loc174 = loc("convolution.278")
#loc175 = loc("multiply.285")
#loc176 = loc("slice.286")
#loc177 = loc("convolution.287")
#loc178 = loc("multiply.294")
#loc179 = loc("convolution.295")
#loc180 = loc("multiply.302")
#loc181 = loc("reshape.303")
#loc182 = loc("reshape.175_in_0_layout")
#loc183 = loc("reshape.175")
#loc184 = loc("add.181")
#loc185 = loc("reshape.182")
#loc186 = loc("dot.304")
#loc187 = loc("slice.383")
#loc188 = loc("convolution.372")
#loc189 = loc("reduce.312")
#loc190 = loc("divide.344")
#loc191 = loc("add.349")
#loc192 = loc("logistic.350")
#loc193 = loc("broadcast.380")
#loc194 = loc("reshape.382")
#loc195 = loc("multiply.381")
#loc196 = loc("concatenate.384")
#loc197 = loc("convolution.385")
#loc198 = loc("multiply.392")
#loc199 = loc("transpose.393")
#loc200 = loc("dot.394")
#loc201 = loc("transpose.396")
#loc202 = loc("dot.397")
#loc203 = loc("concatenate.399")
#loc204 = loc("concatenate.399_in_0_layout")
#loc205 = loc("convolution.400")
#loc206 = loc("multiply.407")
#loc207 = loc("slice.408")
#loc208 = loc("convolution.409")
#loc209 = loc("multiply.416")
#loc210 = loc("convolution.417")
#loc211 = loc("multiply.424")
#loc212 = loc("reshape.425")
#loc213 = loc("add.48")
#loc214 = loc("reshape.49")
#loc215 = loc("dot.426")
#loc216 = loc("slice.505")
#loc217 = loc("convolution.494")
#loc218 = loc("reduce.434")
#loc219 = loc("divide.466")
#loc220 = loc("add.471")
#loc221 = loc("logistic.472")
#loc222 = loc("broadcast.502")
#loc223 = loc("reshape.504")
#loc224 = loc("multiply.503")
#loc225 = loc("concatenate.506")
#loc226 = loc("convolution.507")
#loc227 = loc("multiply.514")
#loc228 = loc("convolution.634")
#loc229 = loc("multiply.641")
#loc230 = loc("concatenate.642")
#loc231 = loc("convolution.643")
#loc232 = loc("multiply.650")
#loc233 = loc("slice.651")
#loc234 = loc("convolution.652")
#loc235 = loc("multiply.659")
#loc236 = loc("convolution.660")
#loc237 = loc("multiply.667")
#loc238 = loc("reshape.668")
#loc239 = loc("add.560")
#loc240 = loc("reshape.561")
#loc241 = loc("dot.669")
#loc242 = loc("slice.748")
#loc243 = loc("convolution.737")
#loc244 = loc("reduce.677")
#loc245 = loc("divide.709")
#loc246 = loc("add.714")
#loc247 = loc("logistic.715")
#loc248 = loc("broadcast.745")
#loc249 = loc("reshape.747")
#loc250 = loc("multiply.746")
#loc251 = loc("concatenate.749")
#loc252 = loc("convolution.750")
#loc253 = loc("multiply.757")
#loc254 = loc("convolution.877")
#loc255 = loc("multiply.884")
#loc256 = loc("concatenate.885_in_0_layout")
#loc257 = loc("concatenate.885")
#loc258 = loc("convolution.886")
#loc259 = loc("multiply.893")
#loc260 = loc("slice.894")
#loc261 = loc("convolution.895")
#loc262 = loc("multiply.902")
#loc263 = loc("convolution.903")
#loc264 = loc("multiply.910")
#loc265 = loc("reshape.911")
#loc266 = loc("add.803")
#loc267 = loc("reshape.804")
#loc268 = loc("dot.912")
#loc269 = loc("slice.991")
#loc270 = loc("convolution.980")
#loc271 = loc("reduce.920")
#loc272 = loc("divide.952")
#loc273 = loc("add.957")
#loc274 = loc("logistic.958")
#loc275 = loc("broadcast.988")
#loc276 = loc("reshape.990")
#loc277 = loc("multiply.989")
#loc278 = loc("concatenate.992")
#loc279 = loc("convolution.993")
#loc280 = loc("multiply.1000")
#loc281 = loc("convert.260_workaround"(#loc3))
#loc282 = loc("transpose.267_tm0"(#loc5))
#loc283 = loc("convert.127_workaround"(#loc38))
#loc284 = loc("transpose.134_tm0"(#loc40))
#loc285 = loc("dot.272_reshapeLhs"(#loc169))
#loc286 = loc("dot.272_reshapeOutput"(#loc169))
#loc287 = loc("dot.275_reshapeLhs"(#loc171))
#loc288 = loc("dot.275_reshapeOutput"(#loc171))
#loc289 = loc("convolution.278_workaround"(#loc174))
#loc290 = loc("convolution.278_reshape"(#loc174))
#loc291 = loc("convolution.287_workaround"(#loc177))
#loc292 = loc("convolution.287_reshape"(#loc177))
#loc293 = loc("convolution.295_workaround"(#loc179))
#loc294 = loc("convolution.295_reshape"(#loc179))
#loc295 = loc("add.181_decomp_matmul"(#loc184))
#loc296 = loc("add.181_decomp_add"(#loc184))
#loc297 = loc("dot.304_permuteLhs"(#loc186))
#loc298 = loc("dot.304_permuteRhs"(#loc186))
#loc299 = loc("dot.304_reshapeLhs"(#loc186))
#loc300 = loc("dot.304_reshapeOutput"(#loc186))
#loc301 = loc("convolution.372_input"(#loc188))
#loc302 = loc("convolution.372_workaround"(#loc188))
#loc303 = loc("convolution.372_reshape"(#loc188))
#loc304 = loc("reshape.382_tm1"(#loc194))
#loc305 = loc("convolution.385_workaround"(#loc197))
#loc306 = loc("convolution.385_reshape"(#loc197))
#loc307 = loc("dot.394_reshapeLhs"(#loc200))
#loc308 = loc("dot.394_reshapeOutput"(#loc200))
#loc309 = loc("dot.397_reshapeLhs"(#loc202))
#loc310 = loc("dot.397_reshapeOutput"(#loc202))
#loc311 = loc("convolution.400_workaround"(#loc205))
#loc312 = loc("convolution.400_reshape"(#loc205))
#loc313 = loc("convolution.409_workaround"(#loc208))
#loc314 = loc("convolution.409_reshape"(#loc208))
#loc315 = loc("convolution.417_workaround"(#loc210))
#loc316 = loc("convolution.417_reshape"(#loc210))
#loc317 = loc("add.48_decomp_matmul"(#loc213))
#loc318 = loc("add.48_decomp_add"(#loc213))
#loc319 = loc("dot.426_permuteLhs"(#loc215))
#loc320 = loc("dot.426_permuteRhs"(#loc215))
#loc321 = loc("dot.426_reshapeLhs"(#loc215))
#loc322 = loc("dot.426_reshapeOutput"(#loc215))
#loc323 = loc("convolution.494_input"(#loc217))
#loc324 = loc("convolution.494_workaround"(#loc217))
#loc325 = loc("convolution.494_reshape"(#loc217))
#loc326 = loc("reshape.504_tm1"(#loc223))
#loc327 = loc("convolution.507_workaround"(#loc226))
#loc328 = loc("convolution.507_reshape"(#loc226))
#loc329 = loc("convolution.634_input"(#loc228))
#loc330 = loc("convolution.634_workaround"(#loc228))
#loc331 = loc("convolution.634_reshape"(#loc228))
#loc332 = loc("convolution.643_workaround"(#loc231))
#loc333 = loc("convolution.643_reshape"(#loc231))
#loc334 = loc("convolution.652_workaround"(#loc234))
#loc335 = loc("convolution.652_reshape"(#loc234))
#loc336 = loc("convolution.660_workaround"(#loc236))
#loc337 = loc("convolution.660_reshape"(#loc236))
#loc338 = loc("add.560_decomp_matmul"(#loc239))
#loc339 = loc("add.560_decomp_add"(#loc239))
#loc340 = loc("dot.669_permuteLhs"(#loc241))
#loc341 = loc("dot.669_permuteRhs"(#loc241))
#loc342 = loc("dot.669_reshapeLhs"(#loc241))
#loc343 = loc("dot.669_reshapeOutput"(#loc241))
#loc344 = loc("convolution.737_input"(#loc243))
#loc345 = loc("convolution.737_workaround"(#loc243))
#loc346 = loc("convolution.737_reshape"(#loc243))
#loc347 = loc("reshape.747_tm1"(#loc249))
#loc348 = loc("convolution.750_workaround"(#loc252))
#loc349 = loc("convolution.750_reshape"(#loc252))
#loc350 = loc("convolution.877_input"(#loc254))
#loc351 = loc("convolution.877_workaround"(#loc254))
#loc352 = loc("convolution.877_reshape"(#loc254))
#loc353 = loc("convolution.886_workaround"(#loc258))
#loc354 = loc("convolution.886_reshape"(#loc258))
#loc355 = loc("convolution.895_workaround"(#loc261))
#loc356 = loc("convolution.895_reshape"(#loc261))
#loc357 = loc("convolution.903_workaround"(#loc263))
#loc358 = loc("convolution.903_reshape"(#loc263))
#loc359 = loc("add.803_decomp_matmul"(#loc266))
#loc360 = loc("add.803_decomp_add"(#loc266))
#loc361 = loc("dot.912_permuteLhs"(#loc268))
#loc362 = loc("dot.912_permuteRhs"(#loc268))
#loc363 = loc("dot.912_reshapeLhs"(#loc268))
#loc364 = loc("dot.912_reshapeOutput"(#loc268))
#loc365 = loc("convolution.980_input"(#loc270))
#loc366 = loc("convolution.980_workaround"(#loc270))
#loc367 = loc("convolution.980_reshape"(#loc270))
#loc368 = loc("reshape.990_tm1"(#loc276))
#loc369 = loc("convolution.993_workaround"(#loc279))
#loc370 = loc("convolution.993_reshape"(#loc279))
#loc371 = loc("transpose.267_tm0_tm1"(#loc282))
#loc372 = loc("transpose.134_tm0_tm1"(#loc284))
#loc373 = loc("convolution.372_input_reshape"(#loc301))
#loc374 = loc("convolution.494_input_reshape"(#loc323))
#loc375 = loc("convolution.634_input_reshape"(#loc329))
#loc376 = loc("convolution.737_input_reshape"(#loc344))
#loc377 = loc("convolution.877_input_reshape"(#loc350))
#loc378 = loc("convolution.980_input_reshape"(#loc365))
#loc379 = loc("transpose.267_tm0_tm1_workaround"(#loc371))
#loc380 = loc("transpose.134_tm0_tm1_workaround"(#loc372))
