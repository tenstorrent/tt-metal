#loc1 = loc("p0.1")
#loc2 = loc("p1.5")
#loc3 = loc("p2.9")
#loc4 = loc("p3.13")
#loc5 = loc("p4.17")
#loc6 = loc("p5.21")
#loc7 = loc("p6.31")
#loc8 = loc("p7.35")
#loc9 = loc("p8.40")
#loc10 = loc("p9.50")
#loc11 = loc("p10.54")
#loc12 = loc("p11.58")
#loc13 = loc("p12.62")
#loc14 = loc("p13.66")
#loc15 = loc("p14.68")
#loc16 = loc("p15.72")
#loc17 = loc("p16.76")
#loc18 = loc("p17.80")
#loc19 = loc("p18.84")
#loc20 = loc("p19.86")
#loc21 = loc("p20.90")
#loc22 = loc("p21.94")
#loc23 = loc("p22.98")
#loc24 = loc("p23.102")
#loc25 = loc("p24.104")
#loc26 = loc("p25.136")
#loc27 = loc("p26.140")
#loc28 = loc("p27.144")
#loc29 = loc("p28.148")
#loc30 = loc("p29.152")
#loc31 = loc("p30.156")
#loc32 = loc("p31.166")
#loc33 = loc("p32.170")
#loc34 = loc("p33.183")
#loc35 = loc("p34.187")
#loc36 = loc("p35.191")
#loc37 = loc("p36.195")
#loc38 = loc("p37.199")
#loc39 = loc("p38.201")
#loc40 = loc("p39.205")
#loc41 = loc("p40.209")
#loc42 = loc("p41.213")
#loc43 = loc("p42.217")
#loc44 = loc("p43.219")
#loc45 = loc("p44.223")
#loc46 = loc("p45.227")
#loc47 = loc("p46.231")
#loc48 = loc("p47.235")
#loc49 = loc("p48.237")
#loc50 = loc("p49.269")
#loc51 = loc("p50.354")
#loc52 = loc("p51.358")
#loc53 = loc("p52.362")
#loc54 = loc("p53.366")
#loc55 = loc("p54.370")
#loc56 = loc("p55.476")
#loc57 = loc("p56.480")
#loc58 = loc("p57.484")
#loc59 = loc("p58.488")
#loc60 = loc("p59.492")
#loc61 = loc("p60.515")
#loc62 = loc("p61.519")
#loc63 = loc("p62.523")
#loc64 = loc("p63.527")
#loc65 = loc("p64.531")
#loc66 = loc("p65.535")
#loc67 = loc("p66.545")
#loc68 = loc("p67.549")
#loc69 = loc("p68.562")
#loc70 = loc("p69.566")
#loc71 = loc("p70.570")
#loc72 = loc("p71.574")
#loc73 = loc("p72.578")
#loc74 = loc("p73.580")
#loc75 = loc("p74.584")
#loc76 = loc("p75.588")
#loc77 = loc("p76.592")
#loc78 = loc("p77.596")
#loc79 = loc("p78.598")
#loc80 = loc("p79.602")
#loc81 = loc("p80.606")
#loc82 = loc("p81.610")
#loc83 = loc("p82.614")
#loc84 = loc("p83.616")
#loc85 = loc("p84.620")
#loc86 = loc("p85.624")
#loc87 = loc("p86.628")
#loc88 = loc("p87.632")
#loc89 = loc("p88.719")
#loc90 = loc("p89.723")
#loc91 = loc("p90.727")
#loc92 = loc("p91.731")
#loc93 = loc("p92.735")
#loc94 = loc("p93.758")
#loc95 = loc("p94.762")
#loc96 = loc("p95.766")
#loc97 = loc("p96.770")
#loc98 = loc("p97.774")
#loc99 = loc("p98.778")
#loc100 = loc("p99.788")
#loc101 = loc("p100.792")
#loc102 = loc("p101.805")
#loc103 = loc("p102.809")
#loc104 = loc("p103.813")
#loc105 = loc("p104.817")
#loc106 = loc("p105.821")
#loc107 = loc("p106.823")
#loc108 = loc("p107.827")
#loc109 = loc("p108.831")
#loc110 = loc("p109.835")
#loc111 = loc("p110.839")
#loc112 = loc("p111.841")
#loc113 = loc("p112.845")
#loc114 = loc("p113.849")
#loc115 = loc("p114.853")
#loc116 = loc("p115.857")
#loc117 = loc("p116.859")
#loc118 = loc("p117.863")
#loc119 = loc("p118.867")
#loc120 = loc("p119.871")
#loc121 = loc("p120.875")
#loc122 = loc("p121.962")
#loc123 = loc("p122.966")
#loc124 = loc("p123.970")
#loc125 = loc("p124.974")
#loc126 = loc("p125.978")
module @SyncTensorsGraph.1002 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.1002 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
      func.func @main(%arg0: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_final_conv_batch_norm2d_running_var"} loc("p0.1"), %arg1: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_final_conv_batch_norm2d_running_mean"} loc("p1.5"), %arg2: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_final_conv_batch_norm2d_bias"} loc("p2.9"), %arg3: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_final_conv_batch_norm2d_weight"} loc("p3.13"), %arg4: tensor<128x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_final_conv_conv_weight"} loc("p4.17"), %arg5: tensor<2xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_attn_block_bias"} loc("p5.21"), %arg6: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_attn_block_guide_fc_bias"} loc("p6.31"), %arg7: tensor<64x512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_attn_block_guide_fc_weight"} loc("p7.35"), %arg8: tensor<1x3x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_3"} loc("p8.40"), %arg9: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv2_batch_norm2d_running_var"} loc("p9.50"), %arg10: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv2_batch_norm2d_running_mean"} loc("p10.54"), %arg11: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv2_batch_norm2d_bias"} loc("p11.58"), %arg12: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv2_batch_norm2d_weight"} loc("p12.62"), %arg13: tensor<64x64x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv2_conv_weight"} loc("p13.66"), %arg14: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv1_batch_norm2d_running_var"} loc("p14.68"), %arg15: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv1_batch_norm2d_running_mean"} loc("p15.72"), %arg16: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv1_batch_norm2d_bias"} loc("p16.76"), %arg17: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv1_batch_norm2d_weight"} loc("p17.80"), %arg18: tensor<64x64x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv1_conv_weight"} loc("p18.84"), %arg19: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_main_conv_batch_norm2d_running_var"} loc("p19.86"), %arg20: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_main_conv_batch_norm2d_running_mean"} loc("p20.90"), %arg21: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_main_conv_batch_norm2d_bias"} loc("p21.94"), %arg22: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_main_conv_batch_norm2d_weight"} loc("p22.98"), %arg23: tensor<128x384x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_main_conv_conv_weight"} loc("p23.102"), %arg24: tensor<1x128x80x80xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("p24.104"), %arg25: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_final_conv_batch_norm2d_running_var"} loc("p25.136"), %arg26: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_final_conv_batch_norm2d_running_mean"} loc("p26.140"), %arg27: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_final_conv_batch_norm2d_bias"} loc("p27.144"), %arg28: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_final_conv_batch_norm2d_weight"} loc("p28.148"), %arg29: tensor<256x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_final_conv_conv_weight"} loc("p29.152"), %arg30: tensor<4xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_attn_block_bias"} loc("p30.156"), %arg31: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_attn_block_guide_fc_bias"} loc("p31.166"), %arg32: tensor<128x512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_attn_block_guide_fc_weight"} loc("p32.170"), %arg33: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv2_batch_norm2d_running_var"} loc("p33.183"), %arg34: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv2_batch_norm2d_running_mean"} loc("p34.187"), %arg35: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv2_batch_norm2d_bias"} loc("p35.191"), %arg36: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv2_batch_norm2d_weight"} loc("p36.195"), %arg37: tensor<128x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv2_conv_weight"} loc("p37.199"), %arg38: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv1_batch_norm2d_running_var"} loc("p38.201"), %arg39: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv1_batch_norm2d_running_mean"} loc("p39.205"), %arg40: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv1_batch_norm2d_bias"} loc("p40.209"), %arg41: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv1_batch_norm2d_weight"} loc("p41.213"), %arg42: tensor<128x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv1_conv_weight"} loc("p42.217"), %arg43: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_main_conv_batch_norm2d_running_var"} loc("p43.219"), %arg44: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_main_conv_batch_norm2d_running_mean"} loc("p44.223"), %arg45: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_main_conv_batch_norm2d_bias"} loc("p45.227"), %arg46: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_main_conv_batch_norm2d_weight"} loc("p46.231"), %arg47: tensor<256x768x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_main_conv_conv_weight"} loc("p47.235"), %arg48: tensor<1x256x40x40xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_1"} loc("p48.237"), %arg49: tensor<1x512x20x20xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_2"} loc("p49.269"), %arg50: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_attn_block_project_conv_batch_norm2d_running_var"} loc("p50.354"), %arg51: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_attn_block_project_conv_batch_norm2d_running_mean"} loc("p51.358"), %arg52: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_attn_block_project_conv_batch_norm2d_bias"} loc("p52.362"), %arg53: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_attn_block_project_conv_batch_norm2d_weight"} loc("p53.366"), %arg54: tensor<128x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_0_attn_block_project_conv_conv_weight"} loc("p54.370"), %arg55: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_attn_block_project_conv_batch_norm2d_running_var"} loc("p55.476"), %arg56: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_attn_block_project_conv_batch_norm2d_running_mean"} loc("p56.480"), %arg57: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_attn_block_project_conv_batch_norm2d_bias"} loc("p57.484"), %arg58: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_attn_block_project_conv_batch_norm2d_weight"} loc("p58.488"), %arg59: tensor<64x64x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_top_down_layers_1_attn_block_project_conv_conv_weight"} loc("p59.492"), %arg60: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_final_conv_batch_norm2d_running_var"} loc("p60.515"), %arg61: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_final_conv_batch_norm2d_running_mean"} loc("p61.519"), %arg62: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_final_conv_batch_norm2d_bias"} loc("p62.523"), %arg63: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_final_conv_batch_norm2d_weight"} loc("p63.527"), %arg64: tensor<256x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_final_conv_conv_weight"} loc("p64.531"), %arg65: tensor<4xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_attn_block_bias"} loc("p65.535"), %arg66: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_attn_block_guide_fc_bias"} loc("p66.545"), %arg67: tensor<128x512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_attn_block_guide_fc_weight"} loc("p67.549"), %arg68: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv2_batch_norm2d_running_var"} loc("p68.562"), %arg69: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv2_batch_norm2d_running_mean"} loc("p69.566"), %arg70: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv2_batch_norm2d_bias"} loc("p70.570"), %arg71: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv2_batch_norm2d_weight"} loc("p71.574"), %arg72: tensor<128x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv2_conv_weight"} loc("p72.578"), %arg73: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv1_batch_norm2d_running_var"} loc("p73.580"), %arg74: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv1_batch_norm2d_running_mean"} loc("p74.584"), %arg75: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv1_batch_norm2d_bias"} loc("p75.588"), %arg76: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv1_batch_norm2d_weight"} loc("p76.592"), %arg77: tensor<128x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv1_conv_weight"} loc("p77.596"), %arg78: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_main_conv_batch_norm2d_running_var"} loc("p78.598"), %arg79: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_main_conv_batch_norm2d_running_mean"} loc("p79.602"), %arg80: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_main_conv_batch_norm2d_bias"} loc("p80.606"), %arg81: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_main_conv_batch_norm2d_weight"} loc("p81.610"), %arg82: tensor<256x384x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_main_conv_conv_weight"} loc("p82.614"), %arg83: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_downsample_layers_0_batch_norm2d_running_var"} loc("p83.616"), %arg84: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_downsample_layers_0_batch_norm2d_running_mean"} loc("p84.620"), %arg85: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_downsample_layers_0_batch_norm2d_bias"} loc("p85.624"), %arg86: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_downsample_layers_0_batch_norm2d_weight"} loc("p86.628"), %arg87: tensor<128x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_downsample_layers_0_conv_weight"} loc("p87.632"), %arg88: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_attn_block_project_conv_batch_norm2d_running_var"} loc("p88.719"), %arg89: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_attn_block_project_conv_batch_norm2d_running_mean"} loc("p89.723"), %arg90: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_attn_block_project_conv_batch_norm2d_bias"} loc("p90.727"), %arg91: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_attn_block_project_conv_batch_norm2d_weight"} loc("p91.731"), %arg92: tensor<128x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_0_attn_block_project_conv_conv_weight"} loc("p92.735"), %arg93: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_final_conv_batch_norm2d_running_var"} loc("p93.758"), %arg94: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_final_conv_batch_norm2d_running_mean"} loc("p94.762"), %arg95: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_final_conv_batch_norm2d_bias"} loc("p95.766"), %arg96: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_final_conv_batch_norm2d_weight"} loc("p96.770"), %arg97: tensor<512x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_final_conv_conv_weight"} loc("p97.774"), %arg98: tensor<8xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_attn_block_bias"} loc("p98.778"), %arg99: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_attn_block_guide_fc_bias"} loc("p99.788"), %arg100: tensor<256x512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_attn_block_guide_fc_weight"} loc("p100.792"), %arg101: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv2_batch_norm2d_running_var"} loc("p101.805"), %arg102: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv2_batch_norm2d_running_mean"} loc("p102.809"), %arg103: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv2_batch_norm2d_bias"} loc("p103.813"), %arg104: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv2_batch_norm2d_weight"} loc("p104.817"), %arg105: tensor<256x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv2_conv_weight"} loc("p105.821"), %arg106: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv1_batch_norm2d_running_var"} loc("p106.823"), %arg107: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv1_batch_norm2d_running_mean"} loc("p107.827"), %arg108: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv1_batch_norm2d_bias"} loc("p108.831"), %arg109: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv1_batch_norm2d_weight"} loc("p109.835"), %arg110: tensor<256x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv1_conv_weight"} loc("p110.839"), %arg111: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_main_conv_batch_norm2d_running_var"} loc("p111.841"), %arg112: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_main_conv_batch_norm2d_running_mean"} loc("p112.845"), %arg113: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_main_conv_batch_norm2d_bias"} loc("p113.849"), %arg114: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_main_conv_batch_norm2d_weight"} loc("p114.853"), %arg115: tensor<512x768x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_main_conv_conv_weight"} loc("p115.857"), %arg116: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_downsample_layers_1_batch_norm2d_running_var"} loc("p116.859"), %arg117: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_downsample_layers_1_batch_norm2d_running_mean"} loc("p117.863"), %arg118: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_downsample_layers_1_batch_norm2d_bias"} loc("p118.867"), %arg119: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_downsample_layers_1_batch_norm2d_weight"} loc("p119.871"), %arg120: tensor<256x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_downsample_layers_1_conv_weight"} loc("p120.875"), %arg121: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_attn_block_project_conv_batch_norm2d_running_var"} loc("p121.962"), %arg122: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_attn_block_project_conv_batch_norm2d_running_mean"} loc("p122.966"), %arg123: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_attn_block_project_conv_batch_norm2d_bias"} loc("p123.970"), %arg124: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_attn_block_project_conv_batch_norm2d_weight"} loc("p124.974"), %arg125: tensor<256x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___neck_bottom_up_layers_1_attn_block_project_conv_conv_weight"} loc("p125.978")) -> (tensor<1x128x80x80xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x256x40x40xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x512x20x20xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<5.656250e+00> : tensor<1x8x20x20xbf16>}> : () -> tensor<1x8x20x20xbf16> loc(#loc)
        %1 = "ttir.constant"() <{value = dense<5.656250e+00> : tensor<1x2x80x80xbf16>}> : () -> tensor<1x2x80x80xbf16> loc(#loc)
        %2 = "ttir.constant"() <{value = dense<[0.000000e+00, 5.000000e-01, 1.000000e+00, 1.500000e+00, 2.000000e+00, 2.500000e+00, 3.000000e+00, 3.500000e+00, 4.000000e+00, 4.500000e+00, 5.000000e+00, 5.500000e+00, 6.000000e+00, 6.500000e+00, 7.000000e+00, 7.500000e+00, 8.000000e+00, 8.500000e+00, 9.000000e+00, 9.500000e+00, 1.000000e+01, 1.050000e+01, 1.100000e+01, 1.150000e+01, 1.200000e+01, 1.250000e+01, 1.300000e+01, 1.350000e+01, 1.400000e+01, 1.450000e+01, 1.500000e+01, 1.550000e+01, 1.600000e+01, 1.650000e+01, 1.700000e+01, 1.750000e+01, 1.800000e+01, 1.850000e+01, 1.900000e+01, 1.950000e+01, 2.000000e+01, 2.050000e+01, 2.100000e+01, 2.150000e+01, 2.200000e+01, 2.250000e+01, 2.300000e+01, 2.350000e+01, 2.400000e+01, 2.450000e+01, 2.500000e+01, 2.550000e+01, 2.600000e+01, 2.650000e+01, 2.700000e+01, 2.750000e+01, 2.800000e+01, 2.850000e+01, 2.900000e+01, 2.950000e+01, 3.000000e+01, 3.050000e+01, 3.100000e+01, 3.150000e+01, 3.200000e+01, 3.250000e+01, 3.300000e+01, 3.350000e+01, 3.400000e+01, 3.450000e+01, 3.500000e+01, 3.550000e+01, 3.600000e+01, 3.650000e+01, 3.700000e+01, 3.750000e+01, 3.800000e+01, 3.850000e+01, 3.900000e+01, 3.950000e+01]> : tensor<80xf64>}> : () -> tensor<80xf64> loc(#loc)
        %3 = "ttir.constant"() <{value = dense<5.656250e+00> : tensor<1x4x40x40xbf16>}> : () -> tensor<1x4x40x40xbf16> loc(#loc)
        %4 = "ttir.constant"() <{value = dense<[0.000000e+00, 5.000000e-01, 1.000000e+00, 1.500000e+00, 2.000000e+00, 2.500000e+00, 3.000000e+00, 3.500000e+00, 4.000000e+00, 4.500000e+00, 5.000000e+00, 5.500000e+00, 6.000000e+00, 6.500000e+00, 7.000000e+00, 7.500000e+00, 8.000000e+00, 8.500000e+00, 9.000000e+00, 9.500000e+00, 1.000000e+01, 1.050000e+01, 1.100000e+01, 1.150000e+01, 1.200000e+01, 1.250000e+01, 1.300000e+01, 1.350000e+01, 1.400000e+01, 1.450000e+01, 1.500000e+01, 1.550000e+01, 1.600000e+01, 1.650000e+01, 1.700000e+01, 1.750000e+01, 1.800000e+01, 1.850000e+01, 1.900000e+01, 1.950000e+01]> : tensor<40xf64>}> : () -> tensor<40xf64> loc(#loc)
        %5 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]> : tensor<40xi64>}> : () -> tensor<40xi64> loc(#loc)
        %6 = "ttir.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]> : tensor<20xi64>}> : () -> tensor<20xi64> loc(#loc)
        %7 = "ttir.reshape"(%arg43) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc127)
        %8 = "ttir.reshape"(%7) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc128)
        %9 = "ttir.reshape"(%arg38) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc129)
        %10 = "ttir.reshape"(%9) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc130)
        %11 = "ttir.reshape"(%arg33) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc131)
        %12 = "ttir.reshape"(%11) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc132)
        %13 = "ttir.permute"(%arg49) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x512x20x20xbf16>) -> tensor<1x512x20x20xbf16> loc(#loc133)
        %14 = "ttir.floor"(%4) : (tensor<40xf64>) -> tensor<40xf64> loc(#loc134)
        %15 = "ttir.typecast"(%14) <{conservative_folding = false}> : (tensor<40xf64>) -> tensor<40xi64> loc(#loc135)
        %16 = "ttir.reshape"(%15) <{shape = [40 : i32, 1 : i32]}> : (tensor<40xi64>) -> tensor<40x1xi64> loc(#loc136)
        %17 = "ttir.broadcast"(%16) <{broadcast_dimensions = array<i64: 1, 20>}> : (tensor<40x1xi64>) -> tensor<40x20xi64> loc(#loc136)
        %18 = "ttir.reshape"(%6) <{shape = [1 : i32, 20 : i32]}> : (tensor<20xi64>) -> tensor<1x20xi64> loc(#loc137)
        %19 = "ttir.broadcast"(%18) <{broadcast_dimensions = array<i64: 40, 1>}> : (tensor<1x20xi64>) -> tensor<40x20xi64> loc(#loc137)
        %20 = "ttir.eq"(%17, %19) : (tensor<40x20xi64>, tensor<40x20xi64>) -> tensor<40x20xi1> loc(#loc138)
        %21 = "ttir.typecast"(%20) <{conservative_folding = false}> : (tensor<40x20xi1>) -> tensor<40x20xi64> loc(#loc139)
        %22 = "ttir.permute"(%21) <{permutation = array<i64: 1, 0>}> : (tensor<40x20xi64>) -> tensor<20x40xi64> loc(#loc140)
        %23 = "ttir.typecast"(%22) <{conservative_folding = false}> : (tensor<20x40xi64>) -> tensor<20x40xbf16> loc(#loc141)
        %24 = "ttir.dot_general"(%13, %23) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x512x20x20xbf16>, tensor<20x40xbf16>) -> tensor<1x512x20x40xbf16> loc(#loc142)
        %25 = "ttir.permute"(%24) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x512x20x40xbf16>) -> tensor<1x512x40x20xbf16> loc(#loc143)
        %26 = "ttir.dot_general"(%25, %23) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x512x40x20xbf16>, tensor<20x40xbf16>) -> tensor<1x512x40x40xbf16> loc(#loc144)
        %27 = "ttir.concat"(%26, %arg48) <{dim = 1 : si32}> : (tensor<1x512x40x40xbf16>, tensor<1x256x40x40xbf16>) -> tensor<1x768x40x40xbf16> loc(#loc145)
        %28 = "ttir.convolution"(%27, %arg47) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x768x40x40xbf16>, tensor<256x768x1x1xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc146)
        %29 = "ttir.reshape"(%arg46) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc147)
        %30 = "ttir.reshape"(%29) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc148)
        %31 = "ttir.reshape"(%arg45) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc149)
        %32 = "ttir.reshape"(%31) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc150)
        %33 = "ttir.reshape"(%arg44) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc151)
        %34 = "ttir.reshape"(%33) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc152)
        %35 = "ttir.batch_norm_inference"(%28, %30, %32, %34, %8) <{dimension = 1 : i32, epsilon = 1.000000e-03 : f32}> : (tensor<1x256x40x40xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc153)
        %36 = "ttir.sigmoid"(%35) : (tensor<1x256x40x40xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc154)
        %37 = "ttir.multiply"(%35, %36) : (tensor<1x256x40x40xbf16>, tensor<1x256x40x40xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc155)
        %38 = "ttir.slice_static"(%37) <{begins = [0 : i32, 128 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 256 : i32, 40 : i32, 40 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x256x40x40xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc156)
        %39 = "ttir.convolution"(%38, %arg42) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x128x40x40xbf16>, tensor<128x128x3x3xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc157)
        %40 = "ttir.reshape"(%arg41) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc158)
        %41 = "ttir.reshape"(%40) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc159)
        %42 = "ttir.reshape"(%arg40) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc160)
        %43 = "ttir.reshape"(%42) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc161)
        %44 = "ttir.reshape"(%arg39) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc162)
        %45 = "ttir.reshape"(%44) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc163)
        %46 = "ttir.batch_norm_inference"(%39, %41, %43, %45, %10) <{dimension = 1 : i32, epsilon = 1.000000e-03 : f32}> : (tensor<1x128x40x40xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc164)
        %47 = "ttir.sigmoid"(%46) : (tensor<1x128x40x40xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc165)
        %48 = "ttir.multiply"(%46, %47) : (tensor<1x128x40x40xbf16>, tensor<1x128x40x40xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc166)
        %49 = "ttir.convolution"(%48, %arg37) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x128x40x40xbf16>, tensor<128x128x3x3xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc167)
        %50 = "ttir.reshape"(%arg36) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc168)
        %51 = "ttir.reshape"(%50) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc169)
        %52 = "ttir.reshape"(%arg35) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc170)
        %53 = "ttir.reshape"(%52) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc171)
        %54 = "ttir.reshape"(%arg34) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc172)
        %55 = "ttir.reshape"(%54) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc173)
        %56 = "ttir.batch_norm_inference"(%49, %51, %53, %55, %12) <{dimension = 1 : i32, epsilon = 1.000000e-03 : f32}> : (tensor<1x128x40x40xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc174)
        %57 = "ttir.sigmoid"(%56) : (tensor<1x128x40x40xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc175)
        %58 = "ttir.multiply"(%56, %57) : (tensor<1x128x40x40xbf16>, tensor<1x128x40x40xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc176)
        %59 = "ttir.reshape"(%58) <{shape = [1 : i32, 4 : i32, 32 : i32, 40 : i32, 40 : i32]}> : (tensor<1x128x40x40xbf16>) -> tensor<1x4x32x40x40xbf16> loc(#loc177)
        %60 = "ttir.reshape"(%arg8) <{shape = [3 : i32, 512 : i32]}> : (tensor<1x3x512xbf16>) -> tensor<3x512xbf16> loc(#loc178)
        %61 = "ttir.reshape"(%arg32) <{shape = [1 : i32, 128 : i32, 512 : i32]}> : (tensor<128x512xbf16>) -> tensor<1x128x512xbf16> loc(#loc179)
        %62 = "ttir.reshape"(%61) <{shape = [128 : i32, 512 : i32]}> : (tensor<1x128x512xbf16>) -> tensor<128x512xbf16> loc(#loc180)
        %63 = "ttir.permute"(%62) <{permutation = array<i64: 1, 0>}> : (tensor<128x512xbf16>) -> tensor<512x128xbf16> loc(#loc181)
        %64 = "ttir.dot_general"(%60, %63) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<3x512xbf16>, tensor<512x128xbf16>) -> tensor<3x128xbf16> loc(#loc182)
        %65 = "ttir.reshape"(%64) <{shape = [1 : i32, 3 : i32, 128 : i32]}> : (tensor<3x128xbf16>) -> tensor<1x3x128xbf16> loc(#loc183)
        %66 = "ttir.reshape"(%arg31) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc184)
        %67 = "ttir.reshape"(%66) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc185)
        %68 = "ttir.reshape"(%67) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc186)
        %69 = "ttir.broadcast"(%68) <{broadcast_dimensions = array<i64: 1, 3, 1>}> : (tensor<1x1x128xbf16>) -> tensor<1x3x128xbf16> loc(#loc186)
        %70 = "ttir.add"(%65, %69) : (tensor<1x3x128xbf16>, tensor<1x3x128xbf16>) -> tensor<1x3x128xbf16> loc(#loc187)
        %71 = "ttir.reshape"(%70) <{shape = [1 : i32, 3 : i32, 4 : i32, 32 : i32]}> : (tensor<1x3x128xbf16>) -> tensor<1x3x4x32xbf16> loc(#loc188)
        %72 = "ttir.dot_general"(%59, %71) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 2>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 3>}> : (tensor<1x4x32x40x40xbf16>, tensor<1x3x4x32xbf16>) -> tensor<1x4x40x40x3xbf16> loc(#loc189)
        %73 = "ttir.reshape"(%arg50) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc190)
        %74 = "ttir.reshape"(%73) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc191)
        %75 = "ttir.reshape"(%arg25) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc192)
        %76 = "ttir.reshape"(%75) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc193)
        %77 = "ttir.reshape"(%arg19) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc194)
        %78 = "ttir.reshape"(%77) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc195)
        %79 = "ttir.reshape"(%arg14) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc196)
        %80 = "ttir.reshape"(%79) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc197)
        %81 = "ttir.reshape"(%arg9) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc198)
        %82 = "ttir.reshape"(%81) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc199)
        %83 = "ttir.slice_static"(%37) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 128 : i32, 40 : i32, 40 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x256x40x40xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc200)
        %84 = "ttir.convolution"(%58, %arg54) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x128x40x40xbf16>, tensor<128x128x3x3xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc201)
        %85 = "ttir.reshape"(%arg53) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc202)
        %86 = "ttir.reshape"(%85) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc203)
        %87 = "ttir.reshape"(%arg52) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc204)
        %88 = "ttir.reshape"(%87) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc205)
        %89 = "ttir.reshape"(%arg51) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc206)
        %90 = "ttir.reshape"(%89) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc207)
        %91 = "ttir.batch_norm_inference"(%84, %86, %88, %90, %74) <{dimension = 1 : i32, epsilon = 1.000000e-03 : f32}> : (tensor<1x128x40x40xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc208)
        %92 = "ttir.reshape"(%91) <{shape = [1 : i32, 4 : i32, 32 : i32, 40 : i32, 40 : i32]}> : (tensor<1x128x40x40xbf16>) -> tensor<1x4x32x40x40xbf16> loc(#loc209)
        %93 = "ttir.max"(%72) <{dim_arg = [4 : i32], keep_dim = false}> : (tensor<1x4x40x40x3xbf16>) -> tensor<1x4x40x40xbf16> loc(#loc210)
        %94 = "ttir.div"(%93, %3) : (tensor<1x4x40x40xbf16>, tensor<1x4x40x40xbf16>) -> tensor<1x4x40x40xbf16> loc(#loc211)
        %95 = "ttir.reshape"(%arg30) <{shape = [1 : i32, 1 : i32, 4 : i32]}> : (tensor<4xbf16>) -> tensor<1x1x4xbf16> loc(#loc212)
        %96 = "ttir.reshape"(%95) <{shape = [1 : i32, 4 : i32]}> : (tensor<1x1x4xbf16>) -> tensor<1x4xbf16> loc(#loc213)
        %97 = "ttir.reshape"(%96) <{shape = [1 : i32, 4 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4xbf16>) -> tensor<1x4x1x1xbf16> loc(#loc214)
        %98 = "ttir.broadcast"(%97) <{broadcast_dimensions = array<i64: 1, 1, 40, 40>}> : (tensor<1x4x1x1xbf16>) -> tensor<1x4x40x40xbf16> loc(#loc214)
        %99 = "ttir.add"(%94, %98) : (tensor<1x4x40x40xbf16>, tensor<1x4x40x40xbf16>) -> tensor<1x4x40x40xbf16> loc(#loc215)
        %100 = "ttir.sigmoid"(%99) : (tensor<1x4x40x40xbf16>) -> tensor<1x4x40x40xbf16> loc(#loc216)
        %101 = "ttir.reshape"(%100) <{shape = [1 : i32, 4 : i32, 1 : i32, 40 : i32, 40 : i32]}> : (tensor<1x4x40x40xbf16>) -> tensor<1x4x1x40x40xbf16> loc(#loc217)
        %102 = "ttir.broadcast"(%101) <{broadcast_dimensions = array<i64: 1, 1, 32, 1, 1>}> : (tensor<1x4x1x40x40xbf16>) -> tensor<1x4x32x40x40xbf16> loc(#loc217)
        %103 = "ttir.multiply"(%92, %102) : (tensor<1x4x32x40x40xbf16>, tensor<1x4x32x40x40xbf16>) -> tensor<1x4x32x40x40xbf16> loc(#loc218)
        %104 = "ttir.reshape"(%103) <{shape = [1 : i32, 128 : i32, 40 : i32, 40 : i32]}> : (tensor<1x4x32x40x40xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc219)
        %105 = "ttir.concat"(%83, %38, %58, %104) <{dim = 1 : si32}> : (tensor<1x128x40x40xbf16>, tensor<1x128x40x40xbf16>, tensor<1x128x40x40xbf16>, tensor<1x128x40x40xbf16>) -> tensor<1x512x40x40xbf16> loc(#loc220)
        %106 = "ttir.convolution"(%105, %arg29) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x512x40x40xbf16>, tensor<256x512x1x1xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc221)
        %107 = "ttir.reshape"(%arg28) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc222)
        %108 = "ttir.reshape"(%107) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc223)
        %109 = "ttir.reshape"(%arg27) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc224)
        %110 = "ttir.reshape"(%109) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc225)
        %111 = "ttir.reshape"(%arg26) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc226)
        %112 = "ttir.reshape"(%111) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc227)
        %113 = "ttir.batch_norm_inference"(%106, %108, %110, %112, %76) <{dimension = 1 : i32, epsilon = 1.000000e-03 : f32}> : (tensor<1x256x40x40xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc228)
        %114 = "ttir.sigmoid"(%113) : (tensor<1x256x40x40xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc229)
        %115 = "ttir.multiply"(%113, %114) : (tensor<1x256x40x40xbf16>, tensor<1x256x40x40xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc230)
        %116 = "ttir.permute"(%115) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x256x40x40xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc231)
        %117 = "ttir.floor"(%2) : (tensor<80xf64>) -> tensor<80xf64> loc(#loc232)
        %118 = "ttir.typecast"(%117) <{conservative_folding = false}> : (tensor<80xf64>) -> tensor<80xi64> loc(#loc233)
        %119 = "ttir.reshape"(%118) <{shape = [80 : i32, 1 : i32]}> : (tensor<80xi64>) -> tensor<80x1xi64> loc(#loc234)
        %120 = "ttir.broadcast"(%119) <{broadcast_dimensions = array<i64: 1, 40>}> : (tensor<80x1xi64>) -> tensor<80x40xi64> loc(#loc234)
        %121 = "ttir.reshape"(%5) <{shape = [1 : i32, 40 : i32]}> : (tensor<40xi64>) -> tensor<1x40xi64> loc(#loc235)
        %122 = "ttir.broadcast"(%121) <{broadcast_dimensions = array<i64: 80, 1>}> : (tensor<1x40xi64>) -> tensor<80x40xi64> loc(#loc235)
        %123 = "ttir.eq"(%120, %122) : (tensor<80x40xi64>, tensor<80x40xi64>) -> tensor<80x40xi1> loc(#loc236)
        %124 = "ttir.typecast"(%123) <{conservative_folding = false}> : (tensor<80x40xi1>) -> tensor<80x40xi64> loc(#loc237)
        %125 = "ttir.permute"(%124) <{permutation = array<i64: 1, 0>}> : (tensor<80x40xi64>) -> tensor<40x80xi64> loc(#loc238)
        %126 = "ttir.typecast"(%125) <{conservative_folding = false}> : (tensor<40x80xi64>) -> tensor<40x80xbf16> loc(#loc239)
        %127 = "ttir.dot_general"(%116, %126) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x256x40x40xbf16>, tensor<40x80xbf16>) -> tensor<1x256x40x80xbf16> loc(#loc240)
        %128 = "ttir.permute"(%127) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x256x40x80xbf16>) -> tensor<1x256x80x40xbf16> loc(#loc241)
        %129 = "ttir.dot_general"(%128, %126) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x256x80x40xbf16>, tensor<40x80xbf16>) -> tensor<1x256x80x80xbf16> loc(#loc242)
        %130 = "ttir.concat"(%129, %arg24) <{dim = 1 : si32}> : (tensor<1x256x80x80xbf16>, tensor<1x128x80x80xbf16>) -> tensor<1x384x80x80xbf16> loc(#loc243)
        %131 = "ttir.convolution"(%130, %arg23) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x384x80x80xbf16>, tensor<128x384x1x1xbf16>) -> tensor<1x128x80x80xbf16> loc(#loc244)
        %132 = "ttir.reshape"(%arg22) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc245)
        %133 = "ttir.reshape"(%132) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc246)
        %134 = "ttir.reshape"(%arg21) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc247)
        %135 = "ttir.reshape"(%134) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc248)
        %136 = "ttir.reshape"(%arg20) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc249)
        %137 = "ttir.reshape"(%136) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc250)
        %138 = "ttir.batch_norm_inference"(%131, %133, %135, %137, %78) <{dimension = 1 : i32, epsilon = 1.000000e-03 : f32}> : (tensor<1x128x80x80xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>) -> tensor<1x128x80x80xbf16> loc(#loc251)
        %139 = "ttir.sigmoid"(%138) : (tensor<1x128x80x80xbf16>) -> tensor<1x128x80x80xbf16> loc(#loc252)
        %140 = "ttir.multiply"(%138, %139) : (tensor<1x128x80x80xbf16>, tensor<1x128x80x80xbf16>) -> tensor<1x128x80x80xbf16> loc(#loc253)
        %141 = "ttir.slice_static"(%140) <{begins = [0 : i32, 64 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 128 : i32, 80 : i32, 80 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x128x80x80xbf16>) -> tensor<1x64x80x80xbf16> loc(#loc254)
        %142 = "ttir.convolution"(%141, %arg18) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x64x80x80xbf16>, tensor<64x64x3x3xbf16>) -> tensor<1x64x80x80xbf16> loc(#loc255)
        %143 = "ttir.reshape"(%arg17) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc256)
        %144 = "ttir.reshape"(%143) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc257)
        %145 = "ttir.reshape"(%arg16) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc258)
        %146 = "ttir.reshape"(%145) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc259)
        %147 = "ttir.reshape"(%arg15) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc260)
        %148 = "ttir.reshape"(%147) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc261)
        %149 = "ttir.batch_norm_inference"(%142, %144, %146, %148, %80) <{dimension = 1 : i32, epsilon = 1.000000e-03 : f32}> : (tensor<1x64x80x80xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>) -> tensor<1x64x80x80xbf16> loc(#loc262)
        %150 = "ttir.sigmoid"(%149) : (tensor<1x64x80x80xbf16>) -> tensor<1x64x80x80xbf16> loc(#loc263)
        %151 = "ttir.multiply"(%149, %150) : (tensor<1x64x80x80xbf16>, tensor<1x64x80x80xbf16>) -> tensor<1x64x80x80xbf16> loc(#loc264)
        %152 = "ttir.convolution"(%151, %arg13) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x64x80x80xbf16>, tensor<64x64x3x3xbf16>) -> tensor<1x64x80x80xbf16> loc(#loc265)
        %153 = "ttir.reshape"(%arg12) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc266)
        %154 = "ttir.reshape"(%153) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc267)
        %155 = "ttir.reshape"(%arg11) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc268)
        %156 = "ttir.reshape"(%155) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc269)
        %157 = "ttir.reshape"(%arg10) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc270)
        %158 = "ttir.reshape"(%157) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc271)
        %159 = "ttir.batch_norm_inference"(%152, %154, %156, %158, %82) <{dimension = 1 : i32, epsilon = 1.000000e-03 : f32}> : (tensor<1x64x80x80xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>) -> tensor<1x64x80x80xbf16> loc(#loc272)
        %160 = "ttir.sigmoid"(%159) : (tensor<1x64x80x80xbf16>) -> tensor<1x64x80x80xbf16> loc(#loc273)
        %161 = "ttir.multiply"(%159, %160) : (tensor<1x64x80x80xbf16>, tensor<1x64x80x80xbf16>) -> tensor<1x64x80x80xbf16> loc(#loc274)
        %162 = "ttir.reshape"(%161) <{shape = [1 : i32, 2 : i32, 32 : i32, 80 : i32, 80 : i32]}> : (tensor<1x64x80x80xbf16>) -> tensor<1x2x32x80x80xbf16> loc(#loc275)
        %163 = "ttir.reshape"(%arg7) <{shape = [1 : i32, 64 : i32, 512 : i32]}> : (tensor<64x512xbf16>) -> tensor<1x64x512xbf16> loc(#loc276)
        %164 = "ttir.reshape"(%163) <{shape = [64 : i32, 512 : i32]}> : (tensor<1x64x512xbf16>) -> tensor<64x512xbf16> loc(#loc277)
        %165 = "ttir.permute"(%164) <{permutation = array<i64: 1, 0>}> : (tensor<64x512xbf16>) -> tensor<512x64xbf16> loc(#loc278)
        %166 = "ttir.dot_general"(%60, %165) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<3x512xbf16>, tensor<512x64xbf16>) -> tensor<3x64xbf16> loc(#loc279)
        %167 = "ttir.reshape"(%166) <{shape = [1 : i32, 3 : i32, 64 : i32]}> : (tensor<3x64xbf16>) -> tensor<1x3x64xbf16> loc(#loc280)
        %168 = "ttir.reshape"(%arg6) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc281)
        %169 = "ttir.reshape"(%168) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc282)
        %170 = "ttir.reshape"(%169) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc283)
        %171 = "ttir.broadcast"(%170) <{broadcast_dimensions = array<i64: 1, 3, 1>}> : (tensor<1x1x64xbf16>) -> tensor<1x3x64xbf16> loc(#loc283)
        %172 = "ttir.add"(%167, %171) : (tensor<1x3x64xbf16>, tensor<1x3x64xbf16>) -> tensor<1x3x64xbf16> loc(#loc284)
        %173 = "ttir.reshape"(%172) <{shape = [1 : i32, 3 : i32, 2 : i32, 32 : i32]}> : (tensor<1x3x64xbf16>) -> tensor<1x3x2x32xbf16> loc(#loc285)
        %174 = "ttir.dot_general"(%162, %173) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 2>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 3>}> : (tensor<1x2x32x80x80xbf16>, tensor<1x3x2x32xbf16>) -> tensor<1x2x80x80x3xbf16> loc(#loc286)
        %175 = "ttir.reshape"(%arg55) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc287)
        %176 = "ttir.reshape"(%175) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc288)
        %177 = "ttir.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc289)
        %178 = "ttir.reshape"(%177) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc290)
        %179 = "ttir.reshape"(%arg83) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc291)
        %180 = "ttir.reshape"(%179) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc292)
        %181 = "ttir.reshape"(%arg78) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc293)
        %182 = "ttir.reshape"(%181) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc294)
        %183 = "ttir.reshape"(%arg73) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc295)
        %184 = "ttir.reshape"(%183) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc296)
        %185 = "ttir.reshape"(%arg68) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc297)
        %186 = "ttir.reshape"(%185) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc298)
        %187 = "ttir.slice_static"(%140) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 64 : i32, 80 : i32, 80 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x128x80x80xbf16>) -> tensor<1x64x80x80xbf16> loc(#loc299)
        %188 = "ttir.convolution"(%161, %arg59) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x64x80x80xbf16>, tensor<64x64x3x3xbf16>) -> tensor<1x64x80x80xbf16> loc(#loc300)
        %189 = "ttir.reshape"(%arg58) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc301)
        %190 = "ttir.reshape"(%189) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc302)
        %191 = "ttir.reshape"(%arg57) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc303)
        %192 = "ttir.reshape"(%191) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc304)
        %193 = "ttir.reshape"(%arg56) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc305)
        %194 = "ttir.reshape"(%193) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc306)
        %195 = "ttir.batch_norm_inference"(%188, %190, %192, %194, %176) <{dimension = 1 : i32, epsilon = 1.000000e-03 : f32}> : (tensor<1x64x80x80xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>) -> tensor<1x64x80x80xbf16> loc(#loc307)
        %196 = "ttir.reshape"(%195) <{shape = [1 : i32, 2 : i32, 32 : i32, 80 : i32, 80 : i32]}> : (tensor<1x64x80x80xbf16>) -> tensor<1x2x32x80x80xbf16> loc(#loc308)
        %197 = "ttir.max"(%174) <{dim_arg = [4 : i32], keep_dim = false}> : (tensor<1x2x80x80x3xbf16>) -> tensor<1x2x80x80xbf16> loc(#loc309)
        %198 = "ttir.div"(%197, %1) : (tensor<1x2x80x80xbf16>, tensor<1x2x80x80xbf16>) -> tensor<1x2x80x80xbf16> loc(#loc310)
        %199 = "ttir.reshape"(%arg5) <{shape = [1 : i32, 1 : i32, 2 : i32]}> : (tensor<2xbf16>) -> tensor<1x1x2xbf16> loc(#loc311)
        %200 = "ttir.reshape"(%199) <{shape = [1 : i32, 2 : i32]}> : (tensor<1x1x2xbf16>) -> tensor<1x2xbf16> loc(#loc312)
        %201 = "ttir.reshape"(%200) <{shape = [1 : i32, 2 : i32, 1 : i32, 1 : i32]}> : (tensor<1x2xbf16>) -> tensor<1x2x1x1xbf16> loc(#loc313)
        %202 = "ttir.broadcast"(%201) <{broadcast_dimensions = array<i64: 1, 1, 80, 80>}> : (tensor<1x2x1x1xbf16>) -> tensor<1x2x80x80xbf16> loc(#loc313)
        %203 = "ttir.add"(%198, %202) : (tensor<1x2x80x80xbf16>, tensor<1x2x80x80xbf16>) -> tensor<1x2x80x80xbf16> loc(#loc314)
        %204 = "ttir.sigmoid"(%203) : (tensor<1x2x80x80xbf16>) -> tensor<1x2x80x80xbf16> loc(#loc315)
        %205 = "ttir.reshape"(%204) <{shape = [1 : i32, 2 : i32, 1 : i32, 80 : i32, 80 : i32]}> : (tensor<1x2x80x80xbf16>) -> tensor<1x2x1x80x80xbf16> loc(#loc316)
        %206 = "ttir.broadcast"(%205) <{broadcast_dimensions = array<i64: 1, 1, 32, 1, 1>}> : (tensor<1x2x1x80x80xbf16>) -> tensor<1x2x32x80x80xbf16> loc(#loc316)
        %207 = "ttir.multiply"(%196, %206) : (tensor<1x2x32x80x80xbf16>, tensor<1x2x32x80x80xbf16>) -> tensor<1x2x32x80x80xbf16> loc(#loc317)
        %208 = "ttir.reshape"(%207) <{shape = [1 : i32, 64 : i32, 80 : i32, 80 : i32]}> : (tensor<1x2x32x80x80xbf16>) -> tensor<1x64x80x80xbf16> loc(#loc318)
        %209 = "ttir.concat"(%187, %141, %161, %208) <{dim = 1 : si32}> : (tensor<1x64x80x80xbf16>, tensor<1x64x80x80xbf16>, tensor<1x64x80x80xbf16>, tensor<1x64x80x80xbf16>) -> tensor<1x256x80x80xbf16> loc(#loc319)
        %210 = "ttir.convolution"(%209, %arg4) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x256x80x80xbf16>, tensor<128x256x1x1xbf16>) -> tensor<1x128x80x80xbf16> loc(#loc320)
        %211 = "ttir.reshape"(%arg3) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc321)
        %212 = "ttir.reshape"(%211) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc322)
        %213 = "ttir.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc323)
        %214 = "ttir.reshape"(%213) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc324)
        %215 = "ttir.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc325)
        %216 = "ttir.reshape"(%215) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc326)
        %217 = "ttir.batch_norm_inference"(%210, %212, %214, %216, %178) <{dimension = 1 : i32, epsilon = 1.000000e-03 : f32}> : (tensor<1x128x80x80xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>) -> tensor<1x128x80x80xbf16> loc(#loc327)
        %218 = "ttir.sigmoid"(%217) : (tensor<1x128x80x80xbf16>) -> tensor<1x128x80x80xbf16> loc(#loc328)
        %219 = "ttir.multiply"(%217, %218) : (tensor<1x128x80x80xbf16>, tensor<1x128x80x80xbf16>) -> tensor<1x128x80x80xbf16> loc(#loc329)
        %220 = "ttir.convolution"(%219, %arg87) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 2, 2>}> : (tensor<1x128x80x80xbf16>, tensor<128x128x3x3xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc330)
        %221 = "ttir.reshape"(%arg86) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc331)
        %222 = "ttir.reshape"(%221) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc332)
        %223 = "ttir.reshape"(%arg85) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc333)
        %224 = "ttir.reshape"(%223) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc334)
        %225 = "ttir.reshape"(%arg84) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc335)
        %226 = "ttir.reshape"(%225) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc336)
        %227 = "ttir.batch_norm_inference"(%220, %222, %224, %226, %180) <{dimension = 1 : i32, epsilon = 1.000000e-03 : f32}> : (tensor<1x128x40x40xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc337)
        %228 = "ttir.sigmoid"(%227) : (tensor<1x128x40x40xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc338)
        %229 = "ttir.multiply"(%227, %228) : (tensor<1x128x40x40xbf16>, tensor<1x128x40x40xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc339)
        %230 = "ttir.concat"(%229, %115) <{dim = 1 : si32}> : (tensor<1x128x40x40xbf16>, tensor<1x256x40x40xbf16>) -> tensor<1x384x40x40xbf16> loc(#loc340)
        %231 = "ttir.convolution"(%230, %arg82) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x384x40x40xbf16>, tensor<256x384x1x1xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc341)
        %232 = "ttir.reshape"(%arg81) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc342)
        %233 = "ttir.reshape"(%232) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc343)
        %234 = "ttir.reshape"(%arg80) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc344)
        %235 = "ttir.reshape"(%234) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc345)
        %236 = "ttir.reshape"(%arg79) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc346)
        %237 = "ttir.reshape"(%236) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc347)
        %238 = "ttir.batch_norm_inference"(%231, %233, %235, %237, %182) <{dimension = 1 : i32, epsilon = 1.000000e-03 : f32}> : (tensor<1x256x40x40xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc348)
        %239 = "ttir.sigmoid"(%238) : (tensor<1x256x40x40xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc349)
        %240 = "ttir.multiply"(%238, %239) : (tensor<1x256x40x40xbf16>, tensor<1x256x40x40xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc350)
        %241 = "ttir.slice_static"(%240) <{begins = [0 : i32, 128 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 256 : i32, 40 : i32, 40 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x256x40x40xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc351)
        %242 = "ttir.convolution"(%241, %arg77) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x128x40x40xbf16>, tensor<128x128x3x3xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc352)
        %243 = "ttir.reshape"(%arg76) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc353)
        %244 = "ttir.reshape"(%243) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc354)
        %245 = "ttir.reshape"(%arg75) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc355)
        %246 = "ttir.reshape"(%245) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc356)
        %247 = "ttir.reshape"(%arg74) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc357)
        %248 = "ttir.reshape"(%247) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc358)
        %249 = "ttir.batch_norm_inference"(%242, %244, %246, %248, %184) <{dimension = 1 : i32, epsilon = 1.000000e-03 : f32}> : (tensor<1x128x40x40xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc359)
        %250 = "ttir.sigmoid"(%249) : (tensor<1x128x40x40xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc360)
        %251 = "ttir.multiply"(%249, %250) : (tensor<1x128x40x40xbf16>, tensor<1x128x40x40xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc361)
        %252 = "ttir.convolution"(%251, %arg72) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x128x40x40xbf16>, tensor<128x128x3x3xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc362)
        %253 = "ttir.reshape"(%arg71) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc363)
        %254 = "ttir.reshape"(%253) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc364)
        %255 = "ttir.reshape"(%arg70) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc365)
        %256 = "ttir.reshape"(%255) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc366)
        %257 = "ttir.reshape"(%arg69) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc367)
        %258 = "ttir.reshape"(%257) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc368)
        %259 = "ttir.batch_norm_inference"(%252, %254, %256, %258, %186) <{dimension = 1 : i32, epsilon = 1.000000e-03 : f32}> : (tensor<1x128x40x40xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc369)
        %260 = "ttir.sigmoid"(%259) : (tensor<1x128x40x40xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc370)
        %261 = "ttir.multiply"(%259, %260) : (tensor<1x128x40x40xbf16>, tensor<1x128x40x40xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc371)
        %262 = "ttir.reshape"(%261) <{shape = [1 : i32, 4 : i32, 32 : i32, 40 : i32, 40 : i32]}> : (tensor<1x128x40x40xbf16>) -> tensor<1x4x32x40x40xbf16> loc(#loc372)
        %263 = "ttir.reshape"(%arg67) <{shape = [1 : i32, 128 : i32, 512 : i32]}> : (tensor<128x512xbf16>) -> tensor<1x128x512xbf16> loc(#loc373)
        %264 = "ttir.reshape"(%263) <{shape = [128 : i32, 512 : i32]}> : (tensor<1x128x512xbf16>) -> tensor<128x512xbf16> loc(#loc374)
        %265 = "ttir.permute"(%264) <{permutation = array<i64: 1, 0>}> : (tensor<128x512xbf16>) -> tensor<512x128xbf16> loc(#loc375)
        %266 = "ttir.dot_general"(%60, %265) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<3x512xbf16>, tensor<512x128xbf16>) -> tensor<3x128xbf16> loc(#loc376)
        %267 = "ttir.reshape"(%266) <{shape = [1 : i32, 3 : i32, 128 : i32]}> : (tensor<3x128xbf16>) -> tensor<1x3x128xbf16> loc(#loc377)
        %268 = "ttir.reshape"(%arg66) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc378)
        %269 = "ttir.reshape"(%268) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc379)
        %270 = "ttir.reshape"(%269) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc380)
        %271 = "ttir.broadcast"(%270) <{broadcast_dimensions = array<i64: 1, 3, 1>}> : (tensor<1x1x128xbf16>) -> tensor<1x3x128xbf16> loc(#loc380)
        %272 = "ttir.add"(%267, %271) : (tensor<1x3x128xbf16>, tensor<1x3x128xbf16>) -> tensor<1x3x128xbf16> loc(#loc381)
        %273 = "ttir.reshape"(%272) <{shape = [1 : i32, 3 : i32, 4 : i32, 32 : i32]}> : (tensor<1x3x128xbf16>) -> tensor<1x3x4x32xbf16> loc(#loc382)
        %274 = "ttir.dot_general"(%262, %273) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 2>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 3>}> : (tensor<1x4x32x40x40xbf16>, tensor<1x3x4x32xbf16>) -> tensor<1x4x40x40x3xbf16> loc(#loc383)
        %275 = "ttir.reshape"(%arg88) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc384)
        %276 = "ttir.reshape"(%275) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc385)
        %277 = "ttir.reshape"(%arg60) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc386)
        %278 = "ttir.reshape"(%277) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc387)
        %279 = "ttir.reshape"(%arg116) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc388)
        %280 = "ttir.reshape"(%279) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc389)
        %281 = "ttir.reshape"(%arg111) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>) -> tensor<1x1x512xbf16> loc(#loc390)
        %282 = "ttir.reshape"(%281) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>) -> tensor<512xbf16> loc(#loc391)
        %283 = "ttir.reshape"(%arg106) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc392)
        %284 = "ttir.reshape"(%283) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc393)
        %285 = "ttir.reshape"(%arg101) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc394)
        %286 = "ttir.reshape"(%285) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc395)
        %287 = "ttir.slice_static"(%240) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 128 : i32, 40 : i32, 40 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x256x40x40xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc396)
        %288 = "ttir.convolution"(%261, %arg92) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x128x40x40xbf16>, tensor<128x128x3x3xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc397)
        %289 = "ttir.reshape"(%arg91) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc398)
        %290 = "ttir.reshape"(%289) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc399)
        %291 = "ttir.reshape"(%arg90) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc400)
        %292 = "ttir.reshape"(%291) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc401)
        %293 = "ttir.reshape"(%arg89) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc402)
        %294 = "ttir.reshape"(%293) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc403)
        %295 = "ttir.batch_norm_inference"(%288, %290, %292, %294, %276) <{dimension = 1 : i32, epsilon = 1.000000e-03 : f32}> : (tensor<1x128x40x40xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc404)
        %296 = "ttir.reshape"(%295) <{shape = [1 : i32, 4 : i32, 32 : i32, 40 : i32, 40 : i32]}> : (tensor<1x128x40x40xbf16>) -> tensor<1x4x32x40x40xbf16> loc(#loc405)
        %297 = "ttir.max"(%274) <{dim_arg = [4 : i32], keep_dim = false}> : (tensor<1x4x40x40x3xbf16>) -> tensor<1x4x40x40xbf16> loc(#loc406)
        %298 = "ttir.div"(%297, %3) : (tensor<1x4x40x40xbf16>, tensor<1x4x40x40xbf16>) -> tensor<1x4x40x40xbf16> loc(#loc407)
        %299 = "ttir.reshape"(%arg65) <{shape = [1 : i32, 1 : i32, 4 : i32]}> : (tensor<4xbf16>) -> tensor<1x1x4xbf16> loc(#loc408)
        %300 = "ttir.reshape"(%299) <{shape = [1 : i32, 4 : i32]}> : (tensor<1x1x4xbf16>) -> tensor<1x4xbf16> loc(#loc409)
        %301 = "ttir.reshape"(%300) <{shape = [1 : i32, 4 : i32, 1 : i32, 1 : i32]}> : (tensor<1x4xbf16>) -> tensor<1x4x1x1xbf16> loc(#loc410)
        %302 = "ttir.broadcast"(%301) <{broadcast_dimensions = array<i64: 1, 1, 40, 40>}> : (tensor<1x4x1x1xbf16>) -> tensor<1x4x40x40xbf16> loc(#loc410)
        %303 = "ttir.add"(%298, %302) : (tensor<1x4x40x40xbf16>, tensor<1x4x40x40xbf16>) -> tensor<1x4x40x40xbf16> loc(#loc411)
        %304 = "ttir.sigmoid"(%303) : (tensor<1x4x40x40xbf16>) -> tensor<1x4x40x40xbf16> loc(#loc412)
        %305 = "ttir.reshape"(%304) <{shape = [1 : i32, 4 : i32, 1 : i32, 40 : i32, 40 : i32]}> : (tensor<1x4x40x40xbf16>) -> tensor<1x4x1x40x40xbf16> loc(#loc413)
        %306 = "ttir.broadcast"(%305) <{broadcast_dimensions = array<i64: 1, 1, 32, 1, 1>}> : (tensor<1x4x1x40x40xbf16>) -> tensor<1x4x32x40x40xbf16> loc(#loc413)
        %307 = "ttir.multiply"(%296, %306) : (tensor<1x4x32x40x40xbf16>, tensor<1x4x32x40x40xbf16>) -> tensor<1x4x32x40x40xbf16> loc(#loc414)
        %308 = "ttir.reshape"(%307) <{shape = [1 : i32, 128 : i32, 40 : i32, 40 : i32]}> : (tensor<1x4x32x40x40xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc415)
        %309 = "ttir.concat"(%287, %241, %261, %308) <{dim = 1 : si32}> : (tensor<1x128x40x40xbf16>, tensor<1x128x40x40xbf16>, tensor<1x128x40x40xbf16>, tensor<1x128x40x40xbf16>) -> tensor<1x512x40x40xbf16> loc(#loc416)
        %310 = "ttir.convolution"(%309, %arg64) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x512x40x40xbf16>, tensor<256x512x1x1xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc417)
        %311 = "ttir.reshape"(%arg63) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc418)
        %312 = "ttir.reshape"(%311) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc419)
        %313 = "ttir.reshape"(%arg62) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc420)
        %314 = "ttir.reshape"(%313) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc421)
        %315 = "ttir.reshape"(%arg61) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc422)
        %316 = "ttir.reshape"(%315) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc423)
        %317 = "ttir.batch_norm_inference"(%310, %312, %314, %316, %278) <{dimension = 1 : i32, epsilon = 1.000000e-03 : f32}> : (tensor<1x256x40x40xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc424)
        %318 = "ttir.sigmoid"(%317) : (tensor<1x256x40x40xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc425)
        %319 = "ttir.multiply"(%317, %318) : (tensor<1x256x40x40xbf16>, tensor<1x256x40x40xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc426)
        %320 = "ttir.convolution"(%319, %arg120) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 2, 2>}> : (tensor<1x256x40x40xbf16>, tensor<256x256x3x3xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc427)
        %321 = "ttir.reshape"(%arg119) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc428)
        %322 = "ttir.reshape"(%321) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc429)
        %323 = "ttir.reshape"(%arg118) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc430)
        %324 = "ttir.reshape"(%323) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc431)
        %325 = "ttir.reshape"(%arg117) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc432)
        %326 = "ttir.reshape"(%325) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc433)
        %327 = "ttir.batch_norm_inference"(%320, %322, %324, %326, %280) <{dimension = 1 : i32, epsilon = 1.000000e-03 : f32}> : (tensor<1x256x20x20xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc434)
        %328 = "ttir.sigmoid"(%327) : (tensor<1x256x20x20xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc435)
        %329 = "ttir.multiply"(%327, %328) : (tensor<1x256x20x20xbf16>, tensor<1x256x20x20xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc436)
        %330 = "ttir.concat"(%329, %arg49) <{dim = 1 : si32}> : (tensor<1x256x20x20xbf16>, tensor<1x512x20x20xbf16>) -> tensor<1x768x20x20xbf16> loc(#loc437)
        %331 = "ttir.convolution"(%330, %arg115) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x768x20x20xbf16>, tensor<512x768x1x1xbf16>) -> tensor<1x512x20x20xbf16> loc(#loc438)
        %332 = "ttir.reshape"(%arg114) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>) -> tensor<1x1x512xbf16> loc(#loc439)
        %333 = "ttir.reshape"(%332) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>) -> tensor<512xbf16> loc(#loc440)
        %334 = "ttir.reshape"(%arg113) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>) -> tensor<1x1x512xbf16> loc(#loc441)
        %335 = "ttir.reshape"(%334) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>) -> tensor<512xbf16> loc(#loc442)
        %336 = "ttir.reshape"(%arg112) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>) -> tensor<1x1x512xbf16> loc(#loc443)
        %337 = "ttir.reshape"(%336) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>) -> tensor<512xbf16> loc(#loc444)
        %338 = "ttir.batch_norm_inference"(%331, %333, %335, %337, %282) <{dimension = 1 : i32, epsilon = 1.000000e-03 : f32}> : (tensor<1x512x20x20xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>) -> tensor<1x512x20x20xbf16> loc(#loc445)
        %339 = "ttir.sigmoid"(%338) : (tensor<1x512x20x20xbf16>) -> tensor<1x512x20x20xbf16> loc(#loc446)
        %340 = "ttir.multiply"(%338, %339) : (tensor<1x512x20x20xbf16>, tensor<1x512x20x20xbf16>) -> tensor<1x512x20x20xbf16> loc(#loc447)
        %341 = "ttir.slice_static"(%340) <{begins = [0 : i32, 256 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 512 : i32, 20 : i32, 20 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x512x20x20xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc448)
        %342 = "ttir.convolution"(%341, %arg110) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x256x20x20xbf16>, tensor<256x256x3x3xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc449)
        %343 = "ttir.reshape"(%arg109) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc450)
        %344 = "ttir.reshape"(%343) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc451)
        %345 = "ttir.reshape"(%arg108) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc452)
        %346 = "ttir.reshape"(%345) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc453)
        %347 = "ttir.reshape"(%arg107) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc454)
        %348 = "ttir.reshape"(%347) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc455)
        %349 = "ttir.batch_norm_inference"(%342, %344, %346, %348, %284) <{dimension = 1 : i32, epsilon = 1.000000e-03 : f32}> : (tensor<1x256x20x20xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc456)
        %350 = "ttir.sigmoid"(%349) : (tensor<1x256x20x20xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc457)
        %351 = "ttir.multiply"(%349, %350) : (tensor<1x256x20x20xbf16>, tensor<1x256x20x20xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc458)
        %352 = "ttir.convolution"(%351, %arg105) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x256x20x20xbf16>, tensor<256x256x3x3xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc459)
        %353 = "ttir.reshape"(%arg104) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc460)
        %354 = "ttir.reshape"(%353) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc461)
        %355 = "ttir.reshape"(%arg103) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc462)
        %356 = "ttir.reshape"(%355) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc463)
        %357 = "ttir.reshape"(%arg102) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc464)
        %358 = "ttir.reshape"(%357) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc465)
        %359 = "ttir.batch_norm_inference"(%352, %354, %356, %358, %286) <{dimension = 1 : i32, epsilon = 1.000000e-03 : f32}> : (tensor<1x256x20x20xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc466)
        %360 = "ttir.sigmoid"(%359) : (tensor<1x256x20x20xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc467)
        %361 = "ttir.multiply"(%359, %360) : (tensor<1x256x20x20xbf16>, tensor<1x256x20x20xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc468)
        %362 = "ttir.reshape"(%361) <{shape = [1 : i32, 8 : i32, 32 : i32, 20 : i32, 20 : i32]}> : (tensor<1x256x20x20xbf16>) -> tensor<1x8x32x20x20xbf16> loc(#loc469)
        %363 = "ttir.reshape"(%arg100) <{shape = [1 : i32, 256 : i32, 512 : i32]}> : (tensor<256x512xbf16>) -> tensor<1x256x512xbf16> loc(#loc470)
        %364 = "ttir.reshape"(%363) <{shape = [256 : i32, 512 : i32]}> : (tensor<1x256x512xbf16>) -> tensor<256x512xbf16> loc(#loc471)
        %365 = "ttir.permute"(%364) <{permutation = array<i64: 1, 0>}> : (tensor<256x512xbf16>) -> tensor<512x256xbf16> loc(#loc472)
        %366 = "ttir.dot_general"(%60, %365) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<3x512xbf16>, tensor<512x256xbf16>) -> tensor<3x256xbf16> loc(#loc473)
        %367 = "ttir.reshape"(%366) <{shape = [1 : i32, 3 : i32, 256 : i32]}> : (tensor<3x256xbf16>) -> tensor<1x3x256xbf16> loc(#loc474)
        %368 = "ttir.reshape"(%arg99) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc475)
        %369 = "ttir.reshape"(%368) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc476)
        %370 = "ttir.reshape"(%369) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc477)
        %371 = "ttir.broadcast"(%370) <{broadcast_dimensions = array<i64: 1, 3, 1>}> : (tensor<1x1x256xbf16>) -> tensor<1x3x256xbf16> loc(#loc477)
        %372 = "ttir.add"(%367, %371) : (tensor<1x3x256xbf16>, tensor<1x3x256xbf16>) -> tensor<1x3x256xbf16> loc(#loc478)
        %373 = "ttir.reshape"(%372) <{shape = [1 : i32, 3 : i32, 8 : i32, 32 : i32]}> : (tensor<1x3x256xbf16>) -> tensor<1x3x8x32xbf16> loc(#loc479)
        %374 = "ttir.dot_general"(%362, %373) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 2>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 3>}> : (tensor<1x8x32x20x20xbf16>, tensor<1x3x8x32xbf16>) -> tensor<1x8x20x20x3xbf16> loc(#loc480)
        %375 = "ttir.reshape"(%arg121) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc481)
        %376 = "ttir.reshape"(%375) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc482)
        %377 = "ttir.reshape"(%arg93) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>) -> tensor<1x1x512xbf16> loc(#loc483)
        %378 = "ttir.reshape"(%377) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>) -> tensor<512xbf16> loc(#loc484)
        %379 = "ttir.slice_static"(%340) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 256 : i32, 20 : i32, 20 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x512x20x20xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc485)
        %380 = "ttir.convolution"(%361, %arg125) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x256x20x20xbf16>, tensor<256x256x3x3xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc486)
        %381 = "ttir.reshape"(%arg124) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc487)
        %382 = "ttir.reshape"(%381) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc488)
        %383 = "ttir.reshape"(%arg123) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc489)
        %384 = "ttir.reshape"(%383) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc490)
        %385 = "ttir.reshape"(%arg122) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc491)
        %386 = "ttir.reshape"(%385) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc492)
        %387 = "ttir.batch_norm_inference"(%380, %382, %384, %386, %376) <{dimension = 1 : i32, epsilon = 1.000000e-03 : f32}> : (tensor<1x256x20x20xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc493)
        %388 = "ttir.reshape"(%387) <{shape = [1 : i32, 8 : i32, 32 : i32, 20 : i32, 20 : i32]}> : (tensor<1x256x20x20xbf16>) -> tensor<1x8x32x20x20xbf16> loc(#loc494)
        %389 = "ttir.max"(%374) <{dim_arg = [4 : i32], keep_dim = false}> : (tensor<1x8x20x20x3xbf16>) -> tensor<1x8x20x20xbf16> loc(#loc495)
        %390 = "ttir.div"(%389, %0) : (tensor<1x8x20x20xbf16>, tensor<1x8x20x20xbf16>) -> tensor<1x8x20x20xbf16> loc(#loc496)
        %391 = "ttir.reshape"(%arg98) <{shape = [1 : i32, 1 : i32, 8 : i32]}> : (tensor<8xbf16>) -> tensor<1x1x8xbf16> loc(#loc497)
        %392 = "ttir.reshape"(%391) <{shape = [1 : i32, 8 : i32]}> : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc498)
        %393 = "ttir.reshape"(%392) <{shape = [1 : i32, 8 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8xbf16>) -> tensor<1x8x1x1xbf16> loc(#loc499)
        %394 = "ttir.broadcast"(%393) <{broadcast_dimensions = array<i64: 1, 1, 20, 20>}> : (tensor<1x8x1x1xbf16>) -> tensor<1x8x20x20xbf16> loc(#loc499)
        %395 = "ttir.add"(%390, %394) : (tensor<1x8x20x20xbf16>, tensor<1x8x20x20xbf16>) -> tensor<1x8x20x20xbf16> loc(#loc500)
        %396 = "ttir.sigmoid"(%395) : (tensor<1x8x20x20xbf16>) -> tensor<1x8x20x20xbf16> loc(#loc501)
        %397 = "ttir.reshape"(%396) <{shape = [1 : i32, 8 : i32, 1 : i32, 20 : i32, 20 : i32]}> : (tensor<1x8x20x20xbf16>) -> tensor<1x8x1x20x20xbf16> loc(#loc502)
        %398 = "ttir.broadcast"(%397) <{broadcast_dimensions = array<i64: 1, 1, 32, 1, 1>}> : (tensor<1x8x1x20x20xbf16>) -> tensor<1x8x32x20x20xbf16> loc(#loc502)
        %399 = "ttir.multiply"(%388, %398) : (tensor<1x8x32x20x20xbf16>, tensor<1x8x32x20x20xbf16>) -> tensor<1x8x32x20x20xbf16> loc(#loc503)
        %400 = "ttir.reshape"(%399) <{shape = [1 : i32, 256 : i32, 20 : i32, 20 : i32]}> : (tensor<1x8x32x20x20xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc504)
        %401 = "ttir.concat"(%379, %341, %361, %400) <{dim = 1 : si32}> : (tensor<1x256x20x20xbf16>, tensor<1x256x20x20xbf16>, tensor<1x256x20x20xbf16>, tensor<1x256x20x20xbf16>) -> tensor<1x1024x20x20xbf16> loc(#loc505)
        %402 = "ttir.convolution"(%401, %arg97) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1024x20x20xbf16>, tensor<512x1024x1x1xbf16>) -> tensor<1x512x20x20xbf16> loc(#loc506)
        %403 = "ttir.reshape"(%arg96) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>) -> tensor<1x1x512xbf16> loc(#loc507)
        %404 = "ttir.reshape"(%403) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>) -> tensor<512xbf16> loc(#loc508)
        %405 = "ttir.reshape"(%arg95) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>) -> tensor<1x1x512xbf16> loc(#loc509)
        %406 = "ttir.reshape"(%405) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>) -> tensor<512xbf16> loc(#loc510)
        %407 = "ttir.reshape"(%arg94) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>) -> tensor<1x1x512xbf16> loc(#loc511)
        %408 = "ttir.reshape"(%407) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>) -> tensor<512xbf16> loc(#loc512)
        %409 = "ttir.batch_norm_inference"(%402, %404, %406, %408, %378) <{dimension = 1 : i32, epsilon = 1.000000e-03 : f32}> : (tensor<1x512x20x20xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>) -> tensor<1x512x20x20xbf16> loc(#loc513)
        %410 = "ttir.sigmoid"(%409) : (tensor<1x512x20x20xbf16>) -> tensor<1x512x20x20xbf16> loc(#loc514)
        %411 = "ttir.multiply"(%409, %410) : (tensor<1x512x20x20xbf16>, tensor<1x512x20x20xbf16>) -> tensor<1x512x20x20xbf16> loc(#loc515)
        return %219, %319, %411 : tensor<1x128x80x80xbf16>, tensor<1x256x40x40xbf16>, tensor<1x512x20x20xbf16> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc127 = loc("reshape.220")
#loc128 = loc("reshape.222")
#loc129 = loc("reshape.202")
#loc130 = loc("reshape.204")
#loc131 = loc("reshape.184")
#loc132 = loc("reshape.186")
#loc133 = loc("transpose.271")
#loc134 = loc("floor.259")
#loc135 = loc("convert.260")
#loc136 = loc("broadcast.264")
#loc137 = loc("broadcast.262")
#loc138 = loc("compare.265")
#loc139 = loc("convert.266")
#loc140 = loc("transpose.267")
#loc141 = loc("convert.268")
#loc142 = loc("dot.272")
#loc143 = loc("transpose.274")
#loc144 = loc("dot.275")
#loc145 = loc("concatenate.277")
#loc146 = loc("convolution.278")
#loc147 = loc("reshape.232")
#loc148 = loc("reshape.234")
#loc149 = loc("reshape.228")
#loc150 = loc("reshape.230")
#loc151 = loc("reshape.224")
#loc152 = loc("reshape.226")
#loc153 = loc("batch-norm-inference.279")
#loc154 = loc("logistic.284")
#loc155 = loc("multiply.285")
#loc156 = loc("slice.286")
#loc157 = loc("convolution.287")
#loc158 = loc("reshape.214")
#loc159 = loc("reshape.216")
#loc160 = loc("reshape.210")
#loc161 = loc("reshape.212")
#loc162 = loc("reshape.206")
#loc163 = loc("reshape.208")
#loc164 = loc("batch-norm-inference.288")
#loc165 = loc("logistic.293")
#loc166 = loc("multiply.294")
#loc167 = loc("convolution.295")
#loc168 = loc("reshape.196")
#loc169 = loc("reshape.198")
#loc170 = loc("reshape.192")
#loc171 = loc("reshape.194")
#loc172 = loc("reshape.188")
#loc173 = loc("reshape.190")
#loc174 = loc("batch-norm-inference.296")
#loc175 = loc("logistic.301")
#loc176 = loc("multiply.302")
#loc177 = loc("reshape.303")
#loc178 = loc("reshape.175")
#loc179 = loc("reshape.171")
#loc180 = loc("reshape.173")
#loc181 = loc("transpose.174")
#loc182 = loc("dot.176")
#loc183 = loc("reshape.177")
#loc184 = loc("reshape.167")
#loc185 = loc("reshape.169")
#loc186 = loc("broadcast.180")
#loc187 = loc("add.181")
#loc188 = loc("reshape.182")
#loc189 = loc("dot.304")
#loc190 = loc("reshape.355")
#loc191 = loc("reshape.357")
#loc192 = loc("reshape.137")
#loc193 = loc("reshape.139")
#loc194 = loc("reshape.87")
#loc195 = loc("reshape.89")
#loc196 = loc("reshape.69")
#loc197 = loc("reshape.71")
#loc198 = loc("reshape.51")
#loc199 = loc("reshape.53")
#loc200 = loc("slice.383")
#loc201 = loc("convolution.372")
#loc202 = loc("reshape.367")
#loc203 = loc("reshape.369")
#loc204 = loc("reshape.363")
#loc205 = loc("reshape.365")
#loc206 = loc("reshape.359")
#loc207 = loc("reshape.361")
#loc208 = loc("batch-norm-inference.373")
#loc209 = loc("reshape.378")
#loc210 = loc("reduce.312")
#loc211 = loc("divide.344")
#loc212 = loc("reshape.157")
#loc213 = loc("reshape.160")
#loc214 = loc("broadcast.348")
#loc215 = loc("add.349")
#loc216 = loc("logistic.350")
#loc217 = loc("broadcast.380")
#loc218 = loc("multiply.381")
#loc219 = loc("reshape.382")
#loc220 = loc("concatenate.384")
#loc221 = loc("convolution.385")
#loc222 = loc("reshape.149")
#loc223 = loc("reshape.151")
#loc224 = loc("reshape.145")
#loc225 = loc("reshape.147")
#loc226 = loc("reshape.141")
#loc227 = loc("reshape.143")
#loc228 = loc("batch-norm-inference.386")
#loc229 = loc("logistic.391")
#loc230 = loc("multiply.392")
#loc231 = loc("transpose.393")
#loc232 = loc("floor.126")
#loc233 = loc("convert.127")
#loc234 = loc("broadcast.131")
#loc235 = loc("broadcast.129")
#loc236 = loc("compare.132")
#loc237 = loc("convert.133")
#loc238 = loc("transpose.134")
#loc239 = loc("convert.135")
#loc240 = loc("dot.394")
#loc241 = loc("transpose.396")
#loc242 = loc("dot.397")
#loc243 = loc("concatenate.399")
#loc244 = loc("convolution.400")
#loc245 = loc("reshape.99")
#loc246 = loc("reshape.101")
#loc247 = loc("reshape.95")
#loc248 = loc("reshape.97")
#loc249 = loc("reshape.91")
#loc250 = loc("reshape.93")
#loc251 = loc("batch-norm-inference.401")
#loc252 = loc("logistic.406")
#loc253 = loc("multiply.407")
#loc254 = loc("slice.408")
#loc255 = loc("convolution.409")
#loc256 = loc("reshape.81")
#loc257 = loc("reshape.83")
#loc258 = loc("reshape.77")
#loc259 = loc("reshape.79")
#loc260 = loc("reshape.73")
#loc261 = loc("reshape.75")
#loc262 = loc("batch-norm-inference.410")
#loc263 = loc("logistic.415")
#loc264 = loc("multiply.416")
#loc265 = loc("convolution.417")
#loc266 = loc("reshape.63")
#loc267 = loc("reshape.65")
#loc268 = loc("reshape.59")
#loc269 = loc("reshape.61")
#loc270 = loc("reshape.55")
#loc271 = loc("reshape.57")
#loc272 = loc("batch-norm-inference.418")
#loc273 = loc("logistic.423")
#loc274 = loc("multiply.424")
#loc275 = loc("reshape.425")
#loc276 = loc("reshape.36")
#loc277 = loc("reshape.38")
#loc278 = loc("transpose.39")
#loc279 = loc("dot.43")
#loc280 = loc("reshape.44")
#loc281 = loc("reshape.32")
#loc282 = loc("reshape.34")
#loc283 = loc("broadcast.47")
#loc284 = loc("add.48")
#loc285 = loc("reshape.49")
#loc286 = loc("dot.426")
#loc287 = loc("reshape.477")
#loc288 = loc("reshape.479")
#loc289 = loc("reshape.2")
#loc290 = loc("reshape.4")
#loc291 = loc("reshape.617")
#loc292 = loc("reshape.619")
#loc293 = loc("reshape.599")
#loc294 = loc("reshape.601")
#loc295 = loc("reshape.581")
#loc296 = loc("reshape.583")
#loc297 = loc("reshape.563")
#loc298 = loc("reshape.565")
#loc299 = loc("slice.505")
#loc300 = loc("convolution.494")
#loc301 = loc("reshape.489")
#loc302 = loc("reshape.491")
#loc303 = loc("reshape.485")
#loc304 = loc("reshape.487")
#loc305 = loc("reshape.481")
#loc306 = loc("reshape.483")
#loc307 = loc("batch-norm-inference.495")
#loc308 = loc("reshape.500")
#loc309 = loc("reduce.434")
#loc310 = loc("divide.466")
#loc311 = loc("reshape.22")
#loc312 = loc("reshape.25")
#loc313 = loc("broadcast.470")
#loc314 = loc("add.471")
#loc315 = loc("logistic.472")
#loc316 = loc("broadcast.502")
#loc317 = loc("multiply.503")
#loc318 = loc("reshape.504")
#loc319 = loc("concatenate.506")
#loc320 = loc("convolution.507")
#loc321 = loc("reshape.14")
#loc322 = loc("reshape.16")
#loc323 = loc("reshape.10")
#loc324 = loc("reshape.12")
#loc325 = loc("reshape.6")
#loc326 = loc("reshape.8")
#loc327 = loc("batch-norm-inference.508")
#loc328 = loc("logistic.513")
#loc329 = loc("multiply.514")
#loc330 = loc("convolution.634")
#loc331 = loc("reshape.629")
#loc332 = loc("reshape.631")
#loc333 = loc("reshape.625")
#loc334 = loc("reshape.627")
#loc335 = loc("reshape.621")
#loc336 = loc("reshape.623")
#loc337 = loc("batch-norm-inference.635")
#loc338 = loc("logistic.640")
#loc339 = loc("multiply.641")
#loc340 = loc("concatenate.642")
#loc341 = loc("convolution.643")
#loc342 = loc("reshape.611")
#loc343 = loc("reshape.613")
#loc344 = loc("reshape.607")
#loc345 = loc("reshape.609")
#loc346 = loc("reshape.603")
#loc347 = loc("reshape.605")
#loc348 = loc("batch-norm-inference.644")
#loc349 = loc("logistic.649")
#loc350 = loc("multiply.650")
#loc351 = loc("slice.651")
#loc352 = loc("convolution.652")
#loc353 = loc("reshape.593")
#loc354 = loc("reshape.595")
#loc355 = loc("reshape.589")
#loc356 = loc("reshape.591")
#loc357 = loc("reshape.585")
#loc358 = loc("reshape.587")
#loc359 = loc("batch-norm-inference.653")
#loc360 = loc("logistic.658")
#loc361 = loc("multiply.659")
#loc362 = loc("convolution.660")
#loc363 = loc("reshape.575")
#loc364 = loc("reshape.577")
#loc365 = loc("reshape.571")
#loc366 = loc("reshape.573")
#loc367 = loc("reshape.567")
#loc368 = loc("reshape.569")
#loc369 = loc("batch-norm-inference.661")
#loc370 = loc("logistic.666")
#loc371 = loc("multiply.667")
#loc372 = loc("reshape.668")
#loc373 = loc("reshape.550")
#loc374 = loc("reshape.552")
#loc375 = loc("transpose.553")
#loc376 = loc("dot.555")
#loc377 = loc("reshape.556")
#loc378 = loc("reshape.546")
#loc379 = loc("reshape.548")
#loc380 = loc("broadcast.559")
#loc381 = loc("add.560")
#loc382 = loc("reshape.561")
#loc383 = loc("dot.669")
#loc384 = loc("reshape.720")
#loc385 = loc("reshape.722")
#loc386 = loc("reshape.516")
#loc387 = loc("reshape.518")
#loc388 = loc("reshape.860")
#loc389 = loc("reshape.862")
#loc390 = loc("reshape.842")
#loc391 = loc("reshape.844")
#loc392 = loc("reshape.824")
#loc393 = loc("reshape.826")
#loc394 = loc("reshape.806")
#loc395 = loc("reshape.808")
#loc396 = loc("slice.748")
#loc397 = loc("convolution.737")
#loc398 = loc("reshape.732")
#loc399 = loc("reshape.734")
#loc400 = loc("reshape.728")
#loc401 = loc("reshape.730")
#loc402 = loc("reshape.724")
#loc403 = loc("reshape.726")
#loc404 = loc("batch-norm-inference.738")
#loc405 = loc("reshape.743")
#loc406 = loc("reduce.677")
#loc407 = loc("divide.709")
#loc408 = loc("reshape.536")
#loc409 = loc("reshape.539")
#loc410 = loc("broadcast.713")
#loc411 = loc("add.714")
#loc412 = loc("logistic.715")
#loc413 = loc("broadcast.745")
#loc414 = loc("multiply.746")
#loc415 = loc("reshape.747")
#loc416 = loc("concatenate.749")
#loc417 = loc("convolution.750")
#loc418 = loc("reshape.528")
#loc419 = loc("reshape.530")
#loc420 = loc("reshape.524")
#loc421 = loc("reshape.526")
#loc422 = loc("reshape.520")
#loc423 = loc("reshape.522")
#loc424 = loc("batch-norm-inference.751")
#loc425 = loc("logistic.756")
#loc426 = loc("multiply.757")
#loc427 = loc("convolution.877")
#loc428 = loc("reshape.872")
#loc429 = loc("reshape.874")
#loc430 = loc("reshape.868")
#loc431 = loc("reshape.870")
#loc432 = loc("reshape.864")
#loc433 = loc("reshape.866")
#loc434 = loc("batch-norm-inference.878")
#loc435 = loc("logistic.883")
#loc436 = loc("multiply.884")
#loc437 = loc("concatenate.885")
#loc438 = loc("convolution.886")
#loc439 = loc("reshape.854")
#loc440 = loc("reshape.856")
#loc441 = loc("reshape.850")
#loc442 = loc("reshape.852")
#loc443 = loc("reshape.846")
#loc444 = loc("reshape.848")
#loc445 = loc("batch-norm-inference.887")
#loc446 = loc("logistic.892")
#loc447 = loc("multiply.893")
#loc448 = loc("slice.894")
#loc449 = loc("convolution.895")
#loc450 = loc("reshape.836")
#loc451 = loc("reshape.838")
#loc452 = loc("reshape.832")
#loc453 = loc("reshape.834")
#loc454 = loc("reshape.828")
#loc455 = loc("reshape.830")
#loc456 = loc("batch-norm-inference.896")
#loc457 = loc("logistic.901")
#loc458 = loc("multiply.902")
#loc459 = loc("convolution.903")
#loc460 = loc("reshape.818")
#loc461 = loc("reshape.820")
#loc462 = loc("reshape.814")
#loc463 = loc("reshape.816")
#loc464 = loc("reshape.810")
#loc465 = loc("reshape.812")
#loc466 = loc("batch-norm-inference.904")
#loc467 = loc("logistic.909")
#loc468 = loc("multiply.910")
#loc469 = loc("reshape.911")
#loc470 = loc("reshape.793")
#loc471 = loc("reshape.795")
#loc472 = loc("transpose.796")
#loc473 = loc("dot.798")
#loc474 = loc("reshape.799")
#loc475 = loc("reshape.789")
#loc476 = loc("reshape.791")
#loc477 = loc("broadcast.802")
#loc478 = loc("add.803")
#loc479 = loc("reshape.804")
#loc480 = loc("dot.912")
#loc481 = loc("reshape.963")
#loc482 = loc("reshape.965")
#loc483 = loc("reshape.759")
#loc484 = loc("reshape.761")
#loc485 = loc("slice.991")
#loc486 = loc("convolution.980")
#loc487 = loc("reshape.975")
#loc488 = loc("reshape.977")
#loc489 = loc("reshape.971")
#loc490 = loc("reshape.973")
#loc491 = loc("reshape.967")
#loc492 = loc("reshape.969")
#loc493 = loc("batch-norm-inference.981")
#loc494 = loc("reshape.986")
#loc495 = loc("reduce.920")
#loc496 = loc("divide.952")
#loc497 = loc("reshape.779")
#loc498 = loc("reshape.782")
#loc499 = loc("broadcast.956")
#loc500 = loc("add.957")
#loc501 = loc("logistic.958")
#loc502 = loc("broadcast.988")
#loc503 = loc("multiply.989")
#loc504 = loc("reshape.990")
#loc505 = loc("concatenate.992")
#loc506 = loc("convolution.993")
#loc507 = loc("reshape.771")
#loc508 = loc("reshape.773")
#loc509 = loc("reshape.767")
#loc510 = loc("reshape.769")
#loc511 = loc("reshape.763")
#loc512 = loc("reshape.765")
#loc513 = loc("batch-norm-inference.994")
#loc514 = loc("logistic.999")
#loc515 = loc("multiply.1000")
