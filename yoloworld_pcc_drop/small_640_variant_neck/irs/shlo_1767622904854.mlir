#loc1 = loc("p0.1")
#loc2 = loc("p1.5")
#loc3 = loc("p2.9")
#loc4 = loc("p3.13")
#loc5 = loc("p4.17")
#loc6 = loc("p5.21")
#loc7 = loc("p6.31")
#loc8 = loc("p7.35")
#loc9 = loc("p8.40")
#loc10 = loc("p9.50")
#loc11 = loc("p10.54")
#loc12 = loc("p11.58")
#loc13 = loc("p12.62")
#loc14 = loc("p13.66")
#loc15 = loc("p14.68")
#loc16 = loc("p15.72")
#loc17 = loc("p16.76")
#loc18 = loc("p17.80")
#loc19 = loc("p18.84")
#loc20 = loc("p19.86")
#loc21 = loc("p20.90")
#loc22 = loc("p21.94")
#loc23 = loc("p22.98")
#loc24 = loc("p23.102")
#loc25 = loc("p24.104")
#loc26 = loc("p25.136")
#loc27 = loc("p26.140")
#loc28 = loc("p27.144")
#loc29 = loc("p28.148")
#loc30 = loc("p29.152")
#loc31 = loc("p30.156")
#loc32 = loc("p31.166")
#loc33 = loc("p32.170")
#loc34 = loc("p33.183")
#loc35 = loc("p34.187")
#loc36 = loc("p35.191")
#loc37 = loc("p36.195")
#loc38 = loc("p37.199")
#loc39 = loc("p38.201")
#loc40 = loc("p39.205")
#loc41 = loc("p40.209")
#loc42 = loc("p41.213")
#loc43 = loc("p42.217")
#loc44 = loc("p43.219")
#loc45 = loc("p44.223")
#loc46 = loc("p45.227")
#loc47 = loc("p46.231")
#loc48 = loc("p47.235")
#loc49 = loc("p48.237")
#loc50 = loc("p49.269")
#loc51 = loc("p50.354")
#loc52 = loc("p51.358")
#loc53 = loc("p52.362")
#loc54 = loc("p53.366")
#loc55 = loc("p54.370")
#loc56 = loc("p55.476")
#loc57 = loc("p56.480")
#loc58 = loc("p57.484")
#loc59 = loc("p58.488")
#loc60 = loc("p59.492")
#loc61 = loc("p60.515")
#loc62 = loc("p61.519")
#loc63 = loc("p62.523")
#loc64 = loc("p63.527")
#loc65 = loc("p64.531")
#loc66 = loc("p65.535")
#loc67 = loc("p66.545")
#loc68 = loc("p67.549")
#loc69 = loc("p68.562")
#loc70 = loc("p69.566")
#loc71 = loc("p70.570")
#loc72 = loc("p71.574")
#loc73 = loc("p72.578")
#loc74 = loc("p73.580")
#loc75 = loc("p74.584")
#loc76 = loc("p75.588")
#loc77 = loc("p76.592")
#loc78 = loc("p77.596")
#loc79 = loc("p78.598")
#loc80 = loc("p79.602")
#loc81 = loc("p80.606")
#loc82 = loc("p81.610")
#loc83 = loc("p82.614")
#loc84 = loc("p83.616")
#loc85 = loc("p84.620")
#loc86 = loc("p85.624")
#loc87 = loc("p86.628")
#loc88 = loc("p87.632")
#loc89 = loc("p88.719")
#loc90 = loc("p89.723")
#loc91 = loc("p90.727")
#loc92 = loc("p91.731")
#loc93 = loc("p92.735")
#loc94 = loc("p93.758")
#loc95 = loc("p94.762")
#loc96 = loc("p95.766")
#loc97 = loc("p96.770")
#loc98 = loc("p97.774")
#loc99 = loc("p98.778")
#loc100 = loc("p99.788")
#loc101 = loc("p100.792")
#loc102 = loc("p101.805")
#loc103 = loc("p102.809")
#loc104 = loc("p103.813")
#loc105 = loc("p104.817")
#loc106 = loc("p105.821")
#loc107 = loc("p106.823")
#loc108 = loc("p107.827")
#loc109 = loc("p108.831")
#loc110 = loc("p109.835")
#loc111 = loc("p110.839")
#loc112 = loc("p111.841")
#loc113 = loc("p112.845")
#loc114 = loc("p113.849")
#loc115 = loc("p114.853")
#loc116 = loc("p115.857")
#loc117 = loc("p116.859")
#loc118 = loc("p117.863")
#loc119 = loc("p118.867")
#loc120 = loc("p119.871")
#loc121 = loc("p120.875")
#loc122 = loc("p121.962")
#loc123 = loc("p122.966")
#loc124 = loc("p123.970")
#loc125 = loc("p124.974")
#loc126 = loc("p125.978")
module @SyncTensorsGraph.1002 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<128xbf16> loc("p0.1"), %arg1: tensor<128xbf16> loc("p1.5"), %arg2: tensor<128xbf16> loc("p2.9"), %arg3: tensor<128xbf16> loc("p3.13"), %arg4: tensor<128x256x1x1xbf16> loc("p4.17"), %arg5: tensor<2xbf16> loc("p5.21"), %arg6: tensor<64xbf16> loc("p6.31"), %arg7: tensor<64x512xbf16> loc("p7.35"), %arg8: tensor<1x3x512xbf16> loc("p8.40"), %arg9: tensor<64xbf16> loc("p9.50"), %arg10: tensor<64xbf16> loc("p10.54"), %arg11: tensor<64xbf16> loc("p11.58"), %arg12: tensor<64xbf16> loc("p12.62"), %arg13: tensor<64x64x3x3xbf16> loc("p13.66"), %arg14: tensor<64xbf16> loc("p14.68"), %arg15: tensor<64xbf16> loc("p15.72"), %arg16: tensor<64xbf16> loc("p16.76"), %arg17: tensor<64xbf16> loc("p17.80"), %arg18: tensor<64x64x3x3xbf16> loc("p18.84"), %arg19: tensor<128xbf16> loc("p19.86"), %arg20: tensor<128xbf16> loc("p20.90"), %arg21: tensor<128xbf16> loc("p21.94"), %arg22: tensor<128xbf16> loc("p22.98"), %arg23: tensor<128x384x1x1xbf16> loc("p23.102"), %arg24: tensor<1x128x80x80xbf16> loc("p24.104"), %arg25: tensor<256xbf16> loc("p25.136"), %arg26: tensor<256xbf16> loc("p26.140"), %arg27: tensor<256xbf16> loc("p27.144"), %arg28: tensor<256xbf16> loc("p28.148"), %arg29: tensor<256x512x1x1xbf16> loc("p29.152"), %arg30: tensor<4xbf16> loc("p30.156"), %arg31: tensor<128xbf16> loc("p31.166"), %arg32: tensor<128x512xbf16> loc("p32.170"), %arg33: tensor<128xbf16> loc("p33.183"), %arg34: tensor<128xbf16> loc("p34.187"), %arg35: tensor<128xbf16> loc("p35.191"), %arg36: tensor<128xbf16> loc("p36.195"), %arg37: tensor<128x128x3x3xbf16> loc("p37.199"), %arg38: tensor<128xbf16> loc("p38.201"), %arg39: tensor<128xbf16> loc("p39.205"), %arg40: tensor<128xbf16> loc("p40.209"), %arg41: tensor<128xbf16> loc("p41.213"), %arg42: tensor<128x128x3x3xbf16> loc("p42.217"), %arg43: tensor<256xbf16> loc("p43.219"), %arg44: tensor<256xbf16> loc("p44.223"), %arg45: tensor<256xbf16> loc("p45.227"), %arg46: tensor<256xbf16> loc("p46.231"), %arg47: tensor<256x768x1x1xbf16> loc("p47.235"), %arg48: tensor<1x256x40x40xbf16> loc("p48.237"), %arg49: tensor<1x512x20x20xbf16> loc("p49.269"), %arg50: tensor<128xbf16> loc("p50.354"), %arg51: tensor<128xbf16> loc("p51.358"), %arg52: tensor<128xbf16> loc("p52.362"), %arg53: tensor<128xbf16> loc("p53.366"), %arg54: tensor<128x128x3x3xbf16> loc("p54.370"), %arg55: tensor<64xbf16> loc("p55.476"), %arg56: tensor<64xbf16> loc("p56.480"), %arg57: tensor<64xbf16> loc("p57.484"), %arg58: tensor<64xbf16> loc("p58.488"), %arg59: tensor<64x64x3x3xbf16> loc("p59.492"), %arg60: tensor<256xbf16> loc("p60.515"), %arg61: tensor<256xbf16> loc("p61.519"), %arg62: tensor<256xbf16> loc("p62.523"), %arg63: tensor<256xbf16> loc("p63.527"), %arg64: tensor<256x512x1x1xbf16> loc("p64.531"), %arg65: tensor<4xbf16> loc("p65.535"), %arg66: tensor<128xbf16> loc("p66.545"), %arg67: tensor<128x512xbf16> loc("p67.549"), %arg68: tensor<128xbf16> loc("p68.562"), %arg69: tensor<128xbf16> loc("p69.566"), %arg70: tensor<128xbf16> loc("p70.570"), %arg71: tensor<128xbf16> loc("p71.574"), %arg72: tensor<128x128x3x3xbf16> loc("p72.578"), %arg73: tensor<128xbf16> loc("p73.580"), %arg74: tensor<128xbf16> loc("p74.584"), %arg75: tensor<128xbf16> loc("p75.588"), %arg76: tensor<128xbf16> loc("p76.592"), %arg77: tensor<128x128x3x3xbf16> loc("p77.596"), %arg78: tensor<256xbf16> loc("p78.598"), %arg79: tensor<256xbf16> loc("p79.602"), %arg80: tensor<256xbf16> loc("p80.606"), %arg81: tensor<256xbf16> loc("p81.610"), %arg82: tensor<256x384x1x1xbf16> loc("p82.614"), %arg83: tensor<128xbf16> loc("p83.616"), %arg84: tensor<128xbf16> loc("p84.620"), %arg85: tensor<128xbf16> loc("p85.624"), %arg86: tensor<128xbf16> loc("p86.628"), %arg87: tensor<128x128x3x3xbf16> loc("p87.632"), %arg88: tensor<128xbf16> loc("p88.719"), %arg89: tensor<128xbf16> loc("p89.723"), %arg90: tensor<128xbf16> loc("p90.727"), %arg91: tensor<128xbf16> loc("p91.731"), %arg92: tensor<128x128x3x3xbf16> loc("p92.735"), %arg93: tensor<512xbf16> loc("p93.758"), %arg94: tensor<512xbf16> loc("p94.762"), %arg95: tensor<512xbf16> loc("p95.766"), %arg96: tensor<512xbf16> loc("p96.770"), %arg97: tensor<512x1024x1x1xbf16> loc("p97.774"), %arg98: tensor<8xbf16> loc("p98.778"), %arg99: tensor<256xbf16> loc("p99.788"), %arg100: tensor<256x512xbf16> loc("p100.792"), %arg101: tensor<256xbf16> loc("p101.805"), %arg102: tensor<256xbf16> loc("p102.809"), %arg103: tensor<256xbf16> loc("p103.813"), %arg104: tensor<256xbf16> loc("p104.817"), %arg105: tensor<256x256x3x3xbf16> loc("p105.821"), %arg106: tensor<256xbf16> loc("p106.823"), %arg107: tensor<256xbf16> loc("p107.827"), %arg108: tensor<256xbf16> loc("p108.831"), %arg109: tensor<256xbf16> loc("p109.835"), %arg110: tensor<256x256x3x3xbf16> loc("p110.839"), %arg111: tensor<512xbf16> loc("p111.841"), %arg112: tensor<512xbf16> loc("p112.845"), %arg113: tensor<512xbf16> loc("p113.849"), %arg114: tensor<512xbf16> loc("p114.853"), %arg115: tensor<512x768x1x1xbf16> loc("p115.857"), %arg116: tensor<256xbf16> loc("p116.859"), %arg117: tensor<256xbf16> loc("p117.863"), %arg118: tensor<256xbf16> loc("p118.867"), %arg119: tensor<256xbf16> loc("p119.871"), %arg120: tensor<256x256x3x3xbf16> loc("p120.875"), %arg121: tensor<256xbf16> loc("p121.962"), %arg122: tensor<256xbf16> loc("p122.966"), %arg123: tensor<256xbf16> loc("p123.970"), %arg124: tensor<256xbf16> loc("p124.974"), %arg125: tensor<256x256x3x3xbf16> loc("p125.978")) -> (tensor<1x128x80x80xbf16>, tensor<1x256x40x40xbf16>, tensor<1x512x20x20xbf16>) {
    %cst = stablehlo.constant dense<5.656250e+00> : tensor<1x8x20x20xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<5.656250e+00> : tensor<1x2x80x80xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<[0.000000e+00, 5.000000e-01, 1.000000e+00, 1.500000e+00, 2.000000e+00, 2.500000e+00, 3.000000e+00, 3.500000e+00, 4.000000e+00, 4.500000e+00, 5.000000e+00, 5.500000e+00, 6.000000e+00, 6.500000e+00, 7.000000e+00, 7.500000e+00, 8.000000e+00, 8.500000e+00, 9.000000e+00, 9.500000e+00, 1.000000e+01, 1.050000e+01, 1.100000e+01, 1.150000e+01, 1.200000e+01, 1.250000e+01, 1.300000e+01, 1.350000e+01, 1.400000e+01, 1.450000e+01, 1.500000e+01, 1.550000e+01, 1.600000e+01, 1.650000e+01, 1.700000e+01, 1.750000e+01, 1.800000e+01, 1.850000e+01, 1.900000e+01, 1.950000e+01, 2.000000e+01, 2.050000e+01, 2.100000e+01, 2.150000e+01, 2.200000e+01, 2.250000e+01, 2.300000e+01, 2.350000e+01, 2.400000e+01, 2.450000e+01, 2.500000e+01, 2.550000e+01, 2.600000e+01, 2.650000e+01, 2.700000e+01, 2.750000e+01, 2.800000e+01, 2.850000e+01, 2.900000e+01, 2.950000e+01, 3.000000e+01, 3.050000e+01, 3.100000e+01, 3.150000e+01, 3.200000e+01, 3.250000e+01, 3.300000e+01, 3.350000e+01, 3.400000e+01, 3.450000e+01, 3.500000e+01, 3.550000e+01, 3.600000e+01, 3.650000e+01, 3.700000e+01, 3.750000e+01, 3.800000e+01, 3.850000e+01, 3.900000e+01, 3.950000e+01]> : tensor<80xf64> loc(#loc)
    %cst_2 = stablehlo.constant dense<5.656250e+00> : tensor<1x4x40x40xbf16> loc(#loc)
    %cst_3 = stablehlo.constant dense<[0.000000e+00, 5.000000e-01, 1.000000e+00, 1.500000e+00, 2.000000e+00, 2.500000e+00, 3.000000e+00, 3.500000e+00, 4.000000e+00, 4.500000e+00, 5.000000e+00, 5.500000e+00, 6.000000e+00, 6.500000e+00, 7.000000e+00, 7.500000e+00, 8.000000e+00, 8.500000e+00, 9.000000e+00, 9.500000e+00, 1.000000e+01, 1.050000e+01, 1.100000e+01, 1.150000e+01, 1.200000e+01, 1.250000e+01, 1.300000e+01, 1.350000e+01, 1.400000e+01, 1.450000e+01, 1.500000e+01, 1.550000e+01, 1.600000e+01, 1.650000e+01, 1.700000e+01, 1.750000e+01, 1.800000e+01, 1.850000e+01, 1.900000e+01, 1.950000e+01]> : tensor<40xf64> loc(#loc)
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]> : tensor<40xi64> loc(#loc)
    %cst_4 = stablehlo.constant dense<0xFF80> : tensor<bf16> loc(#loc)
    %c_5 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]> : tensor<20xi64> loc(#loc)
    %0 = stablehlo.reshape %arg43 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc127)
    %1 = stablehlo.custom_call @tt.mark_argument(%0) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_top_down_layers_0_main_conv_batch_norm2d_running_var"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc128)
    %2 = stablehlo.reshape %1 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc129)
    %3 = stablehlo.reshape %arg38 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc130)
    %4 = stablehlo.custom_call @tt.mark_argument(%3) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv1_batch_norm2d_running_var"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc131)
    %5 = stablehlo.reshape %4 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc132)
    %6 = stablehlo.reshape %arg33 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc133)
    %7 = stablehlo.custom_call @tt.mark_argument(%6) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv2_batch_norm2d_running_var"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc134)
    %8 = stablehlo.reshape %7 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc135)
    %9 = stablehlo.custom_call @tt.mark_argument(%arg49) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_2"}} : (tensor<1x512x20x20xbf16>) -> tensor<1x512x20x20xbf16> loc(#loc136)
    %10 = stablehlo.transpose %9, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,512,20,20]{2,3,1,0}"} : (tensor<1x512x20x20xbf16>) -> tensor<1x512x20x20xbf16> loc(#loc137)
    %11 = stablehlo.floor %cst_3 : tensor<40xf64> loc(#loc138)
    %12 = stablehlo.convert %11 : (tensor<40xf64>) -> tensor<40xi64> loc(#loc139)
    %13 = stablehlo.broadcast_in_dim %12, dims = [0] : (tensor<40xi64>) -> tensor<40x20xi64> loc(#loc140)
    %14 = stablehlo.broadcast_in_dim %c_5, dims = [1] : (tensor<20xi64>) -> tensor<40x20xi64> loc(#loc141)
    %15 = stablehlo.compare  EQ, %13, %14 : (tensor<40x20xi64>, tensor<40x20xi64>) -> tensor<40x20xi1> loc(#loc142)
    %16 = stablehlo.convert %15 : (tensor<40x20xi1>) -> tensor<40x20xi64> loc(#loc143)
    %17 = stablehlo.transpose %16, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "s64[20,40]{0,1}"} : (tensor<40x20xi64>) -> tensor<20x40xi64> loc(#loc144)
    %18 = stablehlo.convert %17 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[20,40]{0,1}"} : (tensor<20x40xi64>) -> tensor<20x40xbf16> loc(#loc145)
    %19 = stablehlo.dot_general %10, %18, contracting_dims = [3] x [0] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x512x20x20xbf16>, tensor<20x40xbf16>) -> tensor<1x512x20x40xbf16> loc(#loc146)
    %20 = stablehlo.transpose %19, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,512,40,20]{2,3,1,0}"} : (tensor<1x512x20x40xbf16>) -> tensor<1x512x40x20xbf16> loc(#loc147)
    %21 = stablehlo.dot_general %20, %18, contracting_dims = [3] x [0] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x512x40x20xbf16>, tensor<20x40xbf16>) -> tensor<1x512x40x40xbf16> loc(#loc148)
    %22 = stablehlo.custom_call @tt.mark_argument(%arg48) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_1"}} : (tensor<1x256x40x40xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc149)
    %23 = stablehlo.concatenate %21, %22, dim = 1 : (tensor<1x512x40x40xbf16>, tensor<1x256x40x40xbf16>) -> tensor<1x768x40x40xbf16> loc(#loc150)
    %24 = stablehlo.custom_call @tt.mark_argument(%arg47) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_0_main_conv_conv_weight"}} : (tensor<256x768x1x1xbf16>) -> tensor<256x768x1x1xbf16> loc(#loc151)
    %25 = stablehlo.convolution(%23, %24) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x768x40x40xbf16>, tensor<256x768x1x1xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc152)
    %26 = stablehlo.reshape %arg46 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc153)
    %27 = stablehlo.custom_call @tt.mark_argument(%26) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_0_main_conv_batch_norm2d_weight"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc154)
    %28 = stablehlo.reshape %27 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc155)
    %29 = stablehlo.reshape %arg45 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc156)
    %30 = stablehlo.custom_call @tt.mark_argument(%29) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_0_main_conv_batch_norm2d_bias"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc157)
    %31 = stablehlo.reshape %30 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc158)
    %32 = stablehlo.reshape %arg44 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc159)
    %33 = stablehlo.custom_call @tt.mark_argument(%32) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_top_down_layers_0_main_conv_batch_norm2d_running_mean"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc160)
    %34 = stablehlo.reshape %33 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc161)
    %35 = "stablehlo.batch_norm_inference"(%25, %28, %31, %34, %2) <{epsilon = 1.000000e-03 : f32, feature_index = 1 : i64}> : (tensor<1x256x40x40xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc162)
    %36 = stablehlo.logistic %35 : tensor<1x256x40x40xbf16> loc(#loc163)
    %37 = stablehlo.multiply %35, %36 : tensor<1x256x40x40xbf16> loc(#loc164)
    %38 = stablehlo.slice %37 [0:1, 128:256, 0:40, 0:40] : (tensor<1x256x40x40xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc165)
    %39 = stablehlo.custom_call @tt.mark_argument(%arg42) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv1_conv_weight"}} : (tensor<128x128x3x3xbf16>) -> tensor<128x128x3x3xbf16> loc(#loc166)
    %40 = stablehlo.convolution(%38, %39) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {pad = [[1, 1], [1, 1]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x128x40x40xbf16>, tensor<128x128x3x3xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc167)
    %41 = stablehlo.reshape %arg41 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc168)
    %42 = stablehlo.custom_call @tt.mark_argument(%41) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv1_batch_norm2d_weight"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc169)
    %43 = stablehlo.reshape %42 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc170)
    %44 = stablehlo.reshape %arg40 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc171)
    %45 = stablehlo.custom_call @tt.mark_argument(%44) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv1_batch_norm2d_bias"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc172)
    %46 = stablehlo.reshape %45 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc173)
    %47 = stablehlo.reshape %arg39 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc174)
    %48 = stablehlo.custom_call @tt.mark_argument(%47) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv1_batch_norm2d_running_mean"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc175)
    %49 = stablehlo.reshape %48 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc176)
    %50 = "stablehlo.batch_norm_inference"(%40, %43, %46, %49, %5) <{epsilon = 1.000000e-03 : f32, feature_index = 1 : i64}> : (tensor<1x128x40x40xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc177)
    %51 = stablehlo.logistic %50 : tensor<1x128x40x40xbf16> loc(#loc178)
    %52 = stablehlo.multiply %50, %51 : tensor<1x128x40x40xbf16> loc(#loc179)
    %53 = stablehlo.custom_call @tt.mark_argument(%arg37) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv2_conv_weight"}} : (tensor<128x128x3x3xbf16>) -> tensor<128x128x3x3xbf16> loc(#loc180)
    %54 = stablehlo.convolution(%52, %53) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {pad = [[1, 1], [1, 1]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x128x40x40xbf16>, tensor<128x128x3x3xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc181)
    %55 = stablehlo.reshape %arg36 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc182)
    %56 = stablehlo.custom_call @tt.mark_argument(%55) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv2_batch_norm2d_weight"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc183)
    %57 = stablehlo.reshape %56 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc184)
    %58 = stablehlo.reshape %arg35 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc185)
    %59 = stablehlo.custom_call @tt.mark_argument(%58) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv2_batch_norm2d_bias"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc186)
    %60 = stablehlo.reshape %59 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc187)
    %61 = stablehlo.reshape %arg34 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc188)
    %62 = stablehlo.custom_call @tt.mark_argument(%61) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_top_down_layers_0_blocks_0_conv2_batch_norm2d_running_mean"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc189)
    %63 = stablehlo.reshape %62 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc190)
    %64 = "stablehlo.batch_norm_inference"(%54, %57, %60, %63, %8) <{epsilon = 1.000000e-03 : f32, feature_index = 1 : i64}> : (tensor<1x128x40x40xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc191)
    %65 = stablehlo.logistic %64 : tensor<1x128x40x40xbf16> loc(#loc192)
    %66 = stablehlo.multiply %64, %65 : tensor<1x128x40x40xbf16> loc(#loc193)
    %67 = stablehlo.reshape %66 : (tensor<1x128x40x40xbf16>) -> tensor<1x4x32x40x40xbf16> loc(#loc194)
    %68 = stablehlo.custom_call @tt.mark_argument(%arg8) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_3"}} : (tensor<1x3x512xbf16>) -> tensor<1x3x512xbf16> loc(#loc195)
    %69 = stablehlo.reshape %68 : (tensor<1x3x512xbf16>) -> tensor<3x512xbf16> loc(#loc196)
    %70 = stablehlo.reshape %arg32 : (tensor<128x512xbf16>) -> tensor<1x128x512xbf16> loc(#loc197)
    %71 = stablehlo.custom_call @tt.mark_argument(%70) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_0_attn_block_guide_fc_weight"}} : (tensor<1x128x512xbf16>) -> tensor<1x128x512xbf16> loc(#loc198)
    %72 = stablehlo.reshape %71 : (tensor<1x128x512xbf16>) -> tensor<128x512xbf16> loc(#loc199)
    %73 = stablehlo.transpose %72, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,128]{0,1}"} : (tensor<128x512xbf16>) -> tensor<512x128xbf16> loc(#loc200)
    %74 = stablehlo.dot_general %69, %73, contracting_dims = [1] x [0] : (tensor<3x512xbf16>, tensor<512x128xbf16>) -> tensor<3x128xbf16> loc(#loc201)
    %75 = stablehlo.reshape %74 : (tensor<3x128xbf16>) -> tensor<1x3x128xbf16> loc(#loc202)
    %76 = stablehlo.reshape %arg31 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc203)
    %77 = stablehlo.custom_call @tt.mark_argument(%76) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_0_attn_block_guide_fc_bias"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc204)
    %78 = stablehlo.reshape %77 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc205)
    %79 = stablehlo.broadcast_in_dim %78, dims = [2] : (tensor<128xbf16>) -> tensor<1x3x128xbf16> loc(#loc206)
    %80 = stablehlo.add %75, %79 : tensor<1x3x128xbf16> loc(#loc207)
    %81 = stablehlo.reshape %80 : (tensor<1x3x128xbf16>) -> tensor<1x3x4x32xbf16> loc(#loc208)
    %82 = stablehlo.dot_general %67, %81, batching_dims = [0, 1] x [0, 2], contracting_dims = [2] x [3] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x4x32x40x40xbf16>, tensor<1x3x4x32xbf16>) -> tensor<1x4x40x40x3xbf16> loc(#loc209)
    %83 = stablehlo.reshape %arg50 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc210)
    %84 = stablehlo.custom_call @tt.mark_argument(%83) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_top_down_layers_0_attn_block_project_conv_batch_norm2d_running_var"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc211)
    %85 = stablehlo.reshape %84 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc212)
    %86 = stablehlo.reshape %arg25 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc213)
    %87 = stablehlo.custom_call @tt.mark_argument(%86) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_top_down_layers_0_final_conv_batch_norm2d_running_var"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc214)
    %88 = stablehlo.reshape %87 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc215)
    %89 = stablehlo.reshape %arg19 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc216)
    %90 = stablehlo.custom_call @tt.mark_argument(%89) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_top_down_layers_1_main_conv_batch_norm2d_running_var"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc217)
    %91 = stablehlo.reshape %90 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc218)
    %92 = stablehlo.reshape %arg14 : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc219)
    %93 = stablehlo.custom_call @tt.mark_argument(%92) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv1_batch_norm2d_running_var"}} : (tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc220)
    %94 = stablehlo.reshape %93 : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc221)
    %95 = stablehlo.reshape %arg9 : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc222)
    %96 = stablehlo.custom_call @tt.mark_argument(%95) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv2_batch_norm2d_running_var"}} : (tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc223)
    %97 = stablehlo.reshape %96 : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc224)
    %98 = stablehlo.slice %37 [0:1, 0:128, 0:40, 0:40] : (tensor<1x256x40x40xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc225)
    %99 = stablehlo.custom_call @tt.mark_argument(%arg54) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_0_attn_block_project_conv_conv_weight"}} : (tensor<128x128x3x3xbf16>) -> tensor<128x128x3x3xbf16> loc(#loc226)
    %100 = stablehlo.convolution(%66, %99) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {pad = [[1, 1], [1, 1]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x128x40x40xbf16>, tensor<128x128x3x3xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc227)
    %101 = stablehlo.reshape %arg53 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc228)
    %102 = stablehlo.custom_call @tt.mark_argument(%101) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_0_attn_block_project_conv_batch_norm2d_weight"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc229)
    %103 = stablehlo.reshape %102 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc230)
    %104 = stablehlo.reshape %arg52 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc231)
    %105 = stablehlo.custom_call @tt.mark_argument(%104) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_0_attn_block_project_conv_batch_norm2d_bias"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc232)
    %106 = stablehlo.reshape %105 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc233)
    %107 = stablehlo.reshape %arg51 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc234)
    %108 = stablehlo.custom_call @tt.mark_argument(%107) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_top_down_layers_0_attn_block_project_conv_batch_norm2d_running_mean"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc235)
    %109 = stablehlo.reshape %108 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc236)
    %110 = "stablehlo.batch_norm_inference"(%100, %103, %106, %109, %85) <{epsilon = 1.000000e-03 : f32, feature_index = 1 : i64}> : (tensor<1x128x40x40xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc237)
    %111 = stablehlo.reshape %110 : (tensor<1x128x40x40xbf16>) -> tensor<1x4x32x40x40xbf16> loc(#loc238)
    %112 = stablehlo.reduce(%82 init: %cst_4) applies stablehlo.maximum across dimensions = [4] : (tensor<1x4x40x40x3xbf16>, tensor<bf16>) -> tensor<1x4x40x40xbf16> loc(#loc239)
    %113 = stablehlo.divide %112, %cst_2 : tensor<1x4x40x40xbf16> loc(#loc240)
    %114 = stablehlo.reshape %arg30 : (tensor<4xbf16>) -> tensor<1x1x4xbf16> loc(#loc241)
    %115 = stablehlo.custom_call @tt.mark_argument(%114) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_0_attn_block_bias"}} : (tensor<1x1x4xbf16>) -> tensor<1x1x4xbf16> loc(#loc242)
    %116 = stablehlo.reshape %115 : (tensor<1x1x4xbf16>) -> tensor<1x4xbf16> loc(#loc243)
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1] : (tensor<1x4xbf16>) -> tensor<1x4x40x40xbf16> loc(#loc244)
    %118 = stablehlo.add %113, %117 : tensor<1x4x40x40xbf16> loc(#loc245)
    %119 = stablehlo.logistic %118 : tensor<1x4x40x40xbf16> loc(#loc246)
    %120 = stablehlo.broadcast_in_dim %119, dims = [0, 1, 3, 4] : (tensor<1x4x40x40xbf16>) -> tensor<1x4x32x40x40xbf16> loc(#loc247)
    %121 = stablehlo.multiply %111, %120 : tensor<1x4x32x40x40xbf16> loc(#loc248)
    %122 = stablehlo.reshape %121 : (tensor<1x4x32x40x40xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc249)
    %123 = stablehlo.concatenate %98, %38, %66, %122, dim = 1 : (tensor<1x128x40x40xbf16>, tensor<1x128x40x40xbf16>, tensor<1x128x40x40xbf16>, tensor<1x128x40x40xbf16>) -> tensor<1x512x40x40xbf16> loc(#loc250)
    %124 = stablehlo.custom_call @tt.mark_argument(%arg29) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_0_final_conv_conv_weight"}} : (tensor<256x512x1x1xbf16>) -> tensor<256x512x1x1xbf16> loc(#loc251)
    %125 = stablehlo.convolution(%123, %124) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x512x40x40xbf16>, tensor<256x512x1x1xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc252)
    %126 = stablehlo.reshape %arg28 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc253)
    %127 = stablehlo.custom_call @tt.mark_argument(%126) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_0_final_conv_batch_norm2d_weight"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc254)
    %128 = stablehlo.reshape %127 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc255)
    %129 = stablehlo.reshape %arg27 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc256)
    %130 = stablehlo.custom_call @tt.mark_argument(%129) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_0_final_conv_batch_norm2d_bias"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc257)
    %131 = stablehlo.reshape %130 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc258)
    %132 = stablehlo.reshape %arg26 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc259)
    %133 = stablehlo.custom_call @tt.mark_argument(%132) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_top_down_layers_0_final_conv_batch_norm2d_running_mean"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc260)
    %134 = stablehlo.reshape %133 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc261)
    %135 = "stablehlo.batch_norm_inference"(%125, %128, %131, %134, %88) <{epsilon = 1.000000e-03 : f32, feature_index = 1 : i64}> : (tensor<1x256x40x40xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc262)
    %136 = stablehlo.logistic %135 : tensor<1x256x40x40xbf16> loc(#loc263)
    %137 = stablehlo.multiply %135, %136 : tensor<1x256x40x40xbf16> loc(#loc264)
    %138 = stablehlo.transpose %137, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,256,40,40]{2,3,1,0}"} : (tensor<1x256x40x40xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc265)
    %139 = stablehlo.floor %cst_1 : tensor<80xf64> loc(#loc266)
    %140 = stablehlo.convert %139 : (tensor<80xf64>) -> tensor<80xi64> loc(#loc267)
    %141 = stablehlo.broadcast_in_dim %140, dims = [0] : (tensor<80xi64>) -> tensor<80x40xi64> loc(#loc268)
    %142 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<40xi64>) -> tensor<80x40xi64> loc(#loc269)
    %143 = stablehlo.compare  EQ, %141, %142 : (tensor<80x40xi64>, tensor<80x40xi64>) -> tensor<80x40xi1> loc(#loc270)
    %144 = stablehlo.convert %143 : (tensor<80x40xi1>) -> tensor<80x40xi64> loc(#loc271)
    %145 = stablehlo.transpose %144, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "s64[40,80]{0,1}"} : (tensor<80x40xi64>) -> tensor<40x80xi64> loc(#loc272)
    %146 = stablehlo.convert %145 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[40,80]{0,1}"} : (tensor<40x80xi64>) -> tensor<40x80xbf16> loc(#loc273)
    %147 = stablehlo.dot_general %138, %146, contracting_dims = [3] x [0] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x256x40x40xbf16>, tensor<40x80xbf16>) -> tensor<1x256x40x80xbf16> loc(#loc274)
    %148 = stablehlo.transpose %147, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,256,80,40]{2,3,1,0}"} : (tensor<1x256x40x80xbf16>) -> tensor<1x256x80x40xbf16> loc(#loc275)
    %149 = stablehlo.dot_general %148, %146, contracting_dims = [3] x [0] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x256x80x40xbf16>, tensor<40x80xbf16>) -> tensor<1x256x80x80xbf16> loc(#loc276)
    %150 = stablehlo.custom_call @tt.mark_argument(%arg24) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_0"}} : (tensor<1x128x80x80xbf16>) -> tensor<1x128x80x80xbf16> loc(#loc277)
    %151 = stablehlo.concatenate %149, %150, dim = 1 : (tensor<1x256x80x80xbf16>, tensor<1x128x80x80xbf16>) -> tensor<1x384x80x80xbf16> loc(#loc278)
    %152 = stablehlo.custom_call @tt.mark_argument(%arg23) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_1_main_conv_conv_weight"}} : (tensor<128x384x1x1xbf16>) -> tensor<128x384x1x1xbf16> loc(#loc279)
    %153 = stablehlo.convolution(%151, %152) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x384x80x80xbf16>, tensor<128x384x1x1xbf16>) -> tensor<1x128x80x80xbf16> loc(#loc280)
    %154 = stablehlo.reshape %arg22 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc281)
    %155 = stablehlo.custom_call @tt.mark_argument(%154) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_1_main_conv_batch_norm2d_weight"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc282)
    %156 = stablehlo.reshape %155 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc283)
    %157 = stablehlo.reshape %arg21 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc284)
    %158 = stablehlo.custom_call @tt.mark_argument(%157) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_1_main_conv_batch_norm2d_bias"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc285)
    %159 = stablehlo.reshape %158 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc286)
    %160 = stablehlo.reshape %arg20 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc287)
    %161 = stablehlo.custom_call @tt.mark_argument(%160) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_top_down_layers_1_main_conv_batch_norm2d_running_mean"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc288)
    %162 = stablehlo.reshape %161 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc289)
    %163 = "stablehlo.batch_norm_inference"(%153, %156, %159, %162, %91) <{epsilon = 1.000000e-03 : f32, feature_index = 1 : i64}> : (tensor<1x128x80x80xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>) -> tensor<1x128x80x80xbf16> loc(#loc290)
    %164 = stablehlo.logistic %163 : tensor<1x128x80x80xbf16> loc(#loc291)
    %165 = stablehlo.multiply %163, %164 : tensor<1x128x80x80xbf16> loc(#loc292)
    %166 = stablehlo.slice %165 [0:1, 64:128, 0:80, 0:80] : (tensor<1x128x80x80xbf16>) -> tensor<1x64x80x80xbf16> loc(#loc293)
    %167 = stablehlo.custom_call @tt.mark_argument(%arg18) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv1_conv_weight"}} : (tensor<64x64x3x3xbf16>) -> tensor<64x64x3x3xbf16> loc(#loc294)
    %168 = stablehlo.convolution(%166, %167) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {pad = [[1, 1], [1, 1]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x64x80x80xbf16>, tensor<64x64x3x3xbf16>) -> tensor<1x64x80x80xbf16> loc(#loc295)
    %169 = stablehlo.reshape %arg17 : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc296)
    %170 = stablehlo.custom_call @tt.mark_argument(%169) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv1_batch_norm2d_weight"}} : (tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc297)
    %171 = stablehlo.reshape %170 : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc298)
    %172 = stablehlo.reshape %arg16 : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc299)
    %173 = stablehlo.custom_call @tt.mark_argument(%172) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv1_batch_norm2d_bias"}} : (tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc300)
    %174 = stablehlo.reshape %173 : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc301)
    %175 = stablehlo.reshape %arg15 : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc302)
    %176 = stablehlo.custom_call @tt.mark_argument(%175) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv1_batch_norm2d_running_mean"}} : (tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc303)
    %177 = stablehlo.reshape %176 : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc304)
    %178 = "stablehlo.batch_norm_inference"(%168, %171, %174, %177, %94) <{epsilon = 1.000000e-03 : f32, feature_index = 1 : i64}> : (tensor<1x64x80x80xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>) -> tensor<1x64x80x80xbf16> loc(#loc305)
    %179 = stablehlo.logistic %178 : tensor<1x64x80x80xbf16> loc(#loc306)
    %180 = stablehlo.multiply %178, %179 : tensor<1x64x80x80xbf16> loc(#loc307)
    %181 = stablehlo.custom_call @tt.mark_argument(%arg13) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv2_conv_weight"}} : (tensor<64x64x3x3xbf16>) -> tensor<64x64x3x3xbf16> loc(#loc308)
    %182 = stablehlo.convolution(%180, %181) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {pad = [[1, 1], [1, 1]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x64x80x80xbf16>, tensor<64x64x3x3xbf16>) -> tensor<1x64x80x80xbf16> loc(#loc309)
    %183 = stablehlo.reshape %arg12 : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc310)
    %184 = stablehlo.custom_call @tt.mark_argument(%183) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv2_batch_norm2d_weight"}} : (tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc311)
    %185 = stablehlo.reshape %184 : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc312)
    %186 = stablehlo.reshape %arg11 : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc313)
    %187 = stablehlo.custom_call @tt.mark_argument(%186) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv2_batch_norm2d_bias"}} : (tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc314)
    %188 = stablehlo.reshape %187 : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc315)
    %189 = stablehlo.reshape %arg10 : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc316)
    %190 = stablehlo.custom_call @tt.mark_argument(%189) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_top_down_layers_1_blocks_0_conv2_batch_norm2d_running_mean"}} : (tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc317)
    %191 = stablehlo.reshape %190 : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc318)
    %192 = "stablehlo.batch_norm_inference"(%182, %185, %188, %191, %97) <{epsilon = 1.000000e-03 : f32, feature_index = 1 : i64}> : (tensor<1x64x80x80xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>) -> tensor<1x64x80x80xbf16> loc(#loc319)
    %193 = stablehlo.logistic %192 : tensor<1x64x80x80xbf16> loc(#loc320)
    %194 = stablehlo.multiply %192, %193 : tensor<1x64x80x80xbf16> loc(#loc321)
    %195 = stablehlo.reshape %194 : (tensor<1x64x80x80xbf16>) -> tensor<1x2x32x80x80xbf16> loc(#loc322)
    %196 = stablehlo.reshape %arg7 : (tensor<64x512xbf16>) -> tensor<1x64x512xbf16> loc(#loc323)
    %197 = stablehlo.custom_call @tt.mark_argument(%196) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_1_attn_block_guide_fc_weight"}} : (tensor<1x64x512xbf16>) -> tensor<1x64x512xbf16> loc(#loc324)
    %198 = stablehlo.reshape %197 : (tensor<1x64x512xbf16>) -> tensor<64x512xbf16> loc(#loc325)
    %199 = stablehlo.transpose %198, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,64]{0,1}"} : (tensor<64x512xbf16>) -> tensor<512x64xbf16> loc(#loc326)
    %200 = stablehlo.dot_general %69, %199, contracting_dims = [1] x [0] : (tensor<3x512xbf16>, tensor<512x64xbf16>) -> tensor<3x64xbf16> loc(#loc327)
    %201 = stablehlo.reshape %200 : (tensor<3x64xbf16>) -> tensor<1x3x64xbf16> loc(#loc328)
    %202 = stablehlo.reshape %arg6 : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc329)
    %203 = stablehlo.custom_call @tt.mark_argument(%202) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_1_attn_block_guide_fc_bias"}} : (tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc330)
    %204 = stablehlo.reshape %203 : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc331)
    %205 = stablehlo.broadcast_in_dim %204, dims = [2] : (tensor<64xbf16>) -> tensor<1x3x64xbf16> loc(#loc332)
    %206 = stablehlo.add %201, %205 : tensor<1x3x64xbf16> loc(#loc333)
    %207 = stablehlo.reshape %206 : (tensor<1x3x64xbf16>) -> tensor<1x3x2x32xbf16> loc(#loc334)
    %208 = stablehlo.dot_general %195, %207, batching_dims = [0, 1] x [0, 2], contracting_dims = [2] x [3] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x2x32x80x80xbf16>, tensor<1x3x2x32xbf16>) -> tensor<1x2x80x80x3xbf16> loc(#loc335)
    %209 = stablehlo.reshape %arg55 : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc336)
    %210 = stablehlo.custom_call @tt.mark_argument(%209) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_top_down_layers_1_attn_block_project_conv_batch_norm2d_running_var"}} : (tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc337)
    %211 = stablehlo.reshape %210 : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc338)
    %212 = stablehlo.reshape %arg0 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc339)
    %213 = stablehlo.custom_call @tt.mark_argument(%212) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_top_down_layers_1_final_conv_batch_norm2d_running_var"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc340)
    %214 = stablehlo.reshape %213 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc341)
    %215 = stablehlo.reshape %arg83 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc342)
    %216 = stablehlo.custom_call @tt.mark_argument(%215) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_downsample_layers_0_batch_norm2d_running_var"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc343)
    %217 = stablehlo.reshape %216 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc344)
    %218 = stablehlo.reshape %arg78 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc345)
    %219 = stablehlo.custom_call @tt.mark_argument(%218) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_bottom_up_layers_0_main_conv_batch_norm2d_running_var"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc346)
    %220 = stablehlo.reshape %219 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc347)
    %221 = stablehlo.reshape %arg73 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc348)
    %222 = stablehlo.custom_call @tt.mark_argument(%221) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv1_batch_norm2d_running_var"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc349)
    %223 = stablehlo.reshape %222 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc350)
    %224 = stablehlo.reshape %arg68 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc351)
    %225 = stablehlo.custom_call @tt.mark_argument(%224) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv2_batch_norm2d_running_var"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc352)
    %226 = stablehlo.reshape %225 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc353)
    %227 = stablehlo.slice %165 [0:1, 0:64, 0:80, 0:80] : (tensor<1x128x80x80xbf16>) -> tensor<1x64x80x80xbf16> loc(#loc354)
    %228 = stablehlo.custom_call @tt.mark_argument(%arg59) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_1_attn_block_project_conv_conv_weight"}} : (tensor<64x64x3x3xbf16>) -> tensor<64x64x3x3xbf16> loc(#loc355)
    %229 = stablehlo.convolution(%194, %228) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {pad = [[1, 1], [1, 1]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x64x80x80xbf16>, tensor<64x64x3x3xbf16>) -> tensor<1x64x80x80xbf16> loc(#loc356)
    %230 = stablehlo.reshape %arg58 : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc357)
    %231 = stablehlo.custom_call @tt.mark_argument(%230) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_1_attn_block_project_conv_batch_norm2d_weight"}} : (tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc358)
    %232 = stablehlo.reshape %231 : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc359)
    %233 = stablehlo.reshape %arg57 : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc360)
    %234 = stablehlo.custom_call @tt.mark_argument(%233) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_1_attn_block_project_conv_batch_norm2d_bias"}} : (tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc361)
    %235 = stablehlo.reshape %234 : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc362)
    %236 = stablehlo.reshape %arg56 : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc363)
    %237 = stablehlo.custom_call @tt.mark_argument(%236) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_top_down_layers_1_attn_block_project_conv_batch_norm2d_running_mean"}} : (tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc364)
    %238 = stablehlo.reshape %237 : (tensor<1x1x64xbf16>) -> tensor<64xbf16> loc(#loc365)
    %239 = "stablehlo.batch_norm_inference"(%229, %232, %235, %238, %211) <{epsilon = 1.000000e-03 : f32, feature_index = 1 : i64}> : (tensor<1x64x80x80xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>) -> tensor<1x64x80x80xbf16> loc(#loc366)
    %240 = stablehlo.reshape %239 : (tensor<1x64x80x80xbf16>) -> tensor<1x2x32x80x80xbf16> loc(#loc367)
    %241 = stablehlo.reduce(%208 init: %cst_4) applies stablehlo.maximum across dimensions = [4] : (tensor<1x2x80x80x3xbf16>, tensor<bf16>) -> tensor<1x2x80x80xbf16> loc(#loc368)
    %242 = stablehlo.divide %241, %cst_0 : tensor<1x2x80x80xbf16> loc(#loc369)
    %243 = stablehlo.reshape %arg5 : (tensor<2xbf16>) -> tensor<1x1x2xbf16> loc(#loc370)
    %244 = stablehlo.custom_call @tt.mark_argument(%243) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_1_attn_block_bias"}} : (tensor<1x1x2xbf16>) -> tensor<1x1x2xbf16> loc(#loc371)
    %245 = stablehlo.reshape %244 : (tensor<1x1x2xbf16>) -> tensor<1x2xbf16> loc(#loc372)
    %246 = stablehlo.broadcast_in_dim %245, dims = [0, 1] : (tensor<1x2xbf16>) -> tensor<1x2x80x80xbf16> loc(#loc373)
    %247 = stablehlo.add %242, %246 : tensor<1x2x80x80xbf16> loc(#loc374)
    %248 = stablehlo.logistic %247 : tensor<1x2x80x80xbf16> loc(#loc375)
    %249 = stablehlo.broadcast_in_dim %248, dims = [0, 1, 3, 4] : (tensor<1x2x80x80xbf16>) -> tensor<1x2x32x80x80xbf16> loc(#loc376)
    %250 = stablehlo.multiply %240, %249 : tensor<1x2x32x80x80xbf16> loc(#loc377)
    %251 = stablehlo.reshape %250 : (tensor<1x2x32x80x80xbf16>) -> tensor<1x64x80x80xbf16> loc(#loc378)
    %252 = stablehlo.concatenate %227, %166, %194, %251, dim = 1 : (tensor<1x64x80x80xbf16>, tensor<1x64x80x80xbf16>, tensor<1x64x80x80xbf16>, tensor<1x64x80x80xbf16>) -> tensor<1x256x80x80xbf16> loc(#loc379)
    %253 = stablehlo.custom_call @tt.mark_argument(%arg4) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_1_final_conv_conv_weight"}} : (tensor<128x256x1x1xbf16>) -> tensor<128x256x1x1xbf16> loc(#loc380)
    %254 = stablehlo.convolution(%252, %253) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x256x80x80xbf16>, tensor<128x256x1x1xbf16>) -> tensor<1x128x80x80xbf16> loc(#loc381)
    %255 = stablehlo.reshape %arg3 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc382)
    %256 = stablehlo.custom_call @tt.mark_argument(%255) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_1_final_conv_batch_norm2d_weight"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc383)
    %257 = stablehlo.reshape %256 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc384)
    %258 = stablehlo.reshape %arg2 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc385)
    %259 = stablehlo.custom_call @tt.mark_argument(%258) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_top_down_layers_1_final_conv_batch_norm2d_bias"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc386)
    %260 = stablehlo.reshape %259 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc387)
    %261 = stablehlo.reshape %arg1 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc388)
    %262 = stablehlo.custom_call @tt.mark_argument(%261) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_top_down_layers_1_final_conv_batch_norm2d_running_mean"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc389)
    %263 = stablehlo.reshape %262 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc390)
    %264 = "stablehlo.batch_norm_inference"(%254, %257, %260, %263, %214) <{epsilon = 1.000000e-03 : f32, feature_index = 1 : i64}> : (tensor<1x128x80x80xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>) -> tensor<1x128x80x80xbf16> loc(#loc391)
    %265 = stablehlo.logistic %264 : tensor<1x128x80x80xbf16> loc(#loc392)
    %266 = stablehlo.multiply %264, %265 : tensor<1x128x80x80xbf16> loc(#loc393)
    %267 = stablehlo.custom_call @tt.mark_argument(%arg87) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_downsample_layers_0_conv_weight"}} : (tensor<128x128x3x3xbf16>) -> tensor<128x128x3x3xbf16> loc(#loc394)
    %268 = stablehlo.convolution(%266, %267) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [2, 2], pad = [[1, 1], [1, 1]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x128x80x80xbf16>, tensor<128x128x3x3xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc395)
    %269 = stablehlo.reshape %arg86 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc396)
    %270 = stablehlo.custom_call @tt.mark_argument(%269) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_downsample_layers_0_batch_norm2d_weight"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc397)
    %271 = stablehlo.reshape %270 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc398)
    %272 = stablehlo.reshape %arg85 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc399)
    %273 = stablehlo.custom_call @tt.mark_argument(%272) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_downsample_layers_0_batch_norm2d_bias"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc400)
    %274 = stablehlo.reshape %273 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc401)
    %275 = stablehlo.reshape %arg84 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc402)
    %276 = stablehlo.custom_call @tt.mark_argument(%275) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_downsample_layers_0_batch_norm2d_running_mean"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc403)
    %277 = stablehlo.reshape %276 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc404)
    %278 = "stablehlo.batch_norm_inference"(%268, %271, %274, %277, %217) <{epsilon = 1.000000e-03 : f32, feature_index = 1 : i64}> : (tensor<1x128x40x40xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc405)
    %279 = stablehlo.logistic %278 : tensor<1x128x40x40xbf16> loc(#loc406)
    %280 = stablehlo.multiply %278, %279 : tensor<1x128x40x40xbf16> loc(#loc407)
    %281 = stablehlo.concatenate %280, %137, dim = 1 : (tensor<1x128x40x40xbf16>, tensor<1x256x40x40xbf16>) -> tensor<1x384x40x40xbf16> loc(#loc408)
    %282 = stablehlo.custom_call @tt.mark_argument(%arg82) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_0_main_conv_conv_weight"}} : (tensor<256x384x1x1xbf16>) -> tensor<256x384x1x1xbf16> loc(#loc409)
    %283 = stablehlo.convolution(%281, %282) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x384x40x40xbf16>, tensor<256x384x1x1xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc410)
    %284 = stablehlo.reshape %arg81 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc411)
    %285 = stablehlo.custom_call @tt.mark_argument(%284) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_0_main_conv_batch_norm2d_weight"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc412)
    %286 = stablehlo.reshape %285 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc413)
    %287 = stablehlo.reshape %arg80 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc414)
    %288 = stablehlo.custom_call @tt.mark_argument(%287) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_0_main_conv_batch_norm2d_bias"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc415)
    %289 = stablehlo.reshape %288 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc416)
    %290 = stablehlo.reshape %arg79 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc417)
    %291 = stablehlo.custom_call @tt.mark_argument(%290) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_bottom_up_layers_0_main_conv_batch_norm2d_running_mean"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc418)
    %292 = stablehlo.reshape %291 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc419)
    %293 = "stablehlo.batch_norm_inference"(%283, %286, %289, %292, %220) <{epsilon = 1.000000e-03 : f32, feature_index = 1 : i64}> : (tensor<1x256x40x40xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc420)
    %294 = stablehlo.logistic %293 : tensor<1x256x40x40xbf16> loc(#loc421)
    %295 = stablehlo.multiply %293, %294 : tensor<1x256x40x40xbf16> loc(#loc422)
    %296 = stablehlo.slice %295 [0:1, 128:256, 0:40, 0:40] : (tensor<1x256x40x40xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc423)
    %297 = stablehlo.custom_call @tt.mark_argument(%arg77) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv1_conv_weight"}} : (tensor<128x128x3x3xbf16>) -> tensor<128x128x3x3xbf16> loc(#loc424)
    %298 = stablehlo.convolution(%296, %297) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {pad = [[1, 1], [1, 1]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x128x40x40xbf16>, tensor<128x128x3x3xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc425)
    %299 = stablehlo.reshape %arg76 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc426)
    %300 = stablehlo.custom_call @tt.mark_argument(%299) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv1_batch_norm2d_weight"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc427)
    %301 = stablehlo.reshape %300 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc428)
    %302 = stablehlo.reshape %arg75 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc429)
    %303 = stablehlo.custom_call @tt.mark_argument(%302) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv1_batch_norm2d_bias"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc430)
    %304 = stablehlo.reshape %303 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc431)
    %305 = stablehlo.reshape %arg74 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc432)
    %306 = stablehlo.custom_call @tt.mark_argument(%305) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv1_batch_norm2d_running_mean"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc433)
    %307 = stablehlo.reshape %306 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc434)
    %308 = "stablehlo.batch_norm_inference"(%298, %301, %304, %307, %223) <{epsilon = 1.000000e-03 : f32, feature_index = 1 : i64}> : (tensor<1x128x40x40xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc435)
    %309 = stablehlo.logistic %308 : tensor<1x128x40x40xbf16> loc(#loc436)
    %310 = stablehlo.multiply %308, %309 : tensor<1x128x40x40xbf16> loc(#loc437)
    %311 = stablehlo.custom_call @tt.mark_argument(%arg72) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv2_conv_weight"}} : (tensor<128x128x3x3xbf16>) -> tensor<128x128x3x3xbf16> loc(#loc438)
    %312 = stablehlo.convolution(%310, %311) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {pad = [[1, 1], [1, 1]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x128x40x40xbf16>, tensor<128x128x3x3xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc439)
    %313 = stablehlo.reshape %arg71 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc440)
    %314 = stablehlo.custom_call @tt.mark_argument(%313) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv2_batch_norm2d_weight"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc441)
    %315 = stablehlo.reshape %314 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc442)
    %316 = stablehlo.reshape %arg70 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc443)
    %317 = stablehlo.custom_call @tt.mark_argument(%316) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv2_batch_norm2d_bias"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc444)
    %318 = stablehlo.reshape %317 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc445)
    %319 = stablehlo.reshape %arg69 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc446)
    %320 = stablehlo.custom_call @tt.mark_argument(%319) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_bottom_up_layers_0_blocks_0_conv2_batch_norm2d_running_mean"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc447)
    %321 = stablehlo.reshape %320 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc448)
    %322 = "stablehlo.batch_norm_inference"(%312, %315, %318, %321, %226) <{epsilon = 1.000000e-03 : f32, feature_index = 1 : i64}> : (tensor<1x128x40x40xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc449)
    %323 = stablehlo.logistic %322 : tensor<1x128x40x40xbf16> loc(#loc450)
    %324 = stablehlo.multiply %322, %323 : tensor<1x128x40x40xbf16> loc(#loc451)
    %325 = stablehlo.reshape %324 : (tensor<1x128x40x40xbf16>) -> tensor<1x4x32x40x40xbf16> loc(#loc452)
    %326 = stablehlo.reshape %arg67 : (tensor<128x512xbf16>) -> tensor<1x128x512xbf16> loc(#loc453)
    %327 = stablehlo.custom_call @tt.mark_argument(%326) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_0_attn_block_guide_fc_weight"}} : (tensor<1x128x512xbf16>) -> tensor<1x128x512xbf16> loc(#loc454)
    %328 = stablehlo.reshape %327 : (tensor<1x128x512xbf16>) -> tensor<128x512xbf16> loc(#loc455)
    %329 = stablehlo.transpose %328, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,128]{0,1}"} : (tensor<128x512xbf16>) -> tensor<512x128xbf16> loc(#loc456)
    %330 = stablehlo.dot_general %69, %329, contracting_dims = [1] x [0] : (tensor<3x512xbf16>, tensor<512x128xbf16>) -> tensor<3x128xbf16> loc(#loc457)
    %331 = stablehlo.reshape %330 : (tensor<3x128xbf16>) -> tensor<1x3x128xbf16> loc(#loc458)
    %332 = stablehlo.reshape %arg66 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc459)
    %333 = stablehlo.custom_call @tt.mark_argument(%332) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_0_attn_block_guide_fc_bias"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc460)
    %334 = stablehlo.reshape %333 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc461)
    %335 = stablehlo.broadcast_in_dim %334, dims = [2] : (tensor<128xbf16>) -> tensor<1x3x128xbf16> loc(#loc462)
    %336 = stablehlo.add %331, %335 : tensor<1x3x128xbf16> loc(#loc463)
    %337 = stablehlo.reshape %336 : (tensor<1x3x128xbf16>) -> tensor<1x3x4x32xbf16> loc(#loc464)
    %338 = stablehlo.dot_general %325, %337, batching_dims = [0, 1] x [0, 2], contracting_dims = [2] x [3] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x4x32x40x40xbf16>, tensor<1x3x4x32xbf16>) -> tensor<1x4x40x40x3xbf16> loc(#loc465)
    %339 = stablehlo.reshape %arg88 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc466)
    %340 = stablehlo.custom_call @tt.mark_argument(%339) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_bottom_up_layers_0_attn_block_project_conv_batch_norm2d_running_var"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc467)
    %341 = stablehlo.reshape %340 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc468)
    %342 = stablehlo.reshape %arg60 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc469)
    %343 = stablehlo.custom_call @tt.mark_argument(%342) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_bottom_up_layers_0_final_conv_batch_norm2d_running_var"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc470)
    %344 = stablehlo.reshape %343 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc471)
    %345 = stablehlo.reshape %arg116 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc472)
    %346 = stablehlo.custom_call @tt.mark_argument(%345) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_downsample_layers_1_batch_norm2d_running_var"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc473)
    %347 = stablehlo.reshape %346 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc474)
    %348 = stablehlo.reshape %arg111 : (tensor<512xbf16>) -> tensor<1x1x512xbf16> loc(#loc475)
    %349 = stablehlo.custom_call @tt.mark_argument(%348) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_bottom_up_layers_1_main_conv_batch_norm2d_running_var"}} : (tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc476)
    %350 = stablehlo.reshape %349 : (tensor<1x1x512xbf16>) -> tensor<512xbf16> loc(#loc477)
    %351 = stablehlo.reshape %arg106 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc478)
    %352 = stablehlo.custom_call @tt.mark_argument(%351) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv1_batch_norm2d_running_var"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc479)
    %353 = stablehlo.reshape %352 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc480)
    %354 = stablehlo.reshape %arg101 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc481)
    %355 = stablehlo.custom_call @tt.mark_argument(%354) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv2_batch_norm2d_running_var"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc482)
    %356 = stablehlo.reshape %355 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc483)
    %357 = stablehlo.slice %295 [0:1, 0:128, 0:40, 0:40] : (tensor<1x256x40x40xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc484)
    %358 = stablehlo.custom_call @tt.mark_argument(%arg92) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_0_attn_block_project_conv_conv_weight"}} : (tensor<128x128x3x3xbf16>) -> tensor<128x128x3x3xbf16> loc(#loc485)
    %359 = stablehlo.convolution(%324, %358) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {pad = [[1, 1], [1, 1]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x128x40x40xbf16>, tensor<128x128x3x3xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc486)
    %360 = stablehlo.reshape %arg91 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc487)
    %361 = stablehlo.custom_call @tt.mark_argument(%360) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_0_attn_block_project_conv_batch_norm2d_weight"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc488)
    %362 = stablehlo.reshape %361 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc489)
    %363 = stablehlo.reshape %arg90 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc490)
    %364 = stablehlo.custom_call @tt.mark_argument(%363) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_0_attn_block_project_conv_batch_norm2d_bias"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc491)
    %365 = stablehlo.reshape %364 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc492)
    %366 = stablehlo.reshape %arg89 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc493)
    %367 = stablehlo.custom_call @tt.mark_argument(%366) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_bottom_up_layers_0_attn_block_project_conv_batch_norm2d_running_mean"}} : (tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc494)
    %368 = stablehlo.reshape %367 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc495)
    %369 = "stablehlo.batch_norm_inference"(%359, %362, %365, %368, %341) <{epsilon = 1.000000e-03 : f32, feature_index = 1 : i64}> : (tensor<1x128x40x40xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc496)
    %370 = stablehlo.reshape %369 : (tensor<1x128x40x40xbf16>) -> tensor<1x4x32x40x40xbf16> loc(#loc497)
    %371 = stablehlo.reduce(%338 init: %cst_4) applies stablehlo.maximum across dimensions = [4] : (tensor<1x4x40x40x3xbf16>, tensor<bf16>) -> tensor<1x4x40x40xbf16> loc(#loc498)
    %372 = stablehlo.divide %371, %cst_2 : tensor<1x4x40x40xbf16> loc(#loc499)
    %373 = stablehlo.reshape %arg65 : (tensor<4xbf16>) -> tensor<1x1x4xbf16> loc(#loc500)
    %374 = stablehlo.custom_call @tt.mark_argument(%373) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_0_attn_block_bias"}} : (tensor<1x1x4xbf16>) -> tensor<1x1x4xbf16> loc(#loc501)
    %375 = stablehlo.reshape %374 : (tensor<1x1x4xbf16>) -> tensor<1x4xbf16> loc(#loc502)
    %376 = stablehlo.broadcast_in_dim %375, dims = [0, 1] : (tensor<1x4xbf16>) -> tensor<1x4x40x40xbf16> loc(#loc503)
    %377 = stablehlo.add %372, %376 : tensor<1x4x40x40xbf16> loc(#loc504)
    %378 = stablehlo.logistic %377 : tensor<1x4x40x40xbf16> loc(#loc505)
    %379 = stablehlo.broadcast_in_dim %378, dims = [0, 1, 3, 4] : (tensor<1x4x40x40xbf16>) -> tensor<1x4x32x40x40xbf16> loc(#loc506)
    %380 = stablehlo.multiply %370, %379 : tensor<1x4x32x40x40xbf16> loc(#loc507)
    %381 = stablehlo.reshape %380 : (tensor<1x4x32x40x40xbf16>) -> tensor<1x128x40x40xbf16> loc(#loc508)
    %382 = stablehlo.concatenate %357, %296, %324, %381, dim = 1 : (tensor<1x128x40x40xbf16>, tensor<1x128x40x40xbf16>, tensor<1x128x40x40xbf16>, tensor<1x128x40x40xbf16>) -> tensor<1x512x40x40xbf16> loc(#loc509)
    %383 = stablehlo.custom_call @tt.mark_argument(%arg64) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_0_final_conv_conv_weight"}} : (tensor<256x512x1x1xbf16>) -> tensor<256x512x1x1xbf16> loc(#loc510)
    %384 = stablehlo.convolution(%382, %383) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x512x40x40xbf16>, tensor<256x512x1x1xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc511)
    %385 = stablehlo.reshape %arg63 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc512)
    %386 = stablehlo.custom_call @tt.mark_argument(%385) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_0_final_conv_batch_norm2d_weight"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc513)
    %387 = stablehlo.reshape %386 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc514)
    %388 = stablehlo.reshape %arg62 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc515)
    %389 = stablehlo.custom_call @tt.mark_argument(%388) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_0_final_conv_batch_norm2d_bias"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc516)
    %390 = stablehlo.reshape %389 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc517)
    %391 = stablehlo.reshape %arg61 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc518)
    %392 = stablehlo.custom_call @tt.mark_argument(%391) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_bottom_up_layers_0_final_conv_batch_norm2d_running_mean"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc519)
    %393 = stablehlo.reshape %392 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc520)
    %394 = "stablehlo.batch_norm_inference"(%384, %387, %390, %393, %344) <{epsilon = 1.000000e-03 : f32, feature_index = 1 : i64}> : (tensor<1x256x40x40xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>) -> tensor<1x256x40x40xbf16> loc(#loc521)
    %395 = stablehlo.logistic %394 : tensor<1x256x40x40xbf16> loc(#loc522)
    %396 = stablehlo.multiply %394, %395 : tensor<1x256x40x40xbf16> loc(#loc523)
    %397 = stablehlo.custom_call @tt.mark_argument(%arg120) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_downsample_layers_1_conv_weight"}} : (tensor<256x256x3x3xbf16>) -> tensor<256x256x3x3xbf16> loc(#loc524)
    %398 = stablehlo.convolution(%396, %397) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [2, 2], pad = [[1, 1], [1, 1]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x256x40x40xbf16>, tensor<256x256x3x3xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc525)
    %399 = stablehlo.reshape %arg119 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc526)
    %400 = stablehlo.custom_call @tt.mark_argument(%399) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_downsample_layers_1_batch_norm2d_weight"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc527)
    %401 = stablehlo.reshape %400 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc528)
    %402 = stablehlo.reshape %arg118 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc529)
    %403 = stablehlo.custom_call @tt.mark_argument(%402) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_downsample_layers_1_batch_norm2d_bias"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc530)
    %404 = stablehlo.reshape %403 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc531)
    %405 = stablehlo.reshape %arg117 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc532)
    %406 = stablehlo.custom_call @tt.mark_argument(%405) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_downsample_layers_1_batch_norm2d_running_mean"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc533)
    %407 = stablehlo.reshape %406 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc534)
    %408 = "stablehlo.batch_norm_inference"(%398, %401, %404, %407, %347) <{epsilon = 1.000000e-03 : f32, feature_index = 1 : i64}> : (tensor<1x256x20x20xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc535)
    %409 = stablehlo.logistic %408 : tensor<1x256x20x20xbf16> loc(#loc536)
    %410 = stablehlo.multiply %408, %409 : tensor<1x256x20x20xbf16> loc(#loc537)
    %411 = stablehlo.concatenate %410, %9, dim = 1 : (tensor<1x256x20x20xbf16>, tensor<1x512x20x20xbf16>) -> tensor<1x768x20x20xbf16> loc(#loc538)
    %412 = stablehlo.custom_call @tt.mark_argument(%arg115) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_1_main_conv_conv_weight"}} : (tensor<512x768x1x1xbf16>) -> tensor<512x768x1x1xbf16> loc(#loc539)
    %413 = stablehlo.convolution(%411, %412) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x768x20x20xbf16>, tensor<512x768x1x1xbf16>) -> tensor<1x512x20x20xbf16> loc(#loc540)
    %414 = stablehlo.reshape %arg114 : (tensor<512xbf16>) -> tensor<1x1x512xbf16> loc(#loc541)
    %415 = stablehlo.custom_call @tt.mark_argument(%414) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_1_main_conv_batch_norm2d_weight"}} : (tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc542)
    %416 = stablehlo.reshape %415 : (tensor<1x1x512xbf16>) -> tensor<512xbf16> loc(#loc543)
    %417 = stablehlo.reshape %arg113 : (tensor<512xbf16>) -> tensor<1x1x512xbf16> loc(#loc544)
    %418 = stablehlo.custom_call @tt.mark_argument(%417) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_1_main_conv_batch_norm2d_bias"}} : (tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc545)
    %419 = stablehlo.reshape %418 : (tensor<1x1x512xbf16>) -> tensor<512xbf16> loc(#loc546)
    %420 = stablehlo.reshape %arg112 : (tensor<512xbf16>) -> tensor<1x1x512xbf16> loc(#loc547)
    %421 = stablehlo.custom_call @tt.mark_argument(%420) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_bottom_up_layers_1_main_conv_batch_norm2d_running_mean"}} : (tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc548)
    %422 = stablehlo.reshape %421 : (tensor<1x1x512xbf16>) -> tensor<512xbf16> loc(#loc549)
    %423 = "stablehlo.batch_norm_inference"(%413, %416, %419, %422, %350) <{epsilon = 1.000000e-03 : f32, feature_index = 1 : i64}> : (tensor<1x512x20x20xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>) -> tensor<1x512x20x20xbf16> loc(#loc550)
    %424 = stablehlo.logistic %423 : tensor<1x512x20x20xbf16> loc(#loc551)
    %425 = stablehlo.multiply %423, %424 : tensor<1x512x20x20xbf16> loc(#loc552)
    %426 = stablehlo.slice %425 [0:1, 256:512, 0:20, 0:20] : (tensor<1x512x20x20xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc553)
    %427 = stablehlo.custom_call @tt.mark_argument(%arg110) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv1_conv_weight"}} : (tensor<256x256x3x3xbf16>) -> tensor<256x256x3x3xbf16> loc(#loc554)
    %428 = stablehlo.convolution(%426, %427) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {pad = [[1, 1], [1, 1]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x256x20x20xbf16>, tensor<256x256x3x3xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc555)
    %429 = stablehlo.reshape %arg109 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc556)
    %430 = stablehlo.custom_call @tt.mark_argument(%429) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv1_batch_norm2d_weight"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc557)
    %431 = stablehlo.reshape %430 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc558)
    %432 = stablehlo.reshape %arg108 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc559)
    %433 = stablehlo.custom_call @tt.mark_argument(%432) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv1_batch_norm2d_bias"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc560)
    %434 = stablehlo.reshape %433 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc561)
    %435 = stablehlo.reshape %arg107 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc562)
    %436 = stablehlo.custom_call @tt.mark_argument(%435) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv1_batch_norm2d_running_mean"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc563)
    %437 = stablehlo.reshape %436 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc564)
    %438 = "stablehlo.batch_norm_inference"(%428, %431, %434, %437, %353) <{epsilon = 1.000000e-03 : f32, feature_index = 1 : i64}> : (tensor<1x256x20x20xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc565)
    %439 = stablehlo.logistic %438 : tensor<1x256x20x20xbf16> loc(#loc566)
    %440 = stablehlo.multiply %438, %439 : tensor<1x256x20x20xbf16> loc(#loc567)
    %441 = stablehlo.custom_call @tt.mark_argument(%arg105) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv2_conv_weight"}} : (tensor<256x256x3x3xbf16>) -> tensor<256x256x3x3xbf16> loc(#loc568)
    %442 = stablehlo.convolution(%440, %441) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {pad = [[1, 1], [1, 1]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x256x20x20xbf16>, tensor<256x256x3x3xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc569)
    %443 = stablehlo.reshape %arg104 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc570)
    %444 = stablehlo.custom_call @tt.mark_argument(%443) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv2_batch_norm2d_weight"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc571)
    %445 = stablehlo.reshape %444 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc572)
    %446 = stablehlo.reshape %arg103 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc573)
    %447 = stablehlo.custom_call @tt.mark_argument(%446) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv2_batch_norm2d_bias"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc574)
    %448 = stablehlo.reshape %447 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc575)
    %449 = stablehlo.reshape %arg102 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc576)
    %450 = stablehlo.custom_call @tt.mark_argument(%449) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_bottom_up_layers_1_blocks_0_conv2_batch_norm2d_running_mean"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc577)
    %451 = stablehlo.reshape %450 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc578)
    %452 = "stablehlo.batch_norm_inference"(%442, %445, %448, %451, %356) <{epsilon = 1.000000e-03 : f32, feature_index = 1 : i64}> : (tensor<1x256x20x20xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc579)
    %453 = stablehlo.logistic %452 : tensor<1x256x20x20xbf16> loc(#loc580)
    %454 = stablehlo.multiply %452, %453 : tensor<1x256x20x20xbf16> loc(#loc581)
    %455 = stablehlo.reshape %454 : (tensor<1x256x20x20xbf16>) -> tensor<1x8x32x20x20xbf16> loc(#loc582)
    %456 = stablehlo.reshape %arg100 : (tensor<256x512xbf16>) -> tensor<1x256x512xbf16> loc(#loc583)
    %457 = stablehlo.custom_call @tt.mark_argument(%456) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_1_attn_block_guide_fc_weight"}} : (tensor<1x256x512xbf16>) -> tensor<1x256x512xbf16> loc(#loc584)
    %458 = stablehlo.reshape %457 : (tensor<1x256x512xbf16>) -> tensor<256x512xbf16> loc(#loc585)
    %459 = stablehlo.transpose %458, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,256]{0,1}"} : (tensor<256x512xbf16>) -> tensor<512x256xbf16> loc(#loc586)
    %460 = stablehlo.dot_general %69, %459, contracting_dims = [1] x [0] : (tensor<3x512xbf16>, tensor<512x256xbf16>) -> tensor<3x256xbf16> loc(#loc587)
    %461 = stablehlo.reshape %460 : (tensor<3x256xbf16>) -> tensor<1x3x256xbf16> loc(#loc588)
    %462 = stablehlo.reshape %arg99 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc589)
    %463 = stablehlo.custom_call @tt.mark_argument(%462) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_1_attn_block_guide_fc_bias"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc590)
    %464 = stablehlo.reshape %463 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc591)
    %465 = stablehlo.broadcast_in_dim %464, dims = [2] : (tensor<256xbf16>) -> tensor<1x3x256xbf16> loc(#loc592)
    %466 = stablehlo.add %461, %465 : tensor<1x3x256xbf16> loc(#loc593)
    %467 = stablehlo.reshape %466 : (tensor<1x3x256xbf16>) -> tensor<1x3x8x32xbf16> loc(#loc594)
    %468 = stablehlo.dot_general %455, %467, batching_dims = [0, 1] x [0, 2], contracting_dims = [2] x [3] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x8x32x20x20xbf16>, tensor<1x3x8x32xbf16>) -> tensor<1x8x20x20x3xbf16> loc(#loc595)
    %469 = stablehlo.reshape %arg121 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc596)
    %470 = stablehlo.custom_call @tt.mark_argument(%469) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_bottom_up_layers_1_attn_block_project_conv_batch_norm2d_running_var"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc597)
    %471 = stablehlo.reshape %470 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc598)
    %472 = stablehlo.reshape %arg93 : (tensor<512xbf16>) -> tensor<1x1x512xbf16> loc(#loc599)
    %473 = stablehlo.custom_call @tt.mark_argument(%472) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_bottom_up_layers_1_final_conv_batch_norm2d_running_var"}} : (tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc600)
    %474 = stablehlo.reshape %473 : (tensor<1x1x512xbf16>) -> tensor<512xbf16> loc(#loc601)
    %475 = stablehlo.slice %425 [0:1, 0:256, 0:20, 0:20] : (tensor<1x512x20x20xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc602)
    %476 = stablehlo.custom_call @tt.mark_argument(%arg125) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_1_attn_block_project_conv_conv_weight"}} : (tensor<256x256x3x3xbf16>) -> tensor<256x256x3x3xbf16> loc(#loc603)
    %477 = stablehlo.convolution(%454, %476) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {pad = [[1, 1], [1, 1]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x256x20x20xbf16>, tensor<256x256x3x3xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc604)
    %478 = stablehlo.reshape %arg124 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc605)
    %479 = stablehlo.custom_call @tt.mark_argument(%478) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_1_attn_block_project_conv_batch_norm2d_weight"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc606)
    %480 = stablehlo.reshape %479 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc607)
    %481 = stablehlo.reshape %arg123 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc608)
    %482 = stablehlo.custom_call @tt.mark_argument(%481) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_1_attn_block_project_conv_batch_norm2d_bias"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc609)
    %483 = stablehlo.reshape %482 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc610)
    %484 = stablehlo.reshape %arg122 : (tensor<256xbf16>) -> tensor<1x1x256xbf16> loc(#loc611)
    %485 = stablehlo.custom_call @tt.mark_argument(%484) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_bottom_up_layers_1_attn_block_project_conv_batch_norm2d_running_mean"}} : (tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc612)
    %486 = stablehlo.reshape %485 : (tensor<1x1x256xbf16>) -> tensor<256xbf16> loc(#loc613)
    %487 = "stablehlo.batch_norm_inference"(%477, %480, %483, %486, %471) <{epsilon = 1.000000e-03 : f32, feature_index = 1 : i64}> : (tensor<1x256x20x20xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc614)
    %488 = stablehlo.reshape %487 : (tensor<1x256x20x20xbf16>) -> tensor<1x8x32x20x20xbf16> loc(#loc615)
    %489 = stablehlo.reduce(%468 init: %cst_4) applies stablehlo.maximum across dimensions = [4] : (tensor<1x8x20x20x3xbf16>, tensor<bf16>) -> tensor<1x8x20x20xbf16> loc(#loc616)
    %490 = stablehlo.divide %489, %cst : tensor<1x8x20x20xbf16> loc(#loc617)
    %491 = stablehlo.reshape %arg98 : (tensor<8xbf16>) -> tensor<1x1x8xbf16> loc(#loc618)
    %492 = stablehlo.custom_call @tt.mark_argument(%491) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_1_attn_block_bias"}} : (tensor<1x1x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc619)
    %493 = stablehlo.reshape %492 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc620)
    %494 = stablehlo.broadcast_in_dim %493, dims = [0, 1] : (tensor<1x8xbf16>) -> tensor<1x8x20x20xbf16> loc(#loc621)
    %495 = stablehlo.add %490, %494 : tensor<1x8x20x20xbf16> loc(#loc622)
    %496 = stablehlo.logistic %495 : tensor<1x8x20x20xbf16> loc(#loc623)
    %497 = stablehlo.broadcast_in_dim %496, dims = [0, 1, 3, 4] : (tensor<1x8x20x20xbf16>) -> tensor<1x8x32x20x20xbf16> loc(#loc624)
    %498 = stablehlo.multiply %488, %497 : tensor<1x8x32x20x20xbf16> loc(#loc625)
    %499 = stablehlo.reshape %498 : (tensor<1x8x32x20x20xbf16>) -> tensor<1x256x20x20xbf16> loc(#loc626)
    %500 = stablehlo.concatenate %475, %426, %454, %499, dim = 1 : (tensor<1x256x20x20xbf16>, tensor<1x256x20x20xbf16>, tensor<1x256x20x20xbf16>, tensor<1x256x20x20xbf16>) -> tensor<1x1024x20x20xbf16> loc(#loc627)
    %501 = stablehlo.custom_call @tt.mark_argument(%arg97) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_1_final_conv_conv_weight"}} : (tensor<512x1024x1x1xbf16>) -> tensor<512x1024x1x1xbf16> loc(#loc628)
    %502 = stablehlo.convolution(%500, %501) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x1024x20x20xbf16>, tensor<512x1024x1x1xbf16>) -> tensor<1x512x20x20xbf16> loc(#loc629)
    %503 = stablehlo.reshape %arg96 : (tensor<512xbf16>) -> tensor<1x1x512xbf16> loc(#loc630)
    %504 = stablehlo.custom_call @tt.mark_argument(%503) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_1_final_conv_batch_norm2d_weight"}} : (tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc631)
    %505 = stablehlo.reshape %504 : (tensor<1x1x512xbf16>) -> tensor<512xbf16> loc(#loc632)
    %506 = stablehlo.reshape %arg95 : (tensor<512xbf16>) -> tensor<1x1x512xbf16> loc(#loc633)
    %507 = stablehlo.custom_call @tt.mark_argument(%506) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___neck_bottom_up_layers_1_final_conv_batch_norm2d_bias"}} : (tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc634)
    %508 = stablehlo.reshape %507 : (tensor<1x1x512xbf16>) -> tensor<512xbf16> loc(#loc635)
    %509 = stablehlo.reshape %arg94 : (tensor<512xbf16>) -> tensor<1x1x512xbf16> loc(#loc636)
    %510 = stablehlo.custom_call @tt.mark_argument(%509) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___neck_bottom_up_layers_1_final_conv_batch_norm2d_running_mean"}} : (tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc637)
    %511 = stablehlo.reshape %510 : (tensor<1x1x512xbf16>) -> tensor<512xbf16> loc(#loc638)
    %512 = "stablehlo.batch_norm_inference"(%502, %505, %508, %511, %474) <{epsilon = 1.000000e-03 : f32, feature_index = 1 : i64}> : (tensor<1x512x20x20xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>) -> tensor<1x512x20x20xbf16> loc(#loc639)
    %513 = stablehlo.logistic %512 : tensor<1x512x20x20xbf16> loc(#loc640)
    %514 = stablehlo.multiply %512, %513 : tensor<1x512x20x20xbf16> loc(#loc641)
    return %266, %396, %514 : tensor<1x128x80x80xbf16>, tensor<1x256x40x40xbf16>, tensor<1x512x20x20xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc127 = loc("reshape.220")
#loc128 = loc("custom-call.221")
#loc129 = loc("reshape.222")
#loc130 = loc("reshape.202")
#loc131 = loc("custom-call.203")
#loc132 = loc("reshape.204")
#loc133 = loc("reshape.184")
#loc134 = loc("custom-call.185")
#loc135 = loc("reshape.186")
#loc136 = loc("custom-call.270")
#loc137 = loc("transpose.271")
#loc138 = loc("floor.259")
#loc139 = loc("convert.260")
#loc140 = loc("broadcast.264")
#loc141 = loc("broadcast.262")
#loc142 = loc("compare.265")
#loc143 = loc("convert.266")
#loc144 = loc("transpose.267")
#loc145 = loc("convert.268")
#loc146 = loc("dot.272")
#loc147 = loc("transpose.274")
#loc148 = loc("dot.275")
#loc149 = loc("custom-call.238")
#loc150 = loc("concatenate.277")
#loc151 = loc("custom-call.236")
#loc152 = loc("convolution.278")
#loc153 = loc("reshape.232")
#loc154 = loc("custom-call.233")
#loc155 = loc("reshape.234")
#loc156 = loc("reshape.228")
#loc157 = loc("custom-call.229")
#loc158 = loc("reshape.230")
#loc159 = loc("reshape.224")
#loc160 = loc("custom-call.225")
#loc161 = loc("reshape.226")
#loc162 = loc("batch-norm-inference.279")
#loc163 = loc("logistic.284")
#loc164 = loc("multiply.285")
#loc165 = loc("slice.286")
#loc166 = loc("custom-call.218")
#loc167 = loc("convolution.287")
#loc168 = loc("reshape.214")
#loc169 = loc("custom-call.215")
#loc170 = loc("reshape.216")
#loc171 = loc("reshape.210")
#loc172 = loc("custom-call.211")
#loc173 = loc("reshape.212")
#loc174 = loc("reshape.206")
#loc175 = loc("custom-call.207")
#loc176 = loc("reshape.208")
#loc177 = loc("batch-norm-inference.288")
#loc178 = loc("logistic.293")
#loc179 = loc("multiply.294")
#loc180 = loc("custom-call.200")
#loc181 = loc("convolution.295")
#loc182 = loc("reshape.196")
#loc183 = loc("custom-call.197")
#loc184 = loc("reshape.198")
#loc185 = loc("reshape.192")
#loc186 = loc("custom-call.193")
#loc187 = loc("reshape.194")
#loc188 = loc("reshape.188")
#loc189 = loc("custom-call.189")
#loc190 = loc("reshape.190")
#loc191 = loc("batch-norm-inference.296")
#loc192 = loc("logistic.301")
#loc193 = loc("multiply.302")
#loc194 = loc("reshape.303")
#loc195 = loc("custom-call.41")
#loc196 = loc("reshape.175")
#loc197 = loc("reshape.171")
#loc198 = loc("custom-call.172")
#loc199 = loc("reshape.173")
#loc200 = loc("transpose.174")
#loc201 = loc("dot.176")
#loc202 = loc("reshape.177")
#loc203 = loc("reshape.167")
#loc204 = loc("custom-call.168")
#loc205 = loc("reshape.169")
#loc206 = loc("broadcast.180")
#loc207 = loc("add.181")
#loc208 = loc("reshape.182")
#loc209 = loc("dot.304")
#loc210 = loc("reshape.355")
#loc211 = loc("custom-call.356")
#loc212 = loc("reshape.357")
#loc213 = loc("reshape.137")
#loc214 = loc("custom-call.138")
#loc215 = loc("reshape.139")
#loc216 = loc("reshape.87")
#loc217 = loc("custom-call.88")
#loc218 = loc("reshape.89")
#loc219 = loc("reshape.69")
#loc220 = loc("custom-call.70")
#loc221 = loc("reshape.71")
#loc222 = loc("reshape.51")
#loc223 = loc("custom-call.52")
#loc224 = loc("reshape.53")
#loc225 = loc("slice.383")
#loc226 = loc("custom-call.371")
#loc227 = loc("convolution.372")
#loc228 = loc("reshape.367")
#loc229 = loc("custom-call.368")
#loc230 = loc("reshape.369")
#loc231 = loc("reshape.363")
#loc232 = loc("custom-call.364")
#loc233 = loc("reshape.365")
#loc234 = loc("reshape.359")
#loc235 = loc("custom-call.360")
#loc236 = loc("reshape.361")
#loc237 = loc("batch-norm-inference.373")
#loc238 = loc("reshape.378")
#loc239 = loc("reduce.312")
#loc240 = loc("divide.344")
#loc241 = loc("reshape.157")
#loc242 = loc("custom-call.158")
#loc243 = loc("reshape.160")
#loc244 = loc("broadcast.348")
#loc245 = loc("add.349")
#loc246 = loc("logistic.350")
#loc247 = loc("broadcast.380")
#loc248 = loc("multiply.381")
#loc249 = loc("reshape.382")
#loc250 = loc("concatenate.384")
#loc251 = loc("custom-call.153")
#loc252 = loc("convolution.385")
#loc253 = loc("reshape.149")
#loc254 = loc("custom-call.150")
#loc255 = loc("reshape.151")
#loc256 = loc("reshape.145")
#loc257 = loc("custom-call.146")
#loc258 = loc("reshape.147")
#loc259 = loc("reshape.141")
#loc260 = loc("custom-call.142")
#loc261 = loc("reshape.143")
#loc262 = loc("batch-norm-inference.386")
#loc263 = loc("logistic.391")
#loc264 = loc("multiply.392")
#loc265 = loc("transpose.393")
#loc266 = loc("floor.126")
#loc267 = loc("convert.127")
#loc268 = loc("broadcast.131")
#loc269 = loc("broadcast.129")
#loc270 = loc("compare.132")
#loc271 = loc("convert.133")
#loc272 = loc("transpose.134")
#loc273 = loc("convert.135")
#loc274 = loc("dot.394")
#loc275 = loc("transpose.396")
#loc276 = loc("dot.397")
#loc277 = loc("custom-call.105")
#loc278 = loc("concatenate.399")
#loc279 = loc("custom-call.103")
#loc280 = loc("convolution.400")
#loc281 = loc("reshape.99")
#loc282 = loc("custom-call.100")
#loc283 = loc("reshape.101")
#loc284 = loc("reshape.95")
#loc285 = loc("custom-call.96")
#loc286 = loc("reshape.97")
#loc287 = loc("reshape.91")
#loc288 = loc("custom-call.92")
#loc289 = loc("reshape.93")
#loc290 = loc("batch-norm-inference.401")
#loc291 = loc("logistic.406")
#loc292 = loc("multiply.407")
#loc293 = loc("slice.408")
#loc294 = loc("custom-call.85")
#loc295 = loc("convolution.409")
#loc296 = loc("reshape.81")
#loc297 = loc("custom-call.82")
#loc298 = loc("reshape.83")
#loc299 = loc("reshape.77")
#loc300 = loc("custom-call.78")
#loc301 = loc("reshape.79")
#loc302 = loc("reshape.73")
#loc303 = loc("custom-call.74")
#loc304 = loc("reshape.75")
#loc305 = loc("batch-norm-inference.410")
#loc306 = loc("logistic.415")
#loc307 = loc("multiply.416")
#loc308 = loc("custom-call.67")
#loc309 = loc("convolution.417")
#loc310 = loc("reshape.63")
#loc311 = loc("custom-call.64")
#loc312 = loc("reshape.65")
#loc313 = loc("reshape.59")
#loc314 = loc("custom-call.60")
#loc315 = loc("reshape.61")
#loc316 = loc("reshape.55")
#loc317 = loc("custom-call.56")
#loc318 = loc("reshape.57")
#loc319 = loc("batch-norm-inference.418")
#loc320 = loc("logistic.423")
#loc321 = loc("multiply.424")
#loc322 = loc("reshape.425")
#loc323 = loc("reshape.36")
#loc324 = loc("custom-call.37")
#loc325 = loc("reshape.38")
#loc326 = loc("transpose.39")
#loc327 = loc("dot.43")
#loc328 = loc("reshape.44")
#loc329 = loc("reshape.32")
#loc330 = loc("custom-call.33")
#loc331 = loc("reshape.34")
#loc332 = loc("broadcast.47")
#loc333 = loc("add.48")
#loc334 = loc("reshape.49")
#loc335 = loc("dot.426")
#loc336 = loc("reshape.477")
#loc337 = loc("custom-call.478")
#loc338 = loc("reshape.479")
#loc339 = loc("reshape.2")
#loc340 = loc("custom-call.3")
#loc341 = loc("reshape.4")
#loc342 = loc("reshape.617")
#loc343 = loc("custom-call.618")
#loc344 = loc("reshape.619")
#loc345 = loc("reshape.599")
#loc346 = loc("custom-call.600")
#loc347 = loc("reshape.601")
#loc348 = loc("reshape.581")
#loc349 = loc("custom-call.582")
#loc350 = loc("reshape.583")
#loc351 = loc("reshape.563")
#loc352 = loc("custom-call.564")
#loc353 = loc("reshape.565")
#loc354 = loc("slice.505")
#loc355 = loc("custom-call.493")
#loc356 = loc("convolution.494")
#loc357 = loc("reshape.489")
#loc358 = loc("custom-call.490")
#loc359 = loc("reshape.491")
#loc360 = loc("reshape.485")
#loc361 = loc("custom-call.486")
#loc362 = loc("reshape.487")
#loc363 = loc("reshape.481")
#loc364 = loc("custom-call.482")
#loc365 = loc("reshape.483")
#loc366 = loc("batch-norm-inference.495")
#loc367 = loc("reshape.500")
#loc368 = loc("reduce.434")
#loc369 = loc("divide.466")
#loc370 = loc("reshape.22")
#loc371 = loc("custom-call.23")
#loc372 = loc("reshape.25")
#loc373 = loc("broadcast.470")
#loc374 = loc("add.471")
#loc375 = loc("logistic.472")
#loc376 = loc("broadcast.502")
#loc377 = loc("multiply.503")
#loc378 = loc("reshape.504")
#loc379 = loc("concatenate.506")
#loc380 = loc("custom-call.18")
#loc381 = loc("convolution.507")
#loc382 = loc("reshape.14")
#loc383 = loc("custom-call.15")
#loc384 = loc("reshape.16")
#loc385 = loc("reshape.10")
#loc386 = loc("custom-call.11")
#loc387 = loc("reshape.12")
#loc388 = loc("reshape.6")
#loc389 = loc("custom-call.7")
#loc390 = loc("reshape.8")
#loc391 = loc("batch-norm-inference.508")
#loc392 = loc("logistic.513")
#loc393 = loc("multiply.514")
#loc394 = loc("custom-call.633")
#loc395 = loc("convolution.634")
#loc396 = loc("reshape.629")
#loc397 = loc("custom-call.630")
#loc398 = loc("reshape.631")
#loc399 = loc("reshape.625")
#loc400 = loc("custom-call.626")
#loc401 = loc("reshape.627")
#loc402 = loc("reshape.621")
#loc403 = loc("custom-call.622")
#loc404 = loc("reshape.623")
#loc405 = loc("batch-norm-inference.635")
#loc406 = loc("logistic.640")
#loc407 = loc("multiply.641")
#loc408 = loc("concatenate.642")
#loc409 = loc("custom-call.615")
#loc410 = loc("convolution.643")
#loc411 = loc("reshape.611")
#loc412 = loc("custom-call.612")
#loc413 = loc("reshape.613")
#loc414 = loc("reshape.607")
#loc415 = loc("custom-call.608")
#loc416 = loc("reshape.609")
#loc417 = loc("reshape.603")
#loc418 = loc("custom-call.604")
#loc419 = loc("reshape.605")
#loc420 = loc("batch-norm-inference.644")
#loc421 = loc("logistic.649")
#loc422 = loc("multiply.650")
#loc423 = loc("slice.651")
#loc424 = loc("custom-call.597")
#loc425 = loc("convolution.652")
#loc426 = loc("reshape.593")
#loc427 = loc("custom-call.594")
#loc428 = loc("reshape.595")
#loc429 = loc("reshape.589")
#loc430 = loc("custom-call.590")
#loc431 = loc("reshape.591")
#loc432 = loc("reshape.585")
#loc433 = loc("custom-call.586")
#loc434 = loc("reshape.587")
#loc435 = loc("batch-norm-inference.653")
#loc436 = loc("logistic.658")
#loc437 = loc("multiply.659")
#loc438 = loc("custom-call.579")
#loc439 = loc("convolution.660")
#loc440 = loc("reshape.575")
#loc441 = loc("custom-call.576")
#loc442 = loc("reshape.577")
#loc443 = loc("reshape.571")
#loc444 = loc("custom-call.572")
#loc445 = loc("reshape.573")
#loc446 = loc("reshape.567")
#loc447 = loc("custom-call.568")
#loc448 = loc("reshape.569")
#loc449 = loc("batch-norm-inference.661")
#loc450 = loc("logistic.666")
#loc451 = loc("multiply.667")
#loc452 = loc("reshape.668")
#loc453 = loc("reshape.550")
#loc454 = loc("custom-call.551")
#loc455 = loc("reshape.552")
#loc456 = loc("transpose.553")
#loc457 = loc("dot.555")
#loc458 = loc("reshape.556")
#loc459 = loc("reshape.546")
#loc460 = loc("custom-call.547")
#loc461 = loc("reshape.548")
#loc462 = loc("broadcast.559")
#loc463 = loc("add.560")
#loc464 = loc("reshape.561")
#loc465 = loc("dot.669")
#loc466 = loc("reshape.720")
#loc467 = loc("custom-call.721")
#loc468 = loc("reshape.722")
#loc469 = loc("reshape.516")
#loc470 = loc("custom-call.517")
#loc471 = loc("reshape.518")
#loc472 = loc("reshape.860")
#loc473 = loc("custom-call.861")
#loc474 = loc("reshape.862")
#loc475 = loc("reshape.842")
#loc476 = loc("custom-call.843")
#loc477 = loc("reshape.844")
#loc478 = loc("reshape.824")
#loc479 = loc("custom-call.825")
#loc480 = loc("reshape.826")
#loc481 = loc("reshape.806")
#loc482 = loc("custom-call.807")
#loc483 = loc("reshape.808")
#loc484 = loc("slice.748")
#loc485 = loc("custom-call.736")
#loc486 = loc("convolution.737")
#loc487 = loc("reshape.732")
#loc488 = loc("custom-call.733")
#loc489 = loc("reshape.734")
#loc490 = loc("reshape.728")
#loc491 = loc("custom-call.729")
#loc492 = loc("reshape.730")
#loc493 = loc("reshape.724")
#loc494 = loc("custom-call.725")
#loc495 = loc("reshape.726")
#loc496 = loc("batch-norm-inference.738")
#loc497 = loc("reshape.743")
#loc498 = loc("reduce.677")
#loc499 = loc("divide.709")
#loc500 = loc("reshape.536")
#loc501 = loc("custom-call.537")
#loc502 = loc("reshape.539")
#loc503 = loc("broadcast.713")
#loc504 = loc("add.714")
#loc505 = loc("logistic.715")
#loc506 = loc("broadcast.745")
#loc507 = loc("multiply.746")
#loc508 = loc("reshape.747")
#loc509 = loc("concatenate.749")
#loc510 = loc("custom-call.532")
#loc511 = loc("convolution.750")
#loc512 = loc("reshape.528")
#loc513 = loc("custom-call.529")
#loc514 = loc("reshape.530")
#loc515 = loc("reshape.524")
#loc516 = loc("custom-call.525")
#loc517 = loc("reshape.526")
#loc518 = loc("reshape.520")
#loc519 = loc("custom-call.521")
#loc520 = loc("reshape.522")
#loc521 = loc("batch-norm-inference.751")
#loc522 = loc("logistic.756")
#loc523 = loc("multiply.757")
#loc524 = loc("custom-call.876")
#loc525 = loc("convolution.877")
#loc526 = loc("reshape.872")
#loc527 = loc("custom-call.873")
#loc528 = loc("reshape.874")
#loc529 = loc("reshape.868")
#loc530 = loc("custom-call.869")
#loc531 = loc("reshape.870")
#loc532 = loc("reshape.864")
#loc533 = loc("custom-call.865")
#loc534 = loc("reshape.866")
#loc535 = loc("batch-norm-inference.878")
#loc536 = loc("logistic.883")
#loc537 = loc("multiply.884")
#loc538 = loc("concatenate.885")
#loc539 = loc("custom-call.858")
#loc540 = loc("convolution.886")
#loc541 = loc("reshape.854")
#loc542 = loc("custom-call.855")
#loc543 = loc("reshape.856")
#loc544 = loc("reshape.850")
#loc545 = loc("custom-call.851")
#loc546 = loc("reshape.852")
#loc547 = loc("reshape.846")
#loc548 = loc("custom-call.847")
#loc549 = loc("reshape.848")
#loc550 = loc("batch-norm-inference.887")
#loc551 = loc("logistic.892")
#loc552 = loc("multiply.893")
#loc553 = loc("slice.894")
#loc554 = loc("custom-call.840")
#loc555 = loc("convolution.895")
#loc556 = loc("reshape.836")
#loc557 = loc("custom-call.837")
#loc558 = loc("reshape.838")
#loc559 = loc("reshape.832")
#loc560 = loc("custom-call.833")
#loc561 = loc("reshape.834")
#loc562 = loc("reshape.828")
#loc563 = loc("custom-call.829")
#loc564 = loc("reshape.830")
#loc565 = loc("batch-norm-inference.896")
#loc566 = loc("logistic.901")
#loc567 = loc("multiply.902")
#loc568 = loc("custom-call.822")
#loc569 = loc("convolution.903")
#loc570 = loc("reshape.818")
#loc571 = loc("custom-call.819")
#loc572 = loc("reshape.820")
#loc573 = loc("reshape.814")
#loc574 = loc("custom-call.815")
#loc575 = loc("reshape.816")
#loc576 = loc("reshape.810")
#loc577 = loc("custom-call.811")
#loc578 = loc("reshape.812")
#loc579 = loc("batch-norm-inference.904")
#loc580 = loc("logistic.909")
#loc581 = loc("multiply.910")
#loc582 = loc("reshape.911")
#loc583 = loc("reshape.793")
#loc584 = loc("custom-call.794")
#loc585 = loc("reshape.795")
#loc586 = loc("transpose.796")
#loc587 = loc("dot.798")
#loc588 = loc("reshape.799")
#loc589 = loc("reshape.789")
#loc590 = loc("custom-call.790")
#loc591 = loc("reshape.791")
#loc592 = loc("broadcast.802")
#loc593 = loc("add.803")
#loc594 = loc("reshape.804")
#loc595 = loc("dot.912")
#loc596 = loc("reshape.963")
#loc597 = loc("custom-call.964")
#loc598 = loc("reshape.965")
#loc599 = loc("reshape.759")
#loc600 = loc("custom-call.760")
#loc601 = loc("reshape.761")
#loc602 = loc("slice.991")
#loc603 = loc("custom-call.979")
#loc604 = loc("convolution.980")
#loc605 = loc("reshape.975")
#loc606 = loc("custom-call.976")
#loc607 = loc("reshape.977")
#loc608 = loc("reshape.971")
#loc609 = loc("custom-call.972")
#loc610 = loc("reshape.973")
#loc611 = loc("reshape.967")
#loc612 = loc("custom-call.968")
#loc613 = loc("reshape.969")
#loc614 = loc("batch-norm-inference.981")
#loc615 = loc("reshape.986")
#loc616 = loc("reduce.920")
#loc617 = loc("divide.952")
#loc618 = loc("reshape.779")
#loc619 = loc("custom-call.780")
#loc620 = loc("reshape.782")
#loc621 = loc("broadcast.956")
#loc622 = loc("add.957")
#loc623 = loc("logistic.958")
#loc624 = loc("broadcast.988")
#loc625 = loc("multiply.989")
#loc626 = loc("reshape.990")
#loc627 = loc("concatenate.992")
#loc628 = loc("custom-call.775")
#loc629 = loc("convolution.993")
#loc630 = loc("reshape.771")
#loc631 = loc("custom-call.772")
#loc632 = loc("reshape.773")
#loc633 = loc("reshape.767")
#loc634 = loc("custom-call.768")
#loc635 = loc("reshape.769")
#loc636 = loc("reshape.763")
#loc637 = loc("custom-call.764")
#loc638 = loc("reshape.765")
#loc639 = loc("batch-norm-inference.994")
#loc640 = loc("logistic.999")
#loc641 = loc("multiply.1000")
