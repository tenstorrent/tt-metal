#loc1 = loc("p0.2")
#loc2 = loc("p1.7")
module @SyncTensorsGraph.16 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<1x12x39x128xbf16> loc("p0.2"), %arg1: tensor<1x12x39x128xbf16> loc("p1.7")) -> tensor<1x12x39x39xbf16> {
    %cst = stablehlo.constant dense<8.837890e-02> : tensor<1x12x39x39xbf16> loc(#loc)
    %0 = stablehlo.custom_call @tt.mark_argument(%arg1) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_1"}} : (tensor<1x12x39x128xbf16>) -> tensor<1x12x39x128xbf16> loc(#loc3)
    %1 = stablehlo.reshape %0 : (tensor<1x12x39x128xbf16>) -> tensor<12x39x128xbf16> loc(#loc4)
    %2 = stablehlo.custom_call @tt.mark_argument(%arg0) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_0"}} : (tensor<1x12x39x128xbf16>) -> tensor<1x12x39x128xbf16> loc(#loc5)
    %3 = stablehlo.transpose %2, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,12,128,39]{2,3,1,0}"} : (tensor<1x12x39x128xbf16>) -> tensor<1x12x128x39xbf16> loc(#loc6)
    %4 = stablehlo.reshape %3 : (tensor<1x12x128x39xbf16>) -> tensor<12x128x39xbf16> loc(#loc7)
    %5 = stablehlo.dot_general %1, %4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<12x39x128xbf16>, tensor<12x128x39xbf16>) -> tensor<12x39x39xbf16> loc(#loc8)
    %6 = stablehlo.reshape %5 : (tensor<12x39x39xbf16>) -> tensor<1x12x39x39xbf16> loc(#loc9)
    %7 = stablehlo.multiply %6, %cst : tensor<1x12x39x39xbf16> loc(#loc10)
    return %7 : tensor<1x12x39x39xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc3 = loc("custom-call.8")
#loc4 = loc("reshape.10")
#loc5 = loc("custom-call.3")
#loc6 = loc("transpose.4")
#loc7 = loc("reshape.6")
#loc8 = loc("dot.11")
#loc9 = loc("reshape.12")
#loc10 = loc("multiply.14")
