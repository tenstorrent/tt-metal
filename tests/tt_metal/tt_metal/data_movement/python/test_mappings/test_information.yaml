tests:
  0:
    name: "DRAM Packet Sizes"

  1:
    name: "DRAM Core Locations"
    comment: |
      This test appears to be broken. The graph is showing numbers that dont make sense.

  3:
    name: "DRAM Directed Ideal"
    comment: |
      This test shows the ideal read and write bandwidth when transfering multiple 8KB packets.
      The read bandwidth is what is expected, however write bandwidth is expected to be 64
      B/cycle rather than 35 B/cycle. There may be some configuration problem with the dram
      controller/phy or this may be the physical limit of the dram.

  4:
    name: "One to One Packet Sizes"

  5:
    name: "One from One Packet Sizes"

  6:
    name: "One to All Unicast 2x2 Packet Sizes"

  7:
    name: "One to All Unicast 5x5 Packet Sizes"

  8:
    name: "One to All Unicast Packet Sizes"

  9:
    name: "One to All Multicast 2x2 Packet Sizes"

  10:
    name: "One to All Multicast 5x5 Packet Sizes"

  11:
    name: "One to All Multicast Packet Sizes"

  12:
    name: "One to All Multicast Linked 2x2 Packet Sizes"

  13:
    name: "One to All Multicast Linked 5x5 Packet Sizes"

  14:
    name: "One to All Multicast Linked Packet Sizes"

  15:
    name: "One from All Packet Sizes"

  16:
    name: "Loopback Packet Sizes"

  17:
    name: "Reshard Hardcoded Small"
    comment: |
      This is a 2 reader reshard. It seems to be getting expected perf based on number of transactions
      and transactions size. Reshard perf is dictated based on the number of transactions and the
      transaction size. A small number of transactions will result in small perf due to large
      round trip latency. It is suggested to use a large number of transactions, with large transaction
      size to get the best performance.

  18:
    name: "Reshard Hardcoded Medium"
    comment: |
      This is a 2 reader reshard. It seems to be getting expected perf based on number of transactions
      and transactions size. Reshard perf is dictated based on the number of transactions and the
      transaction size. A small number of transactions will result in small perf due to large
      round trip latency. It is suggested to use a large number of transactions, with large transaction
      size to get the best performance.

  19:
    name: "Reshard Hardcoded Many Cores"
    comment: |
      This is a 8 reader reshard. It seems to be getting expected perf based on number of transactions
      and transactions size. Reshard perf is dictated based on the number of transactions and the
      transaction size. A small number of transactions will result in small perf due to large
      round trip latency. It is suggested to use a large number of transactions, with large transaction
      size to get the best performance.

  20:
    name: "Reshard Hardcoded 2 Cores to Many Cores"
    comment: |
      This is a 2 core to 8 reader reshard. It seems to be getting expected perf based on number of
      transactions and transactions size. Reshard perf is dictated based on the number of transactions
      and the transaction size. A small number of transactions will result in small perf due to large
      round trip latency. It is suggested to use a large number of transactions, with large transaction
      size to get the best performance.

  21:
    name: "Conv Act with halo 3x3"
    comment: |
      Convolution has a large number of transactions and a small transaction size. The performance is
      similar to what it would be for a similarly configured one from one. Convolution may benefit from
      having multiple cores doing different parts of the convolution at the same time. This would
      result in a larger effective bandwidth.

  22:
    name: "Conv Act with halo 3x3 Small"
    comment: |
      Convolution has a large number of transactions and a small transaction size. The performance is
      similar to what it would be for a similarly configured one from one. Convolution may benefit from
      having multiple cores doing different parts of the convolution at the same time. This would
      result in a larger effective bandwidth.

  23:
    name: "Conv Halo Gather"
    comment: |
      The performance of this test is similar to how other tests perform based on the number of
      transactions and the transaction size, but with extra degradation due to needing to read
      parameters from L1.

  30:
    name: "One from All Directed Ideal"

  50:
    name: "One to One Directed Ideal"

  51:
    name: "One from One Directed Ideal"

  52:
    name: "One to All Unicast Directed Ideal"

  53:
    name: "One to All Multicast Directed Ideal"

  54:
    name: "One to All Multicast Linked Directed Ideal"

  55:
    name: "Loopback Directed Ideal"

  60:
    name: "All to All Packet Sizes"

  70:
    name: "All from All Packet Sizes"

  100:
    name: "Multicast Schemes (Loopback Enabled)"

  101:
    name: "Multicast Schemes (Loopback Disabled)"

  200:
    name: "Deinterleave Single Core"
    comment: |
      With a single core the graphs shows performance increases as the theshold increases.
      This is because frequent flushes dont hide the round trip latency.

  201:
    name: "Deinterleave Multi Core"
    comment: |
      With multiple cores the graph shows that a small theshold always provides bad performance.
      This is because frequent flushes dont hide the round trip latency. At larger thesholds,
      the performance starts to fluctuate due to head-of-line blocking and unfairness in the NOC.
      Performance fluctuates because the flush disturbes the steady state and will randomly create
      traffic that sometimes has head of line blocking, and sometimes not.
