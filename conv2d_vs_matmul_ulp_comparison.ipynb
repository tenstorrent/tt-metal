{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv2D vs Matmul ULP Comparison\n",
    "\n",
    "This notebook compares ULP errors between TTNN and two references:\n",
    "- **PyTorch CUDA reference** (generated on CUDA machine with high precision)\n",
    "- **PyTorch FP32 ground truth** (local CPU float32 computation)\n",
    "\n",
    "For both:\n",
    "- Conv2D operation (3x3 kernel, padding (1,1))\n",
    "- Equivalent matmul operation\n",
    "\n",
    "## Data Flow:\n",
    "1. Load CUDA reference outputs from `torch_reference_outputs/` (generated on CUDA)\n",
    "2. Regenerate and validate input tensors using convenience functions\n",
    "3. Compute PyTorch float32 ground truth using the validated inputs\n",
    "4. Run TTNN operations with the same inputs\n",
    "5. Compare TTNN outputs using ULP metrics:\n",
    "   - **vs CUDA Reference**: Compare against high-precision CUDA outputs\n",
    "   - **vs PyTorch FP32 Ground Truth**: Compare against local CPU float32 computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple\n",
    "from models.common.utility_functions import ulp\n",
    "import os\n",
    "from loguru import logger\n",
    "import ttnn\n",
    "\n",
    "# Import convenience functions for loading reference data\n",
    "from load_torch_reference_outputs import (\n",
    "    ReferenceDataLoader,\n",
    "    get_conv2d_inputs,\n",
    "    get_matmul_inputs,\n",
    "    get_reference_outputs\n",
    ")\n",
    "\n",
    "# Disable TT logging\n",
    "os.environ['TT_LOGGER_LEVEL'] = 'off'\n",
    "os.environ[\"TT_METAL_CACHE\"] = \"/localdev/astancov/tt-metal/built\" \n",
    "logger.disable('ttnn')\n",
    "\n",
    "default_fig_size = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Reference Data Configuration\n",
    "\n",
    "Check what reference data is available from the CUDA generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize reference data loader\n",
    "loader = ReferenceDataLoader(\"torch_reference_outputs\")\n",
    "\n",
    "# Print summary of available data\n",
    "loader.print_summary()\n",
    "\n",
    "# List all available test cases\n",
    "cases = loader.list_available_cases()\n",
    "print(f\"\\nAvailable test cases: {len(cases)}\")\n",
    "for dtype, ic, method in cases[:5]:  # Show first 5\n",
    "    k = loader.ic_to_k[ic]\n",
    "    print(f\"  - dtype={dtype}, input_channels={ic}, method={method}, K={k}\")\n",
    "if len(cases) > 5:\n",
    "    print(f\"  ... and {len(cases) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ulp_error(res, ref):\n",
    "    \"\"\"\n",
    "    Compute the ULP error (in res ULPs) between two tensors.\n",
    "    \n",
    "    Args:\n",
    "        res: Result tensor\n",
    "        ref: Reference tensor\n",
    "        \n",
    "    Returns:\n",
    "        ULP error between res and ref (in ULPs of res)\n",
    "    \"\"\"\n",
    "    res_ulp = ulp(res)\n",
    "    ref_as_res_dtype = ref.to(res.dtype)\n",
    "    return torch.abs((res.to(torch.float64) - ref_as_res_dtype.to(torch.float64)) / res_ulp.to(torch.float64))\n",
    "\n",
    "def compute_ulp_statistics(res, ref):\n",
    "    \"\"\"\n",
    "    Compute ULP error statistics.\n",
    "    \n",
    "    Args:\n",
    "        res: Result tensor\n",
    "        ref: Reference tensor (numpy or torch)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with ULP statistics\n",
    "    \"\"\"\n",
    "    # Convert numpy to torch if needed\n",
    "    if isinstance(ref, np.ndarray):\n",
    "        ref = torch.from_numpy(ref)\n",
    "    \n",
    "    ulp_errors = ulp_error(res, ref)\n",
    "    ulp_vals = ulp_errors.detach().cpu().numpy().flatten()\n",
    "    \n",
    "    return {\n",
    "        'mean': float(np.mean(ulp_vals)),\n",
    "        'median': float(np.median(ulp_vals)),\n",
    "        'max': float(np.max(ulp_vals)),\n",
    "        'min': float(np.min(ulp_vals)),\n",
    "        'p95': float(np.percentile(ulp_vals, 95)),\n",
    "        'p99': float(np.percentile(ulp_vals, 99))\n",
    "    }\n",
    "\n",
    "def plot_ulp_comparison_dual(conv2d_stats, matmul_stats, config_name, ref_type):\n",
    "    \"\"\"\n",
    "    Plot ULP statistics comparison between conv2d and matmul.\n",
    "    \"\"\"\n",
    "    metrics = ['mean', 'median', 'max', 'p95', 'p99']\n",
    "    conv2d_vals = [conv2d_stats[m] for m in metrics]\n",
    "    matmul_vals = [matmul_stats[m] for m in metrics]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=default_fig_size)\n",
    "    bars1 = ax.bar(x - width/2, conv2d_vals, width, label='Conv2D', alpha=0.8)\n",
    "    bars2 = ax.bar(x + width/2, matmul_vals, width, label='Matmul', alpha=0.8)\n",
    "    \n",
    "    ax.set_ylabel('ULP Error')\n",
    "    ax.set_title(f'ULP Error: TTNN vs {ref_type}\\n{config_name}')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.2f}',\n",
    "                   ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_ulp_histogram_comparison(conv2d_res, conv2d_ref, matmul_res, matmul_ref, config_name, ref_type, n_bins=100):\n",
    "    \"\"\"\n",
    "    Plot ULP error histograms for both conv2d and matmul side by side.\n",
    "    \"\"\"\n",
    "    # Convert numpy to torch if needed\n",
    "    if isinstance(conv2d_ref, np.ndarray):\n",
    "        conv2d_ref = torch.from_numpy(conv2d_ref)\n",
    "    if isinstance(matmul_ref, np.ndarray):\n",
    "        matmul_ref = torch.from_numpy(matmul_ref)\n",
    "    \n",
    "    conv2d_ulp = ulp_error(conv2d_res, conv2d_ref).detach().cpu().numpy().flatten()\n",
    "    matmul_ulp = ulp_error(matmul_res, matmul_ref).detach().cpu().numpy().flatten()\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Conv2D histogram\n",
    "    ax1.hist(conv2d_ulp, bins=n_bins, alpha=0.7, edgecolor='black', color='blue')\n",
    "    ax1.set_xlabel('ULP Error')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title(f'Conv2D ULP Error Histogram\\n{config_name} vs {ref_type}')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Matmul histogram\n",
    "    ax2.hist(matmul_ulp, bins=n_bins, alpha=0.7, edgecolor='black', color='green')\n",
    "    ax2.set_xlabel('ULP Error')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title(f'Matmul ULP Error Histogram\\n{config_name} vs {ref_type}')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_ulp_cdf_comparison(conv2d_res, conv2d_ref, matmul_res, matmul_ref, config_name, ref_type):\n",
    "    \"\"\"\n",
    "    Plot ULP error CDFs for both conv2d and matmul on the same plot.\n",
    "    \"\"\"\n",
    "    # Convert numpy to torch if needed\n",
    "    if isinstance(conv2d_ref, np.ndarray):\n",
    "        conv2d_ref = torch.from_numpy(conv2d_ref)\n",
    "    if isinstance(matmul_ref, np.ndarray):\n",
    "        matmul_ref = torch.from_numpy(matmul_ref)\n",
    "    \n",
    "    conv2d_ulp = ulp_error(conv2d_res, conv2d_ref).detach().cpu().numpy().flatten()\n",
    "    matmul_ulp = ulp_error(matmul_res, matmul_ref).detach().cpu().numpy().flatten()\n",
    "    \n",
    "    conv2d_sorted = np.sort(conv2d_ulp)\n",
    "    matmul_sorted = np.sort(matmul_ulp)\n",
    "    \n",
    "    conv2d_cdf = np.arange(1, len(conv2d_sorted) + 1) / len(conv2d_sorted)\n",
    "    matmul_cdf = np.arange(1, len(matmul_sorted) + 1) / len(matmul_sorted)\n",
    "    \n",
    "    plt.figure(figsize=default_fig_size)\n",
    "    plt.plot(conv2d_sorted, conv2d_cdf, linewidth=2, label='Conv2D', alpha=0.8)\n",
    "    plt.plot(matmul_sorted, matmul_cdf, linewidth=2, label='Matmul', alpha=0.8)\n",
    "    plt.xlabel('ULP Error')\n",
    "    plt.ylabel('Cumulative Probability')\n",
    "    plt.title(f'ULP Error CDF: TTNN vs {ref_type}\\n{config_name}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Ground Truth Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_torch_conv2d_ground_truth(input_tensor, weight_tensor, bias_tensor, padding, stride, use_bias=True):\n",
    "    \"\"\"\n",
    "    Run conv2d in PyTorch float32 as ground truth.\n",
    "    \n",
    "    Args:\n",
    "        input_tensor: Input tensor (torch)\n",
    "        weight_tensor: Weight tensor (torch)\n",
    "        bias_tensor: Bias tensor (torch)\n",
    "        padding: Padding tuple\n",
    "        stride: Stride tuple\n",
    "        use_bias: Whether to use bias\n",
    "        \n",
    "    Returns:\n",
    "        Output tensor in float32\n",
    "    \"\"\"\n",
    "    # Convert all inputs to float32\n",
    "    input_f32 = input_tensor.to(torch.float32)\n",
    "    weight_f32 = weight_tensor.to(torch.float32)\n",
    "    bias_f32 = bias_tensor.to(torch.float32) if use_bias else None\n",
    "    \n",
    "    # PyTorch expects bias as 1D\n",
    "    if use_bias and bias_f32.dim() == 4:\n",
    "        bias_f32 = bias_f32.reshape(-1)\n",
    "    \n",
    "    return torch.nn.functional.conv2d(\n",
    "        input_f32,\n",
    "        weight_f32,\n",
    "        bias=bias_f32,\n",
    "        stride=stride,\n",
    "        padding=padding\n",
    "    )\n",
    "\n",
    "def run_torch_matmul_ground_truth(A, B):\n",
    "    \"\"\"\n",
    "    Run matmul in PyTorch float32 as ground truth.\n",
    "    \n",
    "    Args:\n",
    "        A: First input tensor\n",
    "        B: Second input tensor\n",
    "        \n",
    "    Returns:\n",
    "        Output tensor in float32\n",
    "    \"\"\"\n",
    "    # Convert to float32\n",
    "    A_f32 = A.to(torch.float32)\n",
    "    B_f32 = B.to(torch.float32)\n",
    "    \n",
    "    return torch.matmul(A_f32, B_f32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTNN Operation Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ttnn_conv2d(input_tensor, weight_tensor, bias_tensor, padding, stride, fp32_acc=False, \n",
    "                    device=None, use_bias=True):\n",
    "    \"\"\"\n",
    "    Run conv2d in ttnn.\n",
    "    \n",
    "    Args:\n",
    "        input_tensor: Input tensor (torch) in NCHW format\n",
    "        weight_tensor: Weight tensor (torch) in OIHW format\n",
    "        bias_tensor: Bias tensor (torch)\n",
    "        padding: Padding tuple (h, w)\n",
    "        stride: Stride tuple (h, w)\n",
    "        fp32_acc: Whether to use fp32 accumulation\n",
    "        device: TTNN device\n",
    "        use_bias: Whether to use bias\n",
    "        \n",
    "    Returns:\n",
    "        Output tensor (torch) in NCHW format\n",
    "    \"\"\"\n",
    "    create_local_device = device is None\n",
    "    if create_local_device:\n",
    "        device = ttnn.CreateDevice(0, l1_small_size=16384)\n",
    "    \n",
    "    # Extract dimensions\n",
    "    batch_size = input_tensor.shape[0]\n",
    "    in_channels = input_tensor.shape[1]\n",
    "    input_height = input_tensor.shape[2]\n",
    "    input_width = input_tensor.shape[3]\n",
    "    out_channels = weight_tensor.shape[0]\n",
    "    kernel_h = weight_tensor.shape[2]\n",
    "    kernel_w = weight_tensor.shape[3]\n",
    "    \n",
    "    # Handle padding and stride\n",
    "    if isinstance(padding, int):\n",
    "        pad_h, pad_w = padding, padding\n",
    "    else:\n",
    "        pad_h, pad_w = padding[0], padding[1]\n",
    "        \n",
    "    if isinstance(stride, int):\n",
    "        str_h, str_w = stride, stride\n",
    "    else:\n",
    "        str_h, str_w = stride[0], stride[1]\n",
    "    \n",
    "    # Convert input from NCHW to NHWC format\n",
    "    input_nhwc = input_tensor.permute(0, 2, 3, 1).contiguous()\n",
    "    \n",
    "    # Prepare bias - reshape to (1, 1, 1, out_channels) if needed\n",
    "    if use_bias:\n",
    "        if bias_tensor.dim() == 1:\n",
    "            bias_tensor = bias_tensor.reshape(1, 1, 1, -1)\n",
    "        elif bias_tensor.shape != (1, 1, 1, out_channels):\n",
    "            bias_tensor = bias_tensor.reshape(1, 1, 1, -1)\n",
    "    \n",
    "    # Determine ttnn dtype\n",
    "    ttnn_dtype = ttnn.bfloat16 if input_tensor.dtype == torch.bfloat16 else ttnn.float32\n",
    "    \n",
    "    # Convert to ttnn tensors\n",
    "    tt_input = ttnn.from_torch(input_nhwc, dtype=ttnn_dtype, layout=ttnn.ROW_MAJOR_LAYOUT, device=device)\n",
    "    tt_weight = ttnn.from_torch(weight_tensor, dtype=ttnn_dtype)\n",
    "    tt_bias = ttnn.from_torch(bias_tensor, dtype=ttnn_dtype) if use_bias else None\n",
    "    \n",
    "    # Configure compute kernel\n",
    "    compute_config = ttnn.init_device_compute_kernel_config(\n",
    "        device.arch(),\n",
    "        math_fidelity=ttnn.MathFidelity.HiFi4,\n",
    "        math_approx_mode=False,\n",
    "        fp32_dest_acc_en=fp32_acc,\n",
    "        packer_l1_acc=True,  # l1_acc is turned on\n",
    "    )\n",
    "    \n",
    "    # Run conv2d\n",
    "    [tt_output, out_dims, _] = ttnn.conv2d(\n",
    "        input_tensor=tt_input,\n",
    "        weight_tensor=tt_weight,\n",
    "        bias_tensor=tt_bias,\n",
    "        device=device,\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        batch_size=batch_size,\n",
    "        input_height=input_height,\n",
    "        input_width=input_width,\n",
    "        kernel_size=(kernel_h, kernel_w),\n",
    "        stride=(str_h, str_w),\n",
    "        padding=(pad_h, pad_h, pad_w, pad_w),\n",
    "        dilation=(1, 1),\n",
    "        groups=1,\n",
    "        compute_config=compute_config,\n",
    "        return_output_dim=True,\n",
    "        return_weights_and_bias=True,\n",
    "    )\n",
    "    \n",
    "    # Convert back to torch and reshape to NCHW\n",
    "    output_torch = ttnn.to_torch(tt_output)\n",
    "    out_height, out_width = out_dims\n",
    "    output_nchw = output_torch.reshape(batch_size, out_height, out_width, out_channels).permute(0, 3, 1, 2)\n",
    "    \n",
    "    if create_local_device:\n",
    "        ttnn.close_device(device)\n",
    "    \n",
    "    return output_nchw\n",
    "\n",
    "def run_ttnn_matmul(A, B, fp32_acc=False, device=None):\n",
    "    \"\"\"\n",
    "    Run matmul in ttnn.\n",
    "    \n",
    "    Args:\n",
    "        A: First input tensor (torch)\n",
    "        B: Second input tensor (torch)\n",
    "        fp32_acc: Whether to use fp32 accumulation\n",
    "        device: TTNN device\n",
    "        \n",
    "    Returns:\n",
    "        Output tensor (torch)\n",
    "    \"\"\"\n",
    "    create_local_device = device is None\n",
    "    if create_local_device:\n",
    "        device = ttnn.CreateDevice(0)\n",
    "    \n",
    "    tt_a = ttnn.from_torch(A, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "    tt_b = ttnn.from_torch(B, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "    \n",
    "    compute_kernel_config = ttnn.init_device_compute_kernel_config(\n",
    "        device.arch(),\n",
    "        math_fidelity=ttnn.MathFidelity.HiFi4,\n",
    "        math_approx_mode=False,\n",
    "        fp32_dest_acc_en=fp32_acc,\n",
    "        packer_l1_acc=True,  # l1_acc is turned on\n",
    "    )\n",
    "    \n",
    "    tt_c = ttnn.matmul(\n",
    "        tt_a,\n",
    "        tt_b,\n",
    "        core_grid=device.core_grid,\n",
    "        compute_kernel_config=compute_kernel_config,\n",
    "    )\n",
    "    \n",
    "    result = ttnn.to_torch(tt_c)\n",
    "    \n",
    "    if create_local_device:\n",
    "        ttnn.close_device(device)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Configuration\n",
    "\n",
    "Select which test cases to run from the available reference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test configurations\n",
    "# Format: (dtype, input_channels, method, fp32_acc, config_name)\n",
    "test_configs = [\n",
    "    # ('bfloat16', 8, 'rand', False, 'bfloat16_ic8_rand'),\n",
    "    # ('bfloat16', 8, 'rand', True, 'bfloat16_ic8_rand_fp32acc'),\n",
    "    # ('bfloat16', 256, 'rand', False, 'bfloat16_ic256_rand'),\n",
    "    # ('bfloat16', 256, 'rand', True, 'bfloat16_ic256_rand_fp32acc'),\n",
    "    # ('float32', 8, 'rand', False, 'float32_ic8_rand'),\n",
    "    # ('float32', 8, 'rand', True, 'float32_ic8_rand_fp32acc'),\n",
    "    \n",
    "    ('bfloat16', 8, 'rand', True, 'bfloat16_ic8_rand_fp32acc'),\n",
    "    ('bfloat16', 256, 'rand', True, 'bfloat16_ic256_rand_fp32acc'),\n",
    "    ('bfloat16', 1024, 'rand', True, 'bfloat16_ic1024_rand_fp32acc'),\n",
    "    ('bfloat16', 4096, 'rand', True, 'bfloat16_ic4096_rand_fp32acc'),\n",
    "    ('bfloat16', 4096, 'rand', True, 'bfloat16_ic4096_rand_fp32acc'),\n",
    "    # 5120\n",
    "    ('bfloat16', 5120, 'rand', True, 'bfloat16_ic5120_rand_fp32acc'),\n",
    "    # 6144\n",
    "    ('bfloat16', 6144, 'rand', True, 'bfloat16_ic6144_rand_fp32acc'),\n",
    "    # 7168\n",
    "    ('bfloat16', 7168, 'rand', True, 'bfloat16_ic7168_rand_fp32acc'),\n",
    "    # 8192\n",
    "    ('bfloat16', 8192, 'rand', True, 'bfloat16_ic8192_rand_fp32acc'),\n",
    "    \n",
    "\n",
    "]\n",
    "\n",
    "# Print configuration\n",
    "print(\"Test Configurations:\")\n",
    "print(\"=\" * 80)\n",
    "for i, (dtype, ic, method, fp32_acc, name) in enumerate(test_configs, 1):\n",
    "    k = loader.ic_to_k[ic]\n",
    "    print(f\"{i}. {name}\")\n",
    "    print(f\"   dtype={dtype}, input_channels={ic}, K={k}, method={method}, fp32_acc={fp32_acc}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments\n",
    "\n",
    "For each configuration:\n",
    "1. Load CUDA reference outputs\n",
    "2. Get validated input tensors\n",
    "3. Compute PyTorch FP32 ground truth\n",
    "4. Run TTNN operations\n",
    "5. Compare ULP errors (vs CUDA and vs PyTorch FP32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ttnn.CreateDevice(0, l1_small_size=16384)\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Get configuration from loader (assumes all test cases use the same config)\n",
    "config = loader.config\n",
    "padding = tuple(config['padding'])\n",
    "stride = tuple(config['stride'])\n",
    "use_bias = config['use_bias']\n",
    "\n",
    "print(f\"\\nReference data configuration:\")\n",
    "print(f\"  Padding: {padding}\")\n",
    "print(f\"  Stride: {stride}\")\n",
    "print(f\"  Use bias: {use_bias}\")\n",
    "print(f\"  Batch: {config['batch']}\")\n",
    "print(f\"  Input size: {config['input_height']}x{config['input_width']}\")\n",
    "print(f\"  Output channels: {config['out_channels']}\")\n",
    "print(f\"  Kernel: {config['kernel_h']}x{config['kernel_w']}\")\n",
    "\n",
    "for dtype, input_channels, method, fp32_acc, config_name in test_configs:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Running: {config_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load CUDA reference outputs\n",
    "    print(\"Loading CUDA reference outputs...\")\n",
    "    reference = get_reference_outputs(dtype, input_channels, method)\n",
    "    conv2d_cuda_ref = reference['conv2d_output']\n",
    "    matmul_cuda_ref = reference['matmul_output']\n",
    "    metadata = reference['metadata']\n",
    "    \n",
    "    print(f\"  Conv2D CUDA reference shape: {conv2d_cuda_ref.shape}\")\n",
    "    print(f\"  Matmul CUDA reference shape: {matmul_cuda_ref.shape}\")\n",
    "    print(f\"  Generation method: {metadata.get('gen_method', 'unknown')}\")\n",
    "    \n",
    "    # Get validated inputs\n",
    "    print(\"\\nLoading and validating input tensors...\")\n",
    "    conv_input, conv_weight, conv_bias = get_conv2d_inputs(dtype, input_channels, method)\n",
    "    matmul_A, matmul_B = get_matmul_inputs(dtype, input_channels, method)\n",
    "    \n",
    "    print(f\"  ✓ Conv2D inputs loaded and verified\")\n",
    "    print(f\"    Input: {conv_input.shape} {conv_input.dtype}\")\n",
    "    print(f\"    Weight: {conv_weight.shape} {conv_weight.dtype}\")\n",
    "    print(f\"    Bias: {conv_bias.shape} {conv_bias.dtype}\")\n",
    "    print(f\"  ✓ Matmul inputs loaded and verified\")\n",
    "    print(f\"    A: {matmul_A.shape} {matmul_A.dtype}\")\n",
    "    print(f\"    B: {matmul_B.shape} {matmul_B.dtype}\")\n",
    "    \n",
    "    # Compute PyTorch FP32 ground truth\n",
    "    print(\"\\nComputing PyTorch FP32 ground truth...\")\n",
    "    conv2d_torch_gt = run_torch_conv2d_ground_truth(\n",
    "        conv_input, conv_weight, conv_bias, padding, stride, use_bias\n",
    "    )\n",
    "    matmul_torch_gt = run_torch_matmul_ground_truth(matmul_A, matmul_B)\n",
    "    print(f\"  ✓ Conv2D ground truth: {conv2d_torch_gt.shape}\")\n",
    "    print(f\"  ✓ Matmul ground truth: {matmul_torch_gt.shape}\")\n",
    "    \n",
    "    # Run TTNN operations\n",
    "    print(\"\\nRunning TTNN operations...\")\n",
    "    conv2d_ttnn = run_ttnn_conv2d(\n",
    "        conv_input, conv_weight, conv_bias,\n",
    "        padding, stride, fp32_acc, device, use_bias\n",
    "    )\n",
    "    print(f\"  ✓ Conv2D complete: {conv2d_ttnn.shape}\")\n",
    "    \n",
    "    matmul_ttnn = run_ttnn_matmul(matmul_A, matmul_B, fp32_acc, device)\n",
    "    print(f\"  ✓ Matmul complete: {matmul_ttnn.shape}\")\n",
    "    \n",
    "    # Compute ULP statistics vs CUDA reference\n",
    "    print(\"\\nComputing ULP statistics vs CUDA reference...\")\n",
    "    conv2d_stats_cuda = compute_ulp_statistics(conv2d_ttnn, conv2d_cuda_ref)\n",
    "    matmul_stats_cuda = compute_ulp_statistics(matmul_ttnn, matmul_cuda_ref)\n",
    "    \n",
    "    # Compute ULP statistics vs PyTorch FP32 ground truth\n",
    "    print(\"Computing ULP statistics vs PyTorch FP32 ground truth...\")\n",
    "    conv2d_stats_torch = compute_ulp_statistics(conv2d_ttnn, conv2d_torch_gt)\n",
    "    matmul_stats_torch = compute_ulp_statistics(matmul_ttnn, matmul_torch_gt)\n",
    "    \n",
    "    # Store results\n",
    "    results[config_name] = {\n",
    "        # Stats vs CUDA\n",
    "        'conv2d_stats_cuda': conv2d_stats_cuda,\n",
    "        'matmul_stats_cuda': matmul_stats_cuda,\n",
    "        # Stats vs PyTorch FP32\n",
    "        'conv2d_stats_torch': conv2d_stats_torch,\n",
    "        'matmul_stats_torch': matmul_stats_torch,\n",
    "        # Tensors\n",
    "        'conv2d_ttnn': conv2d_ttnn,\n",
    "        'conv2d_cuda_ref': conv2d_cuda_ref,\n",
    "        'conv2d_torch_gt': conv2d_torch_gt,\n",
    "        'matmul_ttnn': matmul_ttnn,\n",
    "        'matmul_cuda_ref': matmul_cuda_ref,\n",
    "        'matmul_torch_gt': matmul_torch_gt,\n",
    "        # Metadata\n",
    "        'metadata': metadata,\n",
    "        'dtype': dtype,\n",
    "        'input_channels': input_channels,\n",
    "        'method': method,\n",
    "        'fp32_acc': fp32_acc\n",
    "    }\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ULP STATISTICS: TTNN vs CUDA Reference\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nConv2D:\")\n",
    "    for key, value in conv2d_stats_cuda.items():\n",
    "        print(f\"  {key:8s}: {value:12.4f}\")\n",
    "    print(f\"\\nMatmul:\")\n",
    "    for key, value in matmul_stats_cuda.items():\n",
    "        print(f\"  {key:8s}: {value:12.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ULP STATISTICS: TTNN vs PyTorch FP32 Ground Truth\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nConv2D:\")\n",
    "    for key, value in conv2d_stats_torch.items():\n",
    "        print(f\"  {key:8s}: {value:12.4f}\")\n",
    "    print(f\"\\nMatmul:\")\n",
    "    for key, value in matmul_stats_torch.items():\n",
    "        print(f\"  {key:8s}: {value:12.4f}\")\n",
    "    \n",
    "    # Generate plots for both comparisons\n",
    "    print(\"\\nGenerating plots...\")\n",
    "    print(\"  Plots vs CUDA Reference...\")\n",
    "    plot_ulp_comparison_dual(conv2d_stats_cuda, matmul_stats_cuda, config_name, \"CUDA Reference\")\n",
    "    plot_ulp_histogram_comparison(conv2d_ttnn, conv2d_cuda_ref, matmul_ttnn, matmul_cuda_ref, config_name, \"CUDA Ref\")\n",
    "    plot_ulp_cdf_comparison(conv2d_ttnn, conv2d_cuda_ref, matmul_ttnn, matmul_cuda_ref, config_name, \"CUDA Ref\")\n",
    "    \n",
    "    print(\"  Plots vs PyTorch FP32 Ground Truth...\")\n",
    "    plot_ulp_comparison_dual(conv2d_stats_torch, matmul_stats_torch, config_name, \"PyTorch FP32 GT\")\n",
    "    plot_ulp_histogram_comparison(conv2d_ttnn, conv2d_torch_gt, matmul_ttnn, matmul_torch_gt, config_name, \"PyTorch FP32 GT\")\n",
    "    plot_ulp_cdf_comparison(conv2d_ttnn, conv2d_torch_gt, matmul_ttnn, matmul_torch_gt, config_name, \"PyTorch FP32 GT\")\n",
    "\n",
    "ttnn.close_device(device)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"All experiments completed!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Comparison: Both References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary comparison for both references\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SUMMARY: ULP Error Comparison\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\n\" + \"-\"*100)\n",
    "print(\"TTNN vs CUDA Reference\")\n",
    "print(\"-\"*100)\n",
    "print(f\"{'Config':<30} {'Operation':<10} {'Mean':<12} {'Median':<12} {'Max':<12} {'P95':<12} {'P99':<12}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for config_name, data in results.items():\n",
    "    conv2d_stats = data['conv2d_stats_cuda']\n",
    "    matmul_stats = data['matmul_stats_cuda']\n",
    "    \n",
    "    print(f\"{config_name:<30} {'Conv2D':<10} {conv2d_stats['mean']:<12.4f} {conv2d_stats['median']:<12.4f} {conv2d_stats['max']:<12.4f} {conv2d_stats['p95']:<12.4f} {conv2d_stats['p99']:<12.4f}\")\n",
    "    print(f\"{'':<30} {'Matmul':<10} {matmul_stats['mean']:<12.4f} {matmul_stats['median']:<12.4f} {matmul_stats['max']:<12.4f} {matmul_stats['p95']:<12.4f} {matmul_stats['p99']:<12.4f}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "print(\"\\n\" + \"-\"*100)\n",
    "print(\"TTNN vs PyTorch FP32 Ground Truth\")\n",
    "print(\"-\"*100)\n",
    "print(f\"{'Config':<30} {'Operation':<10} {'Mean':<12} {'Median':<12} {'Max':<12} {'P95':<12} {'P99':<12}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for config_name, data in results.items():\n",
    "    conv2d_stats = data['conv2d_stats_torch']\n",
    "    matmul_stats = data['matmul_stats_torch']\n",
    "    \n",
    "    print(f\"{config_name:<30} {'Conv2D':<10} {conv2d_stats['mean']:<12.4f} {conv2d_stats['median']:<12.4f} {conv2d_stats['max']:<12.4f} {conv2d_stats['p95']:<12.4f} {conv2d_stats['p99']:<12.4f}\")\n",
    "    print(f\"{'':<30} {'Matmul':<10} {matmul_stats['mean']:<12.4f} {matmul_stats['median']:<12.4f} {matmul_stats['max']:<12.4f} {matmul_stats['p95']:<12.4f} {matmul_stats['p99']:<12.4f}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "# Plot side-by-side comparison\n",
    "config_names = list(results.keys())\n",
    "conv2d_means_cuda = [results[name]['conv2d_stats_cuda']['mean'] for name in config_names]\n",
    "matmul_means_cuda = [results[name]['matmul_stats_cuda']['mean'] for name in config_names]\n",
    "conv2d_means_torch = [results[name]['conv2d_stats_torch']['mean'] for name in config_names]\n",
    "matmul_means_torch = [results[name]['matmul_stats_torch']['mean'] for name in config_names]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "x = np.arange(len(config_names))\n",
    "width = 0.35\n",
    "\n",
    "# CUDA Reference\n",
    "bars1 = ax1.bar(x - width/2, conv2d_means_cuda, width, label='Conv2D', alpha=0.8)\n",
    "bars2 = ax1.bar(x + width/2, matmul_means_cuda, width, label='Matmul', alpha=0.8)\n",
    "ax1.set_ylabel('Mean ULP Error')\n",
    "ax1.set_title('Mean ULP Error: TTNN vs CUDA Reference\\nLower is better')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(config_names, rotation=25, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.2f}',\n",
    "               ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# PyTorch FP32 Ground Truth\n",
    "bars3 = ax2.bar(x - width/2, conv2d_means_torch, width, label='Conv2D', alpha=0.8)\n",
    "bars4 = ax2.bar(x + width/2, matmul_means_torch, width, label='Matmul', alpha=0.8)\n",
    "ax2.set_ylabel('Mean ULP Error')\n",
    "ax2.set_title('Mean ULP Error: TTNN vs PyTorch FP32 GT\\nLower is better')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(config_names, rotation=25, ha='right')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars3, bars4]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.2f}',\n",
    "               ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ratio Analysis: Conv2D vs Matmul (Both References)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ratios for both references\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"RATIO ANALYSIS: Conv2D ULP / Matmul ULP\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "print(\"\\n\" + \"-\"*90)\n",
    "print(\"vs CUDA Reference\")\n",
    "print(\"-\"*90)\n",
    "print(f\"{'Config':<30} {'Mean Ratio':<15} {'Median Ratio':<15} {'Max Ratio':<15}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "ratios_cuda = []\n",
    "for config_name, data in results.items():\n",
    "    conv2d = data['conv2d_stats_cuda']\n",
    "    matmul = data['matmul_stats_cuda']\n",
    "    \n",
    "    mean_r = conv2d['mean'] / matmul['mean'] if matmul['mean'] > 0 else float('inf')\n",
    "    median_r = conv2d['median'] / matmul['median'] if matmul['median'] > 0 else float('inf')\n",
    "    max_r = conv2d['max'] / matmul['max'] if matmul['max'] > 0 else float('inf')\n",
    "    \n",
    "    ratios_cuda.append((mean_r, median_r, max_r))\n",
    "    print(f\"{config_name:<30} {mean_r:<15.4f} {median_r:<15.4f} {max_r:<15.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*90)\n",
    "print(\"vs PyTorch FP32 Ground Truth\")\n",
    "print(\"-\"*90)\n",
    "print(f\"{'Config':<30} {'Mean Ratio':<15} {'Median Ratio':<15} {'Max Ratio':<15}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "ratios_torch = []\n",
    "for config_name, data in results.items():\n",
    "    conv2d = data['conv2d_stats_torch']\n",
    "    matmul = data['matmul_stats_torch']\n",
    "    \n",
    "    mean_r = conv2d['mean'] / matmul['mean'] if matmul['mean'] > 0 else float('inf')\n",
    "    median_r = conv2d['median'] / matmul['median'] if matmul['median'] > 0 else float('inf')\n",
    "    max_r = conv2d['max'] / matmul['max'] if matmul['max'] > 0 else float('inf')\n",
    "    \n",
    "    ratios_torch.append((mean_r, median_r, max_r))\n",
    "    print(f\"{config_name:<30} {mean_r:<15.4f} {median_r:<15.4f} {max_r:<15.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"=\"*90)\n",
    "print(\"Ratio > 1.0 → Conv2D has more ULP error than Matmul\")\n",
    "print(\"Ratio < 1.0 → Conv2D has less ULP error than Matmul\")\n",
    "print(\"Ratio ≈ 1.0 → Conv2D and Matmul have similar error characteristics\")\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Sweep Analysis (Both References)\n",
    "\n",
    "Analyze how ULP error scales with K (accumulation dimension) for both references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group results by dtype, method, and fp32_acc\n",
    "grouped_cuda = {}\n",
    "grouped_torch = {}\n",
    "\n",
    "for config_name, data in results.items():\n",
    "    dtype = data['dtype']\n",
    "    method = data['method']\n",
    "    fp32_acc = data['fp32_acc']\n",
    "    ic = data['input_channels']\n",
    "    k = loader.ic_to_k[ic]\n",
    "    \n",
    "    group_key = f\"{dtype}_{method}_fp32acc={fp32_acc}\"\n",
    "    \n",
    "    # CUDA reference\n",
    "    if group_key not in grouped_cuda:\n",
    "        grouped_cuda[group_key] = {'k_values': [], 'conv2d_mean': [], 'matmul_mean': []}\n",
    "    grouped_cuda[group_key]['k_values'].append(k)\n",
    "    grouped_cuda[group_key]['conv2d_mean'].append(data['conv2d_stats_cuda']['mean'])\n",
    "    grouped_cuda[group_key]['matmul_mean'].append(data['matmul_stats_cuda']['mean'])\n",
    "    \n",
    "    # PyTorch FP32 GT\n",
    "    if group_key not in grouped_torch:\n",
    "        grouped_torch[group_key] = {'k_values': [], 'conv2d_mean': [], 'matmul_mean': []}\n",
    "    grouped_torch[group_key]['k_values'].append(k)\n",
    "    grouped_torch[group_key]['conv2d_mean'].append(data['conv2d_stats_torch']['mean'])\n",
    "    grouped_torch[group_key]['matmul_mean'].append(data['matmul_stats_torch']['mean'])\n",
    "\n",
    "# Sort by K\n",
    "for group_key in grouped_cuda:\n",
    "    group = grouped_cuda[group_key]\n",
    "    sorted_indices = sorted(range(len(group['k_values'])), key=lambda i: group['k_values'][i])\n",
    "    for key in group:\n",
    "        group[key] = [group[key][i] for i in sorted_indices]\n",
    "\n",
    "for group_key in grouped_torch:\n",
    "    group = grouped_torch[group_key]\n",
    "    sorted_indices = sorted(range(len(group['k_values'])), key=lambda i: group['k_values'][i])\n",
    "    for key in group:\n",
    "        group[key] = [group[key][i] for i in sorted_indices]\n",
    "\n",
    "# Plot ULP vs K for both references\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Conv2D - CUDA Reference\n",
    "for group_key, group in grouped_cuda.items():\n",
    "    axes[0, 0].plot(group['k_values'], group['conv2d_mean'], marker='o', linewidth=2, label=group_key)\n",
    "axes[0, 0].set_xlabel('K (Accumulation Dimension)', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Mean ULP Error', fontsize=12)\n",
    "axes[0, 0].set_title('Conv2D: TTNN vs CUDA Reference', fontsize=13)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_xscale('log')\n",
    "axes[0, 0].set_yscale('log')\n",
    "\n",
    "# Matmul - CUDA Reference\n",
    "for group_key, group in grouped_cuda.items():\n",
    "    axes[0, 1].plot(group['k_values'], group['matmul_mean'], marker='s', linewidth=2, label=group_key)\n",
    "axes[0, 1].set_xlabel('K (Accumulation Dimension)', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Mean ULP Error', fontsize=12)\n",
    "axes[0, 1].set_title('Matmul: TTNN vs CUDA Reference', fontsize=13)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_xscale('log')\n",
    "axes[0, 1].set_yscale('log')\n",
    "\n",
    "# Conv2D - PyTorch FP32 GT\n",
    "for group_key, group in grouped_torch.items():\n",
    "    axes[1, 0].plot(group['k_values'], group['conv2d_mean'], marker='o', linewidth=2, label=group_key)\n",
    "axes[1, 0].set_xlabel('K (Accumulation Dimension)', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Mean ULP Error', fontsize=12)\n",
    "axes[1, 0].set_title('Conv2D: TTNN vs PyTorch FP32 GT', fontsize=13)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_xscale('log')\n",
    "axes[1, 0].set_yscale('log')\n",
    "\n",
    "# Matmul - PyTorch FP32 GT\n",
    "for group_key, group in grouped_torch.items():\n",
    "    axes[1, 1].plot(group['k_values'], group['matmul_mean'], marker='s', linewidth=2, label=group_key)\n",
    "axes[1, 1].set_xlabel('K (Accumulation Dimension)', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Mean ULP Error', fontsize=12)\n",
    "axes[1, 1].set_title('Matmul: TTNN vs PyTorch FP32 GT', fontsize=13)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_xscale('log')\n",
    "axes[1, 1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- ULP error generally increases with K (more accumulation = more rounding errors)\")\n",
    "print(\"- fp32acc configurations should show slower error growth\")\n",
    "print(\"- Compare how the two references (CUDA vs PyTorch FP32) differ\")\n",
    "print(\"- CUDA reference may show different behavior due to higher precision settings\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
