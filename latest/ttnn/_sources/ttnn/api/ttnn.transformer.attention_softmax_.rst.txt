ttnn.transformer.attention\_softmax\_
=====================================

.. currentmodule:: ttnn.transformer

.. _ttnn.transformer.attention_softmax_:

.. autooperation:: ttnn.transformer.attention_softmax_