ttnn.transformer.attention\_softmax
===================================

.. currentmodule:: ttnn.transformer

.. _ttnn.transformer.attention_softmax:

.. autooperation:: ttnn.transformer.attention_softmax