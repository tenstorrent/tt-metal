<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ttnn.group_norm &mdash; TT-NN  documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/tt_theme.css" type="text/css" />
    <link rel="shortcut icon" href="../../_static/favicon.png"/>
    <link rel="canonical" href="/tt-metal/latest/ttnn/ttnn/api/ttnn.group_norm.html" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js?v=b3ba4146"></script>
        <script src="../../_static/doctools.js?v=888ff710"></script>
        <script src="../../_static/sphinx_highlight.js?v=4825356b"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="../../_static/posthog.js?v=aa5946f9"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="ttnn.layer_norm" href="ttnn.layer_norm.html" />
    <link rel="prev" title="ttnn.sort" href="ttnn.sort.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >



<a href="https://docs.tenstorrent.com/">
    <img src="../../_static/tt_logo.svg" class="logo" alt="Logo"/>
</a>

<a href="../../index.html">
    TT-NN
</a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">TTNN</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../about.html">What is TT-NN?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installing.html">Install</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage.html">Using TT-NN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensor.html">Tensor</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../api.html">APIs</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../api.html#device">Device</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#memory-config">Memory Config</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../api.html#operations">Operations</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../api.html#core">Core</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#tensor-creation">Tensor Creation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#matrix-multiplication">Matrix Multiplication</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#pointwise-unary">Pointwise Unary</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#pointwise-binary">Pointwise Binary</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#pointwise-ternary">Pointwise Ternary</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#losses">Losses</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#reduction">Reduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#data-movement">Data Movement</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="../api.html#normalization">Normalization</a><ul class="current">
<li class="toctree-l4 current"><a class="current reference internal" href="#">ttnn.group_norm</a></li>
<li class="toctree-l4"><a class="reference internal" href="ttnn.layer_norm.html">ttnn.layer_norm</a></li>
<li class="toctree-l4"><a class="reference internal" href="ttnn.rms_norm.html">ttnn.rms_norm</a></li>
<li class="toctree-l4"><a class="reference internal" href="ttnn.batch_norm.html">ttnn.batch_norm</a></li>
<li class="toctree-l4"><a class="reference internal" href="ttnn.softmax.html">ttnn.softmax</a></li>
<li class="toctree-l4"><a class="reference internal" href="ttnn.scale_mask_softmax.html">ttnn.scale_mask_softmax</a></li>
<li class="toctree-l4"><a class="reference internal" href="ttnn.softmax_in_place.html">ttnn.softmax_in_place</a></li>
<li class="toctree-l4"><a class="reference internal" href="ttnn.scale_mask_softmax_in_place.html">ttnn.scale_mask_softmax_in_place</a></li>
<li class="toctree-l4"><a class="reference internal" href="ttnn.scale_causal_mask_hw_dims_softmax_in_place.html">ttnn.scale_causal_mask_hw_dims_softmax_in_place</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#normalization-program-configs">Normalization Program Configs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#moreh-operations">Moreh Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#transformer">Transformer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#ccl">CCL</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#embedding">Embedding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#convolution">Convolution</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#pooling">Pooling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#vision">Vision</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#kv-cache">KV Cache</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#model-conversion">Model Conversion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#reports">Reports</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#operation-hooks">Operation Hooks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onboarding.html">Onboarding New Functionality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converting_torch_model_to_ttnn.html">Converting PyTorch Model to TT-NN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../adding_new_ttnn_operation.html">Adding New TT-NN Operation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../profiling_ttnn_operations.html">Profiling TT-NN Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demos.html">Building and Uplifting Demos</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../resources/support.html">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../resources/contributing.html">Contributing as a developer</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">TT-NN</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../api.html">APIs</a></li>
      <li class="breadcrumb-item active">ttnn.group_norm</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/ttnn/api/ttnn.group_norm.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="ttnn-group-norm">
<h1>ttnn.group_norm<a class="headerlink" href="#ttnn-group-norm" title="Permalink to this heading"></a>
</h1>
<span class="target" id="id1"></span><dl class="py data">
<dt class="sig sig-object py" id="ttnn.group_norm">
<span class="sig-prename descclassname"><span class="pre">ttnn.</span></span><span class="sig-name descname"><span class="pre">group_norm</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Operation(python_fully_qualified_name='ttnn.group_norm',</span> <span class="pre">function=&lt;ttnn._ttnn.operations.normalization.group_norm_t</span> <span class="pre">object&gt;,</span> <span class="pre">preprocess_golden_function_inputs=&lt;function</span> <span class="pre">default_preprocess_golden_function_inputs&gt;,</span> <span class="pre">golden_function=&lt;function</span> <span class="pre">_golden_function&gt;,</span> <span class="pre">postprocess_golden_function_outputs=&lt;function</span> <span class="pre">_postprocess_golden_function_outputs&gt;,</span> <span class="pre">is_cpp_operation=True,</span> <span class="pre">is_experimental=False)</span></em><a class="headerlink" href="#ttnn.group_norm" title="Permalink to this definition"></a>
</dt>
<dd>
<p><code class="docutils literal notranslate"><span class="pre">ttnn.group_norm(input_tensor:</span> <span class="pre">ttnn.Tensor,</span> <span class="pre">num_groups:</span> <span class="pre">int,</span> <span class="pre">epsilon:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">1e-12,</span> <span class="pre">input_mask:</span> <span class="pre">Optional[ttnn.Tensor]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">weight:</span> <span class="pre">Optional[ttnn.Tensor]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">bias:</span> <span class="pre">Optional[ttnn.Tensor]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">reciprocals:</span> <span class="pre">Optional[ttnn.Tensor]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">memory_config:</span> <span class="pre">Optional[ttnn.MemoryConfig]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">dtype:</span> <span class="pre">Optional[ttnn.DataType]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">core_grid:</span> <span class="pre">Optional[ttnn.CoreGrid]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">inplace:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True,</span> <span class="pre">output_layout:</span> <span class="pre">Optional[ttnn.Layout]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">num_out_blocks:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">compute_kernel_config:</span> <span class="pre">Optional[ttnn.DeviceComputeKernelConfig]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">negative_mask:</span> <span class="pre">Optional[ttnn.Tensor]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">use_welford:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False)</span> <span class="pre">-&gt;</span> <span class="pre">ttnn.Tensor</span></code></p>
<blockquote>
<div>
<p>Computes group_norm over <code class="xref py py-attr docutils literal notranslate"><span class="pre">input_tensor</span></code>.
See <a class="reference external" href="https://arxiv.org/abs/1803.08494">Group Normalization</a> for more details.</p>
<div class="math notranslate nohighlight">
\[\text{group_norm}(x, \gamma, \beta, \epsilon) = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} \cdot \gamma + \beta\]</div>
<dl class="simple">
<dt>Where:</dt>
<dd>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> are the mean and variance of the input tensor, respectively</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are the learnable scale and shift parameters, respectively</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> is a small constant.</p></li>
</ul>
</dd>
</dl>
<p>GroupNorm traditionally operates by splitting the input tensor’s channels into groups, and then computing the mean and variance of each group.
This implementation is slightly different, in that it forms the groups using the tensor’s last dimension.
Concretely, the input tensor is expected to have a shape of [N, 1, H*W, C], where C is the dimension along which the groups are formed.</p>
<dl class="simple">
<dt>TTNN provides utility functions to help prepare this op’s inputs.</dt>
<dd>
<ul class="simple">
<li><p>When using sharded input tensors, <code class="xref py py-func docutils literal notranslate"><span class="pre">ttnn.determine_expected_group_norm_sharded_config_and_grid_size()</span></code> can provide the appropriate memory configuration and grid size.</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">ttnn.create_group_norm_input_mask()</span></code> creates the appropriate input mask for a given tensor dimension and group size.</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">ttnn.create_group_norm_weight_bias_rm()</span></code> converts the weight and bias tensors into appropriately padded and tiled inputs</p></li>
</ul>
</dd>
</dl>
<p>See the sharded example in this document for more details on how to properly prepare the op’s inputs using these functions.</p>
</div>
</blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span>
</dt>
<dd class="field-odd">
<p><strong>input_tensor</strong> (<em>ttnn.Tensor</em>) – the input tensor.</p>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span>
</dt>
<dd class="field-even">
<ul class="simple">
<li><p><strong>num_groups</strong> (<em>int</em>) – Number of groups to split the tensor’s channels into.</p></li>
<li><p><strong>epsilon</strong> (<em>float</em>) – Defaults to 1e-12.</p></li>
<li><p><strong>input_mask</strong> (<em>ttnn.Tensor</em><em>, </em><em>optional</em>) – Defaults to <cite>None</cite>. When processing the inputs, the mask is used to only look at the elements of the current group.</p></li>
<li><p><strong>weight</strong> (<em>ttnn.Tensor</em><em>, </em><em>optional</em>) – Defaults to <cite>None</cite>.</p></li>
<li><p><strong>bias</strong> (<em>ttnn.Tensor</em><em>, </em><em>optional</em>) – Defaults to <cite>None</cite>.</p></li>
<li><p><strong>memory_config</strong> (<em>ttnn.MemoryConfig</em><em>, </em><em>optional</em>) – Memory configuration for the operation. Defaults to <cite>None</cite>.</p></li>
<li><p><strong>dtype</strong> (<em>ttnn.DataType</em><em>, </em><em>optional</em>) – Defaults to <cite>None</cite>.</p></li>
<li><p><strong>core_grid</strong> (<em>CoreGrid</em><em>, </em><em>optional</em>) – Defaults to <cite>None</cite>.</p></li>
<li><p><strong>inplace</strong> (<em>bool</em><em>, </em><em>optional</em>) – Defaults to <cite>True</cite>.</p></li>
<li><p><strong>output_layout</strong> (<em>ttnn.Layout</em><em>, </em><em>optional</em>) – Defaults to <cite>None</cite>.</p></li>
<li><p><strong>num_out_blocks</strong> (<em>int</em><em>, </em><em>optional</em>) – Defaults to <cite>None</cite>.</p></li>
<li><p><strong>compute_kernel_config</strong> (<em>ttnn.DeviceComputeKernelConfig</em><em>, </em><em>optional</em>) – Compute kernel configuration for the op. Defaults to <cite>None</cite>.</p></li>
<li><p><strong>negative_mask</strong> (<em>ttnn.Tensor</em><em>, </em><em>optional</em>) – Defaults to <cite>None</cite>. Can be used only in row-major sharded input/output tensors. Used to reduce the number of CB’s used in the sharded version of the kernel by overlapping the CB’s used for tilized input and output. (The kernel is in fact row major variant, but is internally tilizing RM into tilized inputs).</p></li>
<li><p><strong>use_welford</strong> (<em>bool</em><em>, </em><em>optional</em>) – Defaults to <cite>False</cite>. If <cite>True</cite>, the Welford’s algorithm is used to compute the mean and variance.</p></li>
<li><p><strong>reciprocals</strong> (<em>ttnn.Tensor</em><em>, </em><em>optional</em>) – Defaults to <cite>None</cite>. FP32 tensor containing pre-computed reciprocal values. Only valid when use_welford is True. Must be sharded to L1 memory in each core.</p></li>
</ul>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span>
</dt>
<dd class="field-odd">
<p><em>ttnn.Tensor</em> – the output tensor.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The supported input data types and layouts:</p>
<table class="docutils align-default" id="id2">
<caption>
<span class="caption-text">input_tensor</span><a class="headerlink" href="#id2" title="Permalink to this table"></a>
</caption>
<thead>
<tr class="row-odd">
<th class="head"><p>dtype</p></th>
<th class="head"><p>layout</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even">
<td><p>BFLOAT16</p></td>
<td><p>TILE, ROW_MAJOR</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default" id="id3">
<caption>
<span class="caption-text">weight (gamma) and bias (beta)</span><a class="headerlink" href="#id3" title="Permalink to this table"></a>
</caption>
<thead>
<tr class="row-odd">
<th class="head"><p>dtype</p></th>
<th class="head"><p>layout</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even">
<td><p>BFLOAT16</p></td>
<td><p>ROW_MAJOR</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default" id="id4">
<caption>
<span class="caption-text">input_mask</span><a class="headerlink" href="#id4" title="Permalink to this table"></a>
</caption>
<thead>
<tr class="row-odd">
<th class="head"><p>dtype</p></th>
<th class="head"><p>layout</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even">
<td><p>BFLOAT16, BFLOAT8_B</p></td>
<td><p>TILE</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default" id="id5">
<caption>
<span class="caption-text">output_tensor</span><a class="headerlink" href="#id5" title="Permalink to this table"></a>
</caption>
<thead>
<tr class="row-odd">
<th class="head"><p>dtype</p></th>
<th class="head"><p>layout</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even">
<td><p>BFLOAT16</p></td>
<td><p>TILE, ROW_MAJOR</p></td>
</tr>
</tbody>
</table>
</div>
<dl>
<dt>Memory Support:</dt>
<dd>
<ul class="simple">
<li><p>Interleaved: DRAM and L1</p></li>
<li><p>Sharded (L1): Height and Block sharded</p></li>
</ul>
</dd>
<dt>Limitations:</dt>
<dd>
<ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">input_tensor</span></code> is a 4D tensor of shape [N, 1, H*W, C] and is allocated on the device</p></li>
<li><p>For the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input_tensor</span></code>, N*H*W must be a multiple of the tile size (32) and C must divide evenly into <code class="xref py py-attr docutils literal notranslate"><span class="pre">num_groups</span></code>.</p></li>
<li><p>For the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input_mask</span></code>, C must match the number of groups, H must match a tile’s height, and W must be a multiple of a tile’s width.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">gamma</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">beta</span></code> must be provided</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">inplace</span></code> is not supported for TILE-layout inputs and requires input and output layouts to be identical.</p></li>
<li><p>When generating inputs (e.g. weight, bias) for block sharded tensors, the number of cores in a column should draw upon core.x rather than core.y.</p></li>
<li><p>When generating inputs (e.g. weight, bias) for height sharded tensors, the number of cores in a column should be 1 rather than core.y.</p></li>
<li><p>Width-sharding is not supported</p></li>
</ul>
</dd>
<dt>Example (Sharded Input):</dt>
<dd>
<div class="highlight-python notranslate">
<div class="highlight"><pre><span></span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">num_groups</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># Prepare random inputs</span>
<span class="n">torch_input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
<span class="n">torch_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="n">C</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
<span class="n">torch_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="n">C</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>

<span class="c1"># Generate random inputs and prepare reference output</span>
<span class="n">torch_output_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span>
    <span class="n">torch_input_tensor</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">torch_weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">torch_bias</span>
<span class="p">)</span>

<span class="c1"># Permute the torch output to match the TTNN format, so they can be compared</span>
<span class="n">torch_output_tensor</span> <span class="o">=</span> <span class="n">torch_output_tensor</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">W</span> <span class="o">*</span> <span class="n">H</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>

<span class="c1">#Prepare TTNN input</span>
<span class="c1"># Determine how to shard the input tensor</span>
<span class="n">sharded_mem_config</span><span class="p">,</span> <span class="n">grid_size</span> <span class="o">=</span> <span class="n">ttnn</span><span class="o">.</span><span class="n">determine_expected_group_norm_sharded_config_and_grid_size</span><span class="p">(</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">,</span>
    <span class="n">num_channels</span> <span class="o">=</span> <span class="n">C</span><span class="p">,</span>
    <span class="n">num_groups</span> <span class="o">=</span> <span class="n">num_groups</span><span class="p">,</span>
    <span class="n">input_nhw</span> <span class="o">=</span> <span class="n">N</span> <span class="o">*</span> <span class="n">H</span> <span class="o">*</span> <span class="n">W</span><span class="p">,</span>
    <span class="n">is_height_sharded</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">is_row_major</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span>

<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">ttnn</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span>
    <span class="n">torch_input_tensor</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">W</span> <span class="o">*</span> <span class="n">H</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">ttnn</span><span class="o">.</span><span class="n">DataType</span><span class="o">.</span><span class="n">BFLOAT16</span><span class="p">,</span>
    <span class="n">layout</span><span class="o">=</span><span class="n">ttnn</span><span class="o">.</span><span class="n">ROW_MAJOR_LAYOUT</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="n">memory_config</span><span class="o">=</span><span class="n">sharded_mem_config</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Input mask - this is needed for each group to be able to select the correct elements of the input tensor</span>
<span class="c1"># In general, it will have dimensions of [1, num_groups, 32, 32*block_wt]</span>

<span class="c1"># In this example, C=64 and num_groups=2, so each group is 32 channels (i.e. one tile) wide</span>
<span class="c1"># As a result, the input_mask_tensor is a [1, 2, 32, 32] tensor where every value is 1</span>

<span class="c1"># If instead num_groups was 4, each group would be 16 channels (i.e. half a tile) wide</span>
<span class="c1"># As a result, the input_mask_tensor would be a [1, 4, 32, 32] tensor that selects either the first or second half of the tile</span>
<span class="c1"># e.g. The mask at [0][0][:][:] would be a 32x32 tensor where the left half is 1 and the right half is 0</span>
<span class="c1"># While [0][1][:][:] would be a 32x32 tensor where the left half is 0 and the right half is 1</span>
<span class="n">input_mask_tensor</span> <span class="o">=</span> <span class="n">ttnn</span><span class="o">.</span><span class="n">create_group_norm_input_mask</span><span class="p">(</span>
    <span class="n">num_channels</span><span class="o">=</span><span class="n">C</span><span class="p">,</span>
    <span class="n">num_groups</span><span class="o">=</span><span class="n">num_groups</span><span class="p">,</span>
    <span class="n">num_cores_across_channel</span><span class="o">=</span><span class="mi">1</span> <span class="c1"># As explained in the Limitations, supply 1 for height sharded input tensors</span>
<span class="p">)</span>

<span class="n">input_mask_tensor</span> <span class="o">=</span> <span class="n">ttnn</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span>
    <span class="n">input_mask_tensor</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">ttnn</span><span class="o">.</span><span class="n">DataType</span><span class="o">.</span><span class="n">BFLOAT8_B</span><span class="p">,</span>
    <span class="n">layout</span><span class="o">=</span><span class="n">ttnn</span><span class="o">.</span><span class="n">TILE_LAYOUT</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="n">memory_config</span><span class="o">=</span><span class="n">ttnn</span><span class="o">.</span><span class="n">DRAM_MEMORY_CONFIG</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Prepare gamma and beta for TTNN. Currently these are just 1D tensors of size [C], which isn't compatible with tile based processing</span>
<span class="c1"># First they will zero padded if needed (does not apply to this example)</span>
<span class="c1"># Then reshaped to be [1, 1, tiles_per_core_total, 32], which in this case will be [1, 1, 2, 32]</span>

<span class="c1"># As with the input mask, we supply a core count of 1 for height sharded input tensors</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="n">ttnn</span><span class="o">.</span><span class="n">create_group_norm_weight_bias_rm</span><span class="p">(</span><span class="n">input_tensor</span> <span class="o">=</span><span class="n">torch_weight</span><span class="p">,</span> <span class="n">num_channels</span> <span class="o">=</span> <span class="n">C</span><span class="p">,</span> <span class="n">num_cores_x</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">ttnn</span><span class="o">.</span><span class="n">create_group_norm_weight_bias_rm</span><span class="p">(</span><span class="n">input_tensor</span> <span class="o">=</span><span class="n">torch_bias</span><span class="p">,</span> <span class="n">num_channels</span> <span class="o">=</span> <span class="n">C</span><span class="p">,</span> <span class="n">num_cores_x</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">gamma_t</span> <span class="o">=</span> <span class="n">ttnn</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span>
    <span class="n">gamma</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">ttnn</span><span class="o">.</span><span class="n">DataType</span><span class="o">.</span><span class="n">BFLOAT16</span><span class="p">,</span>
    <span class="n">layout</span><span class="o">=</span><span class="n">ttnn</span><span class="o">.</span><span class="n">ROW_MAJOR_LAYOUT</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="n">memory_config</span><span class="o">=</span><span class="n">ttnn</span><span class="o">.</span><span class="n">DRAM_MEMORY_CONFIG</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">beta_t</span> <span class="o">=</span> <span class="n">ttnn</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span>
    <span class="n">beta</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">ttnn</span><span class="o">.</span><span class="n">DataType</span><span class="o">.</span><span class="n">BFLOAT16</span><span class="p">,</span>
    <span class="n">layout</span><span class="o">=</span><span class="n">ttnn</span><span class="o">.</span><span class="n">ROW_MAJOR_LAYOUT</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="n">memory_config</span><span class="o">=</span><span class="n">ttnn</span><span class="o">.</span><span class="n">DRAM_MEMORY_CONFIG</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Compute the TTNN output and compare with the reference output</span>
<span class="n">output_tensor</span> <span class="o">=</span> <span class="n">ttnn</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span>
    <span class="n">input_tensor</span><span class="p">,</span>
    <span class="n">num_groups</span><span class="o">=</span><span class="n">num_groups</span><span class="p">,</span>
    <span class="n">input_mask</span><span class="o">=</span><span class="n">input_mask_tensor</span><span class="p">,</span>
    <span class="n">weight</span><span class="o">=</span><span class="n">gamma_t</span><span class="p">,</span>
    <span class="n">bias</span><span class="o">=</span><span class="n">beta_t</span><span class="p">,</span>
    <span class="n">memory_config</span><span class="o">=</span><span class="n">sharded_mem_config</span><span class="p">,</span>
    <span class="n">core_grid</span><span class="o">=</span><span class="n">grid_size</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">output_tensor</span> <span class="o">=</span> <span class="n">ttnn</span><span class="o">.</span><span class="n">to_torch</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">)</span>
<span class="n">assert_with_pcc</span><span class="p">(</span><span class="n">torch_output_tensor</span><span class="p">,</span> <span class="n">output_tensor</span><span class="p">,</span> <span class="mf">0.9999</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<div class="admonition-example admonition">
<p class="admonition-title">Example</p>
<div class="highlight-python notranslate">
<div class="highlight"><pre><span></span><span class="n">tile_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">480</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">64</span>
<span class="n">grid_size</span> <span class="o">=</span> <span class="n">ttnn</span><span class="o">.</span><span class="n">CoreGrid</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">num_out_blocks</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">num_groups</span> <span class="o">=</span> <span class="mi">8</span> <span class="c1"># This must be a multiple of grid_size.y (1 in this example)</span>

<span class="n">input_tensor_row_major</span> <span class="o">=</span> <span class="n">ttnn</span><span class="o">.</span><span class="n">rand</span><span class="p">([</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">H</span><span class="o">*</span><span class="n">W</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">ttnn</span><span class="o">.</span><span class="n">DataType</span><span class="o">.</span><span class="n">BFLOAT16</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="n">ttnn</span><span class="o">.</span><span class="n">ROW_MAJOR_LAYOUT</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">input_tensor_tilized</span> <span class="o">=</span> <span class="n">ttnn</span><span class="o">.</span><span class="n">tilize_with_zero_padding</span><span class="p">(</span><span class="n">input_tensor_row_major</span><span class="p">,</span> <span class="n">use_multicore</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># input mask</span>
<span class="n">width_per_group</span> <span class="o">=</span> <span class="n">C</span> <span class="o">//</span> <span class="n">num_groups</span> <span class="c1"># C must be a multiple of num_groups</span>
<span class="n">max_tiles_group_can_span</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">width_per_group</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">tile_size</span><span class="p">)</span>
<span class="n">input_mask_tensor</span> <span class="o">=</span> <span class="n">ttnn</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="n">tile_size</span><span class="p">,</span> <span class="n">max_tiles_group_can_span</span> <span class="o">*</span> <span class="n">tile_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">ttnn</span><span class="o">.</span><span class="n">DataType</span><span class="o">.</span><span class="n">BFLOAT8_B</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="n">ttnn</span><span class="o">.</span><span class="n">TILE_LAYOUT</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># gamma/beta</span>
<span class="n">values_per_chunk</span> <span class="o">=</span> <span class="n">C</span> <span class="o">//</span> <span class="n">grid_size</span><span class="o">.</span><span class="n">y</span> <span class="c1"># 480 / 1 = 480. Note that 480 is a multiple of 32, so no padding up to the next tile is needed.</span>
<span class="n">values_per_chunk_per_tile</span> <span class="o">=</span> <span class="n">values_per_chunk</span> <span class="o">//</span> <span class="n">tile_size</span> <span class="c1"># 480 / 32 = 15</span>

<span class="n">gamma_beta</span> <span class="o">=</span> <span class="n">ttnn</span><span class="o">.</span><span class="n">rand</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">values_per_chunk_per_tile</span><span class="p">,</span> <span class="mi">32</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">ttnn</span><span class="o">.</span><span class="n">DataType</span><span class="o">.</span><span class="n">BFLOAT16</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="n">ttnn</span><span class="o">.</span><span class="n">ROW_MAJOR_LAYOUT</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># groupnorm</span>
<span class="n">output_tensor</span> <span class="o">=</span> <span class="n">ttnn</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span>
    <span class="n">input_tensor_tilized</span><span class="p">,</span>
    <span class="n">num_groups</span><span class="o">=</span><span class="n">num_groups</span><span class="p">,</span>
    <span class="n">input_mask</span><span class="o">=</span><span class="n">input_mask_tensor</span><span class="p">,</span>
    <span class="n">weight</span><span class="o">=</span><span class="n">gamma_beta</span><span class="p">,</span>
    <span class="n">bias</span><span class="o">=</span><span class="n">gamma_beta</span><span class="p">,</span>
    <span class="n">output_layout</span><span class="o">=</span><span class="n">ttnn</span><span class="o">.</span><span class="n">TILE_LAYOUT</span><span class="p">,</span>
    <span class="n">core_grid</span><span class="o">=</span><span class="n">grid_size</span><span class="p">,</span>
    <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">num_out_blocks</span><span class="o">=</span><span class="n">num_out_blocks</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</dd>
</dl>

</section>



           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="ttnn.sort.html" class="btn btn-neutral float-left" title="ttnn.sort" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ttnn.layer_norm.html" class="btn btn-neutral float-right" title="ttnn.layer_norm" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Tenstorrent.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
        Version: <span id="current-version">latest</span>
        <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
        <dl id="version-list">
            <dt>Versions</dt>
        </dl>
        <br>
        </dl>
    </div>
</div>

<script>
const VERSIONS_URL = 'https://raw.githubusercontent.com/tenstorrent/tt-metal/refs/heads/main/docs/published_versions.json';

async function loadVersions() {
    try {
        const response = await fetch(VERSIONS_URL);
        const data = await response.json();
        const versionList = document.getElementById('version-list');
        const projectCode = location.pathname.split('/')[3];

        data.versions.forEach(version => {
            const dd = document.createElement('dd');
            const link = document.createElement('a');
            link.href = `https://docs.tenstorrent.com/tt-metal/${version}/${projectCode}/index.html`;
            link.textContent = version;
            dd.appendChild(link);
            versionList.appendChild(dd);
        });
    } catch (error) {
        console.error('Error loading versions:', error);
    }
}

loadVersions();

function getCurrentVersion() {
    return window.location.pathname.split('/')[2];
}
document.getElementById('current-version').textContent = getCurrentVersion();

const versionEl = document.createElement("span");
versionEl.innerText = getCurrentVersion();
versionEl.className = "project-versions";
const wySideSearchEl = document.getElementsByClassName("wy-side-nav-search").item(0);
if (wySideSearchEl) {
    const projectNameEl = wySideSearchEl.children.item(1);
    if (projectNameEl) projectNameEl.appendChild(versionEl);
}

</script><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>