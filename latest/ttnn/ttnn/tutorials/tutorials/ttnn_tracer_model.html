<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>TT-NN Tracer and BERT Model Visualization Tutorial &mdash; TT-NN  documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/tt_theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/nbsphinx-code-cells.css" type="text/css" />
    <link rel="shortcut icon" href="../../../_static/favicon.png"/>
    <link rel="canonical" href="/tt-metal/latest/ttnn/ttnn/tutorials/tutorials/ttnn_tracer_model.html" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js?v=b3ba4146"></script>
        <script src="../../../_static/doctools.js?v=888ff710"></script>
        <script src="../../../_static/sphinx_highlight.js?v=4825356b"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="../../../_static/posthog.js?v=aa5946f9"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Onboarding New Functionality" href="../../onboarding.html" />
    <link rel="prev" title="TT-NN Visualizer" href="ttnn_visualizer.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >



<a href="https://docs.tenstorrent.com/">
    <img src="../../../_static/tt_logo.svg" class="logo" alt="Logo"/>
</a>

<a href="../../../index.html">
    TT-NN
</a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">TTNN</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../about.html">What is TT-NN?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installing.html">Install</a></li>
<li class="toctree-l1"><a class="reference internal" href="ttnn_intro.html">TT-NN Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api.html">APIs</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../tutorials.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="ttnn_add_tensors.html">Add Tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="ttnn_basic_operations.html">Basic Tensor Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="ttnn_basic_matrix_multiplication.html">Matrix Multiplication</a></li>
<li class="toctree-l2"><a class="reference internal" href="ttnn_mlp_inference_mnist.html">MLP Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="ttnn_multihead_attention.html">Multi-Head Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="ttnn_basic_conv.html">Basic Convolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="ttnn_simplecnn_inference.html">Running a Simple CNN Inference on CIFAR-10</a></li>
<li class="toctree-l2"><a class="reference internal" href="ttnn_clip_zero_shot_classification.html">Building CLIP Model for Zero-Shot Image Classification with TT-NN</a></li>
<li class="toctree-l2"><a class="reference internal" href="ttnn_visualizer.html">TT-NN Visualizer</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">TT-NN Tracer and BERT Model Visualization Tutorial</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Import-Libraries">Import Libraries</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Set-program-config">Set program config</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Example-1:-Tracing-PyTorch-Operations">Example 1: Tracing PyTorch Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Example-2:-Tracing-TT-NN-Tensor-Operations">Example 2: Tracing TT-NN Tensor Operations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Model-and-Config-downloading">Model and Config downloading</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Example-3:-Tracing-a-BERT-Layer">Example 3: Tracing a BERT Layer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Example-4:-Trace-models-written-using-ttnn">Example 4: Trace models written using ttnn</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Close-the-device">Close the device</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../onboarding.html">Onboarding New Functionality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../converting_torch_model_to_ttnn.html">Converting PyTorch Model to TT-NN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../adding_new_ttnn_operation.html">Adding New TT-NN Operation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../profiling_ttnn_operations.html">Profiling TT-NN Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../demos.html">Building and Uplifting Demos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tools/index.html">Tools</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../resources/support.html">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../resources/contributing.html">Contributing as a developer</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">TT-NN</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../tutorials.html">Tutorials</a></li>
      <li class="breadcrumb-item active">TT-NN Tracer and BERT Model Visualization Tutorial</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/ttnn/tutorials/tutorials/ttnn_tracer_model.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="TT-NN-Tracer-and-BERT-Model-Visualization-Tutorial">
<h1>TT-NN Tracer and BERT Model Visualization Tutorial<a class="headerlink" href="#TT-NN-Tracer-and-BERT-Model-Visualization-Tutorial" title="Permalink to this heading"></a>
</h1>
<p>This tutorial demonstrates how to use TT-NN’s tracer functionality to visualize tensor operations and computational graphs. We’ll explore: 1. Basic tensor operation tracing with PyTorch tensors 2. TT-NN tensor operations with reshaping 3. Tracing a BERT self-attention layer 4. Running and tracing a full BERT model for question answering</p>
<p>The tracer is a powerful debugging and optimization tool that helps understand how operations are executed on Tenstorrent hardware.</p>
<section id="Import-Libraries">
<h2>Import Libraries<a class="headerlink" href="#Import-Libraries" title="Permalink to this heading"></a>
</h2>
<p>Disable fast runtime mode to ensure all operations are properly traced. Fast runtime mode may skip some operations for performance, which we don’t want when debugging.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate">
<div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate">
<div class="highlight"><pre><span></span>import os
from pathlib import Path
os.environ["TTNN_CONFIG_OVERRIDES"] = "{\"enable_fast_runtime_mode\": false}"

import torch
import transformers

import ttnn
from ttnn.tracer import trace, visualize
</pre></div>
</div>
</div>
</section>
<section id="Set-program-config">
<h2>Set program config<a class="headerlink" href="#Set-program-config" title="Permalink to this heading"></a>
</h2>
<p>Suppress transformer library warnings for cleaner output.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate">
<div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate">
<div class="highlight"><pre><span></span>transformers.logging.set_verbosity_error()
</pre></div>
</div>
</div>
</section>
</section>

<section id="Example-1:-Tracing-PyTorch-Operations">
<h1>Example 1: Tracing PyTorch Operations<a class="headerlink" href="#Example-1:-Tracing-PyTorch-Operations" title="Permalink to this heading"></a>
</h1>
<p>The tracer context manager captures all operations performed within its scope.</p>
<p>This example shows how basic PyTorch operations are tracked.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate">
<div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate">
<div class="highlight"><pre><span></span>with trace():
    # Create a random integer tensor of shape (1, 64) with values between 0-99
    tensor = torch.randint(0, 100, (1, 64), dtype=torch.int32)
    # Apply exponential function element-wise
    # This demonstrates how mathematical operations are captured
    tensor = torch.exp(tensor)

# Visualize the computational graph of the traced operations
# This will show the flow from random tensor creation to exp operation
visualize(tensor)
</pre></div>
</div>
</div>
</section>

<section id="Example-2:-Tracing-TT-NN-Tensor-Operations">
<h1>Example 2: Tracing TT-NN Tensor Operations<a class="headerlink" href="#Example-2:-Tracing-TT-NN-Tensor-Operations" title="Permalink to this heading"></a>
</h1>
<p>This example demonstrates tracing operations that involve TT-NN tensors.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate">
<div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate">
<div class="highlight"><pre><span></span>with trace():
    # Create a PyTorch tensor with shape (4, 64)
    tensor = torch.randint(0, 100, (4, 64), dtype=torch.int32)

    # Convert PyTorch tensor to TT-NN format
    # This operation moves data to the TT-NN representation
    tensor = ttnn.from_torch(tensor)

    # Reshape the tensor from (4, 64) to (2, 4, 32)
    # This demonstrates how reshape operations are handled in TT-NN
    tensor = ttnn.reshape(tensor, (2, 4, 32))

    # Convert back to PyTorch for visualization
    tensor = ttnn.to_torch(tensor)

# Visualize the graph showing PyTorch → TT-NN → reshape → PyTorch conversion
visualize(tensor)
</pre></div>
</div>
</div>
<section id="Model-and-Config-downloading">
<h2>Model and Config downloading<a class="headerlink" href="#Model-and-Config-downloading" title="Permalink to this heading"></a>
</h2>
<p>We define three functions to download the weights and configuration from Hugging Face.</p>
<p>For practical purposes, we can also specify a <code class="docutils literal notranslate"><span class="pre">TTNN_TUTORIALS_MODELS_CLIP_PATH</span></code> environment variable to avoid downloading the model. If it is defined, then model and configuration will be loaded from the location indicated by <code class="docutils literal notranslate"><span class="pre">TTNN_TUTORIALS_MODELS_CLIP_PATH</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate">
<div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate">
<div class="highlight"><pre><span></span>def download_google_bert_model_and_config(
    model_name: str,
) -&gt; tuple[transformers.models.bert.modeling_bert.BertSelfOutput, transformers.BertConfig]:
    model_location = model_name  # By default, download from Hugging Face

    # If TTNN_TUTORIALS_MODELS_TRACER_PATH is set, use it as the cache directory to avoid requests to Hugging Face
    cache_dir = os.getenv("TTNN_TUTORIALS_MODELS_TRACER_PATH")
    if cache_dir is not None:
        model_location = Path(cache_dir) / Path("config_google_bert.json")

    # Load model weights (download if cache_dir was not set)
    config = transformers.BertConfig.from_pretrained(model_location)
    model = transformers.models.bert.modeling_bert.BertSelfOutput(config).eval()

    return model, config


def download_ttnn_bert_config(model_name: str) -&gt; transformers.BertConfig:
    config_location = model_name  # By default, download from Hugging Face

    # If TTNN_TUTORIALS_MODELS_TRACER_PATH is set, use it as the cache directory to avoid requests to Hugging Face
    cache_dir = os.getenv("TTNN_TUTORIALS_MODELS_TRACER_PATH")
    if cache_dir is not None:
        config_location = Path(cache_dir) / Path("config_ttnn_bert.json")

    # Load config (download if cache_dir was not set)
    config = transformers.BertConfig.from_pretrained(config_location)

    return config


def download_ttnn_bert_model(model_name: str, config: transformers.BertConfig) -&gt; transformers.BertForQuestionAnswering:
    model_location = model_name  # By default, download from Hugging Face

    # If TTNN_TUTORIALS_MODELS_TRACER_PATH is set, use it as the cache directory to avoid requests to Hugging Face
    cache_dir = os.getenv("TTNN_TUTORIALS_MODELS_TRACER_PATH")
    if cache_dir is not None:
        model_location = Path(cache_dir)

    # Load model weights (download if cache_dir was not set)
    model = transformers.BertForQuestionAnswering.from_pretrained(model_location, config=config).eval()

    return model
</pre></div>
</div>
</div>
</section>
</section>

<section id="Example-3:-Tracing-a-BERT-Layer">
<h1>Example 3: Tracing a BERT Layer<a class="headerlink" href="#Example-3:-Tracing-a-BERT-Layer" title="Permalink to this heading"></a>
</h1>
<p>Load a small BERT configuration for demonstration. This is a tiny BERT model with only 4 layers, 256 hidden dimensions, and 4 attention heads.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate">
<div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate">
<div class="highlight"><pre><span></span>model_name = "google/bert_uncased_L-4_H-256_A-4"
model, config = download_google_bert_model_and_config(model_name)

# Trace the BERT self-output layer operations
with trace():
    # Create dummy inputs matching the expected dimensions
    # hidden_states: output from self-attention (batch=1, seq_len=64, hidden_size=256)
    hidden_states = torch.rand((1, 64, config.hidden_size))
    # input_tensor: residual connection input
    input_tensor = torch.rand((1, 64, config.hidden_size))

    # Run the layer forward pass
    output = model(hidden_states, input_tensor)

# Visualize the BERT layer computation graph
visualize(output)
</pre></div>
</div>
</div>
<section id="Example-4:-Trace-models-written-using-ttnn">
<h2>Example 4: Trace models written using ttnn<a class="headerlink" href="#Example-4:-Trace-models-written-using-ttnn" title="Permalink to this heading"></a>
</h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate">
<div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate">
<div class="highlight"><pre><span></span># Configure the dispatch core type based on the architecture
# ETH cores are used on newer architectures, WORKER cores on Grayskull
dispatch_core_type = ttnn.device.DispatchCoreType.ETH
if os.environ.get("ARCH_NAME") and "grayskull" in os.environ.get("ARCH_NAME"):
    dispatch_core_type = ttnn.device.DispatchCoreType.WORKER

# Open device with custom configuration
# - l1_small_size: Set L1 memory allocation to 8KB for small tensors
# - dispatch_core_config: Configure which cores handle dispatch operations
device = ttnn.open_device(
    device_id=0,
    l1_small_size=8192,
    dispatch_core_config=ttnn.device.DispatchCoreConfig(dispatch_core_type)
)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate">
<div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate">
<div class="highlight"><pre><span></span>from models.demos.bert.tt import ttnn_bert
from models.demos.bert.tt import ttnn_optimized_bert
from ttnn.model_preprocessing import preprocess_model_parameters


def ttnn_bert(bert):
    """
    Run and trace a complete BERT model for question answering.

    Args:
        bert: Either ttnn_bert or ttnn_optimized_bert module
    """
    # Use a larger BERT model fine-tuned for question answering
    model_name = "phiyodr/bert-large-finetuned-squad2"
    config = download_ttnn_bert_config(model_name)

    # Limit to 1 layer for faster execution in this demo
    # Full BERT-large has 24 layers
    config.num_hidden_layers = 1

    # Set batch size and sequence length for input
    batch_size = 8
    sequence_size = 384  # Standard for question answering tasks

    # ===== Model Parameter Preprocessing =====
    # Convert model parameters to TT-NN format and optimize for device
    # This includes weight packing, layout conversion, and memory placement
    model = download_ttnn_bert_model(model_name, config)
    parameters = preprocess_model_parameters(
        initialize_model=lambda: model,
        custom_preprocessor=bert.custom_preprocessor,
        device=device,
    )

    # ===== Trace BERT Inference =====
    with trace():
        # Create dummy input tensors
        # input_ids: Token IDs from vocabulary
        input_ids = torch.randint(0, config.vocab_size, (batch_size, sequence_size)).to(torch.uint32)

        # token_type_ids: Segment IDs (0 for question, 1 for context in QA)
        torch_token_type_ids = torch.ones((batch_size, sequence_size), dtype=torch.uint32)

        # position_ids: Position embeddings (usually just 0 to sequence_length-1)
        torch_position_ids = torch.ones((batch_size, sequence_size), dtype=torch.uint32)

        # attention_mask: Mask for padding tokens (only for optimized version)
        # Shape differs between regular and optimized BERT implementations
        torch_attention_mask = torch.zeros(1, sequence_size, dtype=torch.bfloat16) if bert == ttnn_optimized_bert else None

        # Preprocess inputs for TT-NN format
        # This converts PyTorch tensors to device tensors with appropriate layout
        ttnn_bert_inputs = bert.preprocess_inputs(
            input_ids,
            torch_token_type_ids,
            torch_position_ids,
            torch_attention_mask,
            device=device,
        )

        # Run BERT model for question answering
        # Returns start and end logits for answer span prediction
        output = bert.bert_for_question_answering(
            config,
            *ttnn_bert_inputs,
            parameters=parameters,
        )

        # Move output back from device to host for visualization
        output = ttnn.from_device(output)

    # Visualize the complete BERT computation graph
    return visualize(output)


# Run the optimized BERT implementation
# This version includes TT-NN specific optimizations for better performance
ttnn_bert(ttnn_optimized_bert)
</pre></div>
</div>
</div>
</section>
<section id="Close-the-device">
<h2>Close the device<a class="headerlink" href="#Close-the-device" title="Permalink to this heading"></a>
</h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate">
<div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate">
<div class="highlight"><pre><span></span>ttnn.close_device(device)
</pre></div>
</div>
</div>
</section>
</section>



           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="ttnn_visualizer.html" class="btn btn-neutral float-left" title="TT-NN Visualizer" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../../onboarding.html" class="btn btn-neutral float-right" title="Onboarding New Functionality" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Tenstorrent.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
        Version: <span id="current-version">latest</span>
        <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
        <dl id="version-list">
            <dt>Versions</dt>
        </dl>
        <br>
        </dl>
    </div>
</div>

<script>
const VERSIONS_URL = 'https://raw.githubusercontent.com/tenstorrent/tt-metal/refs/heads/main/docs/published_versions.json';

async function loadVersions() {
    try {
        const response = await fetch(VERSIONS_URL);
        const data = await response.json();
        const versionList = document.getElementById('version-list');
        const projectCode = location.pathname.split('/')[3];

        data.versions.forEach(version => {
            const dd = document.createElement('dd');
            const link = document.createElement('a');
            link.href = `https://docs.tenstorrent.com/tt-metal/${version}/${projectCode}/index.html`;
            link.textContent = version;
            dd.appendChild(link);
            versionList.appendChild(dd);
        });
    } catch (error) {
        console.error('Error loading versions:', error);
    }
}

loadVersions();

function getCurrentVersion() {
    return window.location.pathname.split('/')[2];
}
document.getElementById('current-version').textContent = getCurrentVersion();

const versionEl = document.createElement("span");
versionEl.innerText = getCurrentVersion();
versionEl.className = "project-versions";
const wySideSearchEl = document.getElementsByClassName("wy-side-nav-search").item(0);
if (wySideSearchEl) {
    const projectNameEl = wySideSearchEl.children.item(1);
    if (projectNameEl) projectNameEl.appendChild(versionEl);
}

</script><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>