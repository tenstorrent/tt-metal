Search.setIndex({"docnames": ["index", "resources/contributing", "resources/support", "tools/index", "ttnn/about", "ttnn/adding_new_ttnn_operation", "ttnn/api", "ttnn/api/ttnn.Conv2dConfig", "ttnn/api/ttnn.Conv2dSliceConfig", "ttnn/api/ttnn.GetDefaultDevice", "ttnn/api/ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig", "ttnn/api/ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig", "ttnn/api/ttnn.MatmulMultiCoreReuseMultiCastProgramConfig", "ttnn/api/ttnn.MatmulMultiCoreReuseProgramConfig", "ttnn/api/ttnn.SetDefaultDevice", "ttnn/api/ttnn.SoftmaxDefaultProgramConfig", "ttnn/api/ttnn.SoftmaxProgramConfig", "ttnn/api/ttnn.SoftmaxShardedMultiCoreProgramConfig", "ttnn/api/ttnn.abs", "ttnn/api/ttnn.abs_bw", "ttnn/api/ttnn.acos", "ttnn/api/ttnn.acos_bw", "ttnn/api/ttnn.acosh", "ttnn/api/ttnn.acosh_bw", "ttnn/api/ttnn.add", "ttnn/api/ttnn.add_bw", "ttnn/api/ttnn.addalpha", "ttnn/api/ttnn.addalpha_bw", "ttnn/api/ttnn.addcdiv", "ttnn/api/ttnn.addcdiv_bw", "ttnn/api/ttnn.addcmul", "ttnn/api/ttnn.addcmul_bw", "ttnn/api/ttnn.addmm", "ttnn/api/ttnn.all_gather", "ttnn/api/ttnn.all_reduce", "ttnn/api/ttnn.alt_complex_rotate90", "ttnn/api/ttnn.angle", "ttnn/api/ttnn.angle_bw", "ttnn/api/ttnn.arange", "ttnn/api/ttnn.argmax", "ttnn/api/ttnn.as_tensor", "ttnn/api/ttnn.asin", "ttnn/api/ttnn.asin_bw", "ttnn/api/ttnn.asinh", "ttnn/api/ttnn.asinh_bw", "ttnn/api/ttnn.assign_bw", "ttnn/api/ttnn.atan", "ttnn/api/ttnn.atan2", "ttnn/api/ttnn.atan2_bw", "ttnn/api/ttnn.atan_bw", "ttnn/api/ttnn.atanh", "ttnn/api/ttnn.atanh_bw", "ttnn/api/ttnn.avg_pool2d", "ttnn/api/ttnn.batch_norm", "ttnn/api/ttnn.bias_gelu_bw", "ttnn/api/ttnn.bitcast", "ttnn/api/ttnn.bitwise_and", "ttnn/api/ttnn.bitwise_left_shift", "ttnn/api/ttnn.bitwise_not", "ttnn/api/ttnn.bitwise_or", "ttnn/api/ttnn.bitwise_right_shift", "ttnn/api/ttnn.bitwise_xor", "ttnn/api/ttnn.cbrt", "ttnn/api/ttnn.ceil", "ttnn/api/ttnn.ceil_bw", "ttnn/api/ttnn.celu", "ttnn/api/ttnn.celu_bw", "ttnn/api/ttnn.clamp", "ttnn/api/ttnn.clamp_bw", "ttnn/api/ttnn.clip", "ttnn/api/ttnn.clip_bw", "ttnn/api/ttnn.clone", "ttnn/api/ttnn.close_device", "ttnn/api/ttnn.concat", "ttnn/api/ttnn.concat_bw", "ttnn/api/ttnn.conj", "ttnn/api/ttnn.conj_bw", "ttnn/api/ttnn.conv1d", "ttnn/api/ttnn.conv2d", "ttnn/api/ttnn.conv_transpose2d", "ttnn/api/ttnn.cos", "ttnn/api/ttnn.cos_bw", "ttnn/api/ttnn.cosh", "ttnn/api/ttnn.cosh_bw", "ttnn/api/ttnn.create_sharded_memory_config", "ttnn/api/ttnn.cumprod", "ttnn/api/ttnn.cumsum", "ttnn/api/ttnn.deallocate", "ttnn/api/ttnn.deg2rad", "ttnn/api/ttnn.deg2rad_bw", "ttnn/api/ttnn.digamma", "ttnn/api/ttnn.digamma_bw", "ttnn/api/ttnn.div", "ttnn/api/ttnn.div_bw", "ttnn/api/ttnn.div_no_nan", "ttnn/api/ttnn.div_no_nan_bw", "ttnn/api/ttnn.dump_tensor", "ttnn/api/ttnn.elu", "ttnn/api/ttnn.elu_bw", "ttnn/api/ttnn.ema", "ttnn/api/ttnn.embedding", "ttnn/api/ttnn.embedding_bw", "ttnn/api/ttnn.empty", "ttnn/api/ttnn.empty_like", "ttnn/api/ttnn.eq", "ttnn/api/ttnn.eq_", "ttnn/api/ttnn.eqz", "ttnn/api/ttnn.erf", "ttnn/api/ttnn.erf_bw", "ttnn/api/ttnn.erfc", "ttnn/api/ttnn.erfc_bw", "ttnn/api/ttnn.erfinv", "ttnn/api/ttnn.erfinv_bw", "ttnn/api/ttnn.exp", "ttnn/api/ttnn.exp2", "ttnn/api/ttnn.exp2_bw", "ttnn/api/ttnn.exp_bw", "ttnn/api/ttnn.experimental.conv3d", "ttnn/api/ttnn.experimental.dropout", "ttnn/api/ttnn.experimental.gelu_bw", "ttnn/api/ttnn.experimental.rotary_embedding", "ttnn/api/ttnn.expm1", "ttnn/api/ttnn.expm1_bw", "ttnn/api/ttnn.fill", "ttnn/api/ttnn.fill_bw", "ttnn/api/ttnn.fill_ones_rm", "ttnn/api/ttnn.fill_rm", "ttnn/api/ttnn.fill_zero_bw", "ttnn/api/ttnn.floor", "ttnn/api/ttnn.floor_bw", "ttnn/api/ttnn.floor_div", "ttnn/api/ttnn.fmod", "ttnn/api/ttnn.fmod_bw", "ttnn/api/ttnn.frac", "ttnn/api/ttnn.frac_bw", "ttnn/api/ttnn.from_buffer", "ttnn/api/ttnn.from_device", "ttnn/api/ttnn.from_torch", "ttnn/api/ttnn.full", "ttnn/api/ttnn.full_like", "ttnn/api/ttnn.gather", "ttnn/api/ttnn.gcd", "ttnn/api/ttnn.ge", "ttnn/api/ttnn.ge_", "ttnn/api/ttnn.geglu", "ttnn/api/ttnn.gelu", "ttnn/api/ttnn.gelu_bw", "ttnn/api/ttnn.gez", "ttnn/api/ttnn.global_avg_pool2d", "ttnn/api/ttnn.glu", "ttnn/api/ttnn.group_norm", "ttnn/api/ttnn.gt", "ttnn/api/ttnn.gt_", "ttnn/api/ttnn.gtz", "ttnn/api/ttnn.hardshrink", "ttnn/api/ttnn.hardshrink_bw", "ttnn/api/ttnn.hardsigmoid", "ttnn/api/ttnn.hardsigmoid_bw", "ttnn/api/ttnn.hardswish", "ttnn/api/ttnn.hardswish_bw", "ttnn/api/ttnn.hardtanh", "ttnn/api/ttnn.hardtanh_bw", "ttnn/api/ttnn.heaviside", "ttnn/api/ttnn.hypot", "ttnn/api/ttnn.hypot_bw", "ttnn/api/ttnn.i0", "ttnn/api/ttnn.i0_bw", "ttnn/api/ttnn.identity", "ttnn/api/ttnn.imag", "ttnn/api/ttnn.imag_bw", "ttnn/api/ttnn.indexed_fill", "ttnn/api/ttnn.is_imag", "ttnn/api/ttnn.is_real", "ttnn/api/ttnn.isclose", "ttnn/api/ttnn.isfinite", "ttnn/api/ttnn.isinf", "ttnn/api/ttnn.isnan", "ttnn/api/ttnn.isneginf", "ttnn/api/ttnn.isposinf", "ttnn/api/ttnn.kv_cache.fill_cache_for_user_", "ttnn/api/ttnn.kv_cache.update_cache_for_token_", "ttnn/api/ttnn.l1_loss", "ttnn/api/ttnn.layer_norm", "ttnn/api/ttnn.layer_norm_post_all_gather", "ttnn/api/ttnn.layer_norm_pre_all_gather", "ttnn/api/ttnn.lcm", "ttnn/api/ttnn.ldexp", "ttnn/api/ttnn.ldexp_bw", "ttnn/api/ttnn.le", "ttnn/api/ttnn.le_", "ttnn/api/ttnn.leaky_relu", "ttnn/api/ttnn.leaky_relu_bw", "ttnn/api/ttnn.lerp", "ttnn/api/ttnn.lerp_bw", "ttnn/api/ttnn.lez", "ttnn/api/ttnn.lgamma", "ttnn/api/ttnn.lgamma_bw", "ttnn/api/ttnn.linear", "ttnn/api/ttnn.load_tensor", "ttnn/api/ttnn.log", "ttnn/api/ttnn.log10", "ttnn/api/ttnn.log10_bw", "ttnn/api/ttnn.log1p", "ttnn/api/ttnn.log1p_bw", "ttnn/api/ttnn.log2", "ttnn/api/ttnn.log2_bw", "ttnn/api/ttnn.log_bw", "ttnn/api/ttnn.log_sigmoid", "ttnn/api/ttnn.log_sigmoid_bw", "ttnn/api/ttnn.logaddexp", "ttnn/api/ttnn.logaddexp2", "ttnn/api/ttnn.logaddexp2_bw", "ttnn/api/ttnn.logaddexp_bw", "ttnn/api/ttnn.logical_and", "ttnn/api/ttnn.logical_and_", "ttnn/api/ttnn.logical_not", "ttnn/api/ttnn.logical_not_", "ttnn/api/ttnn.logical_or", "ttnn/api/ttnn.logical_or_", "ttnn/api/ttnn.logical_xor", "ttnn/api/ttnn.logical_xor_", "ttnn/api/ttnn.logit", "ttnn/api/ttnn.logit_bw", "ttnn/api/ttnn.logiteps_bw", "ttnn/api/ttnn.lt", "ttnn/api/ttnn.lt_", "ttnn/api/ttnn.ltz", "ttnn/api/ttnn.mac", "ttnn/api/ttnn.manage_device", "ttnn/api/ttnn.manual_seed", "ttnn/api/ttnn.matmul", "ttnn/api/ttnn.max", "ttnn/api/ttnn.max_bw", "ttnn/api/ttnn.max_pool2d", "ttnn/api/ttnn.maximum", "ttnn/api/ttnn.mean", "ttnn/api/ttnn.min", "ttnn/api/ttnn.min_bw", "ttnn/api/ttnn.minimum", "ttnn/api/ttnn.mish", "ttnn/api/ttnn.model_preprocessing.preprocess_model", "ttnn/api/ttnn.model_preprocessing.preprocess_model_parameters", "ttnn/api/ttnn.moe", "ttnn/api/ttnn.mse_loss", "ttnn/api/ttnn.mul_bw", "ttnn/api/ttnn.multigammaln", "ttnn/api/ttnn.multigammaln_bw", "ttnn/api/ttnn.multiply", "ttnn/api/ttnn.ne", "ttnn/api/ttnn.ne_", "ttnn/api/ttnn.neg", "ttnn/api/ttnn.neg_bw", "ttnn/api/ttnn.nextafter", "ttnn/api/ttnn.nez", "ttnn/api/ttnn.nonzero", "ttnn/api/ttnn.normalize_global", "ttnn/api/ttnn.normalize_hw", "ttnn/api/ttnn.ones", "ttnn/api/ttnn.ones_like", "ttnn/api/ttnn.open_device", "ttnn/api/ttnn.outer", "ttnn/api/ttnn.pad", "ttnn/api/ttnn.pad_to_tile_shape", "ttnn/api/ttnn.permute", "ttnn/api/ttnn.polar", "ttnn/api/ttnn.polar_bw", "ttnn/api/ttnn.polygamma", "ttnn/api/ttnn.polygamma_bw", "ttnn/api/ttnn.polyval", "ttnn/api/ttnn.pow", "ttnn/api/ttnn.pow_bw", "ttnn/api/ttnn.prelu", "ttnn/api/ttnn.prepare_conv_bias", "ttnn/api/ttnn.prepare_conv_transpose2d_bias", "ttnn/api/ttnn.prepare_conv_transpose2d_weights", "ttnn/api/ttnn.prepare_conv_weights", "ttnn/api/ttnn.prod", "ttnn/api/ttnn.prod_bw", "ttnn/api/ttnn.rad2deg", "ttnn/api/ttnn.rad2deg_bw", "ttnn/api/ttnn.rand", "ttnn/api/ttnn.rdiv", "ttnn/api/ttnn.rdiv_bw", "ttnn/api/ttnn.real", "ttnn/api/ttnn.real_bw", "ttnn/api/ttnn.reallocate", "ttnn/api/ttnn.reciprocal", "ttnn/api/ttnn.reciprocal_bw", "ttnn/api/ttnn.reduce_scatter", "ttnn/api/ttnn.register_post_operation_hook", "ttnn/api/ttnn.register_pre_operation_hook", "ttnn/api/ttnn.reglu", "ttnn/api/ttnn.relu", "ttnn/api/ttnn.relu6", "ttnn/api/ttnn.relu6_bw", "ttnn/api/ttnn.relu_bw", "ttnn/api/ttnn.relu_max", "ttnn/api/ttnn.relu_min", "ttnn/api/ttnn.remainder", "ttnn/api/ttnn.remainder_bw", "ttnn/api/ttnn.repeat", "ttnn/api/ttnn.repeat_bw", "ttnn/api/ttnn.repeat_interleave", "ttnn/api/ttnn.reshape", "ttnn/api/ttnn.rms_norm", "ttnn/api/ttnn.rms_norm_post_all_gather", "ttnn/api/ttnn.rms_norm_pre_all_gather", "ttnn/api/ttnn.round", "ttnn/api/ttnn.round_bw", "ttnn/api/ttnn.rpow", "ttnn/api/ttnn.rpow_bw", "ttnn/api/ttnn.rsqrt", "ttnn/api/ttnn.rsqrt_bw", "ttnn/api/ttnn.rsub", "ttnn/api/ttnn.rsub_bw", "ttnn/api/ttnn.scale_causal_mask_hw_dims_softmax_in_place", "ttnn/api/ttnn.scale_mask_softmax", "ttnn/api/ttnn.scale_mask_softmax_in_place", "ttnn/api/ttnn.scatter", "ttnn/api/ttnn.selu", "ttnn/api/ttnn.selu_bw", "ttnn/api/ttnn.set_printoptions", "ttnn/api/ttnn.sigmoid", "ttnn/api/ttnn.sigmoid_accurate", "ttnn/api/ttnn.sigmoid_bw", "ttnn/api/ttnn.sign", "ttnn/api/ttnn.sign_bw", "ttnn/api/ttnn.signbit", "ttnn/api/ttnn.silu", "ttnn/api/ttnn.silu_bw", "ttnn/api/ttnn.sin", "ttnn/api/ttnn.sin_bw", "ttnn/api/ttnn.sinh", "ttnn/api/ttnn.sinh_bw", "ttnn/api/ttnn.slice", "ttnn/api/ttnn.softmax", "ttnn/api/ttnn.softmax_in_place", "ttnn/api/ttnn.softplus", "ttnn/api/ttnn.softplus_bw", "ttnn/api/ttnn.softshrink", "ttnn/api/ttnn.softshrink_bw", "ttnn/api/ttnn.softsign", "ttnn/api/ttnn.softsign_bw", "ttnn/api/ttnn.sort", "ttnn/api/ttnn.sparse_matmul", "ttnn/api/ttnn.split_work_to_cores", "ttnn/api/ttnn.sqrt", "ttnn/api/ttnn.sqrt_bw", "ttnn/api/ttnn.square", "ttnn/api/ttnn.square_bw", "ttnn/api/ttnn.squared_difference", "ttnn/api/ttnn.squared_difference_bw", "ttnn/api/ttnn.std", "ttnn/api/ttnn.sub_bw", "ttnn/api/ttnn.subalpha", "ttnn/api/ttnn.subalpha_bw", "ttnn/api/ttnn.subtract", "ttnn/api/ttnn.sum", "ttnn/api/ttnn.swiglu", "ttnn/api/ttnn.swish", "ttnn/api/ttnn.synchronize_device", "ttnn/api/ttnn.tan", "ttnn/api/ttnn.tan_bw", "ttnn/api/ttnn.tanh", "ttnn/api/ttnn.tanh_bw", "ttnn/api/ttnn.tanhshrink", "ttnn/api/ttnn.tanhshrink_bw", "ttnn/api/ttnn.threshold", "ttnn/api/ttnn.threshold_bw", "ttnn/api/ttnn.tilize", "ttnn/api/ttnn.tilize_with_val_padding", "ttnn/api/ttnn.to_device", "ttnn/api/ttnn.to_layout", "ttnn/api/ttnn.to_memory_config", "ttnn/api/ttnn.to_torch", "ttnn/api/ttnn.topk", "ttnn/api/ttnn.transformer.attention_softmax", "ttnn/api/ttnn.transformer.attention_softmax_", "ttnn/api/ttnn.transformer.concatenate_heads", "ttnn/api/ttnn.transformer.scaled_dot_product_attention", "ttnn/api/ttnn.transformer.scaled_dot_product_attention_decode", "ttnn/api/ttnn.transformer.split_query_key_value_and_split_heads", "ttnn/api/ttnn.tril", "ttnn/api/ttnn.triu", "ttnn/api/ttnn.trunc", "ttnn/api/ttnn.trunc_bw", "ttnn/api/ttnn.unary_chain", "ttnn/api/ttnn.untilize", "ttnn/api/ttnn.untilize_with_unpadding", "ttnn/api/ttnn.upsample", "ttnn/api/ttnn.var", "ttnn/api/ttnn.where", "ttnn/api/ttnn.where_bw", "ttnn/api/ttnn.xlogy", "ttnn/api/ttnn.xlogy_bw", "ttnn/api/ttnn.zeros", "ttnn/api/ttnn.zeros_like", "ttnn/converting_torch_model_to_ttnn", "ttnn/demos", "ttnn/get_started", "ttnn/installing", "ttnn/onboarding", "ttnn/profiling_ttnn_operations", "ttnn/tensor", "ttnn/tutorials", "ttnn/tutorials/tutorials/ttnn_add_tensors", "ttnn/tutorials/tutorials/ttnn_basic_conv", "ttnn/tutorials/tutorials/ttnn_basic_matrix_multiplication", "ttnn/tutorials/tutorials/ttnn_basic_operations", "ttnn/tutorials/tutorials/ttnn_clip_zero_shot_classification", "ttnn/tutorials/tutorials/ttnn_mlp_inference_mnist", "ttnn/tutorials/tutorials/ttnn_multihead_attention", "ttnn/tutorials/tutorials/ttnn_simplecnn_inference", "ttnn/tutorials/tutorials/ttnn_tracer_model", "ttnn/tutorials/tutorials/ttnn_visualizer", "ttnn/usage"], "filenames": ["index.rst", "resources/contributing.rst", "resources/support.rst", "tools/index.rst", "ttnn/about.rst", "ttnn/adding_new_ttnn_operation.rst", "ttnn/api.rst", "ttnn/api/ttnn.Conv2dConfig.rst", "ttnn/api/ttnn.Conv2dSliceConfig.rst", "ttnn/api/ttnn.GetDefaultDevice.rst", "ttnn/api/ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.rst", "ttnn/api/ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig.rst", "ttnn/api/ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.rst", "ttnn/api/ttnn.MatmulMultiCoreReuseProgramConfig.rst", "ttnn/api/ttnn.SetDefaultDevice.rst", "ttnn/api/ttnn.SoftmaxDefaultProgramConfig.rst", "ttnn/api/ttnn.SoftmaxProgramConfig.rst", "ttnn/api/ttnn.SoftmaxShardedMultiCoreProgramConfig.rst", "ttnn/api/ttnn.abs.rst", "ttnn/api/ttnn.abs_bw.rst", "ttnn/api/ttnn.acos.rst", "ttnn/api/ttnn.acos_bw.rst", "ttnn/api/ttnn.acosh.rst", "ttnn/api/ttnn.acosh_bw.rst", "ttnn/api/ttnn.add.rst", "ttnn/api/ttnn.add_bw.rst", "ttnn/api/ttnn.addalpha.rst", "ttnn/api/ttnn.addalpha_bw.rst", "ttnn/api/ttnn.addcdiv.rst", "ttnn/api/ttnn.addcdiv_bw.rst", "ttnn/api/ttnn.addcmul.rst", "ttnn/api/ttnn.addcmul_bw.rst", "ttnn/api/ttnn.addmm.rst", "ttnn/api/ttnn.all_gather.rst", "ttnn/api/ttnn.all_reduce.rst", "ttnn/api/ttnn.alt_complex_rotate90.rst", "ttnn/api/ttnn.angle.rst", "ttnn/api/ttnn.angle_bw.rst", "ttnn/api/ttnn.arange.rst", "ttnn/api/ttnn.argmax.rst", "ttnn/api/ttnn.as_tensor.rst", "ttnn/api/ttnn.asin.rst", "ttnn/api/ttnn.asin_bw.rst", "ttnn/api/ttnn.asinh.rst", "ttnn/api/ttnn.asinh_bw.rst", "ttnn/api/ttnn.assign_bw.rst", "ttnn/api/ttnn.atan.rst", "ttnn/api/ttnn.atan2.rst", "ttnn/api/ttnn.atan2_bw.rst", "ttnn/api/ttnn.atan_bw.rst", "ttnn/api/ttnn.atanh.rst", "ttnn/api/ttnn.atanh_bw.rst", "ttnn/api/ttnn.avg_pool2d.rst", "ttnn/api/ttnn.batch_norm.rst", "ttnn/api/ttnn.bias_gelu_bw.rst", "ttnn/api/ttnn.bitcast.rst", "ttnn/api/ttnn.bitwise_and.rst", "ttnn/api/ttnn.bitwise_left_shift.rst", "ttnn/api/ttnn.bitwise_not.rst", "ttnn/api/ttnn.bitwise_or.rst", "ttnn/api/ttnn.bitwise_right_shift.rst", "ttnn/api/ttnn.bitwise_xor.rst", "ttnn/api/ttnn.cbrt.rst", "ttnn/api/ttnn.ceil.rst", "ttnn/api/ttnn.ceil_bw.rst", "ttnn/api/ttnn.celu.rst", "ttnn/api/ttnn.celu_bw.rst", "ttnn/api/ttnn.clamp.rst", "ttnn/api/ttnn.clamp_bw.rst", "ttnn/api/ttnn.clip.rst", "ttnn/api/ttnn.clip_bw.rst", "ttnn/api/ttnn.clone.rst", "ttnn/api/ttnn.close_device.rst", "ttnn/api/ttnn.concat.rst", "ttnn/api/ttnn.concat_bw.rst", "ttnn/api/ttnn.conj.rst", "ttnn/api/ttnn.conj_bw.rst", "ttnn/api/ttnn.conv1d.rst", "ttnn/api/ttnn.conv2d.rst", "ttnn/api/ttnn.conv_transpose2d.rst", "ttnn/api/ttnn.cos.rst", "ttnn/api/ttnn.cos_bw.rst", "ttnn/api/ttnn.cosh.rst", "ttnn/api/ttnn.cosh_bw.rst", "ttnn/api/ttnn.create_sharded_memory_config.rst", "ttnn/api/ttnn.cumprod.rst", "ttnn/api/ttnn.cumsum.rst", "ttnn/api/ttnn.deallocate.rst", "ttnn/api/ttnn.deg2rad.rst", "ttnn/api/ttnn.deg2rad_bw.rst", "ttnn/api/ttnn.digamma.rst", "ttnn/api/ttnn.digamma_bw.rst", "ttnn/api/ttnn.div.rst", "ttnn/api/ttnn.div_bw.rst", "ttnn/api/ttnn.div_no_nan.rst", "ttnn/api/ttnn.div_no_nan_bw.rst", "ttnn/api/ttnn.dump_tensor.rst", "ttnn/api/ttnn.elu.rst", "ttnn/api/ttnn.elu_bw.rst", "ttnn/api/ttnn.ema.rst", "ttnn/api/ttnn.embedding.rst", "ttnn/api/ttnn.embedding_bw.rst", "ttnn/api/ttnn.empty.rst", "ttnn/api/ttnn.empty_like.rst", "ttnn/api/ttnn.eq.rst", "ttnn/api/ttnn.eq_.rst", "ttnn/api/ttnn.eqz.rst", "ttnn/api/ttnn.erf.rst", "ttnn/api/ttnn.erf_bw.rst", "ttnn/api/ttnn.erfc.rst", "ttnn/api/ttnn.erfc_bw.rst", "ttnn/api/ttnn.erfinv.rst", "ttnn/api/ttnn.erfinv_bw.rst", "ttnn/api/ttnn.exp.rst", "ttnn/api/ttnn.exp2.rst", "ttnn/api/ttnn.exp2_bw.rst", "ttnn/api/ttnn.exp_bw.rst", "ttnn/api/ttnn.experimental.conv3d.rst", "ttnn/api/ttnn.experimental.dropout.rst", "ttnn/api/ttnn.experimental.gelu_bw.rst", "ttnn/api/ttnn.experimental.rotary_embedding.rst", "ttnn/api/ttnn.expm1.rst", "ttnn/api/ttnn.expm1_bw.rst", "ttnn/api/ttnn.fill.rst", "ttnn/api/ttnn.fill_bw.rst", "ttnn/api/ttnn.fill_ones_rm.rst", "ttnn/api/ttnn.fill_rm.rst", "ttnn/api/ttnn.fill_zero_bw.rst", "ttnn/api/ttnn.floor.rst", "ttnn/api/ttnn.floor_bw.rst", "ttnn/api/ttnn.floor_div.rst", "ttnn/api/ttnn.fmod.rst", "ttnn/api/ttnn.fmod_bw.rst", "ttnn/api/ttnn.frac.rst", "ttnn/api/ttnn.frac_bw.rst", "ttnn/api/ttnn.from_buffer.rst", "ttnn/api/ttnn.from_device.rst", "ttnn/api/ttnn.from_torch.rst", "ttnn/api/ttnn.full.rst", "ttnn/api/ttnn.full_like.rst", "ttnn/api/ttnn.gather.rst", "ttnn/api/ttnn.gcd.rst", "ttnn/api/ttnn.ge.rst", "ttnn/api/ttnn.ge_.rst", "ttnn/api/ttnn.geglu.rst", "ttnn/api/ttnn.gelu.rst", "ttnn/api/ttnn.gelu_bw.rst", "ttnn/api/ttnn.gez.rst", "ttnn/api/ttnn.global_avg_pool2d.rst", "ttnn/api/ttnn.glu.rst", "ttnn/api/ttnn.group_norm.rst", "ttnn/api/ttnn.gt.rst", "ttnn/api/ttnn.gt_.rst", "ttnn/api/ttnn.gtz.rst", "ttnn/api/ttnn.hardshrink.rst", "ttnn/api/ttnn.hardshrink_bw.rst", "ttnn/api/ttnn.hardsigmoid.rst", "ttnn/api/ttnn.hardsigmoid_bw.rst", "ttnn/api/ttnn.hardswish.rst", "ttnn/api/ttnn.hardswish_bw.rst", "ttnn/api/ttnn.hardtanh.rst", "ttnn/api/ttnn.hardtanh_bw.rst", "ttnn/api/ttnn.heaviside.rst", "ttnn/api/ttnn.hypot.rst", "ttnn/api/ttnn.hypot_bw.rst", "ttnn/api/ttnn.i0.rst", "ttnn/api/ttnn.i0_bw.rst", "ttnn/api/ttnn.identity.rst", "ttnn/api/ttnn.imag.rst", "ttnn/api/ttnn.imag_bw.rst", "ttnn/api/ttnn.indexed_fill.rst", "ttnn/api/ttnn.is_imag.rst", "ttnn/api/ttnn.is_real.rst", "ttnn/api/ttnn.isclose.rst", "ttnn/api/ttnn.isfinite.rst", "ttnn/api/ttnn.isinf.rst", "ttnn/api/ttnn.isnan.rst", "ttnn/api/ttnn.isneginf.rst", "ttnn/api/ttnn.isposinf.rst", "ttnn/api/ttnn.kv_cache.fill_cache_for_user_.rst", "ttnn/api/ttnn.kv_cache.update_cache_for_token_.rst", "ttnn/api/ttnn.l1_loss.rst", "ttnn/api/ttnn.layer_norm.rst", "ttnn/api/ttnn.layer_norm_post_all_gather.rst", "ttnn/api/ttnn.layer_norm_pre_all_gather.rst", "ttnn/api/ttnn.lcm.rst", "ttnn/api/ttnn.ldexp.rst", "ttnn/api/ttnn.ldexp_bw.rst", "ttnn/api/ttnn.le.rst", "ttnn/api/ttnn.le_.rst", "ttnn/api/ttnn.leaky_relu.rst", "ttnn/api/ttnn.leaky_relu_bw.rst", "ttnn/api/ttnn.lerp.rst", "ttnn/api/ttnn.lerp_bw.rst", "ttnn/api/ttnn.lez.rst", "ttnn/api/ttnn.lgamma.rst", "ttnn/api/ttnn.lgamma_bw.rst", "ttnn/api/ttnn.linear.rst", "ttnn/api/ttnn.load_tensor.rst", "ttnn/api/ttnn.log.rst", "ttnn/api/ttnn.log10.rst", "ttnn/api/ttnn.log10_bw.rst", "ttnn/api/ttnn.log1p.rst", "ttnn/api/ttnn.log1p_bw.rst", "ttnn/api/ttnn.log2.rst", "ttnn/api/ttnn.log2_bw.rst", "ttnn/api/ttnn.log_bw.rst", "ttnn/api/ttnn.log_sigmoid.rst", "ttnn/api/ttnn.log_sigmoid_bw.rst", "ttnn/api/ttnn.logaddexp.rst", "ttnn/api/ttnn.logaddexp2.rst", "ttnn/api/ttnn.logaddexp2_bw.rst", "ttnn/api/ttnn.logaddexp_bw.rst", "ttnn/api/ttnn.logical_and.rst", "ttnn/api/ttnn.logical_and_.rst", "ttnn/api/ttnn.logical_not.rst", "ttnn/api/ttnn.logical_not_.rst", "ttnn/api/ttnn.logical_or.rst", "ttnn/api/ttnn.logical_or_.rst", "ttnn/api/ttnn.logical_xor.rst", "ttnn/api/ttnn.logical_xor_.rst", "ttnn/api/ttnn.logit.rst", "ttnn/api/ttnn.logit_bw.rst", "ttnn/api/ttnn.logiteps_bw.rst", "ttnn/api/ttnn.lt.rst", "ttnn/api/ttnn.lt_.rst", "ttnn/api/ttnn.ltz.rst", "ttnn/api/ttnn.mac.rst", "ttnn/api/ttnn.manage_device.rst", "ttnn/api/ttnn.manual_seed.rst", "ttnn/api/ttnn.matmul.rst", "ttnn/api/ttnn.max.rst", "ttnn/api/ttnn.max_bw.rst", "ttnn/api/ttnn.max_pool2d.rst", "ttnn/api/ttnn.maximum.rst", "ttnn/api/ttnn.mean.rst", "ttnn/api/ttnn.min.rst", "ttnn/api/ttnn.min_bw.rst", "ttnn/api/ttnn.minimum.rst", "ttnn/api/ttnn.mish.rst", "ttnn/api/ttnn.model_preprocessing.preprocess_model.rst", "ttnn/api/ttnn.model_preprocessing.preprocess_model_parameters.rst", "ttnn/api/ttnn.moe.rst", "ttnn/api/ttnn.mse_loss.rst", "ttnn/api/ttnn.mul_bw.rst", "ttnn/api/ttnn.multigammaln.rst", "ttnn/api/ttnn.multigammaln_bw.rst", "ttnn/api/ttnn.multiply.rst", "ttnn/api/ttnn.ne.rst", "ttnn/api/ttnn.ne_.rst", "ttnn/api/ttnn.neg.rst", "ttnn/api/ttnn.neg_bw.rst", "ttnn/api/ttnn.nextafter.rst", "ttnn/api/ttnn.nez.rst", "ttnn/api/ttnn.nonzero.rst", "ttnn/api/ttnn.normalize_global.rst", "ttnn/api/ttnn.normalize_hw.rst", "ttnn/api/ttnn.ones.rst", "ttnn/api/ttnn.ones_like.rst", "ttnn/api/ttnn.open_device.rst", "ttnn/api/ttnn.outer.rst", "ttnn/api/ttnn.pad.rst", "ttnn/api/ttnn.pad_to_tile_shape.rst", "ttnn/api/ttnn.permute.rst", "ttnn/api/ttnn.polar.rst", "ttnn/api/ttnn.polar_bw.rst", "ttnn/api/ttnn.polygamma.rst", "ttnn/api/ttnn.polygamma_bw.rst", "ttnn/api/ttnn.polyval.rst", "ttnn/api/ttnn.pow.rst", "ttnn/api/ttnn.pow_bw.rst", "ttnn/api/ttnn.prelu.rst", "ttnn/api/ttnn.prepare_conv_bias.rst", "ttnn/api/ttnn.prepare_conv_transpose2d_bias.rst", "ttnn/api/ttnn.prepare_conv_transpose2d_weights.rst", "ttnn/api/ttnn.prepare_conv_weights.rst", "ttnn/api/ttnn.prod.rst", "ttnn/api/ttnn.prod_bw.rst", "ttnn/api/ttnn.rad2deg.rst", "ttnn/api/ttnn.rad2deg_bw.rst", "ttnn/api/ttnn.rand.rst", "ttnn/api/ttnn.rdiv.rst", "ttnn/api/ttnn.rdiv_bw.rst", "ttnn/api/ttnn.real.rst", "ttnn/api/ttnn.real_bw.rst", "ttnn/api/ttnn.reallocate.rst", "ttnn/api/ttnn.reciprocal.rst", "ttnn/api/ttnn.reciprocal_bw.rst", "ttnn/api/ttnn.reduce_scatter.rst", "ttnn/api/ttnn.register_post_operation_hook.rst", "ttnn/api/ttnn.register_pre_operation_hook.rst", "ttnn/api/ttnn.reglu.rst", "ttnn/api/ttnn.relu.rst", "ttnn/api/ttnn.relu6.rst", "ttnn/api/ttnn.relu6_bw.rst", "ttnn/api/ttnn.relu_bw.rst", "ttnn/api/ttnn.relu_max.rst", "ttnn/api/ttnn.relu_min.rst", "ttnn/api/ttnn.remainder.rst", "ttnn/api/ttnn.remainder_bw.rst", "ttnn/api/ttnn.repeat.rst", "ttnn/api/ttnn.repeat_bw.rst", "ttnn/api/ttnn.repeat_interleave.rst", "ttnn/api/ttnn.reshape.rst", "ttnn/api/ttnn.rms_norm.rst", "ttnn/api/ttnn.rms_norm_post_all_gather.rst", "ttnn/api/ttnn.rms_norm_pre_all_gather.rst", "ttnn/api/ttnn.round.rst", "ttnn/api/ttnn.round_bw.rst", "ttnn/api/ttnn.rpow.rst", "ttnn/api/ttnn.rpow_bw.rst", "ttnn/api/ttnn.rsqrt.rst", "ttnn/api/ttnn.rsqrt_bw.rst", "ttnn/api/ttnn.rsub.rst", "ttnn/api/ttnn.rsub_bw.rst", "ttnn/api/ttnn.scale_causal_mask_hw_dims_softmax_in_place.rst", "ttnn/api/ttnn.scale_mask_softmax.rst", "ttnn/api/ttnn.scale_mask_softmax_in_place.rst", "ttnn/api/ttnn.scatter.rst", "ttnn/api/ttnn.selu.rst", "ttnn/api/ttnn.selu_bw.rst", "ttnn/api/ttnn.set_printoptions.rst", "ttnn/api/ttnn.sigmoid.rst", "ttnn/api/ttnn.sigmoid_accurate.rst", "ttnn/api/ttnn.sigmoid_bw.rst", "ttnn/api/ttnn.sign.rst", "ttnn/api/ttnn.sign_bw.rst", "ttnn/api/ttnn.signbit.rst", "ttnn/api/ttnn.silu.rst", "ttnn/api/ttnn.silu_bw.rst", "ttnn/api/ttnn.sin.rst", "ttnn/api/ttnn.sin_bw.rst", "ttnn/api/ttnn.sinh.rst", "ttnn/api/ttnn.sinh_bw.rst", "ttnn/api/ttnn.slice.rst", "ttnn/api/ttnn.softmax.rst", "ttnn/api/ttnn.softmax_in_place.rst", "ttnn/api/ttnn.softplus.rst", "ttnn/api/ttnn.softplus_bw.rst", "ttnn/api/ttnn.softshrink.rst", "ttnn/api/ttnn.softshrink_bw.rst", "ttnn/api/ttnn.softsign.rst", "ttnn/api/ttnn.softsign_bw.rst", "ttnn/api/ttnn.sort.rst", "ttnn/api/ttnn.sparse_matmul.rst", "ttnn/api/ttnn.split_work_to_cores.rst", "ttnn/api/ttnn.sqrt.rst", "ttnn/api/ttnn.sqrt_bw.rst", "ttnn/api/ttnn.square.rst", "ttnn/api/ttnn.square_bw.rst", "ttnn/api/ttnn.squared_difference.rst", "ttnn/api/ttnn.squared_difference_bw.rst", "ttnn/api/ttnn.std.rst", "ttnn/api/ttnn.sub_bw.rst", "ttnn/api/ttnn.subalpha.rst", "ttnn/api/ttnn.subalpha_bw.rst", "ttnn/api/ttnn.subtract.rst", "ttnn/api/ttnn.sum.rst", "ttnn/api/ttnn.swiglu.rst", "ttnn/api/ttnn.swish.rst", "ttnn/api/ttnn.synchronize_device.rst", "ttnn/api/ttnn.tan.rst", "ttnn/api/ttnn.tan_bw.rst", "ttnn/api/ttnn.tanh.rst", "ttnn/api/ttnn.tanh_bw.rst", "ttnn/api/ttnn.tanhshrink.rst", "ttnn/api/ttnn.tanhshrink_bw.rst", "ttnn/api/ttnn.threshold.rst", "ttnn/api/ttnn.threshold_bw.rst", "ttnn/api/ttnn.tilize.rst", "ttnn/api/ttnn.tilize_with_val_padding.rst", "ttnn/api/ttnn.to_device.rst", "ttnn/api/ttnn.to_layout.rst", "ttnn/api/ttnn.to_memory_config.rst", "ttnn/api/ttnn.to_torch.rst", "ttnn/api/ttnn.topk.rst", "ttnn/api/ttnn.transformer.attention_softmax.rst", "ttnn/api/ttnn.transformer.attention_softmax_.rst", "ttnn/api/ttnn.transformer.concatenate_heads.rst", "ttnn/api/ttnn.transformer.scaled_dot_product_attention.rst", "ttnn/api/ttnn.transformer.scaled_dot_product_attention_decode.rst", "ttnn/api/ttnn.transformer.split_query_key_value_and_split_heads.rst", "ttnn/api/ttnn.tril.rst", "ttnn/api/ttnn.triu.rst", "ttnn/api/ttnn.trunc.rst", "ttnn/api/ttnn.trunc_bw.rst", "ttnn/api/ttnn.unary_chain.rst", "ttnn/api/ttnn.untilize.rst", "ttnn/api/ttnn.untilize_with_unpadding.rst", "ttnn/api/ttnn.upsample.rst", "ttnn/api/ttnn.var.rst", "ttnn/api/ttnn.where.rst", "ttnn/api/ttnn.where_bw.rst", "ttnn/api/ttnn.xlogy.rst", "ttnn/api/ttnn.xlogy_bw.rst", "ttnn/api/ttnn.zeros.rst", "ttnn/api/ttnn.zeros_like.rst", "ttnn/converting_torch_model_to_ttnn.rst", "ttnn/demos.rst", "ttnn/get_started.rst", "ttnn/installing.md", "ttnn/onboarding.rst", "ttnn/profiling_ttnn_operations.rst", "ttnn/tensor.rst", "ttnn/tutorials.rst", "ttnn/tutorials/tutorials/ttnn_add_tensors.ipynb", "ttnn/tutorials/tutorials/ttnn_basic_conv.ipynb", "ttnn/tutorials/tutorials/ttnn_basic_matrix_multiplication.ipynb", "ttnn/tutorials/tutorials/ttnn_basic_operations.ipynb", "ttnn/tutorials/tutorials/ttnn_clip_zero_shot_classification.ipynb", "ttnn/tutorials/tutorials/ttnn_mlp_inference_mnist.ipynb", "ttnn/tutorials/tutorials/ttnn_multihead_attention.ipynb", "ttnn/tutorials/tutorials/ttnn_simplecnn_inference.ipynb", "ttnn/tutorials/tutorials/ttnn_tracer_model.ipynb", "ttnn/tutorials/tutorials/ttnn_visualizer.md", "ttnn/usage.rst"], "titles": ["Welcome to TT-NN documentation!", "Contributing as a developer", "Support", "Tools", "What is TT-NN?", "Adding New TT-NN Operation", "APIs", "ttnn.Conv2dConfig", "ttnn.Conv2dSliceConfig", "ttnn.GetDefaultDevice", "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig", "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig", "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig", "ttnn.MatmulMultiCoreReuseProgramConfig", "ttnn.SetDefaultDevice", "ttnn.SoftmaxDefaultProgramConfig", "ttnn.SoftmaxProgramConfig", "ttnn.SoftmaxShardedMultiCoreProgramConfig", "ttnn.abs", "ttnn.abs_bw", "ttnn.acos", "ttnn.acos_bw", "ttnn.acosh", "ttnn.acosh_bw", "ttnn.add", "ttnn.add_bw", "ttnn.addalpha", "ttnn.addalpha_bw", "ttnn.addcdiv", "ttnn.addcdiv_bw", "ttnn.addcmul", "ttnn.addcmul_bw", "ttnn.addmm", "ttnn.all_gather", "ttnn.all_reduce", "ttnn.alt_complex_rotate90", "ttnn.angle", "ttnn.angle_bw", "ttnn.arange", "ttnn.argmax", "ttnn.as_tensor", "ttnn.asin", "ttnn.asin_bw", "ttnn.asinh", "ttnn.asinh_bw", "ttnn.assign_bw", "ttnn.atan", "ttnn.atan2", "ttnn.atan2_bw", "ttnn.atan_bw", "ttnn.atanh", "ttnn.atanh_bw", "ttnn.avg_pool2d", "ttnn.batch_norm", "ttnn.bias_gelu_bw", "ttnn.bitcast", "ttnn.bitwise_and", "ttnn.bitwise_left_shift", "ttnn.bitwise_not", "ttnn.bitwise_or", "ttnn.bitwise_right_shift", "ttnn.bitwise_xor", "ttnn.cbrt", "ttnn.ceil", "ttnn.ceil_bw", "ttnn.celu", "ttnn.celu_bw", "ttnn.clamp", "ttnn.clamp_bw", "ttnn.clip", "ttnn.clip_bw", "ttnn.clone", "ttnn.close_device", "ttnn.concat", "ttnn.concat_bw", "ttnn.conj", "ttnn.conj_bw", "ttnn.conv1d", "ttnn.conv2d", "ttnn.conv_transpose2d", "ttnn.cos", "ttnn.cos_bw", "ttnn.cosh", "ttnn.cosh_bw", "ttnn.create_sharded_memory_config", "ttnn.cumprod", "ttnn.cumsum", "ttnn.deallocate", "ttnn.deg2rad", "ttnn.deg2rad_bw", "ttnn.digamma", "ttnn.digamma_bw", "ttnn.div", "ttnn.div_bw", "ttnn.div_no_nan", "ttnn.div_no_nan_bw", "ttnn.dump_tensor", "ttnn.elu", "ttnn.elu_bw", "ttnn.ema", "ttnn.embedding", "ttnn.embedding_bw", "ttnn.empty", "ttnn.empty_like", "ttnn.eq", "ttnn.eq_", "ttnn.eqz", "ttnn.erf", "ttnn.erf_bw", "ttnn.erfc", "ttnn.erfc_bw", "ttnn.erfinv", "ttnn.erfinv_bw", "ttnn.exp", "ttnn.exp2", "ttnn.exp2_bw", "ttnn.exp_bw", "ttnn.experimental.conv3d", "ttnn.experimental.dropout", "ttnn.experimental.gelu_bw", "ttnn.experimental.rotary_embedding", "ttnn.expm1", "ttnn.expm1_bw", "ttnn.fill", "ttnn.fill_bw", "ttnn.fill_ones_rm", "ttnn.fill_rm", "ttnn.fill_zero_bw", "ttnn.floor", "ttnn.floor_bw", "ttnn.floor_div", "ttnn.fmod", "ttnn.fmod_bw", "ttnn.frac", "ttnn.frac_bw", "ttnn.from_buffer", "ttnn.from_device", "ttnn.from_torch", "ttnn.full", "ttnn.full_like", "ttnn.gather", "ttnn.gcd", "ttnn.ge", "ttnn.ge_", "ttnn.geglu", "ttnn.gelu", "ttnn.gelu_bw", "ttnn.gez", "ttnn.global_avg_pool2d", "ttnn.glu", "ttnn.group_norm", "ttnn.gt", "ttnn.gt_", "ttnn.gtz", "ttnn.hardshrink", "ttnn.hardshrink_bw", "ttnn.hardsigmoid", "ttnn.hardsigmoid_bw", "ttnn.hardswish", "ttnn.hardswish_bw", "ttnn.hardtanh", "ttnn.hardtanh_bw", "ttnn.heaviside", "ttnn.hypot", "ttnn.hypot_bw", "ttnn.i0", "ttnn.i0_bw", "ttnn.identity", "ttnn.imag", "ttnn.imag_bw", "ttnn.indexed_fill", "ttnn.is_imag", "ttnn.is_real", "ttnn.isclose", "ttnn.isfinite", "ttnn.isinf", "ttnn.isnan", "ttnn.isneginf", "ttnn.isposinf", "ttnn.kv_cache.fill_cache_for_user_", "ttnn.kv_cache.update_cache_for_token_", "ttnn.l1_loss", "ttnn.layer_norm", "ttnn.layer_norm_post_all_gather", "ttnn.layer_norm_pre_all_gather", "ttnn.lcm", "ttnn.ldexp", "ttnn.ldexp_bw", "ttnn.le", "ttnn.le_", "ttnn.leaky_relu", "ttnn.leaky_relu_bw", "ttnn.lerp", "ttnn.lerp_bw", "ttnn.lez", "ttnn.lgamma", "ttnn.lgamma_bw", "ttnn.linear", "ttnn.load_tensor", "ttnn.log", "ttnn.log10", "ttnn.log10_bw", "ttnn.log1p", "ttnn.log1p_bw", "ttnn.log2", "ttnn.log2_bw", "ttnn.log_bw", "ttnn.log_sigmoid", "ttnn.log_sigmoid_bw", "ttnn.logaddexp", "ttnn.logaddexp2", "ttnn.logaddexp2_bw", "ttnn.logaddexp_bw", "ttnn.logical_and", "ttnn.logical_and_", "ttnn.logical_not", "ttnn.logical_not_", "ttnn.logical_or", "ttnn.logical_or_", "ttnn.logical_xor", "ttnn.logical_xor_", "ttnn.logit", "ttnn.logit_bw", "ttnn.logiteps_bw", "ttnn.lt", "ttnn.lt_", "ttnn.ltz", "ttnn.mac", "ttnn.manage_device", "ttnn.manual_seed", "ttnn.matmul", "ttnn.max", "ttnn.max_bw", "ttnn.max_pool2d", "ttnn.maximum", "ttnn.mean", "ttnn.min", "ttnn.min_bw", "ttnn.minimum", "ttnn.mish", "ttnn.model_preprocessing.preprocess_model", "ttnn.model_preprocessing.preprocess_model_parameters", "ttnn.moe", "ttnn.mse_loss", "ttnn.mul_bw", "ttnn.multigammaln", "ttnn.multigammaln_bw", "ttnn.multiply", "ttnn.ne", "ttnn.ne_", "ttnn.neg", "ttnn.neg_bw", "ttnn.nextafter", "ttnn.nez", "ttnn.nonzero", "ttnn.normalize_global", "ttnn.normalize_hw", "ttnn.ones", "ttnn.ones_like", "ttnn.open_device", "ttnn.outer", "ttnn.pad", "ttnn.pad_to_tile_shape", "ttnn.permute", "ttnn.polar", "ttnn.polar_bw", "ttnn.polygamma", "ttnn.polygamma_bw", "ttnn.polyval", "ttnn.pow", "ttnn.pow_bw", "ttnn.prelu", "ttnn.prepare_conv_bias", "ttnn.prepare_conv_transpose2d_bias", "ttnn.prepare_conv_transpose2d_weights", "ttnn.prepare_conv_weights", "ttnn.prod", "ttnn.prod_bw", "ttnn.rad2deg", "ttnn.rad2deg_bw", "ttnn.rand", "ttnn.rdiv", "ttnn.rdiv_bw", "ttnn.real", "ttnn.real_bw", "ttnn.reallocate", "ttnn.reciprocal", "ttnn.reciprocal_bw", "ttnn.reduce_scatter", "ttnn.register_post_operation_hook", "ttnn.register_pre_operation_hook", "ttnn.reglu", "ttnn.relu", "ttnn.relu6", "ttnn.relu6_bw", "ttnn.relu_bw", "ttnn.relu_max", "ttnn.relu_min", "ttnn.remainder", "ttnn.remainder_bw", "ttnn.repeat", "ttnn.repeat_bw", "ttnn.repeat_interleave", "ttnn.reshape", "ttnn.rms_norm", "ttnn.rms_norm_post_all_gather", "ttnn.rms_norm_pre_all_gather", "ttnn.round", "ttnn.round_bw", "ttnn.rpow", "ttnn.rpow_bw", "ttnn.rsqrt", "ttnn.rsqrt_bw", "ttnn.rsub", "ttnn.rsub_bw", "ttnn.scale_causal_mask_hw_dims_softmax_in_place", "ttnn.scale_mask_softmax", "ttnn.scale_mask_softmax_in_place", "ttnn.scatter", "ttnn.selu", "ttnn.selu_bw", "ttnn.set_printoptions", "ttnn.sigmoid", "ttnn.sigmoid_accurate", "ttnn.sigmoid_bw", "ttnn.sign", "ttnn.sign_bw", "ttnn.signbit", "ttnn.silu", "ttnn.silu_bw", "ttnn.sin", "ttnn.sin_bw", "ttnn.sinh", "ttnn.sinh_bw", "ttnn.slice", "ttnn.softmax", "ttnn.softmax_in_place", "ttnn.softplus", "ttnn.softplus_bw", "ttnn.softshrink", "ttnn.softshrink_bw", "ttnn.softsign", "ttnn.softsign_bw", "ttnn.sort", "ttnn.sparse_matmul", "ttnn.split_work_to_cores", "ttnn.sqrt", "ttnn.sqrt_bw", "ttnn.square", "ttnn.square_bw", "ttnn.squared_difference", "ttnn.squared_difference_bw", "ttnn.std", "ttnn.sub_bw", "ttnn.subalpha", "ttnn.subalpha_bw", "ttnn.subtract", "ttnn.sum", "ttnn.swiglu", "ttnn.swish", "ttnn.synchronize_device", "ttnn.tan", "ttnn.tan_bw", "ttnn.tanh", "ttnn.tanh_bw", "ttnn.tanhshrink", "ttnn.tanhshrink_bw", "ttnn.threshold", "ttnn.threshold_bw", "ttnn.tilize", "ttnn.tilize_with_val_padding", "ttnn.to_device", "ttnn.to_layout", "ttnn.to_memory_config", "ttnn.to_torch", "ttnn.topk", "ttnn.transformer.attention_softmax", "ttnn.transformer.attention_softmax_", "ttnn.transformer.concatenate_heads", "ttnn.transformer.scaled_dot_product_attention", "ttnn.transformer.scaled_dot_product_attention_decode", "ttnn.transformer.split_query_key_value_and_split_heads", "ttnn.tril", "ttnn.triu", "ttnn.trunc", "ttnn.trunc_bw", "ttnn.unary_chain", "ttnn.untilize", "ttnn.untilize_with_unpadding", "ttnn.upsample", "ttnn.var", "ttnn.where", "ttnn.where_bw", "ttnn.xlogy", "ttnn.xlogy_bw", "ttnn.zeros", "ttnn.zeros_like", "Converting PyTorch Model to TT-NN", "Building and Uplifting Demos", "Getting Started", "Install", "Onboarding New Functionality", "Profiling TT-NN Operations", "Tensor", "Tutorials", "Add Tensors", "Basic Convolution", "Matrix Multiplication", "Basic Tensor Operations", "Building CLIP Model for Zero-Shot Image Classification with TT-NN", "MLP Inference", "Multi-Head Attention", "Running a Simple CNN Inference on CIFAR-10", "TT-NN Tracer and BERT Model Visualization Tutorial", "TT-NN Visualizer", "Using TT-NN"], "terms": {"what": [0, 398, 402, 404], "i": [0, 3, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 24, 25, 26, 27, 30, 32, 33, 34, 39, 47, 48, 52, 53, 54, 56, 57, 58, 59, 60, 61, 67, 72, 73, 74, 78, 79, 84, 85, 92, 93, 94, 100, 101, 104, 105, 120, 123, 125, 126, 130, 131, 134, 135, 137, 140, 141, 142, 143, 144, 148, 149, 150, 151, 152, 163, 164, 166, 171, 172, 173, 174, 175, 176, 177, 178, 182, 183, 184, 185, 186, 187, 188, 189, 193, 197, 209, 210, 211, 212, 213, 214, 217, 218, 219, 220, 224, 225, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 240, 241, 242, 244, 246, 247, 248, 249, 252, 259, 260, 261, 262, 264, 266, 268, 269, 271, 276, 277, 281, 286, 288, 289, 290, 291, 296, 297, 298, 302, 303, 304, 305, 306, 313, 314, 315, 316, 317, 334, 335, 336, 343, 344, 345, 350, 351, 352, 353, 354, 355, 356, 357, 358, 360, 361, 362, 363, 365, 372, 374, 375, 376, 377, 379, 380, 381, 389, 390, 391, 393, 394, 397, 398, 399, 400, 401, 402, 403, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415], "get": [0, 7, 9, 36, 75, 168, 283, 322, 375, 378, 397, 400, 404, 406, 407, 409, 413, 414], "start": [0, 38, 192, 334, 397, 400, 402, 404, 409, 411, 413, 414], "1": [0, 7, 8, 10, 15, 17, 18, 19, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 44, 45, 47, 48, 49, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 81, 83, 85, 86, 89, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 104, 105, 107, 108, 109, 110, 112, 113, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 134, 135, 138, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 152, 154, 155, 157, 159, 160, 161, 162, 163, 164, 166, 167, 168, 169, 170, 171, 172, 173, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 196, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 214, 217, 218, 219, 220, 221, 222, 223, 224, 225, 227, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 276, 277, 279, 280, 281, 282, 283, 284, 286, 287, 288, 291, 294, 295, 296, 297, 298, 299, 300, 301, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 316, 317, 318, 319, 320, 322, 323, 324, 326, 329, 331, 333, 334, 335, 336, 337, 338, 339, 340, 342, 343, 344, 345, 346, 347, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 375, 379, 380, 381, 385, 387, 388, 390, 391, 392, 393, 394, 395, 401, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 414], "instal": [0, 3, 398, 402, 404, 414, 415], "build": [0, 5, 404, 406, 414], "2": [0, 7, 8, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 74, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 145, 146, 147, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 257, 258, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 276, 278, 279, 280, 281, 282, 285, 286, 287, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 359, 361, 362, 363, 364, 365, 366, 367, 368, 372, 373, 374, 379, 381, 382, 383, 384, 385, 386, 390, 391, 392, 393, 394, 395, 396, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412], "explor": [0, 407, 413, 414], "our": [0, 3, 398, 400, 401, 403, 407, 408, 409], "model": [0, 3, 4, 240, 241, 369, 398, 401, 402, 404, 405, 406, 410, 411, 412], "demo": [0, 400, 402, 413, 414], "where": [0, 2, 7, 32, 33, 38, 53, 79, 84, 102, 103, 135, 150, 182, 183, 184, 197, 230, 254, 264, 271, 288, 304, 305, 306, 315, 316, 318, 344, 372, 392, 398, 402, 403, 409, 412, 414], "To": [0, 7, 229, 375, 401, 403, 404, 406, 407, 408, 409, 414, 415], "go": [0, 183, 184, 305, 306, 318], "from": [0, 2, 5, 7, 10, 33, 34, 38, 40, 72, 73, 77, 78, 79, 85, 86, 101, 135, 136, 137, 140, 179, 180, 183, 184, 198, 264, 272, 273, 274, 275, 280, 305, 306, 313, 314, 315, 316, 318, 335, 344, 356, 388, 391, 397, 398, 400, 402, 403, 405, 406, 408, 409, 410, 411, 412, 413, 414], "here": [0, 2, 5, 183, 184, 305, 306, 400, 402, 405, 406, 407, 408, 410, 414], "prerequisit": [0, 404], "set": [0, 7, 10, 14, 15, 24, 26, 47, 56, 57, 59, 60, 61, 73, 78, 79, 92, 94, 104, 105, 125, 130, 131, 140, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 229, 230, 234, 238, 246, 247, 248, 249, 252, 260, 268, 269, 271, 272, 273, 276, 298, 303, 313, 321, 344, 345, 350, 354, 356, 360, 369, 375, 378, 381, 393, 397, 402, 404, 407, 409, 410, 412, 414, 415], "up": [0, 4, 5, 24, 26, 47, 56, 57, 59, 60, 61, 92, 94, 104, 105, 126, 130, 131, 141, 142, 143, 150, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 350, 354, 356, 393, 402, 404, 406, 407, 409, 412], "hardwar": [0, 2, 3, 5, 259, 397, 398, 399, 403, 405, 407, 409, 410, 412, 413, 414, 415], "softwar": [0, 406, 407, 408, 410, 411, 412], "depend": [0, 12, 230, 272, 273, 274, 275, 344, 372, 391, 398, 402, 403, 404], "option": [0, 7, 10, 11, 12, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 266, 267, 268, 269, 270, 271, 276, 277, 278, 279, 280, 281, 282, 283, 285, 286, 287, 288, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 402, 406, 409, 412, 414, 415], "script": [0, 398, 402, 404, 405, 406, 407, 408, 410, 411, 412], "recommend": [0, 7, 187, 397, 404, 414, 415], "manual": [0, 5, 7, 229, 397, 404, 415], "metalium": [0, 5], "There": [0, 5, 230, 397, 403], "ar": [0, 7, 8, 10, 12, 13, 14, 24, 25, 26, 32, 33, 47, 52, 53, 56, 57, 59, 60, 61, 71, 73, 85, 86, 92, 93, 94, 104, 105, 106, 126, 130, 131, 140, 141, 142, 143, 147, 150, 151, 152, 153, 163, 171, 172, 173, 174, 175, 176, 177, 178, 182, 183, 184, 185, 186, 188, 189, 194, 197, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 226, 229, 230, 233, 234, 238, 242, 247, 248, 249, 252, 253, 254, 260, 263, 268, 269, 271, 289, 290, 298, 304, 305, 306, 309, 313, 316, 318, 334, 344, 350, 354, 356, 360, 375, 381, 389, 393, 397, 398, 399, 402, 403, 404, 406, 407, 409, 410, 411, 412, 413, 414, 415], "four": 0, "binari": [0, 24, 26, 47, 56, 57, 59, 60, 61, 92, 94, 104, 105, 130, 131, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 350, 354, 356, 393], "step": [0, 38, 162, 183, 184, 305, 306, 334, 398, 401, 409, 414], "latest": [0, 402], "wheel": [0, 415], "For": [0, 3, 5, 7, 19, 24, 26, 29, 40, 47, 54, 56, 57, 59, 60, 61, 78, 85, 86, 92, 93, 94, 99, 104, 105, 130, 131, 132, 137, 140, 141, 142, 143, 146, 150, 151, 152, 163, 164, 173, 183, 184, 185, 186, 188, 189, 193, 201, 203, 205, 206, 208, 209, 210, 211, 212, 213, 214, 217, 218, 219, 220, 222, 223, 224, 225, 230, 234, 238, 242, 247, 248, 249, 252, 260, 268, 269, 270, 271, 277, 282, 287, 298, 299, 305, 306, 313, 316, 317, 336, 338, 343, 347, 350, 354, 356, 375, 381, 391, 393, 394, 397, 398, 401, 402, 403, 405, 409, 410, 412, 413, 414], "user": [0, 5, 7, 32, 167, 197, 229, 230, 240, 241, 259, 344, 370, 398, 399, 401, 406, 407, 408, 410, 411, 412, 414], "onli": [0, 5, 7, 8, 25, 27, 28, 30, 39, 48, 54, 67, 68, 69, 70, 73, 74, 84, 92, 93, 101, 126, 137, 142, 144, 149, 150, 151, 164, 171, 172, 187, 188, 192, 193, 211, 212, 213, 224, 227, 229, 230, 231, 232, 235, 236, 237, 244, 261, 266, 274, 275, 276, 286, 291, 299, 314, 315, 344, 351, 352, 353, 355, 357, 358, 360, 363, 365, 369, 375, 379, 380, 381, 390, 391, 394, 397, 398, 402, 403, 409, 412, 413, 414, 415], "environ": [0, 399, 404, 409, 413, 414, 415], "docker": [0, 415], "releas": [0, 87, 415], "imag": [0, 7, 78, 79, 171, 264, 398, 402, 403, 404, 406, 412, 415], "sourc": [0, 4, 140, 179, 318, 414], "clone": [0, 167, 381, 414], "repositori": [0, 1, 399, 404, 414], "librari": [0, 4, 404, 409, 412], "3": [0, 7, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 100, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 173, 174, 175, 176, 177, 178, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 231, 232, 233, 234, 235, 236, 237, 238, 239, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 257, 258, 261, 263, 264, 265, 266, 267, 268, 269, 270, 271, 276, 278, 279, 280, 281, 282, 285, 286, 287, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 307, 308, 309, 310, 311, 312, 313, 314, 315, 317, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 337, 338, 339, 340, 341, 342, 343, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 372, 373, 374, 375, 381, 382, 383, 384, 385, 386, 390, 391, 392, 393, 394, 395, 396, 403, 404, 405, 406, 409, 410, 411, 412], "virtual": [0, 404, 407], "setup": [0, 12, 17, 53, 183, 184, 304, 305, 306, 315, 316, 317, 398, 399, 404, 414], "anaconda": 0, "packag": 0, "you": [0, 1, 2, 5, 337, 398, 399, 402, 403, 404, 414, 415], "all": [0, 5, 7, 10, 16, 32, 33, 34, 39, 53, 140, 148, 182, 183, 184, 229, 230, 231, 235, 236, 240, 241, 242, 255, 261, 268, 276, 277, 288, 303, 304, 305, 306, 316, 317, 335, 345, 352, 357, 360, 375, 379, 390, 397, 398, 401, 402, 403, 405, 407, 409, 410, 411, 412, 413, 414], "verifi": [0, 401, 412], "your": [0, 398, 399, 402, 404, 414], "try": [0, 7, 398, 406, 409], "execut": [0, 3, 5, 7, 17, 140, 229, 289, 290, 375, 398, 402, 406, 409, 410, 411, 412, 413, 414, 415], "program": [0, 4, 5, 10, 11, 12, 13, 15, 16, 17, 32, 183, 184, 197, 230, 305, 306, 315, 317, 336, 344, 376, 377, 402, 404, 405, 406, 407, 408, 410, 411, 412], "exampl": [0, 7, 345, 398, 399, 401, 402, 403, 404, 414], "interest": 0, "contribut": [0, 2, 399], "multi": [0, 3, 13, 17, 40, 229, 336, 389, 403, 404, 409, 410], "card": [0, 398], "configur": [0, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 190, 191, 192, 193, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 219, 221, 222, 223, 224, 226, 227, 231, 232, 233, 234, 235, 236, 237, 238, 239, 243, 244, 245, 246, 247, 248, 250, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 263, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 335, 336, 337, 338, 339, 340, 341, 342, 343, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 372, 373, 375, 376, 377, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 398, 399, 404, 406, 409, 412, 413, 414], "topologi": [0, 33, 34, 288], "machin": [0, 402, 414], "requir": [0, 5, 7, 10, 25, 27, 35, 45, 53, 74, 84, 93, 125, 126, 138, 150, 240, 241, 244, 257, 276, 314, 335, 353, 355, 392, 395, 398, 402, 406, 408, 409, 411, 412, 414], "overview": [0, 414], "why": [0, 407, 409], "It": [0, 3, 4, 5, 7, 8, 13, 15, 16, 17, 77, 79, 240, 241, 276, 288, 345, 374, 409, 410, 412, 414], "matter": 0, "vm": 0, "us": [0, 4, 5, 7, 8, 9, 10, 12, 13, 14, 15, 16, 24, 26, 32, 33, 34, 47, 52, 54, 56, 57, 59, 60, 61, 67, 69, 73, 77, 79, 84, 92, 93, 94, 100, 101, 103, 104, 105, 107, 109, 113, 118, 119, 120, 126, 130, 131, 135, 137, 139, 140, 141, 142, 143, 145, 146, 150, 151, 152, 163, 164, 167, 173, 183, 184, 185, 186, 188, 189, 197, 199, 200, 202, 204, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 230, 231, 233, 234, 235, 236, 238, 240, 241, 247, 248, 249, 252, 258, 259, 260, 261, 268, 269, 271, 272, 273, 274, 275, 276, 281, 282, 288, 289, 290, 298, 301, 303, 305, 306, 311, 313, 315, 316, 317, 321, 322, 323, 335, 336, 337, 343, 344, 345, 346, 350, 352, 354, 356, 357, 369, 370, 373, 374, 375, 380, 381, 387, 388, 390, 393, 396, 397, 398, 399, 400, 401, 403, 404, 406, 408, 409, 410, 411, 412, 414], "basic": [0, 13, 400, 404, 413], "convert": [0, 4, 5, 7, 10, 40, 55, 71, 88, 137, 150, 240, 241, 264, 272, 273, 274, 275, 278, 343, 373, 374, 403, 406, 407, 408, 409, 410, 411, 412, 413], "torch": [0, 5, 18, 19, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 40, 42, 44, 45, 47, 48, 49, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 74, 75, 76, 81, 83, 89, 90, 91, 92, 93, 94, 95, 97, 98, 100, 101, 104, 105, 107, 108, 109, 110, 112, 113, 115, 116, 118, 119, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 134, 137, 141, 142, 143, 145, 146, 150, 151, 152, 154, 155, 157, 159, 160, 161, 162, 163, 164, 166, 167, 168, 169, 170, 171, 172, 173, 181, 185, 186, 187, 188, 189, 190, 191, 192, 193, 196, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 214, 217, 218, 219, 220, 221, 222, 223, 224, 225, 227, 229, 232, 233, 234, 237, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 254, 260, 264, 265, 266, 267, 268, 269, 270, 271, 277, 279, 281, 282, 283, 284, 286, 287, 294, 295, 296, 297, 298, 299, 300, 301, 303, 307, 308, 309, 310, 311, 312, 313, 314, 318, 319, 320, 322, 323, 324, 326, 329, 331, 333, 337, 338, 339, 340, 342, 343, 344, 346, 347, 349, 350, 351, 353, 354, 355, 356, 362, 363, 364, 365, 366, 367, 368, 371, 374, 375, 381, 385, 386, 391, 392, 393, 394, 397, 403, 406, 408, 409, 410, 411, 412, 413], "tensor": [0, 3, 4, 5, 7, 8, 10, 11, 12, 13, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 232, 233, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 391, 392, 393, 394, 395, 396, 397, 402, 404, 409, 410, 411, 412], "run": [0, 4, 5, 53, 78, 137, 183, 184, 240, 289, 290, 305, 318, 363, 365, 375, 398, 399, 400, 401, 402, 404, 405, 407, 408, 410, 411, 413], "an": [0, 2, 3, 4, 5, 24, 26, 47, 52, 56, 57, 59, 60, 61, 73, 77, 78, 79, 92, 94, 99, 100, 102, 104, 105, 117, 126, 127, 130, 131, 140, 141, 142, 143, 148, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 228, 229, 230, 234, 238, 247, 248, 249, 252, 260, 264, 268, 269, 271, 276, 280, 298, 305, 306, 313, 315, 345, 350, 354, 356, 371, 374, 393, 398, 399, 400, 401, 402, 403, 404, 406, 409, 411, 412, 414], "oper": [0, 3, 4, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 260, 261, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 335, 336, 337, 338, 339, 340, 341, 342, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 372, 374, 375, 376, 377, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 398, 400, 401, 403, 404, 407, 409, 410, 411, 412], "devic": [0, 3, 4, 7, 9, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 382, 383, 384, 385, 386, 387, 388, 390, 391, 392, 393, 394, 395, 396, 397, 398, 400, 402, 403, 404, 411, 414], "__getitem__": 0, "slice": [0, 7, 8, 78, 79], "4": [0, 7, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 74, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 133, 134, 135, 137, 140, 141, 142, 143, 144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 170, 173, 174, 175, 176, 177, 178, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 260, 261, 262, 265, 266, 267, 268, 269, 270, 271, 276, 277, 278, 279, 281, 282, 286, 287, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 303, 307, 308, 309, 310, 311, 312, 313, 314, 317, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 337, 338, 339, 340, 341, 342, 343, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 372, 382, 383, 384, 385, 386, 390, 391, 392, 393, 394, 395, 396, 400, 403, 404, 406, 407, 408, 409, 410, 411, 412, 414], "enabl": [0, 4, 7, 17, 78, 321, 323, 363, 365, 375, 400, 401, 406, 408, 409, 410, 411, 412, 414], "cach": [0, 4, 40, 72, 120, 179, 180, 240, 241, 402, 403, 406, 407, 408, 409, 410, 411, 412, 413, 414], "5": [0, 7, 24, 26, 47, 53, 56, 57, 59, 60, 61, 68, 70, 84, 85, 86, 90, 92, 94, 97, 100, 103, 104, 105, 123, 130, 131, 135, 137, 141, 142, 143, 151, 152, 154, 155, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 221, 224, 225, 227, 234, 238, 245, 247, 248, 249, 252, 260, 268, 269, 271, 276, 282, 298, 313, 318, 339, 340, 343, 350, 354, 356, 391, 393, 400, 408, 409, 410, 411, 412], "debug": [0, 4, 5, 401, 412, 413, 414], "intermedi": [0, 13, 316, 317], "6": [0, 123, 135, 170, 227, 245, 293, 307, 321, 343, 391, 403, 406, 407, 408, 409, 410, 411, 412], "trace": [0, 3, 4, 259, 402, 404], "graph": [0, 3, 4, 240, 404, 413], "7": [0, 138, 227, 229, 307, 345, 391, 406, 407, 408, 409, 410, 411, 412], "tt_lib": [0, 125, 126, 371], "8": [0, 17, 24, 26, 33, 34, 47, 56, 57, 59, 60, 61, 84, 92, 94, 104, 105, 130, 131, 141, 142, 143, 150, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 227, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 288, 298, 313, 315, 317, 344, 345, 350, 354, 356, 391, 393, 402, 403, 407, 408, 409, 412, 413], "log": [0, 195, 196, 202, 203, 207, 208, 209, 210, 211, 212, 245, 246, 393, 394, 402, 405, 407, 410, 412, 413, 414], "9": [0, 38, 67, 69, 82, 100, 227, 332, 391, 409, 410, 411], "support": [0, 1, 3, 4, 7, 8, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 78, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 133, 134, 140, 141, 142, 143, 144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 173, 174, 175, 176, 177, 178, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 260, 261, 265, 266, 267, 268, 269, 270, 271, 274, 275, 276, 277, 278, 279, 280, 281, 282, 286, 287, 291, 292, 293, 294, 295, 296, 297, 298, 299, 301, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 335, 336, 337, 338, 339, 340, 341, 342, 343, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 375, 380, 382, 383, 384, 385, 386, 390, 391, 392, 393, 394, 395, 396, 400, 403, 409, 412, 414], "python": [0, 4, 135, 400, 401, 402, 403, 404, 405, 408, 409, 414], "10": [0, 32, 38, 100, 148, 166, 197, 200, 201, 227, 230, 266, 302, 318, 367, 371, 391, 400, 404, 406, 407, 409, 410, 414], "chang": [0, 229, 303, 369, 370, 372, 373, 387, 388, 403], "string": [0, 54, 92, 146, 240, 241, 281, 282, 321, 409], "represent": [0, 252, 337, 403, 409, 413, 414], "11": [0, 227, 391, 400, 409], "visual": [0, 3, 4, 404, 409], "web": 0, "browser": [0, 414], "12": [0, 7, 17, 74, 150, 182, 183, 227, 261, 304, 305, 315, 317, 391, 397, 400, 404, 409, 411], "regist": [0, 4, 5, 289, 290], "pre": [0, 5, 52, 150, 183, 184, 233, 290, 305, 306, 400, 409, 411, 412], "post": [0, 183, 184, 289, 305, 306, 402, 409], "hook": [0, 289, 290], "13": [0, 406, 408, 409, 410, 411, 412], "queri": [0, 379, 381, 409, 411], "14": [0, 74, 139, 406, 408, 409, 410, 411, 412], "fall": [0, 33, 79, 187, 288, 336], "back": [0, 5, 33, 79, 136, 198, 288, 336, 378, 402, 406, 409, 410, 412, 413], "15": [0, 150, 406, 408, 412], "captur": [0, 413], "c": [0, 4, 8, 24, 26, 47, 52, 53, 56, 57, 59, 60, 61, 77, 78, 79, 92, 94, 99, 104, 105, 117, 125, 126, 130, 131, 141, 142, 143, 150, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 233, 234, 238, 242, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 322, 350, 354, 356, 375, 389, 393, 400, 402, 403, 406, 407, 409, 412], "function": [0, 7, 9, 10, 11, 12, 14, 19, 21, 23, 24, 37, 40, 42, 44, 47, 49, 50, 51, 52, 64, 65, 66, 68, 69, 70, 76, 81, 82, 83, 89, 90, 91, 95, 97, 98, 104, 107, 108, 109, 110, 111, 112, 113, 115, 116, 119, 122, 124, 127, 129, 134, 137, 141, 142, 144, 145, 146, 149, 150, 151, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 166, 167, 169, 181, 185, 186, 188, 190, 191, 195, 196, 197, 201, 203, 205, 206, 208, 209, 210, 213, 216, 217, 219, 221, 222, 223, 224, 230, 234, 238, 239, 240, 241, 243, 245, 246, 247, 248, 251, 255, 256, 262, 265, 266, 267, 270, 271, 276, 277, 279, 282, 284, 287, 291, 292, 293, 294, 295, 296, 297, 301, 308, 310, 312, 313, 319, 320, 322, 323, 324, 326, 328, 329, 331, 332, 333, 335, 336, 337, 338, 339, 340, 341, 342, 345, 347, 349, 350, 356, 358, 359, 362, 364, 365, 366, 367, 368, 382, 383, 385, 393, 397, 398, 403, 404, 406, 408, 410, 412, 413], "buffer": [0, 3, 5, 7, 87, 135, 230, 259, 304, 336, 402, 403, 404], "alloc": [0, 5, 38, 102, 103, 135, 138, 139, 150, 257, 258, 280, 317, 336, 395, 396, 403, 406, 412, 413, 414], "etc": [0, 4, 5, 7, 403, 409, 412], "shape": [0, 10, 12, 24, 26, 32, 33, 34, 39, 40, 47, 52, 53, 56, 57, 59, 60, 61, 73, 79, 84, 85, 86, 92, 94, 99, 102, 103, 104, 105, 123, 125, 126, 130, 131, 135, 136, 137, 138, 139, 140, 141, 142, 143, 148, 150, 151, 152, 163, 170, 173, 182, 184, 185, 186, 188, 189, 197, 198, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 230, 233, 234, 238, 242, 247, 248, 249, 252, 254, 257, 258, 260, 261, 262, 263, 268, 269, 271, 276, 280, 285, 288, 298, 300, 301, 302, 303, 304, 306, 313, 315, 317, 334, 336, 344, 350, 354, 356, 369, 370, 371, 372, 373, 374, 375, 378, 381, 387, 388, 393, 395, 396, 397, 402, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414], "layout": [0, 5, 7, 10, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 74, 78, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 170, 173, 174, 175, 176, 177, 178, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 260, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 286, 287, 288, 291, 292, 293, 294, 295, 296, 297, 298, 299, 301, 302, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 372, 374, 375, 382, 383, 384, 385, 386, 387, 388, 390, 391, 392, 393, 394, 395, 396, 397, 402, 404, 405, 406, 408, 409, 410, 411, 412, 413, 414, 415], "data": [0, 4, 7, 10, 11, 12, 13, 17, 24, 26, 32, 33, 34, 38, 39, 40, 47, 52, 55, 56, 57, 59, 60, 61, 71, 77, 78, 79, 92, 94, 100, 101, 102, 103, 104, 105, 125, 126, 130, 131, 135, 137, 138, 139, 141, 142, 143, 148, 150, 151, 152, 163, 173, 182, 183, 184, 185, 186, 188, 189, 197, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 230, 231, 233, 234, 235, 236, 238, 242, 247, 248, 249, 252, 257, 258, 260, 268, 269, 271, 272, 273, 274, 275, 276, 280, 281, 298, 304, 305, 306, 313, 315, 316, 317, 335, 336, 344, 350, 352, 354, 356, 357, 369, 370, 372, 373, 374, 375, 387, 388, 389, 390, 393, 395, 396, 397, 402, 404, 408, 409, 412, 413, 414, 415], "type": [0, 4, 5, 7, 8, 16, 24, 32, 38, 39, 40, 54, 55, 71, 77, 78, 79, 85, 86, 93, 100, 101, 102, 103, 104, 125, 126, 135, 137, 138, 139, 141, 142, 146, 148, 150, 151, 182, 183, 184, 185, 186, 188, 197, 209, 210, 213, 217, 219, 224, 230, 231, 234, 235, 236, 238, 242, 247, 248, 257, 258, 272, 273, 274, 275, 276, 280, 281, 304, 305, 306, 313, 315, 316, 317, 335, 336, 343, 344, 350, 352, 356, 357, 360, 369, 370, 372, 373, 374, 375, 387, 388, 390, 393, 395, 396, 397, 402, 409, 410, 412, 413, 414], "limit": [0, 19, 29, 39, 53, 54, 85, 86, 93, 99, 132, 146, 150, 164, 182, 183, 184, 197, 201, 203, 205, 206, 208, 211, 212, 222, 223, 242, 270, 276, 277, 282, 287, 296, 297, 299, 304, 305, 306, 315, 316, 317, 335, 336, 338, 347, 375, 394, 398, 413], "bfloat8_b": [0, 7, 17, 18, 19, 20, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 41, 43, 44, 45, 46, 47, 48, 49, 50, 54, 62, 63, 65, 67, 74, 78, 80, 81, 82, 88, 89, 90, 91, 93, 95, 97, 104, 105, 106, 107, 109, 111, 113, 114, 115, 116, 121, 122, 123, 128, 132, 133, 134, 137, 142, 143, 144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 158, 160, 161, 162, 163, 164, 165, 166, 167, 174, 175, 176, 177, 178, 182, 183, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 230, 231, 232, 235, 236, 237, 239, 244, 247, 248, 249, 250, 251, 252, 253, 268, 269, 270, 271, 277, 278, 279, 280, 282, 286, 287, 291, 292, 293, 295, 296, 299, 304, 305, 306, 307, 310, 311, 312, 313, 314, 315, 316, 317, 319, 320, 323, 324, 325, 327, 328, 329, 330, 331, 332, 335, 336, 337, 338, 339, 340, 341, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 365, 366, 374, 382, 383, 384, 386, 390, 391, 392, 394, 397, 402, 411], "storag": [0, 10, 12, 13, 17], "shard": [0, 7, 10, 11, 12, 17, 24, 25, 26, 32, 33, 34, 39, 47, 52, 56, 57, 59, 60, 61, 73, 84, 92, 94, 104, 105, 130, 131, 141, 142, 143, 150, 151, 152, 163, 173, 182, 183, 184, 185, 186, 188, 189, 197, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 230, 231, 233, 234, 235, 236, 238, 242, 247, 248, 249, 252, 260, 268, 269, 271, 272, 273, 274, 275, 276, 298, 304, 305, 306, 313, 317, 335, 336, 344, 350, 352, 354, 356, 357, 371, 373, 375, 381, 390, 393, 409, 414], "memori": [0, 3, 7, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 190, 191, 192, 193, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 219, 221, 222, 223, 224, 226, 227, 231, 232, 233, 234, 235, 236, 237, 238, 239, 242, 243, 244, 245, 246, 247, 248, 250, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 316, 317, 318, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 400, 402, 404, 406, 407, 409, 411, 412, 413], "config": [0, 7, 10, 11, 12, 13, 15, 17, 75, 99, 100, 101, 117, 182, 230, 242, 259, 264, 272, 273, 303, 315, 317, 334, 344, 375, 376, 377, 378, 397, 404, 406, 412, 414, 415], "api": [0, 5, 379, 397, 399, 400, 401, 405, 406, 408, 410, 411, 412, 414, 415], "rank": [0, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 74, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 133, 134, 141, 142, 143, 144, 145, 146, 147, 149, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 173, 174, 175, 176, 177, 178, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 232, 234, 237, 238, 239, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 260, 261, 265, 266, 267, 268, 269, 270, 271, 276, 277, 278, 279, 281, 282, 286, 287, 291, 292, 293, 294, 295, 296, 297, 298, 299, 301, 304, 307, 308, 309, 310, 311, 312, 313, 314, 317, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 337, 338, 339, 340, 341, 342, 344, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 374, 382, 383, 384, 385, 386, 391, 392, 393, 394, 395, 396, 403], "to_rank": [0, 403], "open_devic": [0, 14, 72, 100, 136, 360, 371, 405, 406, 407, 408, 409, 410, 411, 412, 413, 415], "close_devic": [0, 52, 233, 405, 406, 407, 408, 409, 410, 411, 412, 413, 415], "manage_devic": [0, 415], "synchronize_devic": 0, "setdefaultdevic": 0, "getdefaultdevic": 0, "pad_to_tile_shap": 0, "create_sharded_memory_config": [0, 403], "core": [0, 5, 7, 10, 11, 12, 13, 17, 32, 33, 34, 84, 99, 105, 107, 109, 113, 128, 131, 140, 143, 145, 150, 152, 189, 197, 199, 200, 202, 204, 225, 226, 229, 230, 231, 235, 236, 249, 259, 288, 298, 303, 305, 311, 318, 334, 336, 344, 345, 346, 352, 357, 369, 375, 384, 387, 390, 391, 397, 402, 403, 406, 407, 408, 410, 411, 412, 413, 414], "as_tensor": 0, "from_torch": [0, 5, 18, 19, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 42, 44, 45, 47, 48, 49, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 74, 81, 83, 89, 90, 91, 92, 93, 94, 95, 97, 98, 100, 101, 104, 105, 107, 108, 109, 110, 112, 113, 115, 116, 118, 119, 122, 123, 124, 127, 129, 130, 131, 132, 134, 141, 142, 143, 145, 146, 150, 151, 152, 154, 155, 157, 159, 160, 161, 162, 163, 164, 166, 167, 173, 181, 185, 186, 187, 188, 189, 190, 191, 192, 193, 196, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 214, 217, 218, 219, 220, 221, 222, 223, 224, 225, 227, 229, 232, 233, 234, 237, 238, 243, 244, 245, 246, 247, 248, 249, 251, 252, 254, 260, 266, 267, 268, 269, 270, 271, 277, 279, 281, 282, 286, 287, 294, 295, 296, 297, 298, 299, 300, 301, 303, 307, 308, 309, 310, 311, 312, 313, 314, 318, 319, 320, 322, 323, 324, 326, 329, 331, 333, 337, 338, 339, 340, 342, 343, 344, 346, 347, 349, 350, 351, 353, 354, 355, 356, 362, 363, 364, 365, 366, 367, 368, 371, 385, 386, 391, 392, 393, 394, 397, 403, 408, 409, 410, 411, 412, 413, 415], "to_torch": [0, 5, 150, 397, 406, 408, 409, 410, 411, 412, 413, 415], "to_devic": [0, 100, 150, 181, 233, 243, 344, 411], "from_devic": [0, 409, 413], "to_layout": [0, 137, 374, 403, 407, 409, 411, 412], "dump_tensor": [0, 198], "load_tensor": 0, "dealloc": [0, 7, 52, 233, 285, 403, 411, 415], "realloc": [0, 52, 233, 303], "to_memory_config": [0, 17, 315, 317, 403, 409], "split_work_to_cor": [0, 5], "creation": [0, 17, 137, 403, 404, 406, 410, 413], "arang": [0, 303, 408, 409], "empti": [0, 5, 10, 229, 345, 415], "empty_lik": 0, "zero": [0, 52, 77, 78, 79, 92, 106, 118, 125, 126, 127, 147, 150, 153, 171, 172, 194, 226, 228, 242, 253, 254, 261, 272, 273, 274, 275, 281, 286, 344, 396, 403, 404, 406, 408, 413, 415], "zeros_lik": 0, "ones": [0, 258, 403, 408], "ones_lik": 0, "full": [0, 7, 276, 398, 402, 403, 404, 409, 413, 414, 415], "full_lik": 0, "rand": [0, 5, 15, 17, 20, 22, 32, 35, 36, 37, 39, 41, 43, 46, 50, 53, 62, 63, 71, 73, 74, 75, 76, 80, 82, 85, 86, 87, 88, 96, 99, 100, 103, 106, 111, 114, 121, 128, 133, 136, 139, 140, 144, 147, 148, 149, 150, 153, 156, 158, 165, 168, 169, 170, 171, 172, 174, 175, 176, 177, 178, 182, 183, 184, 194, 195, 197, 198, 207, 215, 216, 226, 230, 231, 235, 236, 239, 242, 250, 253, 255, 256, 258, 260, 261, 263, 264, 265, 271, 276, 277, 278, 283, 284, 285, 291, 292, 293, 301, 302, 304, 305, 306, 315, 316, 317, 325, 327, 328, 330, 332, 334, 335, 336, 341, 344, 348, 352, 357, 358, 359, 361, 369, 370, 372, 373, 374, 375, 382, 383, 384, 387, 388, 390, 396, 406, 407, 408, 410, 412, 413, 415], "from_buff": 0, "matrix": [0, 4, 7, 10, 12, 32, 100, 230, 344, 403, 404, 405, 408, 409, 414, 415], "multipl": [0, 4, 5, 7, 12, 13, 32, 71, 87, 126, 138, 144, 149, 150, 183, 185, 230, 240, 242, 244, 247, 257, 272, 273, 274, 275, 291, 305, 318, 358, 372, 375, 395, 400, 402, 404, 408, 409, 414], "matmul": [0, 7, 10, 11, 12, 13, 32, 197, 344, 407, 408, 409, 411, 414], "linear": [0, 144, 149, 291, 337, 358, 397, 409, 410, 411, 412], "addmm": 0, "sparse_matmul": 0, "matmulmulticorereuseprogramconfig": [0, 230], "compute_with_storage_grid_s": [0, 5, 10, 12, 13, 17, 315, 316, 317, 344], "from_json": [0, 10, 11, 12, 13], "in0_block_w": [0, 10, 11, 12, 13, 344], "out_subblock_h": [0, 10, 12, 13, 344], "out_subblock_w": [0, 10, 12, 13, 344], "per_core_m": [0, 10, 11, 12, 13, 344], "per_core_n": [0, 10, 11, 12, 13, 344], "to_json": [0, 10, 11, 12, 13], "matmulmulticorereusemulticastprogramconfig": [0, 230], "fuse_batch": [0, 10, 12, 344], "fused_activ": [0, 10, 11, 12, 197, 230, 344], "out_block_h": [0, 10, 12, 344], "out_block_w": [0, 10, 12, 344], "transpose_mcast": [0, 12], "matmulmulticorereusemulticast1dprogramconfig": [0, 230, 344], "gather_in0": [0, 10], "hop_cor": [0, 10], "mcast_in0": [0, 10, 230, 344], "num_global_cb_receiv": [0, 10], "untilize_out": [0, 10], "matmulmulticorereusemulticastdramshardedprogramconfig": [0, 230], "pointwis": 0, "unari": [0, 5, 282, 386], "ab": [0, 19], "aco": [0, 21], "acosh": [0, 23], "asin": [0, 42], "asinh": [0, 44], "atan": [0, 49], "atanh": [0, 51], "bitcast": 0, "bitwise_not": 0, "bitwise_left_shift": 0, "bitwise_right_shift": 0, "cbrt": 0, "ceil": [0, 52, 64, 150, 233], "celu": [0, 66], "clamp": [0, 68, 221, 293], "clip": [0, 70, 404], "co": 0, "cosh": [0, 83], "deg2rad": [0, 89], "digamma": [0, 91], "experiment": [0, 7, 315, 398], "dropout": 0, "gelu_bw": 0, "elu": [0, 98], "eqz": 0, "erf": [0, 108], "erfc": [0, 110], "erfinv": [0, 112], "exp": [0, 122, 209, 212, 386, 409, 413, 415], "exp2": [0, 115, 210, 211], "expm1": [0, 122], "fill": [0, 124, 125, 126, 127, 138, 139, 170, 257, 258, 280, 300, 334, 395, 396, 402, 405, 409], "floor": [0, 52, 92, 93, 129, 130, 281, 282], "frac": [0, 47, 53, 92, 134, 150, 182, 183, 184, 304, 305, 306, 335, 336], "geglu": 0, "gelu": [0, 7, 12, 54, 119, 144, 146, 397, 409], "glu": 0, "gez": 0, "gtz": 0, "hardshrink": [0, 155, 191], "hardsigmoid": [0, 157], "hardswish": [0, 159], "hardtanh": [0, 161], "heavisid": 0, "i0": [0, 166], "ident": [0, 33, 34, 150, 182, 317], "isfinit": 0, "isinf": 0, "isnan": 0, "isneginf": 0, "isposinf": 0, "leaky_relu": [0, 191], "lez": 0, "lgamma": [0, 196], "log10": [0, 201], "log1p": [0, 203], "log2": [0, 205, 210], "log_sigmoid": 0, "logical_not": [0, 216], "logical_not_": 0, "logit": [0, 222, 223, 409, 410, 412, 413], "ltz": 0, "mish": 0, "multigammaln": 0, "neg": [0, 177, 191, 251, 270], "nez": 0, "normalize_glob": 0, "normalize_hw": 0, "polygamma": [0, 267], "prelu": 0, "rad2deg": [0, 279], "rdiv": [0, 282], "reciproc": [0, 150, 287, 311, 312, 403], "reglu": 0, "relu": [0, 7, 12, 190, 191, 291, 295, 296, 297, 337, 386, 410, 412], "relu_max": 0, "relu_min": 0, "relu6": [0, 294], "remaind": [0, 131, 299], "round": [0, 11, 45, 92, 93, 281, 282, 308, 403], "rsqrt": 0, "selu": [0, 320], "sigmoid": [0, 7, 156, 157, 207, 208, 323, 324], "sigmoid_accur": 0, "sign": [0, 326, 327], "signbit": 0, "silu": [0, 7, 329, 358, 415], "sin": [0, 331], "sinh": [0, 333], "softplu": [0, 338], "softshrink": [0, 340], "softsign": [0, 342], "sqrt": [0, 53, 150, 163, 182, 183, 184, 304, 305, 306, 311, 409], "squar": [0, 7, 243, 304, 305, 306, 311, 312, 346, 347, 349, 350, 351, 376, 377, 403, 409], "swiglu": 0, "swish": [0, 158, 159, 328, 358], "tan": [0, 362], "tanh": [0, 7, 54, 119, 146, 160, 161, 364, 365, 366], "tanhshrink": [0, 366], "threshold": [0, 337, 338, 368], "tril": 0, "triu": [0, 409], "trunc": [0, 92, 93, 281, 282], "unary_chain": 0, "clamp_bw": 0, "clip_bw": 0, "hardtanh_bw": 0, "threshold_bw": 0, "softplus_bw": 0, "rdiv_bw": 0, "pow_bw": 0, "exp_bw": 0, "tanh_bw": 0, "sqrt_bw": 0, "multigammaln_bw": 0, "lgamma_bw": 0, "fill_bw": 0, "hardsigmoid_bw": 0, "cos_bw": 0, "acosh_bw": 0, "acos_bw": 0, "atan_bw": 0, "rad2deg_bw": 0, "frac_bw": 0, "trunc_bw": 0, "log_sigmoid_bw": 0, "fill_zero_bw": 0, "i0_bw": 0, "tan_bw": 0, "sigmoid_bw": 0, "rsqrt_bw": 0, "neg_bw": 0, "relu_bw": 0, "logit_bw": 0, "hardshrink_bw": 0, "softshrink_bw": 0, "leaky_relu_bw": 0, "elu_bw": 0, "celu_bw": 0, "rpow_bw": 0, "floor_bw": 0, "round_bw": 0, "log_bw": 0, "relu6_bw": 0, "abs_bw": 0, "silu_bw": 0, "selu_bw": 0, "square_bw": 0, "prod_bw": 0, "hardswish_bw": 0, "tanhshrink_bw": 0, "atanh_bw": 0, "asin_bw": 0, "asinh_bw": 0, "sin_bw": 0, "sinh_bw": 0, "log10_bw": 0, "log1p_bw": 0, "erfc_bw": 0, "ceil_bw": 0, "softsign_bw": 0, "cosh_bw": 0, "logiteps_bw": 0, "log2_bw": 0, "sign_bw": 0, "div_no_nan_bw": 0, "exp2_bw": 0, "expm1_bw": 0, "reciprocal_bw": 0, "digamma_bw": 0, "erfinv_bw": 0, "erf_bw": 0, "deg2rad_bw": 0, "polygamma_bw": 0, "repeat_bw": 0, "real": [0, 3, 32, 172, 264, 284, 398], "angl": [0, 37, 264], "is_imag": 0, "is_real": 0, "polar_bw": 0, "imag_bw": 0, "real_bw": 0, "angle_bw": 0, "conj_bw": 0, "conj": [0, 76], "polar": [0, 265], "alt_complex_rotate90": 0, "add": [0, 25, 26, 209, 210, 211, 212, 261, 316, 317, 376, 377, 398, 400, 401, 404, 408, 409, 411, 415], "addalpha": [0, 27], "subalpha": [0, 355], "multipli": [0, 7, 26, 28, 30, 32, 197, 230, 244, 315, 316, 317, 344, 354, 389, 403, 404, 415], "subtract": [0, 313, 314, 353, 354, 381, 415], "div": 0, "div_no_nan": [0, 95], "floor_div": 0, "fmod": [0, 132], "gcd": 0, "lcm": 0, "logical_and_": 0, "logical_or_": 0, "logical_xor_": 0, "rpow": [0, 310], "rsub": 0, "ldexp": [0, 187], "logical_and": 0, "logical_or": 0, "logical_xor": 0, "bitwise_and": [0, 57, 60], "bitwise_or": 0, "bitwise_xor": 0, "logaddexp": [0, 212], "logaddexp2": [0, 211], "hypot": [0, 164], "xlogi": [0, 394], "squared_differ": [0, 351], "gt": 0, "gt_": 0, "lt_": 0, "ge_": 0, "le_": 0, "eq_": 0, "ne_": 0, "ge": 0, "lt": 0, "le": 0, "eq": 0, "ne": 0, "isclos": 0, "nextaft": 0, "maximum": [0, 7, 39, 67, 68, 69, 70, 161, 232, 233], "minimum": [0, 5, 67, 68, 69, 70, 161, 237, 403], "outer": 0, "pow": 0, "polyv": 0, "scatter": [0, 288], "atan2": [0, 48], "add_bw": 0, "assign_bw": 0, "atan2_bw": 0, "bias_gelu_bw": 0, "div_bw": 0, "embedding_bw": 0, "fmod_bw": 0, "remainder_bw": 0, "addalpha_bw": 0, "subalpha_bw": 0, "xlogy_bw": 0, "hypot_bw": 0, "ldexp_bw": 0, "logaddexp_bw": 0, "logaddexp2_bw": 0, "mul_bw": 0, "sub_bw": 0, "squared_difference_bw": 0, "concat_bw": 0, "rsub_bw": 0, "min_bw": 0, "max_bw": 0, "ternari": 0, "addcdiv": [0, 29], "addcmul": [0, 31], "mac": 0, "lerp": [0, 193], "addcmul_bw": 0, "addcdiv_bw": 0, "where_bw": 0, "lerp_bw": 0, "loss": [0, 181, 243], "l1_loss": 0, "mse_loss": 0, "reduct": [0, 4, 7, 34, 39, 181, 243, 276, 318], "cumprod": 0, "ema": 0, "max": [0, 67, 68, 69, 70, 160, 161, 233, 296, 409, 412], "mean": [0, 7, 13, 32, 53, 150, 181, 182, 183, 184, 243, 304, 305, 306, 403, 409, 411], "min": [0, 67, 68, 69, 70, 160, 161, 296, 297], "std": [0, 5, 231, 235, 236, 321, 357, 390, 412], "sum": [0, 34, 86, 242, 318, 409], "var": 0, "argmax": [0, 409, 410, 412], "prod": [0, 84, 277], "topk": [0, 242], "cumsum": 0, "manual_se": [0, 397, 406, 411, 412, 415], "moe": 0, "movement": [0, 414], "concat": [0, 74, 401, 409], "nonzero": 0, "pad": [0, 7, 33, 52, 71, 77, 78, 79, 100, 126, 137, 150, 182, 183, 184, 233, 242, 262, 263, 272, 273, 274, 275, 303, 304, 305, 306, 317, 334, 370, 372, 375, 388, 403, 406, 409, 412, 413], "permut": [0, 52, 150, 233, 381, 406, 409, 411, 412], "reshap": [0, 7, 52, 125, 126, 150, 233, 381, 406, 409, 410, 411, 412, 413], "repeat": [0, 4, 301, 302, 408], "repeat_interleav": 0, "tiliz": [0, 24, 26, 40, 47, 53, 56, 57, 59, 60, 61, 92, 94, 104, 105, 130, 131, 141, 142, 143, 150, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 350, 354, 356, 370, 387, 388, 393, 407, 411], "tilize_with_val_pad": 0, "fill_rm": [0, 125], "fill_ones_rm": 0, "until": [0, 10, 24, 26, 47, 56, 57, 59, 60, 61, 92, 94, 104, 105, 130, 131, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 350, 354, 356, 374, 388, 393], "untilize_with_unpad": 0, "indexed_fil": 0, "gather": [0, 33, 183, 184, 305, 306], "sort": [0, 375, 409], "normal": [0, 53, 150, 167, 182, 183, 184, 255, 256, 304, 305, 306, 386, 409, 410, 412], "group_norm": 0, "layer_norm": [0, 183, 184, 409], "layer_norm_pre_all_gath": [0, 183], "layer_norm_post_all_gath": [0, 184], "rms_norm": [0, 305, 306], "rms_norm_pre_all_gath": [0, 305], "rms_norm_post_all_gath": [0, 306], "batch_norm": 0, "softmax": [0, 15, 16, 17, 242, 315, 316, 317, 336, 376, 377, 409, 411], "scale_mask_softmax": 0, "softmax_in_plac": [0, 15], "scale_mask_softmax_in_plac": [0, 17, 315], "scale_causal_mask_hw_dims_softmax_in_plac": 0, "softmaxprogramconfig": [0, 315, 317, 336, 376, 377], "softmaxdefaultprogramconfig": [0, 315, 317, 336, 376, 377], "softmaxshardedmulticoreprogramconfig": [0, 315, 317], "block_w": [0, 17, 315, 317], "transform": [0, 4, 7, 197, 264, 315, 397, 404, 410, 411, 412, 413], "split_query_key_value_and_split_head": [0, 411], "concatenate_head": [0, 411], "attention_softmax": 0, "attention_softmax_": [0, 411], "rotary_embed": 0, "scaled_dot_product_attent": 0, "scaled_dot_product_attention_decod": 0, "ccl": [0, 4], "all_gath": [0, 183, 184, 305, 306], "reduce_scatt": 0, "all_reduc": 0, "embed": [0, 101, 120, 409, 413], "convolut": [0, 4, 7, 52, 73, 77, 78, 79, 117, 233, 272, 273, 274, 275, 404, 409], "conv1d": [0, 7], "conv2d": [0, 7, 8, 79, 272, 273, 274, 275, 406, 409, 412], "conv3d": 0, "conv_transpose2d": [0, 7], "prepare_conv_weight": [0, 409], "prepare_conv_bia": 0, "prepare_conv_transpose2d_weight": 0, "prepare_conv_transpose2d_bia": 0, "conv2dconfig": [0, 77, 78, 79, 272, 273, 274, 275, 406, 412], "act_block_h_overrid": [0, 7, 412], "act_block_w_div": [0, 7, 412], "activ": [0, 7, 10, 11, 12, 24, 65, 97, 104, 141, 142, 145, 151, 156, 158, 160, 161, 185, 186, 188, 190, 197, 209, 210, 213, 217, 219, 224, 230, 234, 238, 239, 247, 248, 271, 292, 293, 313, 319, 322, 323, 328, 337, 341, 350, 356, 359, 393, 397, 400, 404, 409, 410, 411, 412, 414], "config_tensors_in_dram": [0, 7], "core_grid": [0, 7, 32, 84, 99, 150, 197, 230, 344, 345, 397, 407, 411, 412], "deallocate_activ": [0, 7, 52, 233, 412], "enable_act_double_buff": [0, 7, 412], "enable_activation_reus": [0, 7], "enable_kernel_stride_fold": [0, 7, 412], "enable_weights_double_buff": [0, 7, 412], "force_split_read": [0, 7], "full_inner_dim": [0, 7], "output_layout": [0, 7, 52, 150, 233, 412], "override_output_sharding_config": [0, 7], "override_sharding_config": [0, 7, 412], "reallocate_halo_output": [0, 7, 52, 233, 412], "reshard_if_not_optim": [0, 7, 412], "shard_layout": [0, 7, 412], "transpose_shard": [0, 7, 412], "weights_dtyp": [0, 7, 272, 273, 406, 412], "conv2dsliceconfig": [0, 78, 79], "slicetypeenum": [0, 8], "dramsliceheight": [0, 8], "dramslicewidth": [0, 8], "l1full": [0, 8], "name": [0, 5, 8, 40, 96, 198, 240, 241, 379, 397, 401, 402, 409, 412, 414, 415], "valu": [0, 7, 8, 10, 12, 13, 18, 19, 20, 22, 26, 27, 28, 29, 30, 31, 35, 38, 39, 41, 43, 46, 50, 52, 53, 55, 57, 58, 60, 62, 63, 65, 66, 67, 68, 69, 70, 71, 73, 75, 80, 82, 85, 86, 88, 90, 95, 97, 98, 102, 103, 106, 107, 109, 111, 113, 114, 121, 123, 125, 126, 128, 133, 135, 137, 138, 139, 140, 144, 145, 147, 148, 149, 150, 153, 154, 155, 156, 158, 160, 161, 162, 165, 167, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 183, 190, 191, 194, 195, 199, 200, 202, 204, 207, 215, 216, 221, 223, 226, 229, 231, 233, 234, 235, 236, 238, 239, 245, 246, 250, 252, 253, 255, 256, 257, 258, 259, 261, 263, 266, 269, 270, 278, 280, 281, 282, 286, 291, 292, 293, 296, 297, 303, 305, 307, 309, 310, 311, 318, 319, 321, 322, 323, 325, 327, 328, 330, 332, 334, 337, 338, 339, 340, 341, 343, 344, 346, 348, 352, 354, 355, 357, 358, 359, 361, 363, 365, 367, 368, 370, 375, 379, 381, 382, 383, 384, 386, 390, 391, 395, 396, 398, 403, 404, 405, 408, 409, 410, 411, 412, 413, 414, 415], "num_slic": [0, 8], "slice_typ": [0, 8], "pool": [0, 7, 52, 148, 233, 404], "global_avg_pool2d": 0, "max_pool2d": [0, 412], "avg_pool2d": 0, "vision": [0, 404], "upsampl": 0, "kv": 0, "kv_cach": 0, "fill_cache_for_user_": 0, "update_cache_for_token_": 0, "convers": [0, 55, 89, 279, 404, 407, 410, 412, 413], "model_preprocess": [0, 397, 411, 413], "preprocess_model": 0, "preprocess_model_paramet": [0, 397, 413], "report": [0, 3, 78, 230, 398, 404, 410, 415], "set_printopt": [0, 415], "register_pre_operation_hook": [0, 415], "register_post_operation_hook": [0, 415], "tutori": [0, 3, 399, 405, 406, 407, 408, 409, 410, 411, 412, 414], "import": [0, 5, 10, 397, 398, 400, 402, 403, 404, 411, 415], "open": [0, 4, 136, 228, 259, 371, 404, 409, 411, 413, 414, 415], "tenstorr": [0, 2, 3, 5, 7, 78, 397, 398, 399, 400, 404, 407, 409, 410, 412, 413, 414, 415], "addit": [0, 11, 24, 25, 26, 140, 230, 317, 336, 343, 397, 400, 404, 408, 412, 415], "close": [0, 72, 173, 228, 400, 404, 409, 411], "output": [0, 5, 7, 8, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 260, 261, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 281, 282, 283, 284, 286, 287, 288, 289, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 372, 373, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 396, 397, 398, 400, 402, 403, 404, 409, 413, 414, 415], "host": [0, 77, 123, 136, 261, 334, 360, 400, 402, 403, 404, 406, 409, 413], "tile": [0, 7, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 35, 37, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 74, 78, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 121, 122, 123, 124, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 137, 140, 141, 142, 143, 144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 173, 174, 175, 176, 177, 178, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 230, 231, 232, 234, 235, 236, 237, 238, 239, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 260, 262, 263, 265, 266, 267, 268, 269, 270, 271, 276, 277, 278, 279, 281, 282, 286, 287, 288, 291, 292, 293, 294, 295, 296, 297, 298, 299, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 374, 375, 379, 380, 382, 383, 384, 385, 386, 387, 388, 390, 391, 392, 393, 394, 395, 396, 402, 403, 404, 405, 407, 409, 410, 412, 414], "base": [0, 5, 7, 8, 10, 11, 12, 13, 15, 16, 17, 53, 73, 84, 99, 115, 139, 140, 150, 200, 201, 204, 205, 230, 258, 262, 280, 344, 396, 403, 404, 409, 411, 413], "arithmet": [0, 404], "simul": [0, 404, 406, 407, 410, 411, 412, 414], "broadcast": [0, 10, 12, 24, 25, 26, 33, 47, 56, 57, 59, 60, 61, 92, 93, 94, 104, 105, 130, 131, 140, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 230, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 288, 298, 302, 313, 315, 316, 317, 350, 353, 354, 356, 379, 393, 404, 406, 409, 410, 412, 415], "row": [0, 7, 10, 24, 26, 33, 47, 56, 57, 59, 60, 61, 64, 92, 94, 104, 105, 117, 124, 126, 127, 129, 130, 131, 134, 141, 142, 143, 150, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 230, 234, 238, 247, 248, 249, 252, 254, 260, 268, 269, 271, 288, 298, 308, 313, 326, 344, 345, 350, 354, 356, 385, 393, 402, 403, 404, 407, 409, 414], "vector": [0, 5, 230, 268, 322, 404, 409, 410], "expans": [0, 404], "initi": [0, 103, 229, 240, 241, 280, 397, 404, 405, 406, 408, 409, 410, 411, 414], "b": [0, 5, 24, 26, 47, 56, 57, 59, 60, 61, 92, 94, 99, 104, 105, 120, 130, 131, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 230, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 344, 350, 354, 356, 379, 380, 393, 400, 404, 406, 412], "random": [0, 20, 22, 41, 43, 46, 50, 52, 62, 63, 80, 82, 88, 106, 111, 114, 121, 128, 133, 144, 147, 148, 149, 150, 153, 156, 158, 165, 174, 175, 176, 177, 178, 194, 195, 207, 215, 216, 226, 229, 233, 239, 250, 253, 255, 256, 278, 280, 291, 292, 293, 325, 327, 328, 330, 332, 341, 348, 358, 359, 361, 382, 383, 384, 386, 404, 406, 410, 412, 413], "inspect": [0, 404, 414], "result": [0, 13, 17, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 38, 39, 40, 45, 47, 48, 52, 53, 54, 56, 59, 61, 74, 85, 86, 92, 93, 94, 99, 101, 126, 130, 131, 132, 139, 141, 150, 163, 164, 167, 173, 181, 182, 183, 184, 185, 186, 187, 192, 193, 197, 209, 210, 211, 212, 213, 217, 219, 227, 230, 231, 232, 233, 234, 235, 236, 237, 238, 242, 243, 244, 247, 252, 258, 260, 268, 269, 272, 273, 274, 275, 276, 280, 281, 286, 298, 299, 304, 305, 306, 309, 313, 314, 315, 316, 317, 318, 335, 344, 350, 351, 352, 353, 354, 355, 356, 357, 363, 365, 390, 391, 392, 393, 394, 396, 402, 403, 404, 405, 406, 408, 409, 410, 412], "more": [0, 1, 3, 4, 5, 7, 10, 12, 13, 19, 29, 53, 54, 78, 93, 132, 146, 150, 164, 182, 183, 184, 201, 203, 205, 206, 208, 211, 212, 222, 223, 270, 277, 282, 286, 287, 299, 304, 305, 306, 336, 338, 345, 347, 394, 399, 400, 402, 403, 404, 405, 406, 408, 409, 411, 412, 414], "perform": [0, 3, 7, 10, 11, 12, 13, 17, 19, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 42, 44, 45, 47, 48, 49, 50, 51, 52, 54, 56, 57, 59, 60, 61, 64, 66, 68, 69, 70, 74, 76, 78, 81, 82, 83, 85, 86, 89, 90, 91, 92, 93, 94, 95, 97, 98, 100, 104, 105, 108, 110, 112, 115, 116, 122, 124, 127, 129, 130, 131, 132, 134, 141, 142, 143, 144, 146, 148, 149, 151, 152, 154, 155, 157, 159, 160, 161, 163, 164, 166, 167, 168, 169, 170, 173, 183, 185, 186, 187, 188, 189, 191, 192, 193, 195, 196, 197, 201, 203, 205, 206, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 227, 228, 230, 232, 233, 234, 237, 238, 242, 244, 245, 246, 247, 248, 249, 251, 252, 255, 256, 260, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 279, 281, 282, 283, 284, 287, 291, 294, 295, 298, 299, 301, 305, 308, 310, 312, 313, 314, 315, 316, 317, 318, 319, 320, 322, 324, 326, 329, 331, 332, 333, 334, 338, 339, 340, 342, 347, 349, 350, 351, 353, 354, 355, 356, 358, 362, 363, 364, 365, 366, 367, 368, 369, 375, 382, 383, 385, 391, 392, 393, 394, 397, 398, 400, 401, 404, 405, 406, 408, 409, 410, 411, 412, 413], "mlp": [0, 404, 409], "infer": [0, 3, 53, 78, 344, 398, 402, 404, 413], "load": [0, 3, 4, 7, 10, 12, 198, 402, 404, 409, 413, 414], "mnist": [0, 404], "test": [0, 5, 323, 327, 397, 398, 401, 402, 404, 408, 409, 411, 414, 415], "pretrain": [0, 404, 412], "weight": [0, 7, 53, 77, 78, 79, 100, 101, 117, 192, 197, 242, 271, 272, 273, 274, 275, 306, 397, 404, 406, 411, 413], "accuraci": [0, 398, 404, 412], "track": [0, 404, 412, 413], "loop": [0, 404], "flatten": [0, 404, 406, 409, 412], "head": [0, 376, 377, 378, 379, 381, 404, 409, 413], "attent": [0, 315, 316, 317, 335, 376, 377, 379, 380, 381, 404, 409, 413], "write": [0, 1, 5, 32, 84, 197, 230, 344, 397, 403, 404, 409], "seed": [0, 118, 229, 280, 404], "creat": [0, 5, 7, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 231, 232, 233, 234, 235, 236, 237, 238, 239, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 263, 264, 265, 266, 267, 268, 269, 270, 271, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 317, 318, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 382, 383, 384, 385, 386, 387, 388, 390, 391, 392, 393, 394, 395, 396, 400, 401, 402, 403, 404, 405, 407, 408, 409, 410, 411, 412, 413, 414], "forward": [0, 397, 404, 409, 413], "method": [0, 5, 146, 363, 365, 404], "input": [0, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 137, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 230, 232, 233, 234, 237, 238, 239, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 260, 261, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 281, 282, 283, 284, 285, 286, 287, 288, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 318, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 392, 393, 394, 396, 398, 400, 402, 403, 404, 405, 407, 409, 410, 411, 412, 413, 414, 415], "paramet": [0, 4, 7, 10, 11, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 257, 258, 260, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 400, 404, 409, 411, 412, 413], "simpl": [0, 3, 400, 404, 405, 406, 408, 410], "cnn": [0, 78, 404, 406], "cifar": [0, 404], "dataset": [0, 403, 404, 410, 414], "defin": [0, 5, 10, 32, 40, 52, 68, 70, 125, 126, 170, 183, 184, 197, 233, 270, 276, 277, 282, 301, 305, 306, 310, 335, 344, 368, 401, 403, 404, 409, 413], "stage": [0, 404, 414], "sampl": [0, 19, 21, 23, 37, 42, 44, 49, 51, 64, 66, 68, 70, 76, 81, 83, 89, 91, 95, 98, 108, 110, 112, 115, 116, 122, 124, 127, 129, 134, 146, 155, 157, 159, 161, 166, 169, 191, 196, 201, 203, 205, 206, 208, 222, 223, 246, 251, 265, 267, 270, 277, 279, 282, 284, 287, 294, 295, 301, 308, 310, 312, 320, 324, 326, 329, 331, 333, 338, 340, 342, 347, 349, 362, 364, 366, 368, 385, 404, 410, 414], "shot": [0, 404], "classif": [0, 404, 410, 412], "doe": [0, 13, 71, 137, 150, 229, 276, 374, 398, 402, 404], "manag": [0, 3, 228, 289, 290, 398, 400, 404, 412, 413], "util": [0, 4, 11, 150, 403, 404, 407, 410, 412, 414], "gener": [0, 7, 12, 100, 118, 126, 150, 229, 230, 264, 280, 315, 398, 402, 404, 406, 407, 408, 410, 411, 412], "implement": [0, 7, 10, 12, 33, 77, 150, 230, 261, 288, 379, 380, 381, 397, 401, 402, 404, 411, 413, 414], "architectur": [0, 78, 404, 413, 415], "process": [0, 5, 10, 11, 12, 13, 17, 73, 85, 86, 150, 183, 184, 272, 273, 305, 306, 315, 316, 317, 402, 404, 410, 411, 412], "pipelin": [0, 5, 398, 402, 404, 410], "complet": [0, 24, 26, 47, 56, 57, 59, 60, 61, 92, 94, 104, 105, 130, 131, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 350, 354, 356, 360, 393, 404, 405, 406, 407, 408, 410, 411, 412, 413, 414], "compon": [0, 400, 404], "kei": [0, 4, 381, 397, 403, 404, 411, 412], "preprocess": [0, 5, 7, 40, 77, 78, 79, 240, 241, 272, 273, 274, 275, 403, 404, 412, 413], "download": [0, 400, 404, 410, 412], "token": [0, 100, 180, 379, 380, 404, 413], "text": [0, 53, 92, 150, 173, 182, 183, 184, 252, 260, 304, 305, 306, 335, 336, 398, 404], "profil": [0, 3, 167, 321, 404, 415], "analysi": [0, 402, 404], "upload": [0, 402, 404], "tab": [0, 402, 404], "recap": [0, 404], "tracer": [0, 404, 415], "bert": [0, 397, 402, 404], "pytorch": [0, 4, 77, 79, 230, 242, 272, 273, 274, 275, 375, 379, 381, 404, 406, 408, 409, 410, 412, 415], "layer": [0, 182, 183, 184, 304, 305, 306, 402, 404, 406, 409, 410, 412], "written": [0, 5, 32, 179, 180, 197, 230, 344, 404, 411], "onboard": 0, "new": [0, 33, 103, 139, 258, 300, 303, 396, 398, 400, 402], "rewrit": 0, "switch": [0, 261, 337], "optim": [0, 7, 10, 11, 12, 17, 99, 315, 335, 363, 365, 403, 407, 409, 411, 412, 413, 414], "ad": [0, 32, 77, 78, 79, 197, 272, 273, 274, 275, 401, 409], "faq": 0, "need": [0, 1, 2, 8, 10, 12, 32, 150, 197, 230, 344, 369, 375, 398, 402, 403, 407, 409, 412, 414, 415], "bind": [0, 406, 408, 410, 411, 412], "golden": [0, 397, 414, 415], "usag": [0, 7, 10, 12, 13, 183, 184, 305, 306, 398, 400, 414], "doc": [0, 414], "perf": [0, 7, 398, 414], "header": 0, "profile_thi": 0, "descript": [0, 125, 126, 401, 409], "uplift": 0, "tool": [0, 4, 400, 402, 413, 414], "bug": 0, "featur": [0, 4, 7, 10, 315, 400, 401, 409, 414, 415], "propos": [0, 401], "request": [0, 372, 401, 409, 413], "troubleshoot": [0, 398], "tip": 0, "commun": [0, 10, 12, 400], "develop": [0, 4, 399, 400, 401, 402, 404, 405, 409, 414], "index": [0, 140, 170, 179, 180, 318, 343, 380, 402, 409, 412], "modul": [0, 5, 79, 240, 241, 397, 410, 412, 413, 414], "search": [0, 414], "page": [0, 400, 407, 414], "If": [1, 2, 5, 7, 8, 10, 11, 12, 32, 33, 34, 39, 52, 71, 77, 78, 79, 84, 85, 86, 99, 137, 140, 150, 182, 183, 197, 230, 231, 235, 236, 240, 241, 259, 261, 276, 277, 304, 305, 306, 316, 334, 335, 343, 344, 352, 357, 360, 374, 375, 379, 380, 381, 390, 399, 400, 401, 402, 403, 409, 412, 413, 414, 415], "would": [1, 150, 183, 184, 305, 306, 336, 401, 402, 403, 409], "like": [1, 7, 32, 127, 337, 397, 402, 403, 405, 406, 409, 412, 415], "thi": [1, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 32, 39, 52, 53, 71, 73, 77, 78, 79, 101, 119, 120, 123, 148, 150, 167, 182, 183, 184, 187, 229, 233, 242, 262, 272, 273, 274, 275, 276, 288, 296, 297, 303, 305, 306, 315, 316, 317, 336, 337, 344, 345, 375, 379, 397, 398, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415], "project": [1, 2, 4, 399, 409, 414], "pleas": [1, 2, 230, 398, 399, 401, 415], "review": [1, 399, 401], "standard": [1, 2, 5, 15, 297, 336, 398, 399, 400, 409, 412, 413], "gain": 1, "access": [1, 2, 4, 10, 11, 12, 13, 400, 409], "read": [1, 84, 399, 403], "section": [1, 2, 5, 398, 403, 404, 409, 414], "detail": [1, 3, 19, 29, 53, 54, 93, 132, 146, 150, 164, 182, 183, 184, 201, 203, 205, 206, 208, 211, 212, 222, 223, 270, 277, 282, 287, 299, 304, 305, 306, 338, 347, 394, 399, 406, 412, 414, 415], "contact": 1, "u": [1, 401, 409, 410], "have": [2, 5, 12, 13, 24, 26, 33, 39, 47, 53, 55, 56, 57, 59, 60, 61, 85, 86, 87, 92, 94, 99, 101, 104, 105, 130, 131, 140, 141, 142, 143, 150, 151, 152, 163, 173, 182, 183, 184, 185, 186, 188, 189, 192, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 230, 231, 234, 235, 236, 238, 247, 248, 249, 252, 260, 268, 269, 271, 272, 273, 276, 281, 298, 304, 305, 306, 313, 315, 316, 317, 335, 336, 344, 350, 352, 354, 356, 357, 369, 370, 375, 387, 388, 390, 393, 398, 400, 402, 403, 409, 412, 414], "formal": 2, "permiss": 2, "cloud": 2, "issu": [2, 7, 230, 337, 398, 401, 402, 406], "file": [2, 3, 5, 40, 96, 198, 398, 400, 402, 405, 406, 408, 410, 412, 414, 415], "github": [2, 398, 399, 400, 414], "can": [2, 4, 5, 7, 8, 10, 11, 12, 13, 17, 24, 26, 38, 47, 54, 56, 57, 59, 60, 61, 68, 69, 70, 77, 78, 79, 92, 93, 94, 103, 104, 105, 130, 131, 139, 141, 142, 143, 146, 150, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 230, 234, 238, 247, 248, 249, 252, 258, 260, 261, 268, 269, 271, 272, 273, 274, 275, 281, 282, 289, 290, 298, 313, 321, 337, 350, 354, 356, 360, 393, 396, 397, 400, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415], "check": [2, 3, 5, 104, 106, 142, 147, 151, 153, 171, 172, 173, 174, 175, 176, 177, 178, 188, 194, 224, 226, 230, 248, 253, 398, 400, 410, 411, 412], "out": [2, 3, 5, 7, 32, 85, 86, 99, 118, 140, 242, 297, 334, 343, 375, 405, 406, 411, 412], "relev": [2, 398], "ever": 2, "help": [2, 150, 401, 404, 409, 413, 414], "we": [2, 7, 33, 34, 137, 150, 230, 288, 315, 374, 398, 401, 403, 404, 405, 406, 407, 408, 409, 410, 412, 413, 414, 415], "offici": [2, 414], "discord": 2, "channel": [2, 7, 52, 53, 77, 78, 79, 125, 126, 148, 150, 233, 272, 273, 274, 275, 389, 402, 406, 409, 412], "repres": [2, 4, 402, 403, 409, 412], "both": [2, 7, 10, 11, 12, 13, 25, 55, 77, 78, 79, 85, 86, 150, 230, 233, 269, 272, 273, 274, 275, 336, 397, 398, 402, 403, 409, 414], "metal": [2, 4, 399, 400, 404, 406, 408, 410, 411, 412, 414], "join": 2, "discuss": [2, 398], "board": 2, "member": [2, 8], "bounc": 2, "idea": [2, 398], "off": [2, 296, 307, 397, 403], "each": [2, 5, 8, 10, 11, 12, 13, 26, 33, 52, 53, 63, 73, 126, 128, 140, 141, 148, 150, 163, 185, 186, 209, 210, 233, 261, 268, 300, 302, 309, 313, 325, 327, 334, 344, 348, 350, 354, 391, 393, 400, 402, 403, 404, 409, 410, 412, 414], "other": [2, 5, 7, 10, 24, 26, 47, 56, 57, 59, 60, 61, 92, 94, 104, 105, 130, 131, 140, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 230, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 350, 354, 356, 393, 397, 398, 399, 403, 412, 414, 415], "refer": [2, 3, 4, 19, 29, 32, 54, 78, 87, 93, 103, 123, 132, 146, 150, 164, 181, 183, 184, 201, 203, 205, 206, 208, 211, 212, 222, 223, 230, 243, 246, 270, 277, 282, 287, 299, 305, 306, 338, 347, 394, 400, 401, 403, 407, 412, 414, 415], "code": [2, 5, 242, 289, 290, 375, 381, 399, 400, 401, 402, 403, 406, 408, 409, 410, 411, 412, 415], "conduct": 2, "when": [2, 5, 7, 8, 9, 10, 12, 14, 33, 39, 52, 53, 73, 77, 78, 79, 84, 93, 120, 150, 164, 184, 197, 213, 228, 230, 233, 234, 240, 263, 269, 276, 282, 288, 305, 318, 323, 344, 363, 365, 372, 375, 398, 401, 403, 406, 409, 411, 413, 415], "interact": [2, 3, 414], "tt": [3, 14, 38, 40, 87, 96, 102, 103, 135, 136, 137, 138, 139, 198, 257, 258, 280, 285, 369, 370, 371, 372, 373, 374, 387, 388, 389, 395, 396, 399, 404, 405, 407, 408, 410, 412], "smi": [3, 400], "The": [3, 4, 5, 7, 8, 9, 12, 14, 17, 24, 26, 32, 33, 34, 35, 38, 39, 40, 47, 52, 53, 56, 57, 59, 60, 61, 65, 72, 77, 78, 79, 87, 92, 94, 96, 97, 99, 100, 101, 102, 103, 104, 105, 119, 123, 126, 130, 131, 135, 137, 138, 139, 140, 141, 142, 143, 148, 150, 151, 152, 162, 163, 173, 183, 185, 186, 188, 189, 190, 197, 209, 210, 213, 214, 217, 218, 219, 220, 221, 224, 225, 228, 229, 230, 231, 233, 234, 235, 236, 238, 242, 246, 247, 248, 249, 252, 257, 258, 259, 260, 262, 268, 269, 271, 272, 273, 274, 275, 276, 280, 288, 289, 290, 296, 297, 298, 300, 305, 313, 315, 316, 317, 335, 336, 343, 344, 345, 350, 352, 354, 356, 357, 360, 371, 374, 375, 379, 380, 389, 390, 391, 393, 395, 396, 397, 398, 399, 400, 401, 402, 403, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415], "system": [3, 144, 149, 281, 291, 296, 297, 358, 400, 402, 414], "interfac": [3, 16, 410, 412], "command": [3, 137, 259, 360, 374, 402, 415], "line": 3, "provid": [3, 11, 12, 15, 16, 17, 24, 26, 32, 39, 47, 53, 56, 57, 59, 60, 61, 78, 79, 92, 94, 99, 104, 105, 130, 131, 140, 141, 142, 143, 150, 151, 152, 163, 173, 182, 183, 185, 186, 188, 189, 197, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 229, 230, 231, 234, 235, 236, 238, 240, 241, 247, 248, 249, 252, 260, 268, 269, 271, 276, 277, 298, 304, 305, 306, 313, 315, 316, 317, 335, 344, 350, 352, 354, 356, 357, 360, 370, 375, 379, 390, 393, 398, 400, 401, 402, 403, 404, 409, 410, 411, 412, 414, 415], "wai": [3, 4, 240, 241, 397, 400, 404], "collect": [3, 33, 34, 183, 184, 305, 306, 398, 402, 404, 414], "telemetri": 3, "displai": [3, 409], "firmwar": [3, 400, 407], "inform": [3, 78, 140, 286, 345, 400, 409, 412, 414, 415], "nn": [3, 38, 40, 87, 96, 102, 103, 135, 136, 137, 138, 139, 150, 198, 240, 241, 257, 258, 280, 285, 371, 372, 373, 374, 395, 396, 399, 404, 405, 407, 408, 410, 412], "analyz": [3, 402, 414], "insight": [3, 410, 412, 414], "through": [3, 409, 410, 412, 414], "plot": [3, 414], "view": [3, 150, 303, 344, 402, 410, 412, 414, 415], "flow": [3, 402, 409, 413, 414], "diagram": [3, 409], "instanc": [3, 414], "via": [3, 7, 400, 410, 412, 414], "ssh": [3, 414], "guid": [3, 399, 400, 404, 406, 414], "vllm": 3, "A": [3, 4, 5, 7, 24, 26, 38, 47, 56, 57, 59, 60, 61, 78, 92, 94, 104, 105, 130, 131, 135, 137, 138, 139, 140, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 229, 230, 231, 234, 235, 236, 238, 247, 248, 249, 252, 257, 258, 260, 268, 269, 271, 280, 298, 313, 337, 343, 344, 345, 350, 352, 354, 356, 357, 380, 390, 393, 395, 396, 398, 400, 401, 403, 407, 409, 414], "high": [3, 125, 126, 280, 400, 403, 405, 408, 414, 415], "throughput": 3, "effici": [3, 7, 10, 11, 12, 13, 316, 317, 336, 379, 400, 407, 408, 409, 410], "serv": 3, "engin": [3, 403], "larg": [3, 7, 8, 12, 317, 337, 397, 403, 409, 413], "languag": [3, 409], "llm": 3, "traci": [3, 402, 414], "time": [3, 85, 99, 173, 240, 300, 398, 402, 407, 409, 411, 414, 415], "design": [3, 4, 17, 315, 404, 405, 414], "deep": [3, 409], "learn": [3, 403, 405, 407, 408, 409], "instruct": [3, 398, 399, 400, 402, 404, 415], "neural": [4, 406, 409, 410, 412, 414], "network": [4, 409, 410, 412, 414], "built": [4, 400, 412], "feel": [4, 414], "familiar": 4, "experienc": 4, "includ": [4, 5, 7, 52, 53, 71, 345, 398, 403, 408, 409, 410, 412, 413, 414], "than": [4, 7, 8, 24, 26, 47, 56, 57, 59, 60, 61, 63, 73, 90, 92, 94, 104, 105, 125, 126, 128, 130, 131, 141, 142, 143, 147, 150, 151, 152, 153, 163, 173, 185, 186, 188, 189, 194, 195, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 226, 234, 238, 246, 247, 248, 249, 252, 260, 268, 269, 271, 276, 298, 313, 315, 336, 350, 354, 356, 393, 402, 409, 411, 415], "200": 4, "fuse": [4, 7, 10, 11, 12, 316, 317, 397, 411], "differ": [4, 7, 16, 24, 26, 47, 56, 57, 59, 60, 61, 79, 92, 94, 104, 105, 130, 131, 141, 142, 143, 150, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 229, 230, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 322, 345, 350, 351, 354, 356, 363, 365, 393, 400, 403, 407, 408, 409, 410, 412, 413, 415], "distribut": [4, 10, 11, 12, 13, 32, 84, 183, 184, 197, 230, 280, 305, 306, 344, 345, 403, 409, 414], "abil": [4, 414], "custom": [4, 5, 140, 229, 289, 290, 400, 406, 409, 410, 411, 412, 413, 414], "nativ": [4, 24, 26, 47, 56, 57, 59, 60, 61, 92, 94, 104, 105, 130, 131, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 350, 354, 356, 393, 403, 408, 409], "mesh": [4, 33, 34, 137, 288, 374, 406, 408, 410, 411, 412], "comput": [4, 5, 7, 10, 11, 12, 13, 17, 18, 20, 22, 25, 26, 27, 28, 30, 32, 41, 43, 46, 47, 48, 50, 52, 53, 54, 62, 63, 71, 74, 77, 78, 79, 80, 82, 85, 86, 90, 94, 99, 101, 107, 109, 111, 113, 114, 117, 121, 128, 130, 131, 133, 140, 141, 148, 150, 163, 164, 165, 173, 181, 182, 183, 184, 185, 186, 187, 192, 195, 197, 199, 200, 202, 204, 207, 209, 210, 211, 212, 213, 214, 215, 217, 218, 219, 220, 221, 227, 230, 231, 232, 234, 235, 236, 237, 238, 239, 243, 244, 245, 250, 252, 260, 266, 268, 272, 273, 274, 275, 276, 281, 286, 298, 304, 305, 306, 309, 311, 313, 314, 315, 316, 317, 330, 332, 335, 336, 344, 346, 348, 350, 351, 352, 353, 354, 355, 357, 361, 363, 376, 377, 380, 381, 384, 390, 393, 394, 402, 407, 408, 409, 410, 412, 413, 414], "significantli": [4, 7, 10, 17, 407, 411], "speed": [4, 407], "comparison": [4, 105, 142, 143, 151, 152, 173, 188, 189, 224, 225, 249, 401, 409], "mode": [4, 45, 53, 54, 93, 107, 109, 113, 145, 146, 181, 199, 200, 202, 204, 233, 243, 282, 311, 321, 322, 323, 344, 346, 363, 365, 406, 407, 408, 410, 411, 412, 413, 414], "long": [4, 401], "sequenc": [4, 5, 316, 317, 379, 380, 409, 413], "against": [4, 140, 398, 414, 415], "known": [4, 79], "document": [5, 150, 183, 184, 305, 306, 398, 400, 401, 402, 414], "meant": [5, 7], "contributor": 5, "Not": [5, 52, 176, 249, 253, 280, 397, 415], "mai": [5, 12, 13, 87, 93, 164, 187, 230, 282, 286, 303, 317, 336, 363, 365, 371, 397, 403, 406, 408, 409, 410, 411, 412, 413, 415], "grayskul": [5, 40, 397, 413, 415], "wormhol": [5, 40, 397, 400, 415], "take": [5, 7, 378, 398, 399, 403, 411, 412], "one": [5, 7, 10, 68, 69, 70, 140, 150, 276, 305, 399, 403, 404, 408], "produc": [5, 78, 229, 286, 363, 365, 398, 407, 410], "call": [5, 7, 10, 19, 21, 23, 37, 42, 44, 49, 51, 64, 66, 68, 70, 76, 81, 83, 89, 91, 95, 98, 108, 110, 112, 115, 116, 122, 124, 127, 129, 134, 137, 146, 155, 157, 159, 161, 166, 169, 191, 196, 201, 203, 205, 206, 208, 222, 223, 246, 251, 265, 267, 270, 277, 279, 282, 284, 287, 289, 290, 294, 295, 301, 308, 310, 312, 320, 324, 326, 329, 331, 333, 338, 340, 342, 347, 349, 362, 364, 366, 368, 374, 385, 401, 402, 403, 409, 411, 414, 415], "optiona": 5, "composit": [5, 33, 288], "struct": [5, 409], "specifi": [5, 7, 10, 11, 12, 13, 15, 32, 33, 34, 38, 52, 71, 85, 86, 96, 99, 102, 103, 135, 138, 139, 140, 197, 229, 230, 231, 235, 236, 240, 241, 257, 258, 261, 262, 263, 276, 280, 288, 300, 303, 307, 318, 335, 343, 344, 345, 352, 357, 360, 370, 371, 379, 380, 390, 395, 396, 397, 400, 402, 403, 407, 409, 411, 413], "how": [5, 7, 8, 10, 11, 12, 13, 150, 344, 345, 398, 402, 403, 405, 407, 408, 409, 410, 411, 412, 413, 414, 415], "simpli": [5, 372, 414], "ttnn": [5, 400, 401, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 414, 415], "register_oper": 5, "exist": [5, 259, 400, 402, 410, 412], "bind_registered_oper": 5, "auto": [5, 321, 409], "attach": [5, 240, 241], "attach_golden_funct": 5, "let": [5, 403, 405, 406, 407, 408, 410, 411, 412], "": [5, 7, 17, 71, 77, 78, 150, 182, 230, 231, 235, 236, 240, 241, 304, 306, 316, 318, 352, 357, 379, 380, 390, 391, 398, 400, 403, 407, 408, 409, 410, 412, 413, 414], "just": [5, 150, 272, 273, 274, 275, 413], "copi": [5, 71, 136, 167, 371, 414], "In": [5, 10, 17, 118, 150, 230, 303, 304, 315, 317, 372, 377, 397, 402, 403, 409, 410, 412, 414], "order": [5, 84, 165, 230, 267, 314, 343, 375, 402, 403, 409, 411, 414, 415], "follow": [5, 7, 32, 39, 78, 79, 126, 183, 184, 197, 230, 231, 235, 236, 242, 276, 303, 305, 306, 315, 316, 317, 335, 336, 344, 352, 357, 375, 390, 397, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 414, 415], "directori": [5, 400, 404, 409, 413, 414], "structur": [5, 7, 8, 397, 410, 414], "shown": [5, 403, 405, 406], "below": [5, 230, 344, 398, 402, 403, 405, 406], "cpp": [5, 406, 407, 408, 410, 411, 412, 414], "categori": [5, 409, 414], "operation_nam": 5, "_device_oper": 5, "hpp": 5, "program_factory_0": 5, "_program_factori": 5, "mani": [5, 10, 11, 12, 13, 397, 407], "factori": 5, "But": 5, "concret": [5, 150, 230], "found": [5, 397, 402, 410, 412, 414], "example_device_oper": 5, "spdx": [5, 415], "filecopyrighttext": [5, 415], "2023": 5, "inc": [5, 415], "licens": [5, 415], "identifi": [5, 414, 415], "apach": [5, 415], "0": [5, 7, 8, 14, 17, 24, 26, 27, 28, 29, 30, 31, 32, 33, 38, 52, 53, 56, 57, 59, 60, 61, 66, 67, 68, 69, 70, 72, 74, 78, 79, 85, 86, 90, 94, 95, 98, 99, 100, 101, 106, 118, 125, 126, 135, 136, 137, 138, 147, 150, 153, 154, 155, 160, 161, 170, 173, 191, 192, 194, 195, 215, 223, 226, 228, 229, 231, 233, 235, 236, 242, 247, 253, 257, 258, 259, 261, 263, 268, 270, 271, 276, 277, 280, 282, 293, 296, 297, 301, 302, 303, 307, 310, 315, 316, 317, 318, 334, 337, 338, 339, 340, 344, 345, 348, 352, 354, 356, 357, 360, 367, 368, 370, 371, 374, 381, 382, 383, 390, 391, 392, 395, 396, 397, 400, 403, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415], "pragma": 5, "onc": [5, 272, 273, 274, 275, 407, 409, 410, 414], "variant": [5, 30, 150, 230], "device_oper": 5, "decor": [5, 401], "namespac": 5, "exampledeviceoper": 5, "attribut": [5, 397, 402, 403], "store": [5, 7, 11, 32, 53, 140, 402, 403, 411], "variabl": [5, 400, 409, 413, 414, 415], "aren": [5, 9], "t": [5, 9, 99, 150, 167, 230, 240, 241, 402, 403, 409, 411, 413], "operation_attributes_t": 5, "bool": [5, 7, 25, 27, 39, 45, 52, 53, 74, 77, 78, 79, 84, 85, 86, 87, 92, 93, 107, 109, 113, 140, 145, 150, 173, 181, 197, 199, 200, 202, 204, 230, 231, 233, 235, 236, 240, 241, 243, 244, 261, 274, 275, 276, 303, 311, 314, 315, 316, 317, 322, 323, 335, 336, 343, 344, 345, 346, 352, 353, 355, 357, 369, 370, 375, 376, 377, 379, 380, 381, 387, 388, 390, 392, 412], "int": [5, 17, 32, 33, 34, 38, 39, 52, 73, 74, 77, 78, 79, 84, 85, 86, 100, 102, 120, 123, 125, 126, 137, 139, 140, 144, 149, 150, 170, 179, 180, 197, 228, 230, 233, 259, 261, 262, 266, 269, 272, 273, 274, 275, 276, 277, 280, 288, 291, 301, 307, 318, 321, 322, 334, 335, 343, 344, 345, 358, 360, 374, 376, 377, 379, 380, 381, 389, 403, 406, 412], "some_other_attribut": 5, "argument": [5, 7, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 137, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 219, 221, 222, 223, 224, 225, 226, 227, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 259, 260, 263, 264, 265, 266, 267, 268, 269, 270, 271, 276, 277, 278, 279, 280, 281, 282, 283, 284, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 403], "pass": [5, 7, 10, 77, 119, 120, 126, 289, 290, 381, 397, 401, 403, 406, 409, 410, 412, 413], "don": [5, 230, 409, 413], "thei": [5, 8, 150, 230, 398, 403, 407, 409, 411], "tensor_args_t": 5, "const": [5, 321], "input_tensor": [5, 17, 18, 19, 20, 21, 22, 23, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 45, 46, 49, 50, 51, 52, 53, 54, 55, 58, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 75, 76, 77, 79, 80, 81, 82, 83, 88, 89, 90, 91, 93, 95, 97, 98, 100, 101, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 133, 134, 143, 144, 145, 146, 147, 148, 149, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 166, 167, 168, 169, 171, 172, 174, 175, 176, 177, 178, 179, 189, 190, 191, 194, 195, 196, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 215, 216, 221, 222, 223, 225, 226, 231, 233, 235, 236, 239, 242, 245, 246, 249, 250, 251, 253, 254, 255, 256, 261, 263, 264, 265, 266, 267, 268, 269, 270, 277, 278, 279, 281, 282, 283, 284, 286, 287, 288, 291, 292, 293, 294, 295, 296, 297, 300, 301, 302, 303, 307, 308, 309, 310, 311, 312, 315, 316, 317, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 346, 347, 348, 349, 352, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 376, 377, 378, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 406, 409, 412, 413, 415], "howev": [5, 261], "show": [5, 230, 403, 407, 411, 413, 414], "els": [5, 33, 79, 92, 288, 410, 412, 413], "done": [5, 7, 398, 400, 402, 409], "io_tensor": 5, "optional_output_tensor": [5, 32, 197, 230, 344], "vector_of_tensor": 5, "tupl": [5, 77, 78, 79, 84, 233, 261, 272, 273, 274, 275, 343, 345, 381, 406, 412, 413], "tuple_of_tensor": 5, "vector_of_optional_tensor": 5, "some_crazy_tuple_of_tensor": 5, "return": [5, 9, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 354, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 392, 393, 394, 395, 396, 397, 406, 408, 409, 411, 412, 413], "spec": [5, 24, 26, 47, 56, 57, 59, 60, 61, 84, 92, 94, 104, 105, 130, 131, 137, 141, 142, 143, 151, 152, 163, 173, 182, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 350, 354, 356, 393], "singl": [5, 229, 231, 235, 236, 352, 357, 379, 380, 390, 397, 398, 401, 402, 403], "tensorspec": [5, 137], "spec_return_value_t": 5, "tensor_return_value_t": 5, "note": [5, 7, 76, 140, 150, 169, 230, 257, 258, 284, 303, 400, 402, 403, 409, 411, 414, 415], "should": [5, 7, 10, 12, 32, 52, 79, 99, 140, 144, 149, 150, 167, 173, 183, 184, 192, 197, 230, 233, 254, 261, 269, 281, 291, 305, 306, 318, 358, 375, 398, 401, 402, 405, 409, 412, 414], "same": [5, 7, 24, 26, 28, 34, 47, 53, 55, 56, 57, 59, 60, 61, 77, 79, 85, 86, 92, 94, 99, 101, 103, 104, 105, 123, 125, 130, 139, 140, 141, 142, 143, 151, 152, 163, 167, 173, 182, 183, 185, 186, 188, 189, 192, 197, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 227, 230, 231, 234, 235, 236, 238, 240, 241, 247, 248, 249, 252, 254, 258, 260, 268, 269, 272, 273, 274, 275, 276, 288, 305, 313, 315, 316, 317, 318, 335, 336, 350, 352, 354, 356, 357, 372, 375, 379, 390, 393, 396, 402, 403, 408, 410, 411, 414], "pattern": [5, 7, 10, 11, 12, 55, 73, 240, 315, 401, 410, 412], "e": [5, 7, 12, 39, 113, 121, 150, 182, 229, 230, 231, 235, 236, 242, 276, 315, 318, 335, 336, 344, 352, 357, 390, 400, 402, 403, 409, 410, 412, 414, 415], "singlecor": 5, "share": [5, 10, 11, 12, 13, 403, 409], "between": [5, 7, 24, 26, 38, 47, 53, 56, 57, 59, 60, 61, 77, 78, 79, 92, 94, 99, 104, 105, 130, 131, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 230, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 272, 273, 274, 275, 280, 293, 298, 313, 350, 354, 356, 373, 393, 409, 412, 413, 414], "override_runtime_argu": 5, "shared_variables_t": 5, "tt_metal": [5, 389, 400, 414], "kernelhandl": 5, "unary_reader_kernel_id": 5, "unary_writer_kernel_id": 5, "cached_program_t": 5, "cachedprogram": 5, "static": 5, "operation_attribut": 5, "tensor_arg": 5, "tensor_return_valu": 5, "void": 5, "cached_program": 5, "multicor": [5, 261, 369, 370, 375, 387, 388], "size_t": 5, "num_cor": [5, 7, 183, 345], "num_cores_i": 5, "program_factory_t": 5, "mandatori": 5, "select": [5, 7, 15, 33, 53, 99, 150, 288, 303, 391, 401, 410], "arg": [5, 126, 254, 272, 273, 274, 275, 289, 290, 345, 409, 412, 413, 415], "select_program_factori": 5, "valid": [5, 7, 125, 126, 150, 230, 240, 241, 242, 375, 397, 398, 402, 403, 407], "usual": [5, 413], "validate_on_program_cache_miss": 5, "reus": [5, 7, 13, 272, 273, 274, 275, 317, 409, 414], "less": [5, 128, 188, 189, 194, 224, 225, 226, 276, 345, 369, 402, 415], "validate_on_program_cache_hit": 5, "compute_output_spec": 5, "create_output_tensor": 5, "map": [5, 40, 229, 303, 409], "abl": [5, 150], "prim": 5, "after": [5, 7, 17, 52, 117, 136, 233, 252, 261, 289, 301, 321, 336, 371, 372, 373, 401, 402, 403, 406, 409, 410, 412, 414, 415], "op": [5, 7, 8, 9, 10, 24, 26, 39, 47, 52, 56, 57, 59, 60, 61, 77, 78, 79, 84, 92, 94, 101, 104, 105, 130, 131, 141, 142, 143, 150, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 233, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 350, 354, 356, 360, 375, 379, 380, 386, 393, 401, 402, 406, 414, 415], "invok": 5, "case": [5, 15, 92, 135, 137, 150, 173, 230, 252, 271, 304, 344, 372, 375, 397, 398, 403, 412, 414, 415], "hash": [5, 240, 241, 402], "stl": 5, "hash_t": 5, "compute_program_hash": 5, "create_op_performance_model": 5, "opperformancemodel": 5, "make": [5, 240, 241, 336, 337, 375, 381, 397, 403, 414, 415], "avail": [5, 7, 13, 389, 404, 407, 409, 414, 415], "constexpr": 5, "some_condition_based_on_operation_attributes_and_or_tensor_arg": 5, "true": [5, 7, 10, 12, 19, 21, 23, 25, 27, 29, 31, 39, 42, 44, 45, 48, 49, 51, 52, 53, 54, 64, 66, 68, 70, 74, 77, 78, 79, 81, 83, 84, 87, 89, 91, 92, 93, 95, 98, 101, 107, 108, 109, 110, 112, 113, 115, 116, 119, 122, 124, 127, 129, 132, 134, 140, 145, 146, 150, 155, 157, 159, 161, 164, 166, 187, 191, 193, 196, 199, 200, 201, 202, 203, 204, 205, 206, 208, 211, 212, 222, 223, 230, 231, 232, 233, 235, 236, 237, 244, 246, 251, 267, 270, 276, 277, 279, 282, 287, 294, 295, 299, 301, 303, 308, 310, 311, 312, 314, 316, 320, 321, 322, 324, 326, 329, 331, 333, 335, 336, 338, 340, 342, 343, 344, 346, 347, 349, 351, 352, 353, 355, 357, 362, 363, 364, 365, 366, 368, 369, 370, 375, 379, 380, 381, 385, 387, 388, 390, 392, 394, 397, 403, 409, 410, 412, 414, 415], "logical_shap": 5, "tensorlayout": 5, "dtype": [5, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 121, 122, 123, 124, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 291, 292, 293, 294, 295, 296, 297, 298, 299, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 382, 383, 384, 385, 386, 387, 388, 390, 391, 392, 393, 394, 395, 396, 397, 403, 405, 406, 407, 408, 409, 410, 411, 412, 413, 415], "pageconfig": 5, "memoryconfig": [5, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 190, 191, 192, 193, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 219, 221, 222, 223, 224, 226, 227, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 243, 244, 245, 246, 247, 248, 250, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 335, 337, 338, 339, 340, 341, 342, 343, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 375, 376, 377, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396], "output_spec": 5, "create_device_tensor": 5, "42": [5, 229, 410], "single_core_program_factori": 5, "work_split": 5, "tensor_accessor_arg": 5, "output_tensor": [5, 18, 20, 22, 24, 26, 30, 33, 35, 39, 41, 43, 46, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 67, 73, 80, 88, 92, 100, 101, 104, 106, 107, 109, 111, 113, 114, 116, 118, 121, 123, 124, 128, 133, 138, 139, 141, 142, 145, 146, 147, 150, 151, 153, 156, 158, 162, 165, 167, 170, 174, 175, 176, 177, 178, 181, 185, 186, 188, 190, 194, 199, 200, 202, 204, 207, 209, 210, 213, 215, 217, 219, 224, 226, 234, 238, 239, 242, 243, 247, 248, 250, 251, 253, 258, 269, 270, 278, 281, 286, 288, 292, 293, 296, 297, 307, 309, 311, 312, 313, 322, 323, 325, 327, 328, 329, 330, 335, 337, 341, 346, 347, 348, 350, 354, 356, 359, 361, 363, 364, 365, 373, 375, 384, 386, 391, 392, 393, 396, 415], "src_buffer": 5, "dst_buffer": 5, "dataformat": 5, "cb_data_format": 5, "datatype_to_dataformat_convert": 5, "uint32_t": [5, 118, 229], "single_tile_s": 5, "tile_s": [5, 150], "cb_data_format_output": 5, "single_tile_size_output": 5, "num_til": 5, "physical_volum": 5, "constant": [5, 53, 150, 182, 183, 184, 304, 305, 306, 403], "tile_hw": 5, "corecoord": [5, 17, 315, 317, 344, 345], "y": [5, 12, 13, 17, 26, 47, 150, 163, 186, 209, 210, 252, 264, 315, 316, 317, 345, 350, 354, 393, 394, 402, 403, 407, 411], "all_cor": [5, 345], "core_group_1": [5, 345], "core_group_2": [5, 345], "num_tiles_per_core_group_1": 5, "num_tiles_per_core_group_2": 5, "src0_cb_index": 5, "cbindex": 5, "c_0": 5, "num_input_til": 5, "circularbufferconfig": 5, "cb_src0_config": 5, "set_page_s": 5, "createcircularbuff": 5, "output_cb_index": 5, "c_2": 5, "num_output_til": 5, "cb_output_config": 5, "reader_compile_time_arg": 5, "tensoraccessorarg": 5, "append_to": 5, "writer_compile_time_arg": 5, "createkernel": 5, "eltwis": [5, 131, 271, 298, 405], "kernel": [5, 7, 32, 52, 53, 71, 77, 78, 79, 99, 117, 119, 150, 183, 184, 197, 230, 231, 233, 235, 236, 272, 273, 274, 275, 305, 306, 315, 316, 317, 335, 336, 344, 352, 357, 379, 390, 400, 402, 406, 407, 409, 412, 414], "dataflow": 5, "reader_unary_interleaved_start_id": 5, "readerdatamovementconfig": 5, "writer_unary_interleaved_start_id": 5, "writerdatamovementconfig": 5, "compute_kernel_args_group_1": 5, "per_core_block_cnt": 5, "per_core_block_s": 5, "math_approx_mod": 5, "fals": [5, 7, 10, 39, 52, 53, 77, 78, 79, 84, 85, 86, 92, 107, 109, 113, 140, 145, 150, 173, 197, 199, 200, 202, 204, 213, 230, 231, 233, 235, 236, 276, 311, 315, 316, 317, 321, 322, 323, 343, 344, 345, 346, 352, 357, 363, 365, 369, 375, 376, 377, 386, 390, 397, 409, 410, 411, 412, 413, 414, 415], "eltwise_sfpu": 5, "computeconfig": 5, "math_fidel": 5, "mathfidel": 5, "hifi4": [5, 402], "compile_arg": 5, "rang": [5, 24, 33, 38, 56, 57, 58, 59, 60, 61, 82, 125, 126, 140, 141, 166, 185, 187, 202, 215, 229, 234, 239, 245, 247, 266, 280, 303, 307, 332, 345, 356, 361, 362, 375, 403, 407, 409], "compute_kernel_args_group_2": 5, "num_tiles_written": 5, "num_tiles_per_cor": 5, "contain": [5, 7, 38, 39, 52, 85, 86, 99, 100, 140, 150, 233, 280, 286, 318, 343, 344, 345, 391, 400, 403, 408, 409, 412, 414], "tt_assert": 5, "setruntimearg": 5, "address": [5, 407, 414], "move": [5, 7, 99, 136, 371, 398, 409, 411, 413], "shared_vari": 5, "runtime_arg": 5, "getruntimearg": 5, "multi_core_program_factori": 5, "idevic": [5, 272, 273], "primit": 5, "compositeexampleoper": 5, "composite_exampl": 5, "another_copi": 5, "_pybind": 5, "example_pybind": 5, "pybind": 5, "pybind_fwd": 5, "py": [5, 397, 398, 402, 404, 405, 406, 407, 408, 410, 411, 412, 414, 415], "pybind11": 5, "bind_example_oper": 5, "r": [5, 264, 400, 414], "overload": [5, 276, 345], "expos": 5, "logic": [5, 213, 214, 215, 216, 217, 218, 219, 220, 230, 240, 241], "self": [5, 10, 11, 12, 13, 397, 403, 409, 413], "correct": [5, 150, 231, 235, 236, 352, 357, 390, 410, 412], "specif": [5, 7, 17, 39, 57, 58, 60, 65, 71, 78, 90, 97, 107, 109, 113, 123, 140, 145, 154, 160, 162, 167, 190, 199, 200, 202, 204, 221, 229, 230, 245, 259, 266, 277, 281, 286, 296, 297, 307, 311, 315, 319, 322, 323, 337, 339, 343, 346, 363, 365, 367, 380, 398, 403, 404, 409, 412, 413, 414], "pybind_overload_t": 5, "decltyp": 5, "examples_pybind": 5, "py_modul": 5, "final": [5, 32, 183, 230, 305, 397, 398, 401, 409, 410, 412, 414], "wherev": 5, "want": [5, 400, 407, 413, 415], "compar": [5, 104, 142, 150, 151, 188, 224, 230, 248, 407, 409, 410, 412], "its": [5, 7, 71, 77, 78, 79, 87, 96, 103, 198, 230, 234, 261, 305, 334, 372, 373, 397, 398, 401, 402, 403, 409, 410, 412, 413, 415], "equival": [5, 24, 26, 47, 56, 57, 59, 60, 61, 92, 94, 104, 105, 130, 131, 141, 142, 143, 151, 152, 163, 173, 183, 184, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 242, 247, 248, 249, 252, 260, 268, 269, 271, 298, 305, 306, 313, 350, 354, 356, 375, 381, 393, 403, 404, 407], "signatur": 5, "keep": [5, 39, 231, 235, 236, 276, 352, 357, 390, 397, 403, 409], "mind": [5, 411], "And": [5, 397, 403, 407], "ignor": [5, 32, 409], "kwarg": [5, 272, 273, 274, 275, 289, 290, 345, 415], "def": [5, 397, 406, 408, 409, 411, 412, 413, 415], "golden_funct": 5, "befor": [5, 7, 40, 85, 86, 261, 272, 273, 274, 275, 290, 316, 336, 337, 398, 403, 409], "automat": [5, 7, 15, 33, 78, 228, 288, 372, 398, 400, 402, 403, 407], "some": [5, 254, 344, 360, 400, 413, 415], "postprocess": 5, "pack": [5, 387, 388, 413], "preprocess_golden_function_input": 5, "ttnn_input_tensor": 5, "postprocess_golden_function_output": 5, "torch_output_tensor": [5, 150, 415], "becaus": [5, 403, 407, 411], "wa": [5, 150, 398, 402, 403, 409, 413], "good": [5, 398, 407, 409], "practic": [5, 409, 413], "demonstr": [5, 398, 402, 405, 407, 408, 409, 410, 412, 413], "simplest": [5, 400], "directli": [5, 10, 11, 12, 317, 336, 404, 405, 406, 407, 408, 410, 411, 412], "approach": [5, 337, 397, 401, 415], "difficult": 5, "date": 5, "prevent": [5, 7, 400, 409], "snippet": [5, 410, 415], "being": [5, 276, 289, 290, 345, 402, 403], "better": [5, 13, 315, 322, 409, 413, 414], "place": [5, 8, 17, 53, 105, 143, 152, 179, 180, 189, 216, 225, 249, 307, 315, 317, 336, 371, 377, 403], "dure": [5, 10, 11, 12, 24, 26, 47, 56, 57, 59, 60, 61, 73, 92, 94, 104, 105, 130, 131, 137, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 350, 354, 356, 374, 393, 403, 409, 414], "examples_map": 5, "appear": [5, 240, 241, 414], "function_to_examples_mapping_dict": 5, "dictionari": [5, 397, 409, 412], "test_exampl": 5, "test_example_exampl": 5, "appropri": [5, 15, 24, 26, 47, 56, 57, 59, 60, 61, 92, 94, 104, 105, 130, 131, 141, 142, 143, 150, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 230, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 350, 354, 356, 393, 398, 400, 409, 412, 413], "sure": [5, 375, 415], "top": [5, 242, 375], "test_data_movement_exampl": 5, "data_mov": 5, "test_core_exampl": 5, "pytest": [5, 397, 398, 402, 414, 415], "bfloat16": [5, 7, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 263, 264, 265, 266, 267, 268, 269, 270, 271, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 291, 292, 293, 294, 295, 296, 297, 298, 299, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 382, 383, 384, 385, 386, 387, 388, 390, 391, 392, 393, 394, 395, 396, 397, 402, 403, 406, 407, 408, 409, 410, 411, 412, 415], "row_major_layout": [5, 39, 40, 52, 71, 100, 135, 137, 138, 150, 170, 229, 233, 257, 280, 305, 306, 318, 369, 370, 372, 395, 403, 406, 407, 409, 411, 412], "ensur": [5, 11, 71, 343, 360, 398, 400, 403, 406, 409, 412, 413, 414], "ci": [5, 398], "class": [7, 8, 10, 11, 12, 13, 15, 16, 17, 397, 398, 402, 403, 409, 410], "pybind11_object": [7, 8, 10, 11, 12, 13, 15, 16, 17, 403], "flag": [7, 400], "properti": [7, 8, 10, 11, 12, 13, 17, 403, 409], "control": [7, 10, 12, 13, 17, 33, 303, 400, 409], "size": [7, 8, 10, 12, 13, 17, 24, 26, 38, 47, 52, 55, 56, 57, 59, 60, 61, 77, 78, 79, 85, 86, 92, 94, 99, 100, 101, 104, 105, 130, 131, 140, 141, 142, 143, 150, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 230, 231, 233, 234, 235, 236, 238, 247, 248, 249, 252, 259, 260, 268, 269, 271, 272, 273, 274, 275, 276, 298, 313, 317, 334, 345, 350, 352, 354, 356, 357, 379, 380, 381, 389, 390, 393, 403, 405, 407, 409, 413, 415], "block": [7, 10, 11, 12, 13, 17, 24, 26, 47, 52, 56, 57, 59, 60, 61, 77, 78, 79, 84, 92, 94, 104, 105, 130, 131, 141, 142, 143, 150, 151, 152, 163, 173, 182, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 228, 230, 233, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 272, 273, 274, 275, 286, 298, 313, 317, 350, 354, 356, 393, 403, 409], "height": [7, 8, 10, 12, 13, 17, 24, 26, 47, 52, 56, 57, 59, 60, 61, 71, 73, 77, 78, 79, 84, 92, 94, 104, 105, 125, 126, 130, 131, 138, 141, 142, 143, 148, 150, 151, 152, 163, 173, 182, 183, 184, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 230, 231, 233, 234, 235, 236, 238, 247, 248, 249, 252, 256, 257, 260, 268, 269, 271, 272, 273, 274, 275, 298, 304, 305, 306, 313, 315, 335, 350, 352, 354, 356, 357, 372, 390, 393, 395, 402, 403, 406, 409, 412], "chunk": [7, 10, 13, 379, 380, 407], "l1": [7, 8, 17, 24, 26, 32, 39, 47, 53, 56, 57, 59, 60, 61, 79, 84, 85, 86, 92, 94, 99, 104, 105, 130, 131, 140, 141, 142, 143, 150, 151, 152, 163, 170, 173, 181, 182, 185, 186, 188, 189, 197, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 230, 231, 234, 235, 236, 238, 242, 247, 248, 249, 252, 259, 260, 268, 269, 271, 276, 298, 304, 313, 315, 317, 335, 336, 343, 344, 350, 352, 354, 356, 357, 371, 373, 375, 390, 393, 403, 406, 409, 411, 412, 413, 414], "divid": [7, 8, 12, 13, 92, 93, 150, 345, 376, 377, 409, 414], "among": 7, "also": [7, 24, 26, 32, 47, 56, 57, 59, 60, 61, 77, 78, 79, 85, 86, 92, 94, 99, 104, 105, 130, 131, 141, 142, 143, 151, 152, 163, 173, 183, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 246, 247, 248, 249, 252, 260, 268, 269, 271, 276, 298, 304, 305, 313, 315, 350, 354, 356, 393, 398, 402, 403, 404, 407, 409, 413, 414], "further": [7, 407], "subdivid": 7, "within": [7, 10, 12, 13, 38, 52, 57, 60, 173, 233, 398, 403, 413, 414], "possibl": [7, 230, 374, 397], "which": [7, 10, 12, 32, 33, 52, 55, 57, 60, 78, 84, 85, 86, 135, 138, 139, 140, 150, 197, 229, 230, 231, 233, 235, 236, 240, 241, 257, 258, 280, 281, 288, 318, 334, 335, 343, 344, 352, 357, 379, 380, 390, 395, 396, 397, 402, 403, 405, 406, 407, 409, 412, 413, 414], "equal": [7, 63, 104, 105, 106, 128, 142, 143, 147, 173, 182, 183, 184, 188, 189, 194, 248, 249, 253, 305, 306, 316, 317, 343, 409, 415], "output_matrix_height_per_cor": 7, "lead": 7, "temporari": 7, "circular": [7, 336, 414], "oom": 7, "must": [7, 11, 12, 13, 17, 24, 26, 35, 39, 47, 53, 55, 56, 57, 59, 60, 61, 77, 85, 86, 92, 94, 99, 101, 104, 105, 117, 130, 131, 140, 141, 142, 143, 150, 151, 152, 163, 173, 182, 183, 184, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 229, 230, 231, 234, 235, 236, 238, 242, 246, 247, 248, 249, 252, 260, 268, 269, 270, 271, 272, 273, 276, 281, 298, 303, 304, 305, 306, 313, 315, 316, 317, 318, 334, 335, 336, 343, 344, 350, 352, 354, 356, 357, 369, 370, 371, 375, 387, 388, 390, 391, 393, 400, 403, 407, 409, 414], "32": [7, 13, 15, 17, 24, 26, 32, 33, 34, 35, 36, 37, 39, 47, 55, 56, 57, 59, 60, 61, 71, 73, 74, 75, 76, 92, 94, 104, 105, 117, 126, 130, 131, 138, 141, 142, 143, 144, 148, 149, 150, 151, 152, 163, 168, 169, 170, 171, 172, 173, 182, 183, 184, 185, 186, 188, 189, 197, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 230, 234, 238, 242, 247, 248, 249, 252, 254, 255, 256, 257, 260, 263, 264, 265, 268, 269, 271, 277, 283, 284, 288, 291, 298, 301, 304, 305, 306, 313, 317, 334, 335, 336, 344, 350, 354, 356, 358, 369, 370, 371, 372, 375, 386, 387, 388, 393, 395, 403, 405, 407, 408, 409, 410, 412, 413, 415], "evenli": [7, 12, 13, 38, 150, 345], "reduc": [7, 8, 10, 13, 34, 39, 150, 231, 235, 236, 276, 288, 318, 335, 352, 357, 375, 390], "width": [7, 8, 10, 11, 12, 13, 17, 24, 26, 47, 52, 56, 57, 59, 60, 61, 71, 77, 78, 79, 84, 92, 94, 104, 105, 125, 126, 130, 131, 138, 141, 142, 143, 148, 150, 151, 152, 163, 173, 182, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 230, 231, 233, 234, 235, 236, 238, 247, 248, 249, 252, 256, 257, 260, 268, 269, 271, 272, 273, 274, 275, 298, 313, 315, 316, 317, 350, 352, 354, 356, 357, 372, 375, 378, 390, 393, 395, 402, 406, 409, 412], "greater": [7, 63, 73, 90, 125, 126, 142, 143, 147, 151, 152, 153, 195, 246, 415], "n150": 7, "thats": 7, "64": [7, 15, 32, 35, 39, 73, 84, 99, 125, 126, 144, 149, 150, 182, 197, 230, 242, 263, 291, 304, 334, 335, 344, 358, 369, 370, 371, 375, 387, 388, 403, 409, 410, 411, 412, 413, 415], "2048": [7, 412, 415], "divisor": [7, 12, 13, 52, 141, 282], "halv": 7, "appli": [7, 8, 10, 11, 12, 17, 18, 20, 22, 24, 35, 41, 43, 46, 52, 53, 57, 58, 60, 62, 63, 65, 67, 71, 77, 78, 79, 80, 85, 86, 88, 97, 99, 104, 106, 107, 109, 111, 113, 114, 117, 118, 119, 120, 121, 123, 128, 133, 141, 142, 144, 145, 147, 148, 149, 150, 151, 153, 154, 156, 158, 160, 162, 165, 167, 174, 175, 176, 177, 178, 182, 183, 184, 185, 186, 188, 190, 194, 197, 199, 200, 202, 204, 207, 209, 210, 213, 215, 216, 217, 219, 224, 226, 230, 231, 233, 234, 235, 236, 238, 239, 247, 248, 250, 253, 261, 271, 272, 273, 274, 275, 276, 278, 286, 291, 292, 293, 296, 297, 304, 305, 306, 307, 309, 311, 313, 315, 316, 317, 318, 319, 322, 323, 325, 327, 328, 330, 335, 336, 337, 339, 341, 346, 348, 350, 352, 356, 357, 358, 359, 361, 363, 365, 367, 375, 384, 386, 390, 393, 398, 409, 410, 412, 413], "none": [7, 14, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 219, 221, 222, 223, 224, 225, 226, 227, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 263, 264, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 409, 413], "unarywithparam": [7, 197, 230, 386, 412], "unaryoptyp": [7, 386, 412], "boolean": [7, 171, 172, 344, 363, 365, 375], "determin": [7, 8, 10, 11, 12, 13, 33, 79, 150, 230, 240, 241, 288, 344, 403, 412], "them": [7, 381, 400, 402, 405, 409], "dram": [7, 8, 11, 24, 26, 39, 47, 53, 56, 57, 59, 60, 61, 78, 79, 85, 86, 92, 94, 99, 104, 105, 130, 131, 140, 141, 142, 143, 150, 151, 152, 163, 173, 182, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 230, 231, 234, 235, 236, 238, 242, 247, 248, 249, 252, 260, 268, 269, 271, 276, 298, 303, 304, 313, 335, 343, 344, 350, 352, 354, 356, 357, 371, 373, 375, 390, 393, 403, 409, 414], "l1_small": 7, "2d": [7, 10, 12, 13, 77, 78, 79, 148, 230, 389, 403, 406, 409, 412], "instead": [7, 52, 94, 150, 167, 197, 230, 276, 297, 411], "risc": [7, 402], "grid": [7, 10, 12, 13, 17, 32, 33, 99, 105, 107, 109, 113, 128, 131, 143, 145, 150, 152, 189, 197, 199, 200, 202, 204, 225, 226, 230, 231, 235, 236, 249, 298, 303, 311, 334, 344, 345, 346, 352, 357, 379, 380, 384, 387, 390, 391, 397, 403, 412], "indic": [7, 39, 100, 101, 140, 233, 254, 318, 334, 343, 344, 375, 388, 409, 413, 414], "whether": [7, 10, 12, 39, 52, 85, 86, 87, 197, 230, 231, 233, 235, 236, 240, 241, 274, 275, 315, 316, 317, 335, 336, 344, 345, 352, 357, 369, 370, 375, 380, 381, 387, 388, 390, 412], "conv": [7, 406, 412], "halo": [7, 8, 52, 233], "micro": 7, "anoth": [7, 103, 139, 258, 396, 400, 403], "ha": [7, 32, 57, 60, 79, 230, 240, 241, 274, 275, 360, 381, 397, 398, 402, 403, 404, 409, 413, 414, 415], "effect": 7, "doubl": 7, "allow": [7, 10, 12, 140, 229, 230, 276, 398, 401, 410], "stall": 7, "reader": [7, 240], "improv": [7, 10, 12, 337, 398, 414], "increas": [7, 12, 414], "consecut": [7, 38, 403], "boost": 7, "image2column": 7, "so": [7, 126, 150, 397, 409], "bound": [7, 67, 69, 221, 280], "fold": 7, "adjust": [7, 71, 337, 414], "stride": [7, 52, 77, 78, 79, 233, 272, 273, 274, 275, 406, 409, 412], "default": [7, 9, 10, 12, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 219, 221, 222, 223, 224, 225, 226, 227, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 263, 264, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 285, 286, 287, 288, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 400, 402, 403, 407, 409, 412, 413, 415], "condit": [7, 303, 391, 398], "forc": [7, 87, 321], "disabl": [7, 240, 241, 321, 406, 407, 408, 410, 411, 412, 413, 414, 415], "behavior": [7, 15, 403], "nhwc": [7, 406, 412], "format": [7, 8, 24, 26, 35, 40, 47, 52, 56, 57, 59, 60, 61, 77, 78, 79, 92, 94, 104, 105, 117, 130, 131, 137, 141, 142, 143, 150, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 233, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 272, 273, 274, 275, 286, 298, 313, 343, 350, 354, 356, 393, 401, 403, 406, 409, 410, 412, 413], "n": [7, 8, 10, 11, 12, 13, 32, 52, 53, 73, 77, 78, 79, 85, 86, 117, 125, 126, 150, 231, 233, 235, 236, 242, 254, 267, 268, 304, 305, 306, 344, 352, 357, 375, 389, 390, 400, 402, 407, 408, 409, 412], "h": [7, 8, 52, 53, 77, 78, 79, 117, 125, 126, 150, 233, 242, 304, 375, 389, 403, 406, 409, 412], "w": [7, 8, 52, 53, 77, 78, 79, 117, 125, 126, 150, 182, 183, 184, 233, 242, 304, 375, 389, 402, 406, 409, 412], "ic": 7, "oc": 7, "pad_h": [7, 52, 233], "pad_w": [7, 52, 233], "becom": [7, 372, 414], "met": [7, 303], "dimens": [7, 8, 10, 11, 12, 13, 17, 24, 26, 33, 35, 39, 47, 56, 57, 59, 60, 61, 71, 73, 74, 85, 86, 92, 94, 99, 104, 105, 125, 126, 130, 131, 138, 140, 141, 142, 143, 144, 148, 149, 150, 151, 152, 163, 170, 173, 182, 183, 184, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 230, 231, 234, 235, 236, 238, 242, 247, 248, 249, 252, 255, 256, 257, 260, 261, 263, 268, 269, 271, 276, 277, 288, 291, 298, 300, 302, 303, 306, 313, 315, 316, 317, 318, 334, 335, 336, 343, 345, 350, 352, 354, 356, 357, 358, 372, 374, 375, 378, 379, 380, 381, 390, 393, 395, 403, 406, 407, 408, 409, 410, 413], "kernel_s": [7, 52, 77, 78, 79, 233, 272, 273, 274, 275, 406, 409, 412], "least": [7, 185, 230, 402], "No": [7, 95, 307, 406, 409, 410], "dilat": [7, 77, 78, 79, 233, 272, 273, 274, 275, 409, 412], "divis": [7, 8, 12, 17, 92, 93, 94, 95, 130, 131, 144, 149, 281, 282, 291, 298, 345, 358, 403], "respect": [7, 53, 79, 101, 150, 182, 183, 184, 230, 403, 405], "except": [7, 140, 230, 242, 316, 317, 409, 411], "OR": [7, 59, 217, 218, 303], "particularli": 7, "benefici": [7, 12], "unalign": 7, "g": [7, 12, 79, 150, 229, 230, 318, 400, 402, 403, 409, 410, 412, 414], "small": [7, 53, 150, 182, 183, 184, 259, 304, 305, 306, 406, 409, 413], "count": [7, 125, 126, 150, 182, 402], "rgb": [7, 409], "2x2": [7, 403], "7x7": 7, "1x1": [7, 412], "align": [7, 11, 13, 182, 230, 316, 317, 402], "4x4": [7, 403], "implicitli": [7, 230], "writer": 7, "carri": [7, 297], "bottleneck": [7, 414], "overrid": [7, 414, 415], "split": [7, 73, 144, 149, 150, 291, 345, 358, 381, 403, 409], "heurist": 7, "By": [7, 337, 401, 403, 407, 409, 413], "inner": [7, 10, 11, 12, 13], "dim": [7, 15, 33, 34, 39, 73, 74, 84, 85, 86, 140, 144, 149, 170, 182, 183, 231, 235, 236, 242, 263, 276, 277, 288, 291, 301, 302, 304, 305, 315, 318, 334, 335, 343, 352, 357, 358, 375, 381, 390, 403, 409, 410, 411, 412], "kernel_h": [7, 52, 233], "constraint": [7, 230, 315, 317, 375, 403], "space": [7, 38, 77, 78, 79, 272, 273, 274, 275, 403, 409], "either": [7, 8, 24, 26, 47, 56, 57, 59, 60, 61, 78, 84, 92, 94, 104, 105, 130, 131, 141, 142, 143, 150, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 229, 230, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 350, 354, 356, 372, 393, 400, 403, 413, 414], "row_major": [7, 17, 38, 39, 78, 84, 102, 103, 135, 137, 150, 182, 183, 229, 231, 235, 236, 255, 256, 276, 304, 305, 315, 316, 317, 344, 352, 357, 369, 370, 387, 388, 390, 395, 396, 402, 403, 407], "expect": [7, 10, 52, 79, 117, 126, 150, 183, 233, 381, 398, 401, 406, 409, 410, 413, 414], "next": [7, 150, 252, 400, 403, 412], "impact": [7, 10, 12, 13, 17, 398], "part": [7, 133, 134, 144, 149, 168, 169, 171, 172, 183, 283, 284, 291, 305, 344, 358, 382, 383, 384, 398, 402, 411], "current": [7, 39, 52, 71, 73, 84, 140, 150, 240, 241, 274, 275, 318, 371, 375, 379, 380, 403, 407, 409, 415], "block_shard": [7, 52, 233], "without": [7, 55, 94, 95, 103, 230, 334, 409], "addition": [7, 415], "nhw": [7, 52, 233], "number": [7, 8, 10, 11, 12, 13, 24, 25, 28, 32, 33, 34, 52, 54, 67, 73, 77, 78, 79, 92, 93, 94, 104, 105, 125, 126, 130, 131, 132, 140, 142, 143, 150, 151, 152, 176, 186, 188, 189, 193, 209, 210, 213, 217, 219, 224, 225, 227, 229, 230, 231, 233, 234, 235, 236, 238, 242, 244, 247, 248, 249, 254, 259, 261, 263, 270, 271, 272, 273, 274, 275, 280, 288, 298, 299, 300, 302, 303, 313, 321, 344, 345, 350, 352, 353, 356, 357, 370, 375, 376, 377, 390, 391, 393, 401, 402, 403, 406, 407, 409, 410, 411, 412, 414], "match": [7, 24, 26, 47, 56, 57, 59, 60, 61, 71, 85, 86, 92, 94, 104, 105, 130, 131, 141, 142, 143, 150, 151, 152, 163, 173, 182, 183, 184, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 230, 234, 238, 240, 241, 242, 247, 248, 249, 252, 260, 268, 269, 271, 298, 301, 303, 304, 305, 306, 313, 350, 354, 356, 393, 403, 405, 407, 408, 409, 410, 411, 413], "fragment": [7, 303], "ideal": [7, 52, 233, 375, 403], "face": [7, 403, 409, 413], "reshard": 7, "alreadi": [7, 79, 240, 241, 259, 336, 409, 412], "anywai": 7, "previou": [7, 32, 101, 405, 409, 410, 414], "due": [7, 286, 402, 403, 409], "v": [7, 126, 380, 409, 412, 414], "tensormemorylayout": [7, 17, 52, 170, 233, 315, 317], "own": [7, 403], "height_shard": [7, 17, 315, 317], "width_shard": 7, "orient": [7, 84, 230, 315, 403], "major": [7, 10, 24, 26, 33, 47, 56, 57, 59, 60, 61, 64, 92, 94, 104, 105, 117, 124, 126, 127, 129, 130, 131, 134, 141, 142, 143, 150, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 230, 234, 238, 247, 248, 249, 252, 254, 260, 268, 269, 271, 288, 298, 308, 313, 326, 344, 350, 354, 356, 385, 393, 403, 407, 409, 414], "column": [7, 150, 183, 402, 403, 407, 408], "bia": [7, 53, 54, 77, 78, 79, 117, 272, 273, 274, 275, 397, 406, 409, 410, 411, 412], "respons": [7, 409, 412], "prepar": [7, 150, 398, 406, 409, 412], "unspecifi": [7, 33, 288], "float32": [7, 20, 22, 32, 35, 39, 41, 43, 50, 53, 55, 62, 65, 67, 78, 85, 86, 88, 92, 102, 103, 104, 113, 123, 131, 133, 140, 142, 151, 154, 156, 158, 160, 163, 167, 182, 184, 188, 190, 197, 199, 200, 202, 204, 221, 224, 230, 231, 234, 235, 236, 238, 248, 269, 278, 280, 304, 305, 306, 307, 311, 315, 316, 317, 318, 327, 328, 335, 336, 339, 341, 344, 346, 352, 357, 359, 363, 365, 375, 384, 390, 391, 395, 396, 397, 402, 403, 405, 408, 410, 415], "too": [8, 415], "fit": [8, 302], "conv2d_dram": 8, "version": [8, 67, 79, 150, 240, 241, 276, 315, 323, 369, 380, 400, 402, 404, 406, 407, 408, 410, 411, 412, 413, 414], "happen": 8, "along": [8, 10, 11, 12, 13, 24, 26, 33, 34, 47, 56, 57, 59, 60, 61, 73, 85, 86, 92, 94, 99, 104, 105, 130, 131, 140, 141, 142, 143, 150, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 231, 234, 235, 236, 238, 247, 248, 249, 252, 256, 260, 268, 269, 271, 277, 288, 298, 300, 313, 316, 317, 318, 334, 335, 336, 343, 350, 352, 354, 356, 357, 375, 378, 390, 393, 403, 412], "correspond": [8, 100, 101, 234, 238, 380, 391, 414], "calcul": [8, 32, 47, 52, 84, 163, 252, 402], "last": [8, 35, 39, 71, 99, 138, 144, 149, 150, 182, 183, 184, 242, 257, 291, 303, 304, 305, 306, 316, 317, 335, 336, 343, 358, 372, 375, 379, 381, 395, 402, 403, 409], "smaller": [8, 12, 13, 403, 408], "rest": [8, 126], "sliceheight": 8, "slicewidth": 8, "prefer": [8, 24, 26, 47, 56, 57, 59, 60, 61, 92, 94, 104, 105, 130, 131, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 350, 354, 356, 393, 400, 405, 409, 414], "much": [8, 402, 407], "larger": [8, 13, 408, 413], "_ttnn": [9, 10, 11, 12, 13, 14, 100, 259, 285, 345, 360, 403], "multi_devic": [9, 14, 259, 360], "meshdevic": [9, 14, 40, 72, 77, 78, 79, 102, 103, 135, 137, 138, 139, 198, 228, 229, 257, 258, 259, 280, 360, 371, 374, 395, 396, 412], "plan": [9, 14, 262], "deprec": [9, 14, 262], "futur": [9, 14, 229, 262, 409], "1d": [10, 77, 150, 230, 254, 271, 276, 410], "multicast": [10, 11, 12, 230], "advanc": [10, 33, 303, 400, 409, 414], "veri": [10, 11, 317, 336, 337, 402], "narrow": [10, 11], "interleav": [10, 12, 24, 26, 39, 47, 53, 56, 57, 59, 60, 61, 73, 85, 86, 92, 94, 99, 104, 105, 117, 130, 131, 140, 141, 142, 143, 150, 151, 152, 163, 170, 173, 182, 184, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 230, 231, 234, 235, 236, 238, 242, 247, 248, 249, 252, 260, 268, 269, 271, 276, 298, 302, 304, 306, 313, 315, 318, 335, 343, 344, 350, 352, 354, 356, 357, 371, 373, 375, 381, 390, 393, 403], "capabl": [10, 12, 13, 17, 409], "while": [10, 150, 401, 409, 410], "str": [10, 11, 12, 13, 24, 40, 45, 93, 96, 104, 119, 141, 142, 151, 185, 186, 188, 197, 198, 209, 210, 213, 217, 219, 224, 230, 234, 238, 240, 241, 247, 248, 313, 321, 350, 356, 393, 409, 412, 413], "batch": [10, 12, 17, 52, 53, 77, 78, 79, 125, 126, 170, 230, 233, 272, 273, 274, 275, 315, 316, 317, 380, 402, 406, 409, 412, 413], "incorpor": [10, 398], "elimin": 10, "separ": [10, 12], "overal": [10, 410, 412, 414], "scenario": [10, 11, 12, 230, 403], "intern": [10, 79, 150, 317, 399], "left": [10, 47, 57, 92, 141, 150, 185, 401], "k": [10, 11, 12, 13, 242, 266, 335, 336, 344, 375, 379, 380, 407, 409, 412], "granular": [10, 11, 12, 13, 17], "wide": [10, 11, 12, 13, 150, 336], "input_tensor_a": [10, 11, 12, 21, 23, 24, 25, 26, 27, 28, 29, 31, 42, 44, 45, 47, 48, 49, 51, 52, 54, 56, 57, 59, 60, 61, 64, 66, 74, 81, 83, 89, 91, 92, 93, 94, 98, 104, 105, 108, 110, 112, 115, 122, 127, 129, 130, 131, 132, 134, 141, 142, 143, 151, 152, 155, 157, 159, 163, 164, 166, 170, 173, 185, 186, 187, 188, 189, 191, 193, 196, 201, 203, 205, 206, 208, 209, 210, 211, 212, 213, 214, 217, 218, 219, 220, 222, 223, 224, 225, 227, 232, 233, 234, 237, 238, 244, 246, 247, 248, 249, 252, 260, 268, 270, 271, 279, 282, 287, 294, 295, 298, 299, 308, 313, 314, 320, 324, 326, 331, 333, 340, 342, 349, 350, 351, 353, 354, 355, 356, 362, 366, 385, 392, 393, 394, 415], "input_tensor_b": [10, 11, 12, 24, 25, 26, 27, 28, 29, 31, 45, 47, 48, 54, 56, 57, 59, 60, 61, 74, 92, 93, 94, 104, 105, 130, 131, 132, 141, 142, 143, 151, 152, 163, 164, 170, 173, 185, 186, 187, 188, 189, 193, 209, 210, 211, 212, 213, 214, 217, 218, 219, 220, 224, 225, 227, 232, 234, 237, 238, 244, 247, 248, 249, 252, 260, 271, 298, 299, 313, 314, 350, 351, 353, 354, 355, 356, 392, 393, 394, 415], "affect": [10, 11, 12, 13, 315, 402], "across": [10, 11, 12, 13, 33, 34, 148, 183, 184, 255, 288, 305, 306, 345, 379, 402, 403, 408, 409], "first": [10, 30, 32, 137, 144, 149, 150, 183, 197, 230, 254, 261, 291, 305, 343, 344, 358, 397, 400, 402, 407, 409, 410, 411, 412, 415], "bandwidth": [10, 11], "certain": [10, 12], "m": [10, 11, 12, 13, 32, 344, 400, 407, 414], "subblock": [10, 12, 13], "schedul": [10, 12, 398], "particip": 10, "workload": [10, 399], "balanc": [10, 12, 409], "crucial": 10, "achiev": [10, 398, 401], "subsequ": [10, 407, 409, 410, 411, 415], "special": [11, 315, 398, 403, 409], "signific": [11, 407], "benefit": [11, 401], "avoid": [11, 12, 240, 317, 336, 337, 402, 403, 406, 408, 409, 413, 414], "trip": 11, "chosen": [11, 12, 13, 230, 402], "strategi": [11, 84, 230, 402, 403, 412], "conflict": 11, "compat": [11, 17, 150, 412], "x": [12, 13, 17, 26, 47, 53, 79, 113, 114, 121, 122, 150, 163, 182, 183, 184, 186, 202, 203, 209, 210, 230, 252, 264, 286, 304, 305, 306, 311, 315, 316, 317, 345, 350, 354, 379, 380, 393, 394, 400, 402, 403, 406, 407, 409, 411], "overhead": [12, 408], "explicitli": [12, 15, 24, 26, 47, 56, 57, 59, 60, 61, 87, 92, 94, 104, 105, 130, 131, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 350, 354, 356, 393, 409], "transpos": [12, 79, 120, 197, 230, 263, 381, 403, 409, 410, 412], "direct": [12, 409], "reusabl": 13, "togeth": [13, 344, 405, 406, 407, 408, 410, 411, 412], "suggest": 13, "decreas": 13, "fewer": 13, "work": [13, 230, 334, 345, 397, 398, 400, 415], "total_m": 13, "total_n": 13, "arg0": [14, 403], "device_id": [14, 72, 100, 136, 228, 259, 360, 371, 405, 406, 407, 408, 409, 410, 411, 412, 413, 415], "suitabl": [15, 272, 273, 274, 275], "most": [15, 230, 403, 408, 411], "characterist": [15, 286, 414], "tile_layout": [15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 36, 37, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 74, 75, 76, 80, 81, 82, 83, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 171, 172, 173, 174, 175, 176, 177, 178, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 230, 232, 234, 237, 238, 239, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 257, 258, 264, 265, 266, 267, 268, 269, 270, 271, 277, 278, 279, 280, 281, 282, 283, 284, 286, 287, 291, 292, 293, 294, 295, 296, 297, 298, 299, 301, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 335, 336, 337, 338, 339, 340, 341, 342, 344, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 372, 382, 383, 384, 385, 386, 391, 392, 393, 394, 395, 396, 397, 403, 405, 407, 408, 409, 410, 411, 412, 415], "program_config": [15, 17, 32, 182, 183, 184, 197, 230, 304, 305, 306, 315, 317, 336, 344, 376, 377, 379, 380], "common": [16, 141, 185, 397, 398, 409, 412], "customiz": 17, "fine": [17, 401, 413], "grain": 17, "over": [17, 39, 53, 77, 78, 79, 117, 148, 150, 182, 183, 184, 231, 235, 236, 277, 304, 306, 335, 352, 357, 379, 380, 390, 400, 414], "subblock_w": [17, 315, 317], "sub": [17, 33, 105, 107, 109, 113, 128, 131, 143, 145, 152, 189, 199, 200, 202, 204, 225, 226, 249, 298, 303, 311, 334, 346, 354, 360, 384, 387, 391, 403, 409], "block_h": [17, 315, 317], "vertic": 17, "horizont": 17, "modifi": [17, 165, 317, 321, 336, 337], "proper": [17, 47, 400], "mask": [17, 150, 242, 344, 376, 377, 380, 406, 407, 408, 409, 410, 411, 412, 413, 414], "input_shap": [17, 33, 52, 101, 233, 276, 288, 315, 316, 317], "attention_mask_t": [17, 315, 316, 317], "scale": [17, 26, 32, 53, 118, 150, 182, 183, 184, 231, 235, 236, 304, 305, 306, 315, 316, 317, 319, 337, 352, 354, 357, 379, 380, 390, 409, 412], "tt_output": [17, 52, 233, 316, 317], "logger": [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 263, 264, 265, 266, 267, 268, 269, 270, 271, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 382, 383, 384, 385, 386, 387, 388, 390, 391, 392, 393, 394, 395, 396, 405, 406, 408, 409, 410, 411, 412], "info": [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 263, 264, 265, 266, 267, 268, 269, 270, 271, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 382, 383, 384, 385, 386, 387, 388, 390, 391, 392, 393, 394, 395, 396, 405, 406, 407, 408, 409, 410, 411, 412, 414], "f": [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 36, 37, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 74, 75, 76, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 133, 134, 141, 142, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 190, 191, 192, 193, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 219, 221, 222, 223, 224, 226, 227, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 242, 243, 244, 245, 246, 247, 248, 250, 251, 252, 253, 255, 256, 260, 264, 265, 266, 267, 268, 269, 270, 271, 276, 277, 278, 279, 281, 282, 283, 284, 286, 287, 291, 292, 293, 294, 295, 296, 297, 298, 299, 301, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 375, 382, 383, 384, 385, 386, 390, 391, 392, 393, 394, 406, 407, 408, 409, 410, 411, 412, 415], "compute_grid_s": [17, 315, 316, 317], "fuse_head": [17, 316, 317], "num_cores_r": [17, 315, 316, 317], "384": [17, 315, 316, 317, 397, 411, 413], "768": [17, 315, 316, 317, 409, 411], "grid_coord": [17, 315, 317], "shard_grid": [17, 315, 317], "corerangeset": [17, 33, 84, 105, 107, 109, 113, 128, 131, 140, 143, 145, 152, 189, 199, 200, 202, 204, 225, 226, 231, 235, 236, 249, 298, 303, 311, 315, 317, 318, 334, 345, 346, 352, 357, 369, 375, 384, 387, 390, 391], "corerang": [17, 315, 317, 345], "shard_shap": [17, 315, 317], "shard_spec": [17, 315, 317], "shardspec": [17, 315, 317], "shardorient": [17, 84, 315, 317], "sharded_mem_config": [17, 150, 315, 317], "buffertyp": [17, 170, 315, 317], "input_shard": [17, 315, 317], "24": [17, 315, 317, 412, 413], "complextensor": [18, 19, 25, 36, 37, 75, 76, 92, 93, 168, 169, 171, 172, 244, 264, 265, 283, 284, 286, 287, 353], "memory_config": [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 190, 191, 192, 193, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 219, 221, 222, 223, 224, 226, 227, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 242, 243, 244, 245, 246, 247, 248, 250, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 263, 264, 265, 266, 267, 268, 269, 270, 271, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 316, 318, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 337, 338, 339, 340, 341, 342, 343, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 407, 408, 409, 411, 415], "element": [18, 20, 22, 24, 26, 35, 39, 41, 43, 46, 58, 62, 63, 65, 67, 73, 77, 78, 79, 80, 85, 86, 88, 99, 104, 106, 107, 109, 111, 113, 114, 118, 121, 123, 128, 130, 133, 140, 141, 142, 144, 145, 147, 148, 149, 150, 151, 153, 156, 158, 162, 163, 165, 173, 174, 175, 176, 177, 178, 185, 186, 188, 190, 194, 199, 200, 202, 204, 207, 209, 210, 213, 215, 217, 219, 224, 226, 234, 238, 239, 242, 247, 248, 250, 253, 254, 261, 268, 269, 272, 273, 274, 275, 276, 278, 281, 286, 291, 292, 293, 296, 297, 302, 307, 309, 311, 313, 322, 323, 325, 327, 328, 330, 337, 341, 343, 346, 348, 350, 354, 356, 358, 359, 361, 363, 365, 375, 384, 386, 388, 391, 393, 403, 408, 413], "wise": [18, 20, 22, 24, 35, 41, 43, 46, 58, 62, 63, 65, 67, 80, 88, 104, 106, 107, 109, 111, 113, 114, 118, 121, 123, 128, 130, 133, 142, 144, 145, 147, 149, 151, 153, 156, 158, 162, 165, 173, 174, 175, 176, 177, 178, 188, 190, 194, 199, 200, 202, 204, 207, 213, 215, 217, 219, 224, 226, 234, 238, 239, 247, 248, 250, 253, 269, 278, 281, 286, 291, 292, 293, 296, 297, 307, 309, 311, 322, 323, 325, 327, 328, 330, 337, 341, 345, 346, 348, 356, 358, 359, 361, 363, 365, 384, 386, 408, 413], "mathrm": [18, 19, 20, 21, 22, 23, 24, 26, 35, 41, 42, 43, 44, 46, 47, 49, 50, 51, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 94, 95, 99, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 118, 121, 122, 123, 127, 128, 129, 130, 131, 133, 134, 141, 142, 143, 144, 145, 147, 148, 149, 151, 152, 153, 154, 156, 157, 158, 159, 162, 163, 165, 166, 167, 173, 174, 175, 176, 177, 178, 185, 186, 188, 189, 190, 192, 194, 195, 196, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 213, 214, 215, 216, 217, 218, 219, 220, 222, 224, 225, 226, 239, 245, 246, 247, 248, 249, 250, 252, 253, 255, 256, 260, 267, 268, 269, 271, 278, 279, 286, 287, 291, 292, 293, 294, 295, 296, 297, 298, 307, 308, 309, 310, 311, 312, 313, 320, 322, 323, 324, 325, 326, 327, 328, 330, 331, 332, 333, 337, 339, 341, 342, 346, 348, 349, 350, 354, 356, 358, 359, 361, 362, 363, 365, 366, 367, 368, 382, 383, 384, 385, 386, 393], "_tensor": [18, 19, 20, 21, 22, 23, 24, 26, 35, 41, 42, 43, 44, 46, 47, 49, 50, 51, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 80, 81, 82, 83, 88, 89, 90, 91, 92, 94, 95, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 118, 121, 122, 123, 127, 128, 129, 130, 131, 133, 134, 141, 142, 143, 144, 145, 147, 148, 149, 151, 152, 153, 154, 156, 157, 158, 159, 162, 163, 165, 166, 167, 173, 174, 175, 176, 177, 178, 185, 186, 188, 189, 190, 192, 194, 195, 196, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 213, 214, 215, 216, 217, 218, 219, 220, 222, 224, 225, 226, 239, 245, 246, 247, 248, 249, 250, 252, 253, 255, 256, 260, 267, 268, 269, 271, 278, 279, 286, 287, 291, 292, 293, 294, 295, 296, 297, 298, 307, 308, 309, 310, 311, 312, 313, 320, 322, 323, 324, 325, 326, 327, 328, 330, 331, 332, 333, 337, 339, 341, 342, 346, 348, 349, 350, 354, 356, 358, 359, 361, 362, 363, 365, 366, 367, 368, 382, 383, 384, 385, 386, 393], "_i": [18, 19, 20, 21, 22, 23, 24, 41, 42, 43, 44, 46, 47, 49, 50, 51, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 95, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 118, 121, 122, 123, 127, 128, 129, 133, 134, 141, 142, 144, 145, 147, 148, 149, 151, 153, 154, 156, 157, 158, 159, 162, 163, 165, 166, 167, 174, 175, 176, 177, 178, 185, 188, 190, 194, 195, 196, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 213, 214, 215, 216, 217, 218, 219, 220, 222, 224, 226, 239, 245, 246, 247, 248, 250, 252, 253, 255, 256, 267, 268, 269, 278, 279, 286, 287, 291, 292, 293, 294, 295, 296, 297, 307, 308, 309, 310, 311, 312, 313, 320, 322, 323, 324, 325, 326, 327, 328, 330, 331, 332, 333, 337, 339, 341, 342, 346, 348, 349, 356, 358, 359, 361, 362, 363, 365, 366, 367, 368, 382, 383, 384, 385, 386, 393], "verb": [18, 19, 20, 21, 22, 23, 41, 42, 43, 44, 46, 49, 50, 51, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 80, 81, 82, 83, 88, 89, 90, 91, 94, 95, 108, 110, 111, 112, 114, 115, 118, 121, 122, 123, 127, 128, 129, 130, 131, 133, 134, 141, 144, 149, 154, 156, 157, 158, 159, 162, 165, 166, 167, 174, 175, 176, 177, 178, 185, 186, 190, 192, 195, 196, 201, 203, 205, 206, 207, 208, 209, 210, 216, 222, 239, 245, 246, 250, 255, 256, 267, 271, 278, 279, 286, 287, 291, 292, 293, 294, 295, 296, 297, 298, 307, 308, 309, 310, 312, 320, 322, 323, 324, 325, 326, 327, 328, 330, 331, 332, 333, 339, 341, 342, 348, 349, 350, 358, 359, 361, 362, 366, 367, 368, 382, 383, 384, 385, 386], "keyword": [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 137, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 219, 221, 222, 223, 224, 225, 226, 227, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 259, 260, 261, 263, 264, 265, 266, 267, 268, 269, 270, 271, 276, 277, 278, 279, 280, 281, 282, 283, 284, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394], "prealloc": [18, 20, 22, 24, 25, 26, 27, 30, 33, 35, 39, 41, 43, 45, 46, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 67, 73, 74, 80, 85, 86, 88, 92, 93, 99, 100, 101, 104, 106, 107, 109, 111, 113, 114, 116, 118, 119, 121, 123, 124, 128, 133, 138, 139, 140, 141, 142, 145, 146, 147, 151, 153, 156, 158, 162, 165, 167, 174, 175, 176, 177, 178, 181, 185, 186, 188, 190, 194, 199, 200, 202, 204, 207, 209, 210, 213, 215, 217, 219, 224, 226, 234, 238, 239, 242, 243, 244, 247, 248, 250, 251, 253, 258, 269, 270, 278, 281, 286, 288, 292, 293, 296, 297, 307, 309, 311, 312, 313, 314, 322, 323, 325, 327, 328, 329, 330, 337, 341, 343, 346, 347, 348, 350, 353, 354, 355, 356, 359, 361, 363, 364, 365, 375, 384, 386, 391, 392, 393, 396], "absolut": [18, 19, 173, 181, 369], "grad_tensor": [19, 21, 23, 25, 27, 29, 31, 37, 42, 44, 45, 48, 49, 51, 54, 64, 66, 68, 70, 74, 76, 81, 83, 89, 91, 93, 95, 98, 101, 108, 110, 112, 115, 116, 119, 122, 124, 127, 129, 132, 134, 146, 155, 157, 159, 161, 164, 166, 169, 187, 191, 193, 196, 201, 203, 205, 206, 208, 211, 212, 222, 223, 232, 237, 244, 246, 251, 265, 267, 270, 277, 279, 282, 284, 287, 294, 295, 299, 301, 308, 310, 312, 314, 320, 324, 326, 329, 331, 333, 338, 340, 342, 347, 349, 351, 353, 355, 362, 364, 366, 368, 385, 392, 394], "list": [19, 21, 23, 24, 25, 27, 29, 31, 32, 37, 42, 44, 45, 48, 49, 51, 52, 54, 64, 66, 68, 70, 73, 74, 76, 81, 83, 84, 89, 91, 93, 95, 98, 100, 102, 104, 108, 110, 112, 115, 116, 122, 124, 127, 129, 132, 134, 135, 141, 142, 146, 151, 155, 157, 159, 161, 164, 166, 169, 185, 186, 187, 188, 191, 193, 196, 197, 201, 203, 205, 206, 208, 209, 210, 211, 212, 213, 217, 219, 222, 223, 224, 230, 232, 233, 234, 237, 238, 244, 246, 247, 248, 251, 254, 261, 262, 263, 265, 267, 270, 271, 276, 277, 279, 280, 282, 284, 287, 294, 295, 299, 301, 308, 310, 312, 313, 314, 320, 324, 326, 329, 331, 333, 334, 338, 340, 342, 343, 344, 347, 349, 350, 351, 353, 355, 356, 360, 362, 364, 366, 368, 375, 380, 385, 386, 387, 388, 392, 393, 394, 399, 405, 409, 414], "backward": [19, 21, 23, 25, 27, 29, 31, 37, 42, 44, 45, 48, 49, 51, 54, 64, 66, 68, 70, 74, 76, 81, 83, 89, 91, 93, 95, 98, 101, 108, 110, 112, 115, 116, 119, 122, 124, 127, 129, 132, 134, 146, 155, 157, 159, 161, 164, 166, 169, 187, 191, 193, 196, 201, 203, 205, 206, 208, 211, 212, 222, 223, 232, 237, 244, 246, 251, 265, 267, 270, 277, 279, 282, 284, 287, 294, 295, 299, 301, 308, 310, 312, 314, 320, 324, 326, 329, 331, 333, 338, 340, 342, 347, 349, 351, 353, 355, 362, 364, 366, 368, 385, 392, 394], "given": [19, 21, 23, 25, 27, 29, 31, 37, 39, 42, 44, 45, 48, 49, 51, 54, 64, 66, 68, 70, 74, 76, 79, 81, 83, 85, 86, 89, 91, 93, 95, 98, 99, 103, 108, 110, 112, 115, 116, 122, 124, 127, 129, 132, 134, 146, 150, 155, 157, 159, 161, 164, 166, 169, 187, 191, 193, 196, 201, 203, 205, 206, 208, 211, 212, 222, 223, 232, 237, 240, 241, 244, 246, 251, 259, 262, 265, 267, 270, 277, 279, 280, 282, 284, 287, 294, 295, 299, 301, 302, 308, 310, 312, 314, 318, 320, 324, 326, 329, 331, 333, 338, 340, 342, 347, 349, 351, 353, 355, 362, 364, 366, 368, 375, 380, 385, 389, 392, 394, 397, 408, 409, 412], "grad": [19, 21, 23, 42, 44, 49, 51, 64, 81, 83, 89, 91, 95, 108, 110, 112, 115, 122, 127, 129, 134, 157, 159, 166, 196, 201, 203, 205, 206, 208, 222, 246, 267, 279, 287, 294, 295, 301, 308, 310, 312, 320, 324, 326, 331, 333, 342, 349, 362, 366, 368, 385], "gradient": [19, 21, 23, 25, 27, 29, 31, 37, 42, 44, 45, 48, 49, 51, 54, 64, 66, 68, 70, 74, 76, 79, 81, 83, 89, 91, 93, 95, 98, 101, 108, 110, 112, 115, 116, 119, 122, 124, 127, 129, 132, 134, 140, 146, 155, 157, 159, 161, 164, 166, 169, 187, 191, 193, 196, 201, 203, 205, 206, 208, 211, 212, 222, 223, 232, 237, 244, 246, 251, 265, 267, 270, 277, 279, 282, 284, 287, 294, 295, 299, 301, 308, 310, 312, 314, 320, 324, 326, 329, 331, 333, 338, 340, 342, 347, 349, 351, 353, 355, 362, 364, 366, 368, 385, 392, 394, 409], "about": [19, 29, 54, 132, 146, 201, 203, 205, 206, 208, 211, 212, 222, 223, 270, 277, 286, 287, 299, 338, 345, 347, 394, 403, 405, 407, 408, 411, 414], "requires_grad": [19, 21, 23, 25, 27, 29, 31, 42, 44, 45, 48, 49, 51, 54, 64, 66, 68, 70, 74, 81, 83, 89, 91, 93, 95, 98, 101, 108, 110, 112, 115, 116, 119, 122, 124, 127, 129, 132, 134, 146, 155, 157, 159, 161, 164, 166, 187, 191, 193, 196, 201, 203, 205, 206, 208, 211, 212, 222, 223, 232, 237, 244, 246, 251, 267, 270, 277, 279, 282, 287, 294, 295, 299, 301, 308, 310, 312, 314, 320, 324, 326, 329, 331, 333, 338, 340, 342, 347, 349, 351, 353, 355, 362, 364, 366, 368, 385, 392, 394], "arccosin": [20, 21, 22, 23], "invers": [21, 23, 42, 44, 49, 51, 111, 112], "cosin": [21, 23, 80, 81, 82, 83, 120, 409], "hyperbol": [22, 23, 43, 44, 50, 51, 82, 83, 332, 333, 363, 364], "datatyp": [24, 32, 38, 40, 52, 53, 55, 71, 77, 78, 79, 85, 86, 100, 101, 102, 103, 104, 135, 137, 138, 139, 141, 142, 148, 150, 151, 182, 183, 184, 185, 186, 188, 197, 209, 210, 213, 217, 219, 224, 230, 233, 234, 238, 247, 248, 257, 258, 272, 273, 274, 275, 280, 304, 305, 306, 313, 336, 344, 350, 356, 372, 373, 393, 395, 396, 405, 407, 408, 412, 414], "_a": [24, 26, 47, 56, 57, 59, 60, 61, 92, 94, 104, 105, 130, 131, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 247, 248, 249, 252, 260, 271, 298, 313, 350, 354, 356, 393], "_b": [24, 26, 47, 56, 57, 59, 60, 61, 92, 94, 104, 105, 130, 131, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 247, 248, 249, 252, 260, 271, 298, 313, 350, 354, 356, 393], "elementwis": [24, 26, 47, 56, 57, 59, 60, 61, 92, 94, 104, 105, 130, 131, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 350, 354, 356, 393], "independ": [24, 26, 33, 47, 56, 57, 59, 60, 61, 73, 92, 94, 104, 105, 130, 131, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 350, 354, 356, 393], "operand": [24, 26, 47, 56, 57, 59, 60, 61, 92, 94, 104, 105, 130, 131, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 350, 354, 356, 393], "dnchw": [24, 26, 47, 56, 57, 59, 60, 61, 92, 94, 104, 105, 130, 131, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 350, 354, 356, 393], "ani": [24, 26, 47, 56, 57, 59, 60, 61, 92, 94, 104, 105, 125, 126, 130, 131, 135, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 247, 248, 249, 252, 260, 261, 268, 269, 271, 276, 298, 313, 350, 354, 356, 393, 398, 403, 409, 414], "expand": [24, 26, 47, 56, 57, 59, 60, 61, 92, 94, 104, 105, 130, 131, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 302, 313, 350, 354, 356, 393, 403, 408], "duplic": [24, 26, 47, 56, 57, 59, 60, 61, 92, 94, 104, 105, 130, 131, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 350, 354, 356, 393], "16": [24, 26, 47, 55, 56, 57, 59, 60, 61, 92, 94, 104, 105, 130, 131, 141, 142, 143, 150, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 247, 248, 249, 252, 260, 261, 268, 269, 271, 298, 313, 334, 344, 350, 354, 356, 393, 403, 408, 411, 412], "higher": [24, 26, 47, 56, 57, 59, 60, 61, 92, 94, 104, 105, 130, 131, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 337, 350, 354, 356, 393, 402, 403], "attempt": [24, 26, 47, 56, 57, 59, 60, 61, 92, 94, 104, 105, 130, 131, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 350, 354, 356, 393], "best": [24, 26, 47, 56, 57, 59, 60, 61, 92, 94, 104, 105, 130, 131, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 350, 354, 356, 393, 397, 409], "decis": [24, 26, 47, 56, 57, 59, 60, 61, 92, 94, 104, 105, 130, 131, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 350, 354, 356, 393], "consider": [24, 26, 47, 56, 57, 59, 60, 61, 92, 94, 104, 105, 130, 131, 141, 142, 143, 151, 152, 163, 173, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 234, 238, 247, 248, 249, 252, 260, 268, 269, 271, 298, 313, 350, 354, 356, 393], "int32": [24, 39, 55, 56, 57, 58, 59, 60, 61, 67, 85, 86, 92, 104, 106, 123, 140, 141, 142, 147, 151, 153, 185, 188, 194, 213, 214, 215, 216, 217, 218, 219, 220, 224, 226, 234, 238, 247, 248, 253, 318, 327, 348, 350, 356, 375, 391, 413], "uint32": [24, 39, 55, 56, 57, 59, 60, 61, 85, 86, 100, 101, 104, 106, 123, 140, 167, 170, 213, 214, 215, 216, 217, 218, 219, 220, 229, 247, 248, 253, 343, 348, 350, 356, 375, 402, 403, 409], "4294967295": [24, 215, 356], "uint16": [24, 39, 55, 56, 59, 61, 104, 106, 138, 140, 167, 213, 214, 215, 217, 218, 219, 220, 247, 248, 253, 257, 343, 348, 350, 356, 375, 395, 403], "65535": [24, 56, 59, 61, 215, 247, 356], "two": [24, 26, 45, 47, 56, 59, 61, 71, 73, 92, 94, 104, 105, 130, 131, 141, 142, 143, 144, 149, 151, 152, 163, 173, 183, 184, 185, 186, 188, 189, 209, 210, 213, 214, 217, 218, 219, 220, 224, 225, 230, 234, 238, 242, 247, 248, 249, 252, 260, 269, 291, 298, 303, 305, 306, 313, 316, 317, 343, 344, 350, 354, 356, 358, 372, 381, 391, 393, 397, 400, 403, 404, 405, 409, 410, 411, 412, 414], "tensor1": [24, 25, 26, 27, 28, 29, 30, 31, 32, 45, 47, 48, 54, 56, 57, 59, 60, 61, 73, 74, 92, 93, 94, 104, 105, 130, 131, 132, 141, 142, 143, 151, 152, 163, 164, 173, 185, 186, 187, 188, 189, 192, 193, 209, 210, 211, 212, 213, 214, 217, 218, 219, 220, 224, 225, 227, 230, 232, 234, 237, 238, 244, 247, 248, 249, 252, 260, 269, 271, 298, 299, 313, 314, 344, 350, 351, 353, 354, 355, 356, 391, 392, 393, 394], "tensor2": [24, 25, 26, 27, 28, 29, 30, 31, 32, 45, 47, 48, 54, 56, 57, 59, 60, 61, 73, 74, 92, 93, 94, 104, 105, 130, 131, 132, 141, 142, 143, 151, 152, 163, 164, 173, 185, 186, 187, 188, 189, 192, 193, 209, 210, 211, 212, 213, 214, 217, 218, 219, 220, 224, 225, 227, 230, 232, 234, 237, 238, 244, 247, 248, 249, 252, 260, 269, 298, 299, 313, 314, 344, 350, 351, 353, 354, 355, 356, 391, 392, 393, 394], "are_required_output": [25, 27, 45, 74, 93, 244, 314, 353, 355, 392], "input_grad": [25, 27, 45, 74, 93, 119, 244, 314, 353, 355], "other_grad": [25, 27, 45, 74, 93, 244, 314, 353, 355], "scalar": [25, 28, 30, 67, 69, 92, 93, 95, 132, 138, 139, 193, 213, 229, 231, 234, 235, 236, 244, 267, 271, 276, 281, 282, 299, 352, 353, 357, 390, 415], "bfloat4_b": [25, 27, 28, 32, 35, 48, 54, 74, 93, 137, 164, 187, 192, 193, 197, 211, 212, 227, 230, 232, 237, 244, 280, 314, 344, 351, 353, 355, 363, 365, 374, 391, 394], "alpha": [26, 27, 29, 31, 32, 65, 66, 93, 97, 98, 99, 319, 354, 355], "float": [26, 27, 28, 29, 30, 31, 32, 53, 65, 66, 68, 69, 70, 95, 97, 98, 99, 118, 123, 126, 131, 137, 138, 139, 150, 154, 155, 160, 161, 162, 173, 182, 183, 190, 191, 192, 221, 223, 231, 235, 236, 252, 261, 263, 267, 268, 269, 271, 280, 281, 282, 296, 297, 304, 305, 309, 310, 315, 316, 317, 319, 321, 337, 338, 339, 340, 352, 354, 355, 357, 367, 368, 379, 380, 382, 383, 390, 403, 409], "input_tensor_c": [28, 29, 31, 193, 227, 392], "three": [28, 29, 30, 31, 183, 192, 193, 227, 305, 392, 410, 413], "tensor3": [28, 29, 30, 31, 192, 193, 227, 391, 392], "input_a": [30, 105, 143, 152, 170, 189, 225, 231, 235, 236, 249, 352, 357, 390], "input_b": [30, 105, 143, 152, 170, 189, 225, 249], "input_c": 30, "second": [30, 32, 45, 137, 144, 149, 150, 183, 197, 230, 261, 271, 291, 303, 305, 343, 344, 358, 402, 409, 411, 412, 415], "third": 30, "ttt": 30, "beta": [32, 53, 184, 306, 337, 338], "matmulprogramconfig": [32, 197, 230], "compute_kernel_config": [32, 52, 53, 71, 99, 117, 120, 150, 182, 183, 184, 197, 230, 231, 235, 236, 304, 305, 306, 315, 316, 317, 335, 336, 344, 352, 357, 379, 380, 390], "devicecomputekernelconfig": [32, 52, 53, 77, 78, 79, 117, 120, 150, 182, 183, 184, 197, 230, 272, 273, 274, 275, 304, 305, 306, 315, 316, 317, 335, 336, 344, 379, 380], "coregrid": [32, 84, 99, 150, 197, 230, 344, 407, 411], "output_til": [32, 197, 230, 344], "global_cb": 32, "globalcircularbuff": 32, "sub_device_id": [32, 360], "subdeviceid": [32, 33, 34, 288, 360], "product": [32, 78, 85, 230, 260, 276, 277, 344, 379, 380, 409], "p": [32, 230, 403, 409, 414], "content": [32, 96, 198, 409], "overwritten": 32, "object": [32, 84, 259, 321, 397, 401, 409], "factor": [32, 99, 231, 235, 236, 315, 316, 317, 352, 357, 390, 409], "look": [32, 150, 230, 242, 375, 398, 399, 402, 403, 409, 414], "dram_memory_config": [32, 36, 37, 38, 71, 75, 76, 102, 103, 135, 150, 168, 169, 171, 172, 197, 230, 264, 265, 280, 283, 284, 344, 371, 403, 408, 409], "highest": [32, 409, 410, 412], "precis": [32, 321, 323, 363, 365, 403, 409, 410], "tbd": 32, "128": [32, 99, 197, 230, 402, 406, 408, 410, 411, 412, 415], "cluster_axi": [33, 34, 288], "subdevice_id": [33, 34, 288], "num_link": [33, 34, 288], "sub_core_grid": [33, 105, 107, 109, 113, 128, 131, 140, 143, 145, 152, 189, 199, 200, 202, 204, 225, 226, 229, 231, 235, 236, 249, 298, 303, 311, 318, 334, 346, 352, 357, 369, 375, 384, 387, 390, 391], "cluster": [33, 34, 288, 406, 407, 408, 410, 411, 412, 414], "axi": [33, 34, 85, 86, 288, 302, 408], "concaten": [33, 73, 74, 378, 381, 409], "non": [33, 52, 92, 230, 233, 254, 270, 281, 286, 309, 344, 409], "subdevic": [33, 34, 288], "id": [33, 34, 137, 228, 229, 259, 288, 360, 374, 402, 406, 407, 408, 409, 410, 411, 412, 413, 414], "worker": [33, 34, 288, 406, 408, 410, 411, 412, 413], "link": [33, 34, 288], "fabric": [33, 34, 288], "output_shap": [33, 276, 288], "num_devic": [33, 288], "total": [33, 288, 345, 402, 410, 412], "full_tensor": [33, 34], "randn": [33, 34, 40, 52, 101, 137, 233, 318, 371, 386, 411], "256": [33, 34, 52, 233, 288, 412, 413], "mesh_devic": [33, 34, 406, 408, 410, 411, 412], "open_mesh_devic": [33, 34], "meshshap": [33, 34], "ttnn_tensor": [33, 34, 87, 96, 137, 198, 285, 288, 371, 372, 373, 374], "input_dtyp": [33, 34, 272, 273, 274, 275, 409], "mem_config": [33, 34], "mesh_mapp": [33, 34, 40, 137], "shardtensor2dmesh": [33, 34], "mesh_shap": [33, 34], "print": [33, 34, 228, 259, 288, 321, 345, 402, 403, 406, 407, 412, 415], "otherwis": [34, 52, 173, 276, 360, 375, 391, 411, 415], "_": [35, 99, 397, 402, 411], "2i": 35, "even": [35, 87, 228, 230], "altern": [35, 73, 230, 408, 410, 415], "complex": [35, 36, 37, 75, 76, 168, 169, 171, 172, 264, 265, 283, 284, 406, 412], "rotat": 35, "90": [35, 336, 412], "degre": [35, 88, 89, 278, 279], "real_part": [36, 75, 168, 171, 172, 283], "imag_part": [36, 75, 168, 171, 172, 283], "complex_tensor": [36, 37, 75, 76, 168, 169, 171, 172, 264, 265, 283, 284], "phase": 36, "input_r": [37, 76, 169, 265, 284], "input_imag": [37, 76, 169, 265, 284], "regular": [37, 169, 284, 413], "grad_data": [37, 101, 169, 284], "end": [38, 85, 86, 92, 173, 192, 252, 334, 388, 398, 402, 408, 409, 411, 413, 414], "inclus": [38, 280, 403], "exclus": [38, 261, 276, 280], "keepdim": [39, 231, 235, 236, 276, 352, 357, 390, 409], "tensor_input": [39, 85, 86, 99, 231, 235, 236, 352, 357, 375, 390], "yield": [39, 231, 235, 236, 352, 357, 390, 412], "output_onedim": 39, "onedim": 39, "output_alldim": 39, "alldim": 39, "cache_file_nam": 40, "pathlib": [40, 96, 198, 413], "path": [40, 96, 198, 400, 402, 410, 412, 413, 414, 415], "callabl": [40, 240, 241], "cpptensortomesh": 40, "serial": 40, "tensortomesh": [40, 137], "truncat": [40, 384, 385], "mantissa": 40, "bit": [40, 55, 327], "bfp": [40, 403], "rais": [40, 374, 409], "runtim": [40, 344, 413, 414], "error": [40, 107, 108, 109, 110, 111, 112, 181, 228, 230, 243, 374, 398, 406, 408, 410, 411, 412], "rte": 40, "bfp8": 40, "bfp4": 40, "arcsin": [41, 42, 43, 44], "sine": [42, 44, 120, 330, 331, 332, 333], "round_mod": [45, 92, 93, 281, 282], "assign": [45, 229], "other_tensor": [45, 93], "arctang": [46, 47, 49, 50, 51], "arctan": 47, "right": [47, 60, 92, 141, 150, 185], "quadrant": 47, "handl": [47, 409, 413], "tangent": [49, 51, 361, 362, 363, 364], "batch_siz": [52, 77, 78, 79, 101, 148, 233, 272, 273, 274, 275, 304, 378, 381, 397, 406, 409, 410, 411, 412, 413], "input_h": [52, 233, 412], "input_w": [52, 233, 412], "ceil_mod": [52, 233, 412], "count_include_pad": 52, "divisor_overrid": 52, "applied_shard_schem": [52, 233], "deallocate_input": [52, 233], "averag": [52, 99, 118, 148, 402], "window": [52, 233, 379, 380, 406, 409, 412], "scheme": [52, 233, 272, 273, 274, 275], "convolv": [52, 77, 78, 79, 233, 272, 273, 274, 275], "formula": [52, 66, 98, 155, 191, 192, 223, 338, 340], "avg": 52, "region": [52, 78, 125, 126, 259, 409], "createdevic": [52, 233], "l1_small_siz": [52, 233, 259, 406, 409, 412, 413], "8192": [52, 233, 375, 406, 409, 412, 413], "kernel_w": [52, 233], "stride_h": [52, 233], "stride_w": [52, 233], "nchw_shape": [52, 233], "40": [52, 233], "in_n": [52, 233], "in_c": [52, 233], "in_h": [52, 233], "in_w": [52, 233], "input_perm": [52, 233], "input_reshap": [52, 233], "tt_input": [52, 233], "ep": [53, 221, 223], "1e": [53, 150, 173, 182, 183, 304, 305], "05": [53, 408], "momentum": 53, "running_mean": 53, "running_var": 53, "train": [53, 409, 410, 412, 414], "norm": [53, 150, 182, 183, 184, 304, 305, 306, 409], "see": [53, 150, 182, 183, 184, 304, 305, 306, 344, 398, 399, 401, 402, 405, 406, 408, 410, 411, 412, 414, 415], "spatial": [53, 148, 389, 406], "gamma": [53, 90, 184, 195, 196, 245, 246, 306], "epsilon": [53, 150, 182, 183, 184, 221, 223, 304, 305, 306], "mu": [53, 150, 182, 183, 184], "sigma": [53, 150, 182, 183, 184], "cdot": [53, 85, 86, 150, 182, 183, 184, 304, 305, 306, 393, 408], "varianc": [53, 150, 182, 183, 184, 305, 403], "learnabl": [53, 150, 182, 183, 184, 409], "shift": [53, 57, 60, 150, 182, 183, 184, 304, 305, 306], "updat": [53, 180, 398, 401, 410, 412, 415], "evalu": [53, 268], "These": [53, 230, 398, 402, 404, 408, 409, 410, 414, 415], "approxim": [54, 107, 109, 113, 119, 145, 146, 199, 200, 202, 204, 311, 322, 323, 346, 363, 365], "bias_gelu": 54, "reinterpret": 55, "unlik": [55, 409, 415], "typecast": [55, 409], "pair": [55, 141, 185], "16457": 55, "16429": 55, "32641": 55, "31744": 55, "integ": [56, 57, 58, 59, 60, 61, 63, 85, 128, 141, 185, 229, 261, 269, 280, 380, 384, 413], "bitwis": [56, 57, 58, 59, 60, 61], "AND": [56, 213, 214], "shift_bit": [57, 60], "31": [57, 60, 125, 126, 229, 405, 408], "2147483647": [58, 141], "NOT": [58, 215, 216], "xor": [61, 219, 220], "cube": 62, "root": [62, 304, 305, 306, 311, 312, 346, 347, 376, 377, 410, 412, 414], "smallest": [63, 375], "min_tensor": [67, 69], "max_tensor": [67, 69], "min_val": [68, 70, 160], "max_val": [68, 70, 160], "computekernelconfig": [71, 99, 231, 235, 236, 352, 357, 390], "alter": 71, "target": [71, 276, 398, 412], "l1_memory_config": [71, 371, 373, 397, 403, 407, 411, 415], "unpad": [71, 372, 388], "necessari": [71, 398, 404, 409, 410], "remov": [72, 374, 388], "success": [72, 406, 414], "group": [73, 77, 78, 79, 150, 272, 273, 274, 275, 345, 403, 406, 409, 412], "partit": 73, "recombin": 73, "residu": [73, 184, 306, 409, 413], "concatenated_tensor": 73, "30": [74, 318, 405, 408, 409, 412], "conjug": [75, 76], "grad_real": [76, 265], "grad_imag": [76, 265], "weight_tensor": [77, 78, 79, 117, 274, 275, 406, 409, 412], "in_channel": [77, 78, 79, 272, 273, 274, 275, 406, 409, 412], "out_channel": [77, 78, 79, 272, 273, 274, 275, 406, 409, 412], "input_length": 77, "bias_tensor": [77, 79, 117, 406, 412], "conv_config": [77, 78, 79, 272, 273, 274, 275, 406, 412], "compute_config": [77, 78, 79, 272, 273, 274, 275], "return_output_dim": [77, 78, 79, 409], "return_weights_and_bia": [77, 78, 79, 409], "length": [77, 271, 379, 380, 409, 413], "signal": [77, 78, 117, 148], "compos": [77, 78, 79, 117, 148, 374, 409, 410, 412], "sever": [77, 78, 79, 117, 148, 411, 412], "plane": [77, 78, 79, 117, 148], "kernel_height": [77, 78, 274, 275], "kernel_width": [77, 78, 274, 275], "cross": [77, 79, 272, 273, 274, 275], "correl": [77, 79, 272, 273, 274, 275, 398, 414], "side": [77, 78, 79, 261, 272, 273, 274, 275, 402], "pad_length": 77, "pad_left": [77, 78, 79, 272, 273, 274, 275], "pad_right": [77, 78, 79, 272, 273, 274, 275], "connect": [77, 78, 79, 272, 273, 274, 275, 409, 410, 412, 413, 414], "bias": [77, 79, 182, 183, 184, 397, 409, 410, 411, 412], "input_height": [78, 79, 272, 273, 274, 275, 406, 409, 412], "input_width": [78, 79, 272, 273, 274, 275, 406, 409, 412], "slice_config": 78, "travers": [78, 84], "4d": [78, 150, 242, 335, 375, 409], "dot": [78, 230, 379, 380, 409], "overlap": [78, 150, 409], "tech": 78, "typic": [78, 99, 148, 315, 403, 407, 409, 412], "pad_height": [78, 79, 272, 273, 274, 275], "pad_width": [78, 79, 272, 273, 274, 275], "pad_top": [78, 79, 272, 273, 274, 275], "pad_bottom": [78, 79, 272, 273, 274, 275], "mirror_kernel": 79, "dram_slice_config": 79, "seen": [79, 84, 407, 409], "fraction": [79, 133, 134], "deconvolut": 79, "o": [79, 400, 402, 409, 410, 411, 412, 413], "k_h": 79, "k_w": 79, "equat": 79, "h_out": [79, 409], "h_in": 79, "output_pad": 79, "w_out": [79, 409], "w_in": 79, "mirror": [79, 231, 235, 236, 352, 357, 390], "been": [79, 240, 241, 360, 398, 409, 414], "shardstrategi": 84, "use_height_and_width_as_shard_shap": 84, "math": [84, 150, 402, 409], "320": 84, "reverse_ord": [85, 86], "cumul": [85, 86], "_1": [85, 86], "_2": [85, 86, 409], "desir": [85, 86, 103, 125, 126, 137, 372, 373, 374], "cast": [85, 86], "accumul": [85, 86], "begin": [85, 86, 92, 173, 252, 402, 408], "tensor_output": [85, 86, 99, 231, 235, 236, 242, 352, 357, 390], "With": [85, 86, 99, 404], "preallocated_output": [85, 86, 99], "resourc": [87, 409, 414], "whose": [87, 103], "radian": [88, 89, 264, 278, 279], "logarithm": [90, 199, 200, 201, 204, 205, 206, 207, 246], "deriv": 90, "accurate_mod": 92, "_mode": 92, "pcc": [93, 164, 187, 282, 398, 401, 414], "degrad": [93, 164, 187, 282, 406, 408, 410, 411, 412], "nan": [94, 95, 173, 176], "denomin": [95, 281], "file_nam": [96, 198, 415], "dump": [96, 198, 240, 414, 415], "save": [96, 336], "tensorbin": [96, 198], "exponenti": [99, 113, 115, 116, 269, 323, 413], "_t": 99, "_0": 99, "smooth": 99, "99": [99, 412, 413], "padding_idx": 100, "embeddings_typ": 100, "embeddingstyp": 100, "retriev": [100, 409], "word": 100, "lookup": [100, 409], "output_gradient_tensor": 101, "extract": [101, 140, 325, 382, 383, 409, 412], "vocabulari": [101, 409, 413], "seq_len": [101, 120, 409, 413], "embedding_dim": 101, "num_embed": 101, "1024": [101, 407, 412], "4096": 101, "3200": 101, "input_index": 101, "randint": [101, 170, 318, 413], "weights_shap": 101, "weights_ttnn": 101, "grad_shap": 101, "uniniti": [102, 103], "bfloat_8": 102, "reference_tensor": [103, 139, 258, 396], "inplac": [105, 143, 150, 152, 189, 214, 216, 218, 220, 225, 249], "_tensor_i": [106, 147, 153, 194, 215, 226, 253], "fast_and_approximate_mod": [107, 109, 113, 145, 199, 200, 202, 204, 311, 322, 323, 346, 363, 365], "fast": [107, 109, 113, 145, 199, 200, 202, 204, 311, 322, 323, 346, 413], "complementari": [109, 110], "conv3dconfig": 117, "3d": 117, "d": [117, 400], "kd": 117, "kh": [117, 406], "kw": [117, 406], "c_in": [117, 409], "c_out": [117, 409], "probabl": [118, 409], "rng": 118, "total_elem": 118, "124": 118, "prob": [118, 409], "algorithm": [119, 150, 323, 389], "cod_cach": 120, "sin_cach": 120, "token_index": 120, "rotari": 120, "cos_cach": 120, "token_idx": 120, "assum": [120, 360, 389], "head_dim": [120, 409], "fill_valu": [123, 138, 139, 403, 405, 408, 409], "hone": [125, 126], "wone": [125, 126], "val_hi": [125, 126], "val_lo": [125, 126], "ye": [125, 126], "96": [125, 126, 412, 415], "33": [125, 126], "xt": [125, 126], "tolist": [125, 126], "Ones": 125, "nchw": [126, 412], "hw": [126, 256, 315], "hfill": 126, "wfill": 126, "hi": 126, "lo": 126, "low": [126, 280, 369], "largest": [128, 375, 403], "point": [131, 192, 252, 280, 321, 337, 402, 403], "modulo": 131, "unless": [135, 415], "bfloat4": 135, "bfloat8": [135, 375], "param": [136, 242, 261, 371], "ttnn_tensor_on_devic": 136, "ttnn_tensor_on_host": 136, "pad_valu": [137, 263, 303, 334, 370], "cq_id": [137, 360, 374], "itself": [137, 409], "twice": [137, 402], "purpos": [137, 398, 401, 403, 409, 413, 414], "now": [137, 183, 184, 305, 306, 343, 374, 389, 403, 405, 409, 411], "mapper": 137, "queue": [137, 259, 360, 374], "torch_tensor": [137, 374, 408], "templat": [139, 258, 396, 402], "exce": 140, "do": [140, 345, 398], "sparse_grad": 140, "spars": [140, 344], "ttnn_input": 140, "ttnn_index": 140, "greatest": 141, "2147483648": 141, "gate": [144, 149, 291, 358], "unit": [144, 149, 291, 345, 358, 398, 400, 409], "adapt": 148, "entir": [148, 360, 369, 409, 414], "global": [148, 255, 402, 409, 414], "_avg": 148, "_pool": 148, "num_group": 150, "num_out_block": 150, "negative_mask": 150, "use_welford": 150, "groupnorm": 150, "tradition": 150, "slightli": 150, "form": [150, 264, 389], "determine_expected_group_norm_sharded_config_and_grid_s": 150, "create_group_norm_input_mask": 150, "create_group_norm_weight_bias_rm": 150, "properli": [150, 402, 406, 413], "cb": [150, 402], "fact": 150, "rm": [150, 304, 305, 306, 400], "welford": 150, "fp32": 150, "draw": 150, "upon": [150, 398], "rather": 150, "torch_input_tensor": [150, 415], "torch_weight": 150, "torch_bia": 150, "grid_siz": [150, 345], "num_channel": 150, "input_nhw": 150, "is_height_shard": 150, "is_row_major": 150, "block_wt": 150, "As": [150, 405, 407, 411], "input_mask_tensor": 150, "everi": [150, 276, 398, 402, 408, 411, 414, 415], "half": 150, "32x32": [150, 403, 408, 409], "num_cores_across_channel": 150, "explain": [150, 402], "suppli": 150, "data_typ": [150, 411], "isn": [150, 403], "Then": [150, 381, 397, 400, 414], "tiles_per_core_tot": 150, "num_cores_x": [150, 397, 411], "gamma_t": 150, "beta_t": 150, "480": 150, "input_tensor_row_major": 150, "input_tensor_til": 150, "tilize_with_zero_pad": 150, "use_multicor": [150, 261, 369, 370, 387, 388], "width_per_group": 150, "max_tiles_group_can_span": 150, "values_per_chunk": 150, "values_per_chunk_per_til": 150, "gamma_beta": 150, "lambd": [154, 155, 339, 340], "hard": [154, 155, 156, 157, 158, 159, 160, 161, 337], "shrinkag": [154, 339], "shrink": [154, 155, 339, 340, 365, 366], "lambda": [155, 340, 397, 409, 413], "hypotenus": [163, 164], "zeroth": 165, "bessel": [165, 166, 231, 235, 236, 352, 357, 390], "sfpu": 167, "shouldn": 167, "sinc": [167, 409], "lower": [167, 261, 280, 297, 382], "uint8": 167, "float16": 167, "unchang": 167, "imaginari": [168, 169, 171], "batch_id": 170, "replac": [170, 367], "denot": 170, "input_a_shap": 170, "input_b_shap": 170, "batch_id_ttnn": 170, "pure": [171, 172], "rtol": 173, "05f": 173, "atol": 173, "08f": 173, "equal_nan": 173, "leq": 173, "rel": 173, "toler": 173, "treat": [173, 230, 315], "finit": 174, "infinit": 175, "infin": [177, 178, 403], "posit": [178, 309, 379, 380, 409, 413], "batch_index": 179, "popul": [179, 240, 402], "update_index": 180, "batch_offset": 180, "input_refer": [181, 243], "input_predict": [181, 243], "predict": [181, 243, 410, 412, 413], "programconfig": [182, 183, 184, 304, 305, 306], "unshard": [182, 184, 304, 306, 317, 403], "cannot": [182, 183, 184, 304, 305, 306, 345, 409], "tile_height": [182, 183, 184, 305, 306], "tile_width": [182, 183, 304, 305, 316, 317], "stick": 182, "conjunct": [183, 184, 305, 306], "layernorm": [183, 409], "statist": [183, 305, 409], "distributed_program_config": [183, 184, 305, 306], "layernormdistributeddefaultprogramconfig": [183, 184, 305, 306], "per": [183, 345, 379, 403, 407, 414], "On": [183, 184, 305, 306, 400], "sum_": [184, 268, 304, 305, 306, 335, 336], "Its": [184, 306], "combin": [184, 230, 306, 405, 409, 410], "stat": [184, 306], "32767": 185, "32768": 185, "80": [187, 412], "outsid": 187, "negative_slop": [190, 191], "slope": [190, 191], "leaki": [190, 191], "01": [191, 414], "interpol": [192, 409], "transpose_a": [197, 230], "transpose_b": [197, 230, 409], "behaviour": [197, 230], "loaded_tensor": 198, "natur": [199, 206, 409], "1e7": 202, "use_legaci": 213, "land": [219, 220], "lnot": [219, 220], "lor": [219, 220], "logitep": 223, "context": [228, 289, 290, 409, 413], "exit": 228, "occur": 228, "user_id": 229, "pseudo": 229, "prng": 229, "potenti": 229, "associ": [229, 360, 409, 414], "constrain": 229, "meaning": 229, "seed_tensor": 229, "user_id_tensor": 229, "dimension": [230, 246, 403], "although": 230, "variou": [230, 408], "abov": [230, 400, 403, 409], "criteria": 230, "messag": [230, 402, 412, 414], "unexpect": 230, "obviou": 230, "relat": 230, "swap": 230, "j": [230, 335, 336], "extend": 230, "patch": [230, 409], "leverag": 230, "accord": [230, 263, 300, 318], "those": [230, 402], "n_size": 230, "m_size": 230, "k_size": 230, "though": 230, "carefulli": 230, "fix": [230, 406, 409, 414], "problem": 230, "describ": [230, 344, 398], "origin": [231, 235, 236, 240, 241, 262, 276, 343, 352, 357, 390, 398, 401, 409, 411], "subcor": [231, 235, 236, 352, 357, 390], "nd": [231, 235, 236, 352, 357, 390], "return_indic": 233, "dilation_h": 233, "dilation_w": 233, "tt_input_dev": 233, "16777216": 234, "find": [234, 238, 252, 254, 400, 409, 414], "20": [239, 302, 318, 337, 338, 412, 414], "inf": [239, 245, 409], "initialize_model": [240, 241, 397, 413], "model_nam": [240, 241, 397, 409, 413], "convert_to_ttnn": [240, 241, 397], "custom_preprocessor": [240, 241, 397, 413], "dict": [240, 241, 409, 412], "parameterdict": [240, 241], "prefix": [240, 241, 409], "run_model": 240, "reader_patterns_cach": 240, "git": [240, 241, 400], "doesn": [240, 241, 409], "invalid": [240, 241, 242, 344], "preprocessor": [240, 241], "put": [240, 241, 397, 406, 407, 408, 410, 411, 412], "submodul": [240, 241, 400, 410, 412], "ttnn_module_arg": 240, "tmp": [240, 414], "model_graph": 240, "svg": [240, 415], "recomput": [240, 303], "th": 242, "expert": 242, "val": 242, "ind": 242, "expert_mask_tensor": 242, "topk_mask_tensor": 242, "everyth": [242, 406, 407, 408, 410, 411, 412, 414], "kwtype": [242, 303], "locat": [242, 261, 318, 375, 399, 402, 404, 409, 413, 414, 415], "power": [242, 269, 270, 309, 310, 386, 413, 414], "exactli": 242, "expert_mask": 242, "tope_mask": 242, "mse": 243, "multivari": [245, 246], "mvlgamma": 246, "5f": 246, "inequ": [248, 249], "negat": 251, "_float": 252, "neq": 252, "toward": 252, "well": [254, 400, 401, 409], "input_tensor_ttnn": [254, 343], "nonzero_indic": 254, "trace_region_s": 259, "num_command_queu": 259, "dispatch_core_config": [259, 413], "dispatchcoreconfig": [259, 413], "0x7fee88262d30": 259, "worker_l1_s": 259, "default_l1_small_s": 259, "default_trace_region_s": 259, "dispatch": [259, 402, 413], "allocat": 259, "0x7fbac5bfc1b0": 259, "otim": 260, "mutual": [261, 276], "output_tensor_shap": [261, 370], "input_tensor_start": 261, "union": 261, "padded_tensor": 261, "unpadded_shap": 262, "annot": [262, 398], "fixeds": 262, "padded_shap": 262, "tthe": 263, "broken": 263, "permuted_tensor": 263, "cartesian": 264, "theta": 264, "magnitud": [264, 403, 411], "14159": 264, "coordin": [265, 403], "decim": [266, 307, 321], "coeff": 268, "coeffici": [268, 398], "polynomi": 268, "expon": [269, 270, 309, 310, 403], "arrai": [271, 408, 409], "25": [271, 412], "invoc": [272, 273, 274, 275], "exact": [272, 273, 274, 275, 403, 409], "input_memory_config": [272, 273, 274, 275, 409], "input_layout": [272, 273, 274, 275, 409], "output_dtyp": [272, 273, 274, 275], "convtranspose2d": [273, 274], "conv_tranpose2d": 274, "weights_format": [274, 275, 409], "iohw": 274, "has_bia": [274, 275, 409], "term": [274, 275, 401], "oihw": [275, 409], "squeez": [276, 374], "nich": 276, "nc": 276, "definit": 276, "intend": [276, 401], "output_all_dim": 276, "particular": [277, 397, 415], "taken": [277, 391, 409], "all_dims_output": 277, "uniform": [280, 403], "upper": [280, 296, 383, 403, 409], "reproduc": [280, 406], "uniformli": 280, "consid": 281, "numer": [281, 315, 316, 317, 335, 336, 337, 399, 403, 407, 409], "rounding_mod": 281, "revers": [281, 282, 309, 310, 313, 314, 409], "ttnn_tensor_realloc": 285, "inaccur": [286, 403], "fp": 286, "break": [288, 410, 412], "apart": 288, "upper_limit": 296, "cap": 296, "lower_limit": 297, "modulu": 298, "whb0": 299, "repetition_vector": 300, "smallvector": 300, "repetit": [300, 302], "input_tensor_tt": [300, 303], "repeated_tensor": [300, 302], "2x": 301, "he": 302, "cost": 303, "new_shap": 303, "recreate_mapping_tensor": 303, "allevi": 303, "slow": 303, "reshaped_tensor": 303, "rmsnorm": 305, "subract": 314, "numeric_st": [315, 316, 317, 335, 336, 409, 411], "causal": [315, 316, 317, 376, 377, 379, 409], "d_k": 315, "stabl": [315, 316, 317, 335, 336, 343, 405, 409], "hw_dims_onli": 315, "input_til": 315, "tt_output_shard": 315, "is_causal_mask": [316, 317], "commonli": [316, 317], "mechan": [316, 317], "inherit": [316, 335], "restrict": [317, 369, 400], "src": 318, "scatterreductiontyp": 318, "onto": 318, "amax": 318, "amin": 318, "input_torch": 318, "index_torch": 318, "int64": 318, "source_torch": 318, "input_ttnn": 318, "index_ttnn": 318, "source_ttnn": 318, "0507": 319, "67326": 319, "sci_mod": 321, "scientif": 321, "notat": 321, "detect": 321, "digit": [321, 410], "short": [321, 415], "vector_mod": 322, "rc": [322, 400], "accur": 323, "slice_start": 334, "slice_end": 334, "slice_step": 334, "input_tensor_shap": 334, "unmodifi": 334, "undefin": 334, "sliced_tensor": 334, "x_i": [335, 336, 409], "x_j": [335, 336], "consum": 336, "steep": 337, "steeper": 337, "stabil": [337, 401, 409], "soft": [339, 340], "ascend": 343, "descend": 343, "preserv": 343, "sorted_tensor": 343, "sorted_tensor_desc": 343, "indices_desc": 343, "input_tensor_2d": 343, "input_tensor_2d_ttnn": 343, "sorted_tensor_dim": 343, "indices_dim": 343, "nnz": 344, "is_input_a_spars": 344, "is_input_b_spars": 344, "skip": [344, 380, 413], "tabl": [344, 414], "interpret": 344, "sparsity_bitmask": 344, "randperm": 344, "numel": 344, "units_to_divid": 345, "row_wis": 345, "iter": [345, 410, 411], "involv": [345, 413], "units_per_core_group_1": 345, "units_per_core_group_2": 345, "100": [345, 409, 410, 412, 413], "8x8": 345, "units_1": 345, "units_2": 345, "core_rangeset": 345, "255": 348, "queueid": 360, "synchron": [360, 414, 415], "wait": [360, 402], "ran": [360, 402, 411], "chip": [360, 406, 407, 408, 409, 410, 411, 412, 414], "set_sub_device_stall_group": 360, "queu": 360, "45": [361, 362], "faster": [363, 365, 402, 411, 413], "minor": [363, 365], "approx": [363, 365], "use_low_perf": 369, "acceler": [369, 370, 387, 388, 400, 407, 409, 410], "IF": 369, "IN": 369, "tilized_tensor": [369, 370, 387, 388], "tensor_on_host": 371, "organ": [372, 398, 403, 409], "ttnn_tensor_layout_chang": 372, "ttnn_tensor_memory_config_chang": 373, "torch_rank": [374, 415], "mesh_compos": 374, "cppmeshtotensor": 374, "Will": 374, "reach": 374, "indices_tensor": 375, "output_value_tensor": 375, "output_index_tensor": 375, "fundament": 375, "manipul": [375, 410], "restor": 375, "afterward": 375, "satisfi": 375, "nearest": [375, 389], "65536": 375, "head_siz": [376, 377, 378, 381, 409, 411], "attention_mask": [376, 377, 409, 411, 413], "causal_mask": [376, 377], "num_head": [378, 381, 409, 411], "sequence_s": [378, 381, 397, 409, 411, 413], "input_tensor_q": [379, 380], "input_tensor_k": [379, 380], "input_tensor_v": [379, 380], "attn_mask": [379, 380, 409], "is_caus": [379, 380], "sliding_window_s": [379, 380], "sdpaprogramconfig": [379, 380], "attention_sink": 379, "mimick": 379, "flashattent": 379, "accept": [379, 380, 398, 401, 412], "q": [379, 380, 409], "parallel": [379, 380, 402, 408, 409, 414], "nqh": 379, "dh": [379, 380], "nkv": [379, 380], "impli": 379, "slide": [379, 380, 406, 409, 412], "attend": [379, 409], "center": [379, 409], "sink": 379, "replic": 379, "cur_po": 380, "cur_pos_tensor": 380, "decod": 380, "flash": [380, 400], "mqa": 380, "sdpamulticoreprogramconfig": 380, "nh": 380, "pnh": 380, "kv_input_tensor": 381, "num_kv_head": 381, "transpose_kei": 381, "hidden_s": [381, 397, 409, 411, 413], "readi": [381, 398], "score": [381, 409], "q1": 381, "k1": 381, "v1": 381, "qn": 381, "kn": 381, "vn": 381, "cat": [381, 409, 411], "contigu": [381, 403, 412], "num": 381, "diagon": [382, 383, 409], "triangular": [382, 383, 409], "ops_chain": 386, "chain": 386, "use_pack_until": [387, 388], "untilized_tensor": [387, 388], "output_tensor_end": 388, "scale_factor": 389, "array2d": 389, "true_valu": 391, "false_valu": 391, "entri": 391, "basi": 397, "re": [397, 399, 407, 409, 414], "rewritten": 397, "modeling_bert": [397, 413], "bertintermedi": 397, "__init__": [397, 409], "super": 397, "dens": 397, "intermediate_s": 397, "hidden_st": [397, 409, 411, 413], "tdd": 397, "torch_bert": 397, "utility_funct": 397, "torch_random": 397, "utils_for_test": 397, "assert_with_pcc": 397, "mark": [397, 398], "parametr": 397, "phiyodr": [397, 413], "finetun": [397, 402, 413], "squad2": [397, 413], "test_bert_intermedi": 397, "bertconfig": [397, 413], "from_pretrain": [397, 409, 413], "eval": [397, 413], "torch_hidden_st": [397, 411], "torch_output": [397, 411], "bert_intermedi": 397, "9999": [397, 414, 415], "turn": 397, "ttnn_bert": [397, 413], "999": [397, 410], "someth": 397, "ttnn_optimized_bert": [397, 413], "isinst": [397, 409], "preprocess_linear_weight": [397, 411], "preprocess_linear_bia": [397, 411], "ff1_weight": 397, "ff1_bia": 397, "local": [397, 398, 403, 404, 406, 407, 408, 410, 411, 412, 414], "integr": [397, 398], "incredibli": 398, "excit": 398, "exploratori": 398, "under": [398, 401, 402, 414, 415], "folder": [398, 402], "freedom": 398, "showcas": [398, 407], "few": [398, 403, 412], "question": [398, 413], "answer": [398, 413], "highlight": [398, 403], "successfulli": [398, 405], "migrat": [398, 415], "readm": [398, 400], "md": [398, 400], "credit": 398, "author": 398, "might": [398, 407], "encount": 398, "adequ": 398, "pearson": 398, "metric": 398, "meet": 398, "continu": [398, 400, 401], "commit": 398, "ongo": 398, "complianc": 398, "catch": 398, "regress": [398, 409], "earli": 398, "varieti": 398, "measur": [398, 409], "run_device_perf_model": 398, "run_perform": 398, "sh": [398, 400, 402, 414, 415], "models_device_performance_bare_met": 398, "clear": [398, 401, 406, 408, 410, 411, 412], "autom": 398, "extern": [398, 401, 403], "servic": 398, "workflow": [398, 400, 414], "impl": 398, "yaml": 398, "models_performance_bare_met": 398, "run_demos_single_card_n150_test": 398, "run_demos_single_card_n300_test": 398, "run_t3000_demo_test": 398, "test_ttnn_functional_resnet50": 398, "resnet50testinfra": 398, "friendli": 399, "ml": [399, 400], "http": [399, 400, 409, 414], "com": [399, 400, 409], "choic": 399, "jupyt": 399, "notebook": 399, "comprehens": [400, 414], "stack": 400, "deploy": 400, "asset": [400, 409], "tag": 400, "quick": [400, 412], "curl": 400, "fssl": 400, "chmod": 400, "podman": 400, "warn": [400, 402, 406, 408, 410, 411, 412, 413], "galaxi": 400, "6u": 400, "blackhol": 400, "driver": [400, 406, 407, 408, 410, 411, 412, 414], "kmd": [400, 406, 407, 408, 410, 411, 412, 414], "ubuntu": 400, "22": [400, 406, 408, 410, 411, 412], "04": [400, 408], "v2": 400, "fw_pack": 400, "19": [400, 412], "fwbundl": 400, "v19": 400, "v3": 400, "38": [400, 403, 405, 411], "fw": [400, 402, 406, 407, 408, 410, 411, 412], "visit": [400, 414], "immedi": 400, "ai": [400, 406, 408, 409, 410, 411, 412], "closer": 400, "conveni": 400, "who": [400, 403], "linux": 400, "distro": 400, "glibc": 400, "34": [400, 406, 408, 410, 411, 412, 414], "newer": [400, 413], "pip": [400, 414, 415], "cpu": [400, 402, 406, 408, 409, 410, 411, 412], "governor": 400, "export": [400, 414, 415], "pythonpath": 400, "pwd": 400, "python_env": [400, 414], "dev": 400, "txt": [400, 414], "sudo": [400, 402], "apt": 400, "cpufrequtil": 400, "cpupow": 400, "frequenc": 400, "registri": 400, "pull": [400, 401, 406], "ghcr": 400, "io": [400, 402, 409], "amd64": 400, "bash": 400, "recurs": 400, "install_depend": 400, "build_met": [400, 402, 414], "cmake": 400, "mkdir": 400, "cd": [400, 402], "ninja": 400, "dcmake_build_typ": 400, "relwithdebuginfo": 400, "dcmake_cxx_compil": 400, "compil": [400, 407, 410, 411, 415], "envirion": 400, "python_env_dir": 400, "path_to_your_env_directori": 400, "create_venv": 400, "bin": 400, "driven": [400, 401], "recip": 400, "conda": 400, "forg": 400, "python3": [400, 404, 405, 406, 407, 408, 410, 411, 412], "run_op_on_devic": 400, "eth": [400, 413], "rout": 400, "loudbox": 400, "quietbox": 400, "iommu": [400, 406, 407, 408, 410, 411, 412, 414], "level": [400, 402, 405, 408, 414], "isol": 400, "passthrough": 400, "translat": 400, "viommu": 400, "hypervisor": 400, "secur": 400, "dma": 400, "pcie": [400, 407], "guest": 400, "corrupt": 400, "reliabl": [400, 401], "intel_iommu": 400, "amd_iommu": 400, "provis": 400, "remap": 400, "intel": 400, "vt": 400, "amd": 400, "vi": [400, 414], "maintain": 401, "simultan": [401, 414], "tune": [401, 413], "themselv": [401, 403, 409], "goal": 401, "ask": 401, "popular": 401, "kent": 401, "beck": 401, "submit": 401, "label": [401, 403, 410, 412], "fulli": [401, 402, 410, 412], "fallback": 401, "branch": 401, "brief": 401, "4730": 401, "rst": 401, "referenc": 401, "sweep": 401, "codeown": 401, "pr": 401, "reflect": 401, "merg": [401, 409], "main": [401, 405, 406, 408, 409, 410, 411, 412, 414, 415], "comment": 401, "bert_tini": 402, "tt_metal_hom": [402, 405, 406, 407, 408, 410, 411, 412], "test_demo": 402, "input_data": 402, "json": [402, 413, 414, 415], "mrm8488": 402, "tini": [402, 413], "squadv2": 402, "device_params0": 402, "finish": 402, "csv": [402, 414], "consol": 402, "similar": [402, 409, 414], "give": [402, 409, 414], "shorter": 402, "append": 402, "cli": 402, "reset": 402, "tt_smi": 402, "tensix_reset": 402, "tensix": [402, 407, 408], "skew": 402, "timer": 402, "reboot": 402, "wh": 402, "1000": [402, 406, 408, 410, 411, 412], "fixtur": 402, "ttl": 402, "readdeviceprofil": 402, "drop": 402, "around": 402, "120": [402, 412], "eighth": 402, "receiv": 402, "mention": 402, "come": 402, "python_fallback": 402, "tt_dnn_cpu": 402, "tt_dnn_devic": 402, "fidel": 402, "field": 402, "lofi": 402, "hifi2": 402, "hifi3": 402, "clock": 402, "stamp": 402, "durat": [402, 411, 415], "nanosecond": 402, "end_t": 402, "start_t": 402, "cycl": 402, "earliest": 402, "core_frequ": 402, "marker": 402, "brisc": 402, "ncrisc": 402, "trisc0": 402, "trisc1": 402, "trisc2": 402, "front": 402, "spent": [402, 411], "cb_wait_front": 402, "reserv": 402, "cb_reserve_back": 402, "datamov": 402, "input_0_memori": 402, "z": 402, "channels_last": 402, "dev_0_dram": 402, "dec_0_l1": 402, "noc": 402, "timelin": 402, "npe": 402, "subdirectori": 402, "npe_viz": 402, "traffic": 402, "congest": 402, "item": [402, 409, 410, 412], "aggreg": 402, "timestamp": [402, 414], "ops_perf_results_2025_06_25_14_04_34": 402, "2025_06_25_14_04_34": 402, "actual": [403, 410, 412], "still": 403, "transit": 403, "illustr": [403, 409], "insid": [403, 415], "16x16": 403, "li": 403, "fashion": 403, "face0": 403, "face1": 403, "face2": 403, "face3": 403, "pictur": 403, "reason": 403, "matric": [403, 407], "decompos": 403, "transpose_til": 403, "col": 403, "torch_t": 403, "byte": [403, 409], "That": [403, 407], "sizeof": 403, "introduc": 403, "observ": [403, 412], "flush": 403, "instabl": 403, "extrem": 403, "domin": 403, "caus": 403, "lose": 403, "7014118346046923e": 403, "frequent": 403, "occurr": 403, "deal": 403, "critic": 403, "applic": 403, "homogen": 403, "unsuit": 403, "inher": 403, "owned_host_storag": 403, "borrowed_host_storag": 403, "borrow": 403, "numpi": [403, 408, 410], "device_storag": 403, "abstract": 403, "awai": 403, "compress": 403, "remain": 403, "128x128": 403, "subset": 403, "know": [403, 409], "understand": [403, 409, 413, 414], "physic": [403, 407], "task": [404, 409, 410, 413], "smoothli": 404, "lightweight": 404, "minim": 404, "standalon": 404, "basic_python": [404, 405, 406, 407, 408, 410, 411, 412], "llama": 405, "mistral": 405, "diffus": 405, "ttnn_add_tensor": [405, 407], "loguru": [405, 406, 408, 409, 410, 411, 412], "tt_tensor1": 405, "tt_tensor2": 405, "tt_result": 405, "2025": [405, 406, 407, 408, 410, 411, 412, 414], "06": [405, 411], "23": [405, 407], "09": [405, 406, 407, 414], "36": 405, "58": 405, "211": 405, "__main__": [405, 406, 408, 410, 411, 412], "29": [405, 408], "00000": 405, "37": 405, "00": [405, 410, 412], "524": 405, "525": 405, "ttnn_basic_conv": 406, "state": [406, 409], "8kb": [406, 413], "enough": [406, 412], "32kb": 406, "bchw": 406, "permuted_input": 406, "flat": 406, "reshaped_input": 406, "out_torch": 406, "07": [406, 408, 410, 411, 412], "02": [406, 414], "649": 406, "silicondriv": [406, 408, 410, 411, 412, 414], "pci": [406, 408, 410, 411, 412, 414], "pci_devic": [406, 407, 408, 410, 411, 412, 414], "198": [406, 408, 410, 411, 412], "651": 406, "658": 406, "tt_cluster": [406, 407, 408, 410, 411, 412, 414], "190": [406, 408, 410, 411, 412], "659": 406, "666": 406, "667": 406, "673": 406, "harvest": [406, 407, 408, 410, 411, 412, 414], "0x100": [406, 408, 410, 411, 412], "noc0": [406, 407, 408, 410, 411, 412, 414], "0x0": [406, 407, 408, 410, 411, 412, 414], "282": [406, 408, 410, 411, 412], "772": 406, "817": 406, "remot": [406, 407, 408, 410, 411, 412, 414], "147": [406, 408, 410, 411, 412], "828": 406, "ethernet": [406, 407, 408, 410, 411, 412], "1039": [406, 408, 410, 411, 412], "915": 406, "clk": [406, 408, 410, 411, 412], "mhz": [406, 408, 410, 411, 412], "metal_context": [406, 408, 410, 411, 412], "487": 406, "428": [406, 408, 410, 411, 412], "489": 406, "unabl": [406, 408, 410, 411, 412], "thread": [406, 408, 410, 411, 412, 414, 415], "hardware_command_queu": [406, 408, 410, 411, 412], "74": [406, 408, 410, 411, 412], "921": 406, "reprocess": 406, "563": 406, "922": 406, "582": 406, "390": 406, "78": 406, "488": [406, 408, 410, 411, 412], "391": 406, "468": [406, 408, 410, 411, 412], "783": [406, 408, 410, 411, 412], "392": 406, "ll": [407, 413, 414], "longer": 407, "conver": 407, "effeci": 407, "could": 407, "enjoi": 407, "massiv": 407, "ttnn_basic_matrix_multipl": 407, "03": [407, 410], "21": [407, 412], "386": 407, "209": 407, "512": 407, "umd": 407, "0x20": 407, "394": 407, "751": 407, "252": 407, "18": 407, "232": 407, "174": 407, "177": 407, "752": 407, "1085": 407, "765": 407, "pin": 407, "hugepag": 407, "0x7f5480000000": 407, "0x40000000": 407, "0x4c0000000": 407, "536": 407, "258": 407, "0000": [407, 408], "260": 407, "266": 407, "272": 407, "46": 407, "028": 407, "426": 407, "ttnn_basic_oper": 408, "np": [408, 410], "host_rand": 408, "helper": [408, 409], "to_tt_til": 408, "tt_t1": 408, "transfer": 408, "tt_t2": 408, "tt_t3": 408, "tt_t4": 408, "t5": 408, "tt_t5": 408, "add_result": 408, "mul_result": 408, "mul": [408, 411], "matmul_result": 408, "bmatrix": 408, "rightarrow": 408, "broadcast_vector": 408, "broadcast_tt": 408, "broadcast_add_result": 408, "850": 408, "852": [408, 410], "859": 408, "860": 408, "866": 408, "867": 408, "873": 408, "970": 408, "015": 408, "025": 408, "111": 408, "678": 408, "680": 408, "537": 408, "564": 408, "47": 408, "08": [408, 414], "072": 408, "49": [408, 409], "82812": 408, "04688": 408, "32812": 408, "00781": 408, "39844": 408, "03906": 408, "14844": 408, "24219": 408, "65625": 408, "31250": 408, "21094": 408, "21875": 408, "33594": 408, "37500": 408, "62500": 408, "670": 408, "52": 408, "12500": 408, "23438": 408, "96875": 408, "02600": 408, "97656": 408, "18164": 408, "87891": 408, "44531": 408, "59375": 408, "48438": 408, "50781": 408, "35938": 408, "229": 408, "55": [408, 411], "50000": 408, "25000": 408, "56250": 408, "43750": 408, "57": 408, "231": 408, "59": 408, "233": 408, "63": 408, "8242": 408, "0469": 408, "2500": 408, "3750": 408, "3945": 408, "0391": 408, "5625": 408, "1250": 408, "2188": 408, "8750": 408, "4375": 408, "7500": 408, "6250": 408, "7422": 408, "1484": 408, "9531": 408, "5000": 408, "6562": 408, "3281": 408, "0938": 408, "2158": 408, "3359": 408, "8438": 408, "234": 408, "contrast": 409, "foundat": 409, "multimod": 409, "openai": 409, "concept": 409, "supervis": 409, "tradit": 409, "classifi": 409, "arbitrari": 409, "bridg": 409, "gap": 409, "textual": 409, "consist": 409, "encod": 409, "vit": 409, "never": 409, "prompt": 409, "dog": 409, "resiz": 409, "patch32": 409, "focu": 409, "hug": [409, 413], "pil": 409, "torchvis": [409, 410, 412], "cliptoken": 409, "clipmodel": 409, "bytesio": 409, "safetensor": 409, "centercrop": 409, "totensor": [409, 410, 412], "interpolationmod": 409, "simplifi": 409, "asid": 409, "portion": 409, "fail": 409, "lack": 409, "open_ttnn": 409, "close_ttnn": 409, "clean": 409, "get_devic": 409, "vice": 409, "versa": 409, "state_dict": 409, "convert_model_to_ttnn": 409, "ttnn_state_dict": 409, "elif": 409, "maxim": 409, "modal": 409, "perceptron": [409, 410], "deeper": 409, "multiheadattent": 409, "q_proj_weight": 409, "q_proj": 409, "q_proj_bia": 409, "k_proj_weight": 409, "k_proj": 409, "k_proj_bia": 409, "v_proj_weight": 409, "v_proj": 409, "v_proj_bia": 409, "out_proj_weight": 409, "out_proj": 409, "out_proj_bia": 409, "hidden": [409, 413], "bring": 409, "satur": 409, "overflow": 409, "attn_weight": 409, "attn_output": 409, "dense_out": 409, "multilayerperceptron": 409, "mlp_c_fc_weight": 409, "fc1": [409, 412], "mlp_c_fc_bia": 409, "mlp_c_proj_weight": 409, "fc2": [409, 412], "mlp_c_proj_bia": 409, "residualattentionblock": 409, "self_attn": 409, "layer_norm_1_weight": 409, "layer_norm1": 409, "layer_norm_1_bia": 409, "layer_norm_2_weight": 409, "layer_norm2": 409, "layer_norm_2_bia": 409, "multihead": 409, "d_model": 409, "n_head": 409, "need_weight": 409, "x_post_layer_norm": 409, "num_lay": 409, "text_model": 409, "vision_model": 409, "len": 409, "prepend": 409, "visiontransform": 409, "num_vision_lay": 409, "output_dim": 409, "conv2_state_dict_nam": 409, "patch_embed": 409, "vision_width": 409, "patch_siz": 409, "vision_head": 409, "class_embed": 409, "positional_embed": 409, "position_embed": 409, "proj": 409, "visual_project": 409, "conv1_weight": 409, "to_dtyp": 409, "ln_pre_weight": 409, "pre_layrnorm": 409, "ln_pre_bia": 409, "ln_post_weight": 409, "post_layernorm": 409, "ln_post_bia": 409, "rearrang": [409, 412], "224x224": 409, "224": [409, 412], "depthwis": 409, "unflatten": 409, "num_patch": 409, "embed_dim": 409, "cl": 409, "slower": 409, "capac": 409, "emb": 409, "patch_1": 409, "patch_2": 409, "patch_49": 409, "instanti": 409, "encode_text": 409, "encode_imag": 409, "token_embed": 409, "text_project": 409, "context_length": 409, "vocab_s": [409, 413], "transformer_width": 409, "final_layer_norm": 409, "transformer_head": 409, "ln_final_weight": 409, "ln_final_bia": 409, "logit_scal": 409, "hardcod": 409, "num_text_lay": 409, "build_attention_mask": 409, "cheat": 409, "essenti": [409, 410], "autoregress": 409, "triangl": 409, "exclud": 409, "thu": 409, "eot": 409, "yet": 409, "torch_token": 409, "torch_x": 409, "eot_indic": 409, "torch_selected_featur": 409, "logits_per_imag": 409, "batch_size_imag": 409, "batch_size_text": 409, "logits_per_text": 409, "text_featur": 409, "batch_text": 409, "image_featur": 409, "batch_imag": 409, "l2": 409, "norm_image_featur": 409, "moreh": 409, "norm_text_featur": 409, "temperatur": 409, "sharp": 409, "color": 409, "pixel": [409, 412], "bicub": 409, "crop": 409, "imagenet": 409, "regardless": 409, "preprocess_imag": 409, "model_resolut": 409, "_convert_image_to_rgb": 409, "transform_fn": 409, "48145466": 409, "4578275": 409, "40821073": 409, "26862954": 409, "26130258": 409, "27577711": 409, "url": 409, "download_imag": 409, "timeout": 409, "raise_for_statu": 409, "bad": 409, "statu": [409, 410], "requestexcept": 409, "ttnn_tutorials_models_clip_path": [409, 413], "download_model": 409, "clip_model_loc": 409, "cache_dir": [409, 413], "getenv": [409, 413], "download_token": 409, "tokenizer_nam": 409, "clip_tokenizer_loc": 409, "408": 409, "eo": 409, "77": 409, "image_url": 409, "media": 409, "githubusercont": 409, "ref": 409, "clip_tutori": 409, "png": 409, "unsqueez": 409, "preferred_dtyp": 409, "tt_imag": 409, "max_length": 409, "return_tensor": 409, "pt": [409, 410, 412], "tokenized_input": 409, "tokens_pretrained_host": 409, "input_id": [409, 413], "num_prompt": 409, "tokens_pretrain": 409, "time_start": 409, "time_end": 409, "3f": 409, "probs_torch": 409, "enumer": [409, 410, 412], "4f": 409, "2f": [409, 410, 412], "ttnn_mlp_inference_mnist": 410, "disk": [410, 412, 414, 415], "throughout": [410, 412, 414], "backend": 410, "outcom": 410, "28x28": 410, "grayscal": 410, "dataload": [410, 412], "testset": [410, 412], "testload": [410, 412], "shuffl": [410, 412], "train_and_export_mlp": 410, "poor": [410, 412], "mlp_mnist_weight": 410, "w1": 410, "b1": 410, "w2": 410, "b2": 410, "w3": [410, 412], "b3": [410, 412], "correctli": 410, "28": 410, "five": [410, 412], "counter": [410, 412], "sequenti": 410, "raw": 410, "_layout": 410, "image_tt": 410, "1x128": 410, "w1_final": 410, "b1_final": 410, "out1": 410, "w2_final": 410, "b2_final": 410, "out2": 410, "w3_final": 410, "b3_final": 410, "out3": 410, "predicted_label": [410, 412], "ntt": [410, 412], "41": 410, "990": 410, "992": 410, "998": 410, "006": 410, "007": 410, "013": 410, "110": 410, "172": 410, "182": [410, 412], "268": 410, "886": 410, "888": 410, "44": 410, "48": 410, "677": 410, "87": [410, 412], "682": 410, "686": 410, "690": 410, "695": 410, "89": [410, 412], "696": 410, "697": 410, "six": 411, "similarli": 411, "Be": 411, "multi_head_attent": 411, "query_weight": 411, "query_bia": 411, "key_weight": 411, "key_bia": 411, "value_weight": 411, "value_bia": 411, "output_weight": 411, "output_bia": 411, "fallback_reshap": 411, "get_fallback_funct": [411, 415], "attention_scor": 411, "attention_prob": 411, "context_lay": 411, "self_output": 411, "torch_attention_mask": [411, 413], "torch_query_weight": 411, "torch_query_bia": 411, "torch_key_weight": 411, "torch_key_bia": 411, "torch_value_weight": 411, "torch_value_bia": 411, "torch_output_weight": 411, "torch_output_bia": 411, "fly": 411, "fortun": 411, "ahead": 411, "optimized_multi_head_attent": 411, "fused_qkv_weight": 411, "fused_qkv_bia": 411, "self_output_weight": 411, "self_output_bia": 411, "fused_qkv_output": 411, "context_layer_after_concatenate_head": 411, "qkv": 411, "torch_qkv_weight": 411, "torch_qkv_bia": 411, "qkv_weight": 411, "qkv_bia": 411, "optimized_output": 411, "torch_optimized_output": 411, "assert": [411, 415], "allclos": 411, "ttnn_multihead_attent": 411, "769": 411, "776": [411, 414], "777": 411, "784": 411, "790": 411, "887": 411, "931": 411, "942": 411, "39": 411, "027": [411, 412], "603": 411, "605": 411, "51": [411, 414], "001": 411, "132": [411, 412], "265338897705078": 411, "056": 411, "151": [411, 412], "05480194091796875": 411, "363": 411, "259": 411, "2866740226745605": 411, "366": 411, "274": 411, "002416849136352539": 411, "417": 411, "418": 411, "460": 411, "scratchpad": 412, "kb": 412, "cifar10": 412, "train_and_export_cnn": 412, "simple_cnn_cifar10_weight": 412, "conv1": 412, "conv2": 412, "conv_pool_stag": 412, "encapsul": 412, "undergo": 412, "metadata": 412, "sizegur": 412, "again": 412, "record": 412, "modular": 412, "flexibl": [412, 414], "input_nhwc": 412, "conv_outchannel": 412, "weight_str": 412, "bias_str": 412, "log_first_sampl": 412, "conv_kernel_s": 412, "conv_strid": 412, "conv_pad": 412, "conv1_out": 412, "max_pool2d_kernel_s": 412, "max_pool2d_strid": 412, "max_pool2d_pad": 412, "max_pool2d_dil": 412, "max_pool2d_out": 412, "simplecnn": 412, "obtain": 412, "ttnn_imag": 412, "ttnn_image_permu": 412, "log_thi": 412, "conv1_pool": 412, "conv2_pool": 412, "fc": 412, "out_flat": 412, "w4": 412, "b4": 412, "w3_tt": 412, "b3_tt": 412, "x_tt": 412, "w4_tt": 412, "b4_tt": 412, "ttnn_simplecnn_infer": 412, "17": 412, "041": 412, "043": 412, "050": 412, "051": 412, "057": 412, "058": 412, "064": 412, "161": 412, "235": 412, "321": 412, "889": 412, "891": 412, "734": 412, "471": 412, "075": 412, "86": 412, "88": 412, "076": 412, "91": 412, "92": 412, "93": 412, "94": 412, "95": 412, "97": 412, "98": 412, "nullopt": 412, "enable_split_read": 412, "enable_subblock_pad": 412, "101": 412, "960": 412, "129": 412, "130": 412, "131": 412, "961": 412, "133": 412, "134": 412, "135": 412, "136": 412, "137": 412, "138": 412, "139": 412, "026": 412, "157": [412, 414], "158": 412, "121": 412, "669": 412, "238": 412, "166": 412, "181": 412, "240": 412, "183": 412, "ttnn_config_overrid": [413, 414, 415], "enable_fast_runtime_mod": [413, 414, 415], "suppress": 413, "cleaner": 413, "set_verbosity_error": 413, "scope": 413, "mathemat": 413, "download_google_bert_model_and_config": 413, "bertselfoutput": 413, "model_loc": 413, "ttnn_tutorials_models_tracer_path": 413, "config_google_bert": 413, "download_ttnn_bert_config": 413, "config_loc": 413, "config_ttnn_bert": 413, "download_ttnn_bert_model": 413, "bertforquestionansw": 413, "googl": 413, "bert_uncased_l": 413, "4_h": 413, "256_a": 413, "dummi": 413, "dispatch_core_typ": 413, "dispatchcoretyp": 413, "arch_nam": 413, "num_hidden_lay": 413, "placement": [413, 414], "token_type_id": 413, "segment": 413, "qa": 413, "torch_token_type_id": 413, "position_id": 413, "sequence_length": 413, "torch_position_id": 413, "ttnn_bert_input": 413, "preprocess_input": 413, "span": 413, "bert_for_question_answ": 413, "offer": 414, "intuit": 414, "depth": 414, "searchabl": 414, "peak": 414, "hierarch": 414, "server": 414, "opportun": 414, "watch": 414, "walkthrough": 414, "video": 414, "offlin": 414, "launch": 414, "localhost": 414, "8000": 414, "chrome": 414, "greet": 414, "homepag": 414, "yolov4": 414, "320x320": 414, "coco": 414, "predefin": 414, "wrap": 414, "ttnn_config_path": [414, 415], "inlin": 414, "past": 414, "enable_log": [414, 415], "report_nam": [414, 415], "ttnn_visualizer_tutori": 414, "enable_graph_report": [414, 415], "enable_detailed_buffer_report": [414, 415], "enable_detailed_tensor_report": [414, 415], "enable_comparison_mod": [414, 415], "free": 414, "test_ttnn_yolov4": 414, "test_yolov4": 414, "pretrained_weight_tru": 414, "At": 414, "664": 414, "73": 414, "665": 414, "83": 414, "cache_path": 414, "model_cache_path": 414, "tmp_dir": 414, "enable_model_cach": 414, "throw_exception_on_fallback": 414, "comparison_mode_should_raise_except": 414, "comparison_mode_pcc": [414, 415], "root_report_path": 414, "4042956046390500517": 414, "754": 414, "197": 414, "758": 414, "192": 414, "761": 414, "764": 414, "0x80": 414, "295": 414, "836": 414, "navig": 414, "db": 414, "sqlite": 414, "termin": 414, "session": 414, "unset": 414, "regener": 414, "Or": 414, "731": 414, "process_ops_log": 414, "generate_report": 414, "905": 414, "2025_08_01_10_51_02": 414, "ops_perf_results_2025_08_01_10_51_02": 414, "diredtori": 414, "ops_perf_results_": 414, "device_profile_log": 414, "bottom": 414, "filter": 414, "click": 414, "breakdown": 414, "relationship": 414, "easi": 414, "candid": 414, "chart": 414, "lifetim": 414, "estim": 414, "headroom": 414, "pinpoint": 414, "ineffici": 414, "node": 414, "edg": 414, "zoom": 414, "pan": 414, "subnetwork": 414, "flop": 414, "underutil": 414, "toggl": 414, "hint": 414, "suboptim": 414, "summar": 414, "deepli": 414, "2024": 415, "torch_input_tensor_a": 415, "torch_input_tensor_b": 415, "matmul_output_tensor": 415, "torch_matmul_output_tensor": 415, "start_tim": 415, "end_tim": 415, "stdout": 415, "6391518115997314": 415, "0007393360137939453": 415, "manage_config": 415, "9998": 415, "construct": 415, "exp_trac": 415, "miss": 415, "tt_logger_typ": 415, "tt_logger_level": 415, "substitut": 415, "implementaiton": 415, "app": 415, "pre_hook_to_print_args_and_kwarg": 415, "post_hook_to_print_output": 415, "query_registered_oper": 415, "begin_graph_captur": 415, "runmod": 415, "no_dispatch": 415, "captured_graph": 415, "end_graph_captur": 415, "pretty_print": 415}, "objects": {"ttnn": [[7, 0, 1, "", "Conv2dConfig"], [8, 0, 1, "", "Conv2dSliceConfig"], [9, 3, 1, "", "GetDefaultDevice"], [10, 0, 1, "", "MatmulMultiCoreReuseMultiCast1DProgramConfig"], [11, 0, 1, "", "MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig"], [12, 0, 1, "", "MatmulMultiCoreReuseMultiCastProgramConfig"], [13, 0, 1, "", "MatmulMultiCoreReuseProgramConfig"], [14, 3, 1, "", "SetDefaultDevice"], [403, 0, 1, "", "Shape"], [15, 0, 1, "", "SoftmaxDefaultProgramConfig"], [16, 0, 1, "", "SoftmaxProgramConfig"], [17, 0, 1, "", "SoftmaxShardedMultiCoreProgramConfig"], [18, 3, 1, "", "abs"], [19, 3, 1, "", "abs_bw"], [20, 3, 1, "", "acos"], [21, 3, 1, "", "acos_bw"], [22, 3, 1, "", "acosh"], [23, 3, 1, "", "acosh_bw"], [24, 3, 1, "", "add"], [25, 3, 1, "", "add_bw"], [26, 3, 1, "", "addalpha"], [27, 3, 1, "", "addalpha_bw"], [28, 3, 1, "", "addcdiv"], [29, 3, 1, "", "addcdiv_bw"], [30, 3, 1, "", "addcmul"], [31, 3, 1, "", "addcmul_bw"], [32, 3, 1, "", "addmm"], [33, 3, 1, "", "all_gather"], [34, 3, 1, "", "all_reduce"], [35, 3, 1, "", "alt_complex_rotate90"], [36, 3, 1, "", "angle"], [37, 3, 1, "", "angle_bw"], [38, 3, 1, "", "arange"], [39, 3, 1, "", "argmax"], [40, 3, 1, "", "as_tensor"], [41, 3, 1, "", "asin"], [42, 3, 1, "", "asin_bw"], [43, 3, 1, "", "asinh"], [44, 3, 1, "", "asinh_bw"], [45, 3, 1, "", "assign_bw"], [46, 3, 1, "", "atan"], [47, 3, 1, "", "atan2"], [48, 3, 1, "", "atan2_bw"], [49, 3, 1, "", "atan_bw"], [50, 3, 1, "", "atanh"], [51, 3, 1, "", "atanh_bw"], [52, 3, 1, "", "avg_pool2d"], [53, 3, 1, "", "batch_norm"], [54, 3, 1, "", "bias_gelu_bw"], [55, 3, 1, "", "bitcast"], [56, 3, 1, "", "bitwise_and"], [57, 3, 1, "", "bitwise_left_shift"], [58, 3, 1, "", "bitwise_not"], [59, 3, 1, "", "bitwise_or"], [60, 3, 1, "", "bitwise_right_shift"], [61, 3, 1, "", "bitwise_xor"], [62, 3, 1, "", "cbrt"], [63, 3, 1, "", "ceil"], [64, 3, 1, "", "ceil_bw"], [65, 3, 1, "", "celu"], [66, 3, 1, "", "celu_bw"], [67, 3, 1, "", "clamp"], [68, 3, 1, "", "clamp_bw"], [69, 3, 1, "", "clip"], [70, 3, 1, "", "clip_bw"], [71, 3, 1, "", "clone"], [72, 3, 1, "", "close_device"], [73, 3, 1, "", "concat"], [74, 3, 1, "", "concat_bw"], [75, 3, 1, "", "conj"], [76, 3, 1, "", "conj_bw"], [77, 3, 1, "", "conv1d"], [78, 3, 1, "", "conv2d"], [79, 3, 1, "", "conv_transpose2d"], [80, 3, 1, "", "cos"], [81, 3, 1, "", "cos_bw"], [82, 3, 1, "", "cosh"], [83, 3, 1, "", "cosh_bw"], [84, 3, 1, "", "create_sharded_memory_config"], [85, 3, 1, "", "cumprod"], [86, 3, 1, "", "cumsum"], [87, 3, 1, "", "deallocate"], [88, 3, 1, "", "deg2rad"], [89, 3, 1, "", "deg2rad_bw"], [90, 3, 1, "", "digamma"], [91, 3, 1, "", "digamma_bw"], [92, 3, 1, "", "div"], [93, 3, 1, "", "div_bw"], [94, 3, 1, "", "div_no_nan"], [95, 3, 1, "", "div_no_nan_bw"], [96, 3, 1, "", "dump_tensor"], [97, 3, 1, "", "elu"], [98, 3, 1, "", "elu_bw"], [99, 3, 1, "", "ema"], [100, 3, 1, "", "embedding"], [101, 3, 1, "", "embedding_bw"], [102, 3, 1, "", "empty"], [103, 3, 1, "", "empty_like"], [104, 3, 1, "", "eq"], [105, 3, 1, "", "eq_"], [106, 3, 1, "", "eqz"], [107, 3, 1, "", "erf"], [108, 3, 1, "", "erf_bw"], [109, 3, 1, "", "erfc"], [110, 3, 1, "", "erfc_bw"], [111, 3, 1, "", "erfinv"], [112, 3, 1, "", "erfinv_bw"], [113, 3, 1, "", "exp"], [114, 3, 1, "", "exp2"], [115, 3, 1, "", "exp2_bw"], [116, 3, 1, "", "exp_bw"], [121, 3, 1, "", "expm1"], [122, 3, 1, "", "expm1_bw"], [123, 3, 1, "", "fill"], [124, 3, 1, "", "fill_bw"], [125, 3, 1, "", "fill_ones_rm"], [126, 3, 1, "", "fill_rm"], [127, 3, 1, "", "fill_zero_bw"], [128, 3, 1, "", "floor"], [129, 3, 1, "", "floor_bw"], [130, 3, 1, "", "floor_div"], [131, 3, 1, "", "fmod"], [132, 3, 1, "", "fmod_bw"], [133, 3, 1, "", "frac"], [134, 3, 1, "", "frac_bw"], [135, 3, 1, "", "from_buffer"], [136, 3, 1, "", "from_device"], [137, 3, 1, "", "from_torch"], [138, 3, 1, "", "full"], [139, 3, 1, "", "full_like"], [140, 3, 1, "", "gather"], [141, 3, 1, "", "gcd"], [142, 3, 1, "", "ge"], [143, 3, 1, "", "ge_"], [144, 3, 1, "", "geglu"], [145, 3, 1, "", "gelu"], [146, 3, 1, "", "gelu_bw"], [147, 3, 1, "", "gez"], [148, 3, 1, "", "global_avg_pool2d"], [149, 3, 1, "", "glu"], [150, 3, 1, "", "group_norm"], [151, 3, 1, "", "gt"], [152, 3, 1, "", "gt_"], [153, 3, 1, "", "gtz"], [154, 3, 1, "", "hardshrink"], [155, 3, 1, "", "hardshrink_bw"], [156, 3, 1, "", "hardsigmoid"], [157, 3, 1, "", "hardsigmoid_bw"], [158, 3, 1, "", "hardswish"], [159, 3, 1, "", "hardswish_bw"], [160, 3, 1, "", "hardtanh"], [161, 3, 1, "", "hardtanh_bw"], [162, 3, 1, "", "heaviside"], [163, 3, 1, "", "hypot"], [164, 3, 1, "", "hypot_bw"], [165, 3, 1, "", "i0"], [166, 3, 1, "", "i0_bw"], [167, 3, 1, "", "identity"], [168, 3, 1, "", "imag"], [169, 3, 1, "", "imag_bw"], [170, 3, 1, "", "indexed_fill"], [171, 3, 1, "", "is_imag"], [172, 3, 1, "", "is_real"], [173, 3, 1, "", "isclose"], [174, 3, 1, "", "isfinite"], [175, 3, 1, "", "isinf"], [176, 3, 1, "", "isnan"], [177, 3, 1, "", "isneginf"], [178, 3, 1, "", "isposinf"], [181, 3, 1, "", "l1_loss"], [182, 3, 1, "", "layer_norm"], [183, 3, 1, "", "layer_norm_post_all_gather"], [184, 3, 1, "", "layer_norm_pre_all_gather"], [185, 3, 1, "", "lcm"], [186, 3, 1, "", "ldexp"], [187, 3, 1, "", "ldexp_bw"], [188, 3, 1, "", "le"], [189, 3, 1, "", "le_"], [190, 3, 1, "", "leaky_relu"], [191, 3, 1, "", "leaky_relu_bw"], [192, 3, 1, "", "lerp"], [193, 3, 1, "", "lerp_bw"], [194, 3, 1, "", "lez"], [195, 3, 1, "", "lgamma"], [196, 3, 1, "", "lgamma_bw"], [197, 3, 1, "", "linear"], [198, 3, 1, "", "load_tensor"], [199, 3, 1, "", "log"], [200, 3, 1, "", "log10"], [201, 3, 1, "", "log10_bw"], [202, 3, 1, "", "log1p"], [203, 3, 1, "", "log1p_bw"], [204, 3, 1, "", "log2"], [205, 3, 1, "", "log2_bw"], [206, 3, 1, "", "log_bw"], [207, 3, 1, "", "log_sigmoid"], [208, 3, 1, "", "log_sigmoid_bw"], [209, 3, 1, "", "logaddexp"], [210, 3, 1, "", "logaddexp2"], [211, 3, 1, "", "logaddexp2_bw"], [212, 3, 1, "", "logaddexp_bw"], [213, 3, 1, "", "logical_and"], [214, 3, 1, "", "logical_and_"], [215, 3, 1, "", "logical_not"], [216, 3, 1, "", "logical_not_"], [217, 3, 1, "", "logical_or"], [218, 3, 1, "", "logical_or_"], [219, 3, 1, "", "logical_xor"], [220, 3, 1, "", "logical_xor_"], [221, 3, 1, "", "logit"], [222, 3, 1, "", "logit_bw"], [223, 3, 1, "", "logiteps_bw"], [224, 3, 1, "", "lt"], [225, 3, 1, "", "lt_"], [226, 3, 1, "", "ltz"], [227, 3, 1, "", "mac"], [228, 3, 1, "", "manage_device"], [229, 3, 1, "", "manual_seed"], [230, 3, 1, "", "matmul"], [231, 3, 1, "", "max"], [232, 3, 1, "", "max_bw"], [233, 3, 1, "", "max_pool2d"], [234, 3, 1, "", "maximum"], [235, 3, 1, "", "mean"], [236, 3, 1, "", "min"], [237, 3, 1, "", "min_bw"], [238, 3, 1, "", "minimum"], [239, 3, 1, "", "mish"], [242, 3, 1, "", "moe"], [243, 3, 1, "", "mse_loss"], [244, 3, 1, "", "mul_bw"], [245, 3, 1, "", "multigammaln"], [246, 3, 1, "", "multigammaln_bw"], [247, 3, 1, "", "multiply"], [248, 3, 1, "", "ne"], [249, 3, 1, "", "ne_"], [250, 3, 1, "", "neg"], [251, 3, 1, "", "neg_bw"], [252, 3, 1, "", "nextafter"], [253, 3, 1, "", "nez"], [254, 3, 1, "", "nonzero"], [255, 3, 1, "", "normalize_global"], [256, 3, 1, "", "normalize_hw"], [257, 3, 1, "", "ones"], [258, 3, 1, "", "ones_like"], [259, 3, 1, "", "open_device"], [260, 3, 1, "", "outer"], [261, 3, 1, "", "pad"], [262, 3, 1, "", "pad_to_tile_shape"], [263, 3, 1, "", "permute"], [264, 3, 1, "", "polar"], [265, 3, 1, "", "polar_bw"], [266, 3, 1, "", "polygamma"], [267, 3, 1, "", "polygamma_bw"], [268, 3, 1, "", "polyval"], [269, 3, 1, "", "pow"], [270, 3, 1, "", "pow_bw"], [271, 3, 1, "", "prelu"], [272, 3, 1, "", "prepare_conv_bias"], [273, 3, 1, "", "prepare_conv_transpose2d_bias"], [274, 3, 1, "", "prepare_conv_transpose2d_weights"], [275, 3, 1, "", "prepare_conv_weights"], [276, 3, 1, "", "prod"], [277, 3, 1, "", "prod_bw"], [278, 3, 1, "", "rad2deg"], [279, 3, 1, "", "rad2deg_bw"], [280, 3, 1, "", "rand"], [281, 3, 1, "", "rdiv"], [282, 3, 1, "", "rdiv_bw"], [283, 3, 1, "", "real"], [284, 3, 1, "", "real_bw"], [285, 3, 1, "", "reallocate"], [286, 3, 1, "", "reciprocal"], [287, 3, 1, "", "reciprocal_bw"], [288, 3, 1, "", "reduce_scatter"], [289, 3, 1, "", "register_post_operation_hook"], [290, 3, 1, "", "register_pre_operation_hook"], [291, 3, 1, "", "reglu"], [292, 3, 1, "", "relu"], [293, 3, 1, "", "relu6"], [294, 3, 1, "", "relu6_bw"], [295, 3, 1, "", "relu_bw"], [296, 3, 1, "", "relu_max"], [297, 3, 1, "", "relu_min"], [298, 3, 1, "", "remainder"], [299, 3, 1, "", "remainder_bw"], [300, 3, 1, "", "repeat"], [301, 3, 1, "", "repeat_bw"], [302, 3, 1, "", "repeat_interleave"], [303, 3, 1, "", "reshape"], [304, 3, 1, "", "rms_norm"], [305, 3, 1, "", "rms_norm_post_all_gather"], [306, 3, 1, "", "rms_norm_pre_all_gather"], [307, 3, 1, "", "round"], [308, 3, 1, "", "round_bw"], [309, 3, 1, "", "rpow"], [310, 3, 1, "", "rpow_bw"], [311, 3, 1, "", "rsqrt"], [312, 3, 1, "", "rsqrt_bw"], [313, 3, 1, "", "rsub"], [314, 3, 1, "", "rsub_bw"], [315, 3, 1, "", "scale_causal_mask_hw_dims_softmax_in_place"], [316, 3, 1, "", "scale_mask_softmax"], [317, 3, 1, "", "scale_mask_softmax_in_place"], [318, 3, 1, "", "scatter"], [319, 3, 1, "", "selu"], [320, 3, 1, "", "selu_bw"], [321, 3, 1, "", "set_printoptions"], [322, 3, 1, "", "sigmoid"], [323, 3, 1, "", "sigmoid_accurate"], [324, 3, 1, "", "sigmoid_bw"], [325, 3, 1, "", "sign"], [326, 3, 1, "", "sign_bw"], [327, 3, 1, "", "signbit"], [328, 3, 1, "", "silu"], [329, 3, 1, "", "silu_bw"], [330, 3, 1, "", "sin"], [331, 3, 1, "", "sin_bw"], [332, 3, 1, "", "sinh"], [333, 3, 1, "", "sinh_bw"], [334, 3, 1, "", "slice"], [335, 3, 1, "", "softmax"], [336, 3, 1, "", "softmax_in_place"], [337, 3, 1, "", "softplus"], [338, 3, 1, "", "softplus_bw"], [339, 3, 1, "", "softshrink"], [340, 3, 1, "", "softshrink_bw"], [341, 3, 1, "", "softsign"], [342, 3, 1, "", "softsign_bw"], [343, 3, 1, "", "sort"], [344, 3, 1, "", "sparse_matmul"], [345, 3, 1, "", "split_work_to_cores"], [346, 3, 1, "", "sqrt"], [347, 3, 1, "", "sqrt_bw"], [348, 3, 1, "", "square"], [349, 3, 1, "", "square_bw"], [350, 3, 1, "", "squared_difference"], [351, 3, 1, "", "squared_difference_bw"], [352, 3, 1, "", "std"], [353, 3, 1, "", "sub_bw"], [354, 3, 1, "", "subalpha"], [355, 3, 1, "", "subalpha_bw"], [356, 3, 1, "", "subtract"], [357, 3, 1, "", "sum"], [358, 3, 1, "", "swiglu"], [359, 3, 1, "", "swish"], [360, 3, 1, "", "synchronize_device"], [361, 3, 1, "", "tan"], [362, 3, 1, "", "tan_bw"], [363, 3, 1, "", "tanh"], [364, 3, 1, "", "tanh_bw"], [365, 3, 1, "", "tanhshrink"], [366, 3, 1, "", "tanhshrink_bw"], [367, 3, 1, "", "threshold"], [368, 3, 1, "", "threshold_bw"], [369, 3, 1, "", "tilize"], [370, 3, 1, "", "tilize_with_val_padding"], [371, 3, 1, "", "to_device"], [372, 3, 1, "", "to_layout"], [373, 3, 1, "", "to_memory_config"], [374, 3, 1, "", "to_torch"], [375, 3, 1, "", "topk"], [382, 3, 1, "", "tril"], [383, 3, 1, "", "triu"], [384, 3, 1, "", "trunc"], [385, 3, 1, "", "trunc_bw"], [386, 3, 1, "", "unary_chain"], [387, 3, 1, "", "untilize"], [388, 3, 1, "", "untilize_with_unpadding"], [389, 3, 1, "", "upsample"], [390, 3, 1, "", "var"], [391, 3, 1, "", "where"], [392, 3, 1, "", "where_bw"], [393, 3, 1, "", "xlogy"], [394, 3, 1, "", "xlogy_bw"], [395, 3, 1, "", "zeros"], [396, 3, 1, "", "zeros_like"]], "ttnn.Conv2dConfig": [[7, 1, 1, "", "act_block_h_override"], [7, 1, 1, "", "act_block_w_div"], [7, 1, 1, "", "activation"], [7, 1, 1, "", "config_tensors_in_dram"], [7, 1, 1, "", "core_grid"], [7, 1, 1, "", "deallocate_activation"], [7, 1, 1, "", "enable_act_double_buffer"], [7, 1, 1, "", "enable_activation_reuse"], [7, 1, 1, "", "enable_kernel_stride_folding"], [7, 1, 1, "", "enable_weights_double_buffer"], [7, 1, 1, "", "force_split_reader"], [7, 1, 1, "", "full_inner_dim"], [7, 1, 1, "", "output_layout"], [7, 1, 1, "", "override_output_sharding_config"], [7, 1, 1, "", "override_sharding_config"], [7, 1, 1, "", "reallocate_halo_output"], [7, 1, 1, "", "reshard_if_not_optimal"], [7, 1, 1, "", "shard_layout"], [7, 1, 1, "", "transpose_shards"], [7, 1, 1, "", "weights_dtype"]], "ttnn.Conv2dSliceConfig": [[8, 0, 1, "", "SliceTypeEnum"], [8, 1, 1, "", "num_slices"], [8, 1, 1, "", "slice_type"]], "ttnn.Conv2dSliceConfig.SliceTypeEnum": [[8, 2, 1, "", "DRAMSliceHeight"], [8, 2, 1, "", "DRAMSliceWidth"], [8, 2, 1, "", "L1Full"], [8, 1, 1, "", "name"], [8, 1, 1, "", "value"]], "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig": [[10, 1, 1, "", "compute_with_storage_grid_size"], [10, 4, 1, "", "from_json"], [10, 1, 1, "", "fuse_batch"], [10, 1, 1, "", "fused_activation"], [10, 1, 1, "", "gather_in0"], [10, 1, 1, "", "hop_cores"], [10, 1, 1, "", "in0_block_w"], [10, 1, 1, "", "mcast_in0"], [10, 1, 1, "", "num_global_cb_receivers"], [10, 1, 1, "", "out_block_h"], [10, 1, 1, "", "out_block_w"], [10, 1, 1, "", "out_subblock_h"], [10, 1, 1, "", "out_subblock_w"], [10, 1, 1, "", "per_core_M"], [10, 1, 1, "", "per_core_N"], [10, 4, 1, "", "to_json"], [10, 1, 1, "", "untilize_out"]], "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig": [[11, 4, 1, "", "from_json"], [11, 1, 1, "", "fused_activation"], [11, 1, 1, "", "in0_block_w"], [11, 1, 1, "", "per_core_M"], [11, 1, 1, "", "per_core_N"], [11, 4, 1, "", "to_json"]], "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig": [[12, 1, 1, "", "compute_with_storage_grid_size"], [12, 4, 1, "", "from_json"], [12, 1, 1, "", "fuse_batch"], [12, 1, 1, "", "fused_activation"], [12, 1, 1, "", "in0_block_w"], [12, 1, 1, "", "out_block_h"], [12, 1, 1, "", "out_block_w"], [12, 1, 1, "", "out_subblock_h"], [12, 1, 1, "", "out_subblock_w"], [12, 1, 1, "", "per_core_M"], [12, 1, 1, "", "per_core_N"], [12, 4, 1, "", "to_json"], [12, 1, 1, "", "transpose_mcast"]], "ttnn.MatmulMultiCoreReuseProgramConfig": [[13, 1, 1, "", "compute_with_storage_grid_size"], [13, 4, 1, "", "from_json"], [13, 1, 1, "", "in0_block_w"], [13, 1, 1, "", "out_subblock_h"], [13, 1, 1, "", "out_subblock_w"], [13, 1, 1, "", "per_core_M"], [13, 1, 1, "", "per_core_N"], [13, 4, 1, "", "to_json"]], "ttnn.Shape": [[403, 1, 1, "", "rank"], [403, 4, 1, "", "to_rank"]], "ttnn.SoftmaxShardedMultiCoreProgramConfig": [[17, 1, 1, "", "block_w"]], "ttnn.experimental": [[117, 3, 1, "", "conv3d"], [118, 3, 1, "", "dropout"], [119, 3, 1, "", "gelu_bw"], [120, 3, 1, "", "rotary_embedding"]], "ttnn.kv_cache": [[179, 3, 1, "", "fill_cache_for_user_"], [180, 3, 1, "", "update_cache_for_token_"]], "ttnn.model_preprocessing": [[240, 3, 1, "", "preprocess_model"], [241, 3, 1, "", "preprocess_model_parameters"]], "ttnn.transformer": [[376, 3, 1, "", "attention_softmax"], [377, 3, 1, "", "attention_softmax_"], [378, 3, 1, "", "concatenate_heads"], [379, 3, 1, "", "scaled_dot_product_attention"], [380, 3, 1, "", "scaled_dot_product_attention_decode"], [381, 3, 1, "", "split_query_key_value_and_split_heads"]]}, "objtypes": {"0": "py:class", "1": "py:property", "2": "py:attribute", "3": "py:function", "4": "py:method"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "property", "Python property"], "2": ["py", "attribute", "Python attribute"], "3": ["py", "function", "Python function"], "4": ["py", "method", "Python method"]}, "titleterms": {"welcom": 0, "tt": [0, 4, 5, 397, 400, 402, 409, 411, 413, 414, 415], "nn": [0, 4, 5, 397, 400, 402, 409, 411, 413, 414, 415], "document": 0, "ttnn": [0, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 413], "resourc": 0, "indic": 0, "tabl": 0, "contribut": [1, 400], "develop": 1, "support": [2, 230, 344, 415], "report": [2, 6, 402, 414], "bug": 2, "featur": 2, "propos": 2, "request": 2, "troubleshoot": 2, "debug": [2, 415], "tip": 2, "commun": 2, "tool": 3, "what": [4, 5, 409], "i": [4, 5], "ad": 5, "new": [5, 401], "oper": [5, 6, 397, 402, 405, 406, 408, 413, 414, 415], "faq": 5, "step": [5, 397, 400], "ar": [5, 400], "need": 5, "add": [5, 24, 405], "c": [5, 415], "python": [5, 415], "exampl": [5, 9, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 382, 383, 384, 385, 386, 387, 388, 390, 391, 392, 393, 394, 395, 396, 397, 400, 405, 406, 407, 408, 410, 411, 412, 413, 415], "devic": [5, 6, 78, 405, 406, 407, 408, 409, 410, 412, 413, 415], "implement": [5, 409], "1": [5, 315, 397, 399, 400, 413, 415], "2": [5, 397, 399, 400, 413, 415], "bind": 5, "option": [5, 316, 317, 400], "golden": 5, "function": [5, 401, 409, 415], "3": [5, 397, 400, 413, 415], "usag": 5, "doc": 5, "api": [6, 403], "memori": [6, 230, 344, 403, 414], "config": [6, 403, 407, 413], "core": 6, "tensor": [6, 39, 231, 235, 236, 315, 316, 317, 352, 357, 390, 403, 405, 406, 407, 408, 413, 414, 415], "creation": [6, 405, 408], "matrix": [6, 407], "multipl": [6, 403, 407], "pointwis": 6, "unari": 6, "binari": [6, 400], "ternari": 6, "loss": 6, "reduct": 6, "data": [6, 403, 410], "movement": 6, "normal": 6, "program": [6, 400, 413, 415], "transform": [6, 376, 377, 378, 379, 380, 381, 409], "ccl": 6, "embed": [6, 100], "convolut": [6, 406, 412], "pool": [6, 412], "vision": [6, 409], "kv": 6, "cach": [6, 415], "model": [6, 397, 399, 400, 409, 413, 414], "convers": [6, 405, 408, 409], "hook": [6, 415], "conv2dconfig": 7, "conv2dsliceconfig": 8, "getdefaultdevic": 9, "matmulmulticorereusemulticast1dprogramconfig": 10, "matmulmulticorereusemulticastdramshardedprogramconfig": 11, "matmulmulticorereusemulticastprogramconfig": 12, "matmulmulticorereuseprogramconfig": 13, "setdefaultdevic": 14, "softmaxdefaultprogramconfig": 15, "softmaxprogramconfig": 16, "softmaxshardedmulticoreprogramconfig": 17, "ab": 18, "abs_bw": 19, "aco": 20, "acos_bw": 21, "acosh": 22, "acosh_bw": 23, "add_bw": 25, "addalpha": 26, "addalpha_bw": 27, "addcdiv": 28, "addcdiv_bw": 29, "addcmul": 30, "addcmul_bw": 31, "addmm": 32, "input_tensor": [32, 78, 150, 182, 183, 184, 276, 304, 305, 306, 375], "mat1_tensor": 32, "mat2_tensor": 32, "all_gath": 33, "all_reduc": 34, "alt_complex_rotate90": 35, "angl": 36, "angle_bw": 37, "arang": 38, "argmax": 39, "input": [39, 231, 235, 236, 315, 316, 317, 352, 357, 390, 406], "output": [39, 405, 406, 407, 408, 410, 411, 412], "as_tensor": 40, "asin": 41, "asin_bw": 42, "asinh": 43, "asinh_bw": 44, "assign_bw": 45, "atan": 46, "atan2": 47, "atan2_bw": 48, "atan_bw": 49, "atanh": 50, "atanh_bw": 51, "avg_pool2d": 52, "batch_norm": 53, "bias_gelu_bw": 54, "bitcast": 55, "bitwise_and": 56, "bitwise_left_shift": 57, "bitwise_not": 58, "bitwise_or": 59, "bitwise_right_shift": 60, "bitwise_xor": 61, "cbrt": 62, "ceil": 63, "ceil_bw": 64, "celu": 65, "celu_bw": 66, "clamp": 67, "clamp_bw": 68, "clip": [69, 409], "clip_bw": 70, "clone": [71, 400], "close_devic": 72, "concat": 73, "concat_bw": 74, "conj": 75, "conj_bw": 76, "conv1d": 77, "conv2d": 78, "output_tensor": [78, 182, 276, 304], "weights_tensor": 78, "host": [78, 408], "prepar": 78, "bias_tensor": 78, "conv_transpose2d": 79, "co": 80, "cos_bw": 81, "cosh": 82, "cosh_bw": 83, "create_sharded_memory_config": 84, "cumprod": 85, "cumsum": 86, "dealloc": 87, "deg2rad": 88, "deg2rad_bw": 89, "digamma": 90, "digamma_bw": 91, "div": 92, "div_bw": 93, "div_no_nan": 94, "div_no_nan_bw": 95, "dump_tensor": 96, "elu": 97, "elu_bw": 98, "ema": 99, "embedding_bw": 101, "empti": 102, "empty_lik": 103, "eq": 104, "eq_": 105, "eqz": 106, "erf": 107, "erf_bw": 108, "erfc": 109, "erfc_bw": 110, "erfinv": 111, "erfinv_bw": 112, "exp": 113, "exp2": 114, "exp2_bw": 115, "exp_bw": 116, "experiment": [117, 118, 119, 120], "conv3d": 117, "dropout": 118, "gelu_bw": [119, 146], "rotary_embed": 120, "expm1": 121, "expm1_bw": 122, "fill": 123, "fill_bw": 124, "fill_ones_rm": 125, "fill_rm": 126, "fill_zero_bw": 127, "floor": 128, "floor_bw": 129, "floor_div": 130, "fmod": 131, "fmod_bw": 132, "frac": 133, "frac_bw": 134, "from_buff": 135, "from_devic": 136, "from_torch": 137, "full": [138, 405, 406, 407, 408, 410, 411, 412], "full_lik": 139, "gather": 140, "gcd": 141, "ge": 142, "ge_": 143, "geglu": 144, "gelu": 145, "gez": 147, "global_avg_pool2d": 148, "glu": 149, "group_norm": 150, "weight": [150, 182, 183, 304, 305, 409, 410, 412], "gamma": [150, 182, 183, 304, 305], "bia": [150, 182, 183, 197, 304, 305], "beta": [150, 182, 183, 304, 305], "input_mask": 150, "gt": 151, "gt_": 152, "gtz": 153, "hardshrink": 154, "hardshrink_bw": 155, "hardsigmoid": 156, "hardsigmoid_bw": 157, "hardswish": 158, "hardswish_bw": 159, "hardtanh": 160, "hardtanh_bw": 161, "heavisid": 162, "hypot": 163, "hypot_bw": 164, "i0": 165, "i0_bw": 166, "ident": 167, "imag": [168, 400, 409, 410], "imag_bw": 169, "indexed_fil": 170, "is_imag": 171, "is_real": 172, "isclos": 173, "isfinit": 174, "isinf": 175, "isnan": 176, "isneginf": 177, "isposinf": 178, "kv_cach": [179, 180], "fill_cache_for_user_": 179, "update_cache_for_token_": 180, "l1_loss": 181, "layer_norm": 182, "residual_input_tensor": [182, 184, 304, 306], "layer_norm_post_all_gath": 183, "stat": [183, 305], "layer_norm_pre_all_gath": 184, "lcm": 185, "ldexp": 186, "ldexp_bw": 187, "le": 188, "le_": 189, "leaky_relu": 190, "leaky_relu_bw": 191, "lerp": 192, "lerp_bw": 193, "lez": 194, "lgamma": 195, "lgamma_bw": 196, "linear": 197, "input_tensor_a": [197, 230, 344], "input_tensor_b": [197, 230, 344], "load_tensor": 198, "log": [199, 415], "log10": 200, "log10_bw": 201, "log1p": 202, "log1p_bw": 203, "log2": 204, "log2_bw": 205, "log_bw": 206, "log_sigmoid": 207, "log_sigmoid_bw": 208, "logaddexp": 209, "logaddexp2": 210, "logaddexp2_bw": 211, "logaddexp_bw": 212, "logical_and": 213, "logical_and_": 214, "logical_not": 215, "logical_not_": 216, "logical_or": 217, "logical_or_": 218, "logical_xor": 219, "logical_xor_": 220, "logit": 221, "logit_bw": 222, "logiteps_bw": 223, "lt": 224, "lt_": 225, "ltz": 226, "mac": 227, "manage_devic": 228, "manual_se": 229, "matmul": 230, "configur": [230, 344, 400, 407, 411], "max": 231, "max_bw": 232, "max_pool2d": 233, "maximum": 234, "mean": 235, "min": 236, "min_bw": 237, "minimum": 238, "mish": 239, "model_preprocess": [240, 241], "preprocess_model": 240, "preprocess_model_paramet": 241, "moe": 242, "mse_loss": 243, "mul_bw": 244, "multigammaln": 245, "multigammaln_bw": 246, "multipli": [247, 407], "ne": 248, "ne_": 249, "neg": 250, "neg_bw": 251, "nextaft": 252, "nez": 253, "nonzero": 254, "normalize_glob": 255, "normalize_hw": 256, "ones": 257, "ones_lik": 258, "open_devic": 259, "outer": 260, "pad": 261, "pad_to_tile_shap": 262, "permut": 263, "polar": 264, "polar_bw": 265, "polygamma": 266, "polygamma_bw": 267, "polyv": 268, "pow": 269, "pow_bw": 270, "prelu": 271, "prepare_conv_bia": 272, "prepare_conv_transpose2d_bia": 273, "prepare_conv_transpose2d_weight": 274, "prepare_conv_weight": 275, "prod": 276, "prod_bw": 277, "rad2deg": 278, "rad2deg_bw": 279, "rand": 280, "rdiv": 281, "rdiv_bw": 282, "real": 283, "real_bw": 284, "realloc": 285, "reciproc": 286, "reciprocal_bw": 287, "reduce_scatt": 288, "register_post_operation_hook": 289, "register_pre_operation_hook": 290, "reglu": 291, "relu": 292, "relu6": 293, "relu6_bw": 294, "relu_bw": 295, "relu_max": 296, "relu_min": 297, "remaind": 298, "remainder_bw": 299, "repeat": 300, "repeat_bw": 301, "repeat_interleav": 302, "reshap": 303, "rms_norm": 304, "rms_norm_post_all_gath": 305, "rms_norm_pre_all_gath": 306, "round": 307, "round_bw": 308, "rpow": 309, "rpow_bw": 310, "rsqrt": 311, "rsqrt_bw": 312, "rsub": 313, "rsub_bw": 314, "scale_causal_mask_hw_dims_softmax_in_plac": 315, "shard": [315, 403], "mask": [315, 316, 317], "h": 315, "w": 315, "scale_mask_softmax": 316, "scale_mask_softmax_in_plac": 317, "scatter": 318, "selu": 319, "selu_bw": 320, "set_printopt": 321, "sigmoid": 322, "sigmoid_accur": 323, "sigmoid_bw": 324, "sign": 325, "sign_bw": 326, "signbit": 327, "silu": 328, "silu_bw": 329, "sin": 330, "sin_bw": 331, "sinh": 332, "sinh_bw": 333, "slice": [334, 415], "softmax": 335, "softmax_in_plac": 336, "softplu": 337, "softplus_bw": 338, "softshrink": 339, "softshrink_bw": 340, "softsign": 341, "softsign_bw": 342, "sort": 343, "sparse_matmul": 344, "sparsiti": 344, "split_work_to_cor": 345, "sqrt": 346, "sqrt_bw": 347, "squar": 348, "square_bw": 349, "squared_differ": 350, "squared_difference_bw": 351, "std": 352, "sub_bw": 353, "subalpha": 354, "subalpha_bw": 355, "subtract": 356, "sum": 357, "swiglu": 358, "swish": 359, "synchronize_devic": 360, "tan": 361, "tan_bw": 362, "tanh": 363, "tanh_bw": 364, "tanhshrink": 365, "tanhshrink_bw": 366, "threshold": 367, "threshold_bw": 368, "tiliz": 369, "tilize_with_val_pad": 370, "to_devic": 371, "to_layout": 372, "to_memory_config": 373, "to_torch": 374, "topk": 375, "index_tensor": 375, "attention_softmax": 376, "attention_softmax_": 377, "concatenate_head": 378, "scaled_dot_product_attent": 379, "scaled_dot_product_attention_decod": 380, "split_query_key_value_and_split_head": 381, "tril": 382, "triu": 383, "trunc": 384, "trunc_bw": 385, "unary_chain": 386, "until": 387, "untilize_with_unpad": 388, "upsampl": 389, "var": 390, "where": [391, 399], "where_bw": 392, "xlogi": 393, "xlogy_bw": 394, "zero": [395, 409], "zeros_lik": 396, "convert": [397, 415], "pytorch": [397, 413], "rewrit": 397, "switch": 397, "optim": 397, "more": [397, 407], "build": [398, 399, 400, 409], "uplift": 398, "demo": [398, 399], "get": 399, "start": 399, "instal": [399, 400], "explor": 399, "our": 399, "To": [399, 400], "go": 399, "from": [399, 415], "here": 399, "prerequisit": [400, 414], "set": [400, 406, 413], "up": 400, "hardwar": 400, "softwar": 400, "depend": [400, 409], "script": 400, "recommend": 400, "manual": [400, 406], "metalium": 400, "There": 400, "four": 400, "latest": 400, "wheel": 400, "For": 400, "user": 400, "onli": 400, "environ": 400, "docker": 400, "releas": 400, "sourc": 400, "repositori": 400, "librari": [400, 405, 406, 407, 408, 410, 413], "virtual": 400, "setup": [400, 412], "anaconda": 400, "packag": 400, "you": 400, "all": [400, 415], "verifi": 400, "your": 400, "try": 400, "execut": 400, "interest": 400, "multi": [400, 411], "card": 400, "topologi": 400, "machin": 400, "requir": [400, 403], "overview": 400, "why": 400, "It": 400, "matter": 400, "vm": 400, "onboard": 401, "profil": [402, 414], "perf": 402, "header": 402, "profile_thi": 402, "descript": 402, "us": [402, 407, 413, 415], "perform": [402, 407, 414], "visual": [402, 413, 414, 415], "shape": 403, "layout": [403, 407], "type": 403, "width": 403, "limit": 403, "bfloat8_b": 403, "storag": 403, "tutori": [404, 413], "import": [405, 406, 407, 408, 409, 410, 412, 413], "open": [405, 406, 407, 408, 410, 412], "tenstorr": 405, "addit": 405, "close": [405, 406, 407, 408, 410, 412, 413], "basic": [406, 408, 415], "seed": 406, "creat": 406, "forward": 406, "method": [406, 409], "paramet": 406, "run": [406, 409, 412, 414, 415], "initi": [407, 412], "b": 407, "random": 407, "valu": 407, "inspect": 407, "result": [407, 414], "tile": 408, "base": 408, "arithmet": 408, "simul": 408, "broadcast": 408, "row": 408, "vector": 408, "expans": 408, "shot": 409, "classif": 409, "doe": 409, "manag": 409, "util": 409, "gener": [409, 414], "architectur": 409, "process": 409, "pipelin": 409, "complet": 409, "compon": 409, "kei": 409, "preprocess": 409, "download": [409, 413], "token": 409, "infer": [409, 410, 412], "text": 409, "mlp": 410, "load": [410, 412], "mnist": 410, "test": [410, 412], "pretrain": 410, "accuraci": 410, "track": 410, "loop": 410, "flatten": 410, "head": 411, "attent": 411, "write": 411, "simpl": 412, "cnn": 412, "cifar": 412, "10": [412, 415], "dataset": 412, "defin": 412, "stage": 412, "sampl": 412, "tracer": 413, "bert": 413, "trace": [413, 415], "layer": 413, "4": [413, 415], "written": 413, "analysi": 414, "upload": 414, "tab": 414, "buffer": [414, 415], "graph": [414, 415], "recap": 414, "torch": 415, "an": 415, "__getitem__": 415, "enabl": 415, "5": 415, "intermedi": 415, "6": 415, "7": 415, "tt_lib": 415, "8": 415, "9": 415, "chang": 415, "string": 415, "represent": 415, "11": 415, "web": 415, "browser": 415, "12": 415, "regist": 415, "pre": 415, "post": 415, "13": 415, "queri": 415, "14": 415, "fall": 415, "back": 415, "15": 415, "captur": 415, "alloc": 415, "etc": 415}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "nbsphinx": 4, "sphinx": 58}, "alltitles": {"Welcome to TT-NN documentation!": [[0, "welcome-to-tt-nn-documentation"]], "TTNN": [[0, null]], "Resources": [[0, null]], "Indices and tables": [[0, "indices-and-tables"]], "Contributing as a developer": [[1, "contributing-as-a-developer"]], "Support": [[2, "support"]], "Reporting bugs, feature proposals, or support requests": [[2, "reporting-bugs-feature-proposals-or-support-requests"]], "Troubleshooting and debugging tips": [[2, "troubleshooting-and-debugging-tips"]], "Community": [[2, "community"]], "Tools": [[3, "tools"]], "What is TT-NN?": [[4, "what-is-tt-nn"]], "Adding New TT-NN Operation": [[5, "adding-new-tt-nn-operation"]], "FAQ": [[5, "faq"]], "What is a TT-NN operation?": [[5, "what-is-a-tt-nn-operation"]], "What steps are needed to add TT-NN operation in C++?": [[5, "what-steps-are-needed-to-add-tt-nn-operation-in-c"]], "What steps are needed to add TT-NN operation in Python?": [[5, "what-steps-are-needed-to-add-tt-nn-operation-in-python"]], "Example of Adding a new Device Operation": [[5, "example-of-adding-a-new-device-operation"]], "C++ Implementation": [[5, "c-implementation"]], "Step 1: Implement device operation": [[5, "step-1-implement-device-operation"]], "Step 2: Implement the operation in C++": [[5, "step-2-implement-the-operation-in-c"]], "Python Implementation": [[5, "python-implementation"]], "Step 1: Add Python binding": [[5, "step-1-add-python-binding"]], "Step 2: (Optional) Add golden function for the operation in Python": [[5, "step-2-optional-add-golden-function-for-the-operation-in-python"]], "Step 3: (Optional) Add example usage to docs": [[5, "step-3-optional-add-example-usage-to-docs"]], "APIs": [[6, "apis"], [403, "apis"]], "Device": [[6, "device"]], "Memory Config": [[6, "memory-config"], [403, "memory-config"]], "Operations": [[6, "operations"]], "Core": [[6, "core"]], "Tensor Creation": [[6, "tensor-creation"], [405, "Tensor-Creation"]], "Matrix Multiplication": [[6, "matrix-multiplication"], [407, "Matrix-Multiplication"]], "Pointwise Unary": [[6, "pointwise-unary"]], "Pointwise Binary": [[6, "pointwise-binary"]], "Pointwise Ternary": [[6, "pointwise-ternary"]], "Losses": [[6, "losses"]], "Reduction": [[6, "reduction"]], "Data Movement": [[6, "data-movement"]], "Normalization": [[6, "normalization"]], "Normalization Program Configs": [[6, "normalization-program-configs"]], "Transformer": [[6, "transformer"]], "CCL": [[6, "ccl"]], "Embedding": [[6, "embedding"]], "Convolution": [[6, "convolution"]], "Pooling": [[6, "pooling"]], "Vision": [[6, "vision"]], "KV Cache": [[6, "kv-cache"]], "Model Conversion": [[6, "model-conversion"]], "Reports": [[6, "reports"]], "Operation Hooks": [[6, "operation-hooks"]], "ttnn.Conv2dConfig": [[7, "ttnn-conv2dconfig"]], "ttnn.Conv2dSliceConfig": [[8, "ttnn-conv2dsliceconfig"]], "ttnn.GetDefaultDevice": [[9, "ttnn-getdefaultdevice"]], "Example": [[9, null], [14, null], [15, null], [17, null], [18, null], [19, null], [20, null], [21, null], [22, null], [23, null], [24, null], [25, null], [26, null], [27, null], [28, null], [29, null], [30, null], [31, null], [32, null], [33, null], [34, null], [35, null], [36, null], [37, null], [38, null], [39, null], [40, null], [41, null], [42, null], [43, null], [44, null], [45, null], [46, null], [47, null], [48, null], [49, null], [50, null], [51, null], [52, null], [53, null], [54, null], [55, null], [56, null], [57, null], [58, null], [59, null], [60, null], [61, null], [62, null], [63, null], [64, null], [65, null], [66, null], [67, null], [68, null], [69, null], [70, null], [71, null], [72, null], [73, null], [74, null], [75, null], [76, null], [80, null], [81, null], [82, null], [83, null], [84, null], [85, null], [86, null], [87, null], [88, null], [89, null], [90, null], [91, null], [92, null], [93, null], [94, null], [95, null], [96, null], [97, null], [98, null], [99, null], [100, null], [101, null], [102, null], [103, null], [104, null], [105, null], [106, null], [107, null], [108, null], [109, null], [110, null], [111, null], [112, null], [113, null], [114, null], [115, null], [116, null], [118, null], [119, null], [121, null], [122, null], [123, null], [124, null], [125, null], [126, null], [127, null], [128, null], [129, null], [130, null], [131, null], [132, null], [133, null], [134, null], [135, null], [136, null], [137, null], [138, null], [139, null], [140, null], [141, null], [142, null], [143, null], [144, null], [145, null], [146, null], [147, null], [148, null], [149, null], [150, null], [151, null], [152, null], [153, null], [154, null], [155, null], [156, null], [157, null], [158, null], [159, null], [160, null], [161, null], [162, null], [163, null], [164, null], [165, null], [166, null], [167, null], [168, null], [169, null], [170, null], [171, null], [172, null], [173, null], [174, null], [175, null], [176, null], [177, null], [178, null], [181, null], [182, null], [183, null], [184, null], [185, null], [186, null], [187, null], [188, null], [189, null], [190, null], [191, null], [192, null], [193, null], [194, null], [195, null], [196, null], [197, null], [198, null], [199, null], [200, null], [201, null], [202, null], [203, null], [204, null], [205, null], [206, null], [207, null], [208, null], [209, null], [210, null], [211, null], [212, null], [213, null], [214, null], [215, null], [216, null], [217, null], [218, null], [219, null], [220, null], [221, null], [221, null], [222, null], [223, null], [224, null], [225, null], [226, null], [227, null], [228, null], [229, null], [230, null], [231, null], [232, null], [233, null], [234, null], [235, null], [236, null], [237, null], [238, null], [239, null], [242, null], [243, null], [244, null], [245, null], [246, null], [247, null], [248, null], [249, null], [250, null], [251, null], [252, null], [253, null], [254, null], [255, null], [256, null], [257, null], [258, null], [259, null], [260, null], [261, null], [262, null], [263, null], [264, null], [265, null], [266, null], [267, null], [268, null], [269, null], [270, null], [271, null], [276, null], [277, null], [278, null], [279, null], [280, null], [281, null], [282, null], [283, null], [284, null], [285, null], [286, null], [287, null], [288, null], [291, null], [292, null], [293, null], [294, null], [295, null], [296, null], [297, null], [298, null], [299, null], [300, null], [301, null], [302, null], [303, null], [304, null], [305, null], [306, null], [307, null], [308, null], [309, null], [310, null], [311, null], [312, null], [313, null], [314, null], [315, null], [316, null], [317, null], [318, null], [319, null], [320, null], [322, null], [323, null], [324, null], [325, null], [326, null], [327, null], [328, null], [329, null], [330, null], [331, null], [332, null], [333, null], [334, null], [335, null], [336, null], [337, null], [338, null], [339, null], [340, null], [341, null], [342, null], [343, null], [344, null], [346, null], [347, null], [348, null], [349, null], [350, null], [351, null], [352, null], [353, null], [354, null], [355, null], [356, null], [357, null], [358, null], [359, null], [360, null], [361, null], [362, null], [363, null], [364, null], [365, null], [366, null], [367, null], [368, null], [369, null], [370, null], [371, null], [372, null], [373, null], [374, null], [375, null], [382, null], [383, null], [384, null], [385, null], [386, null], [387, null], [388, null], [390, null], [391, null], [392, null], [393, null], [394, null], [395, null], [396, null]], "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig": [[10, "ttnn-matmulmulticorereusemulticast1dprogramconfig"]], "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig": [[11, "ttnn-matmulmulticorereusemulticastdramshardedprogramconfig"]], "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig": [[12, "ttnn-matmulmulticorereusemulticastprogramconfig"]], "ttnn.MatmulMultiCoreReuseProgramConfig": [[13, "ttnn-matmulmulticorereuseprogramconfig"]], "ttnn.SetDefaultDevice": [[14, "ttnn-setdefaultdevice"]], "ttnn.SoftmaxDefaultProgramConfig": [[15, "ttnn-softmaxdefaultprogramconfig"]], "ttnn.SoftmaxProgramConfig": [[16, "ttnn-softmaxprogramconfig"]], "ttnn.SoftmaxShardedMultiCoreProgramConfig": [[17, "ttnn-softmaxshardedmulticoreprogramconfig"]], "ttnn.abs": [[18, "ttnn-abs"]], "ttnn.abs_bw": [[19, "ttnn-abs-bw"]], "ttnn.acos": [[20, "ttnn-acos"]], "ttnn.acos_bw": [[21, "ttnn-acos-bw"]], "ttnn.acosh": [[22, "ttnn-acosh"]], "ttnn.acosh_bw": [[23, "ttnn-acosh-bw"]], "ttnn.add": [[24, "ttnn-add"]], "ttnn.add_bw": [[25, "ttnn-add-bw"]], "ttnn.addalpha": [[26, "ttnn-addalpha"]], "ttnn.addalpha_bw": [[27, "ttnn-addalpha-bw"]], "ttnn.addcdiv": [[28, "ttnn-addcdiv"]], "ttnn.addcdiv_bw": [[29, "ttnn-addcdiv-bw"]], "ttnn.addcmul": [[30, "ttnn-addcmul"]], "ttnn.addcmul_bw": [[31, "ttnn-addcmul-bw"]], "ttnn.addmm": [[32, "ttnn-addmm"]], "input_tensor": [[32, "id2"], [78, "id2"], [150, "id2"], [182, "id2"], [183, "id2"], [184, "id2"], [276, "id2"], [304, "id2"], [305, "id2"], [306, "id2"], [375, "id2"]], "mat1_tensor": [[32, "id3"]], "mat2_tensor": [[32, "id4"]], "ttnn.all_gather": [[33, "ttnn-all-gather"]], "ttnn.all_reduce": [[34, "ttnn-all-reduce"]], "ttnn.alt_complex_rotate90": [[35, "ttnn-alt-complex-rotate90"]], "ttnn.angle": [[36, "ttnn-angle"]], "ttnn.angle_bw": [[37, "ttnn-angle-bw"]], "ttnn.arange": [[38, "ttnn-arange"]], "ttnn.argmax": [[39, "ttnn-argmax"]], "Input Tensor": [[39, "id2"], [231, "id2"], [235, "id2"], [236, "id2"], [316, "id2"], [317, "id2"], [352, "id2"], [357, "id2"], [390, "id2"]], "Output Tensor": [[39, "id3"]], "ttnn.as_tensor": [[40, "ttnn-as-tensor"]], "ttnn.asin": [[41, "ttnn-asin"]], "ttnn.asin_bw": [[42, "ttnn-asin-bw"]], "ttnn.asinh": [[43, "ttnn-asinh"]], "ttnn.asinh_bw": [[44, "ttnn-asinh-bw"]], "ttnn.assign_bw": [[45, "ttnn-assign-bw"]], "ttnn.atan": [[46, "ttnn-atan"]], "ttnn.atan2": [[47, "ttnn-atan2"]], "ttnn.atan2_bw": [[48, "ttnn-atan2-bw"]], "ttnn.atan_bw": [[49, "ttnn-atan-bw"]], "ttnn.atanh": [[50, "ttnn-atanh"]], "ttnn.atanh_bw": [[51, "ttnn-atanh-bw"]], "ttnn.avg_pool2d": [[52, "ttnn-avg-pool2d"]], "ttnn.batch_norm": [[53, "ttnn-batch-norm"]], "ttnn.bias_gelu_bw": [[54, "ttnn-bias-gelu-bw"]], "ttnn.bitcast": [[55, "ttnn-bitcast"]], "ttnn.bitwise_and": [[56, "ttnn-bitwise-and"]], "ttnn.bitwise_left_shift": [[57, "ttnn-bitwise-left-shift"]], "ttnn.bitwise_not": [[58, "ttnn-bitwise-not"]], "ttnn.bitwise_or": [[59, "ttnn-bitwise-or"]], "ttnn.bitwise_right_shift": [[60, "ttnn-bitwise-right-shift"]], "ttnn.bitwise_xor": [[61, "ttnn-bitwise-xor"]], "ttnn.cbrt": [[62, "ttnn-cbrt"]], "ttnn.ceil": [[63, "ttnn-ceil"]], "ttnn.ceil_bw": [[64, "ttnn-ceil-bw"]], "ttnn.celu": [[65, "ttnn-celu"]], "ttnn.celu_bw": [[66, "ttnn-celu-bw"]], "ttnn.clamp": [[67, "ttnn-clamp"]], "ttnn.clamp_bw": [[68, "ttnn-clamp-bw"]], "ttnn.clip": [[69, "ttnn-clip"]], "ttnn.clip_bw": [[70, "ttnn-clip-bw"]], "ttnn.clone": [[71, "ttnn-clone"]], "ttnn.close_device": [[72, "ttnn-close-device"]], "ttnn.concat": [[73, "ttnn-concat"]], "ttnn.concat_bw": [[74, "ttnn-concat-bw"]], "ttnn.conj": [[75, "ttnn-conj"]], "ttnn.conj_bw": [[76, "ttnn-conj-bw"]], "ttnn.conv1d": [[77, "ttnn-conv1d"]], "ttnn.conv2d": [[78, "ttnn-conv2d"]], "output_tensor": [[78, "id3"], [182, "id5"], [276, "id3"], [304, "id5"]], "weights_tensor (host)": [[78, "id4"]], "weights_tensor (prepared on device)": [[78, "id5"]], "bias_tensor (host)": [[78, "id6"]], "bias_tensor (prepared on device)": [[78, "id7"]], "ttnn.conv_transpose2d": [[79, "ttnn-conv-transpose2d"]], "ttnn.cos": [[80, "ttnn-cos"]], "ttnn.cos_bw": [[81, "ttnn-cos-bw"]], "ttnn.cosh": [[82, "ttnn-cosh"]], "ttnn.cosh_bw": [[83, "ttnn-cosh-bw"]], "ttnn.create_sharded_memory_config": [[84, "ttnn-create-sharded-memory-config"]], "ttnn.cumprod": [[85, "ttnn-cumprod"]], "ttnn.cumsum": [[86, "ttnn-cumsum"]], "ttnn.deallocate": [[87, "ttnn-deallocate"]], "ttnn.deg2rad": [[88, "ttnn-deg2rad"]], "ttnn.deg2rad_bw": [[89, "ttnn-deg2rad-bw"]], "ttnn.digamma": [[90, "ttnn-digamma"]], "ttnn.digamma_bw": [[91, "ttnn-digamma-bw"]], "ttnn.div": [[92, "ttnn-div"]], "ttnn.div_bw": [[93, "ttnn-div-bw"]], "ttnn.div_no_nan": [[94, "ttnn-div-no-nan"]], "ttnn.div_no_nan_bw": [[95, "ttnn-div-no-nan-bw"]], "ttnn.dump_tensor": [[96, "ttnn-dump-tensor"]], "ttnn.elu": [[97, "ttnn-elu"]], "ttnn.elu_bw": [[98, "ttnn-elu-bw"]], "ttnn.ema": [[99, "ttnn-ema"]], "ttnn.embedding": [[100, "ttnn-embedding"]], "ttnn.embedding_bw": [[101, "ttnn-embedding-bw"]], "ttnn.empty": [[102, "ttnn-empty"]], "ttnn.empty_like": [[103, "ttnn-empty-like"]], "ttnn.eq": [[104, "ttnn-eq"]], "ttnn.eq_": [[105, "ttnn-eq"]], "ttnn.eqz": [[106, "ttnn-eqz"]], "ttnn.erf": [[107, "ttnn-erf"]], "ttnn.erf_bw": [[108, "ttnn-erf-bw"]], "ttnn.erfc": [[109, "ttnn-erfc"]], "ttnn.erfc_bw": [[110, "ttnn-erfc-bw"]], "ttnn.erfinv": [[111, "ttnn-erfinv"]], "ttnn.erfinv_bw": [[112, "ttnn-erfinv-bw"]], "ttnn.exp": [[113, "ttnn-exp"]], "ttnn.exp2": [[114, "ttnn-exp2"]], "ttnn.exp2_bw": [[115, "ttnn-exp2-bw"]], "ttnn.exp_bw": [[116, "ttnn-exp-bw"]], "ttnn.experimental.conv3d": [[117, "ttnn-experimental-conv3d"]], "ttnn.experimental.dropout": [[118, "ttnn-experimental-dropout"]], "ttnn.experimental.gelu_bw": [[119, "ttnn-experimental-gelu-bw"]], "ttnn.experimental.rotary_embedding": [[120, "ttnn-experimental-rotary-embedding"]], "ttnn.expm1": [[121, "ttnn-expm1"]], "ttnn.expm1_bw": [[122, "ttnn-expm1-bw"]], "ttnn.fill": [[123, "ttnn-fill"]], "ttnn.fill_bw": [[124, "ttnn-fill-bw"]], "ttnn.fill_ones_rm": [[125, "ttnn-fill-ones-rm"]], "ttnn.fill_rm": [[126, "ttnn-fill-rm"]], "ttnn.fill_zero_bw": [[127, "ttnn-fill-zero-bw"]], "ttnn.floor": [[128, "ttnn-floor"]], "ttnn.floor_bw": [[129, "ttnn-floor-bw"]], "ttnn.floor_div": [[130, "ttnn-floor-div"]], "ttnn.fmod": [[131, "ttnn-fmod"]], "ttnn.fmod_bw": [[132, "ttnn-fmod-bw"]], "ttnn.frac": [[133, "ttnn-frac"]], "ttnn.frac_bw": [[134, "ttnn-frac-bw"]], "ttnn.from_buffer": [[135, "ttnn-from-buffer"]], "ttnn.from_device": [[136, "ttnn-from-device"]], "ttnn.from_torch": [[137, "ttnn-from-torch"]], "ttnn.full": [[138, "ttnn-full"]], "ttnn.full_like": [[139, "ttnn-full-like"]], "ttnn.gather": [[140, "ttnn-gather"]], "ttnn.gcd": [[141, "ttnn-gcd"]], "ttnn.ge": [[142, "ttnn-ge"]], "ttnn.ge_": [[143, "ttnn-ge"]], "ttnn.geglu": [[144, "ttnn-geglu"]], "ttnn.gelu": [[145, "ttnn-gelu"]], "ttnn.gelu_bw": [[146, "ttnn-gelu-bw"]], "ttnn.gez": [[147, "ttnn-gez"]], "ttnn.global_avg_pool2d": [[148, "ttnn-global-avg-pool2d"]], "ttnn.glu": [[149, "ttnn-glu"]], "ttnn.group_norm": [[150, "ttnn-group-norm"]], "weight (gamma) and bias (beta)": [[150, "id3"], [182, "id4"], [183, "id4"], [304, "id4"], [305, "id4"]], "input_mask": [[150, "id4"]], "ttnn.gt": [[151, "ttnn-gt"]], "ttnn.gt_": [[152, "ttnn-gt"]], "ttnn.gtz": [[153, "ttnn-gtz"]], "ttnn.hardshrink": [[154, "ttnn-hardshrink"]], "ttnn.hardshrink_bw": [[155, "ttnn-hardshrink-bw"]], "ttnn.hardsigmoid": [[156, "ttnn-hardsigmoid"]], "ttnn.hardsigmoid_bw": [[157, "ttnn-hardsigmoid-bw"]], "ttnn.hardswish": [[158, "ttnn-hardswish"]], "ttnn.hardswish_bw": [[159, "ttnn-hardswish-bw"]], "ttnn.hardtanh": [[160, "ttnn-hardtanh"]], "ttnn.hardtanh_bw": [[161, "ttnn-hardtanh-bw"]], "ttnn.heaviside": [[162, "ttnn-heaviside"]], "ttnn.hypot": [[163, "ttnn-hypot"]], "ttnn.hypot_bw": [[164, "ttnn-hypot-bw"]], "ttnn.i0": [[165, "ttnn-i0"]], "ttnn.i0_bw": [[166, "ttnn-i0-bw"]], "ttnn.identity": [[167, "ttnn-identity"]], "ttnn.imag": [[168, "ttnn-imag"]], "ttnn.imag_bw": [[169, "ttnn-imag-bw"]], "ttnn.indexed_fill": [[170, "ttnn-indexed-fill"]], "ttnn.is_imag": [[171, "ttnn-is-imag"]], "ttnn.is_real": [[172, "ttnn-is-real"]], "ttnn.isclose": [[173, "ttnn-isclose"]], "ttnn.isfinite": [[174, "ttnn-isfinite"]], "ttnn.isinf": [[175, "ttnn-isinf"]], "ttnn.isnan": [[176, "ttnn-isnan"]], "ttnn.isneginf": [[177, "ttnn-isneginf"]], "ttnn.isposinf": [[178, "ttnn-isposinf"]], "ttnn.kv_cache.fill_cache_for_user_": [[179, "ttnn-kv-cache-fill-cache-for-user"]], "ttnn.kv_cache.update_cache_for_token_": [[180, "ttnn-kv-cache-update-cache-for-token"]], "ttnn.l1_loss": [[181, "ttnn-l1-loss"]], "ttnn.layer_norm": [[182, "ttnn-layer-norm"]], "residual_input_tensor": [[182, "id3"], [184, "id3"], [304, "id3"], [306, "id3"]], "ttnn.layer_norm_post_all_gather": [[183, "ttnn-layer-norm-post-all-gather"]], "stats": [[183, "id3"], [305, "id3"]], "ttnn.layer_norm_pre_all_gather": [[184, "ttnn-layer-norm-pre-all-gather"]], "ttnn.lcm": [[185, "ttnn-lcm"]], "ttnn.ldexp": [[186, "ttnn-ldexp"]], "ttnn.ldexp_bw": [[187, "ttnn-ldexp-bw"]], "ttnn.le": [[188, "ttnn-le"]], "ttnn.le_": [[189, "ttnn-le"]], "ttnn.leaky_relu": [[190, "ttnn-leaky-relu"]], "ttnn.leaky_relu_bw": [[191, "ttnn-leaky-relu-bw"]], "ttnn.lerp": [[192, "ttnn-lerp"]], "ttnn.lerp_bw": [[193, "ttnn-lerp-bw"]], "ttnn.lez": [[194, "ttnn-lez"]], "ttnn.lgamma": [[195, "ttnn-lgamma"]], "ttnn.lgamma_bw": [[196, "ttnn-lgamma-bw"]], "ttnn.linear": [[197, "ttnn-linear"]], "input_tensor_a": [[197, "id2"], [230, "id2"], [344, "id2"]], "input_tensor_b": [[197, "id3"], [230, "id3"], [344, "id3"]], "bias": [[197, "id4"]], "ttnn.load_tensor": [[198, "ttnn-load-tensor"]], "ttnn.log": [[199, "ttnn-log"]], "ttnn.log10": [[200, "ttnn-log10"]], "ttnn.log10_bw": [[201, "ttnn-log10-bw"]], "ttnn.log1p": [[202, "ttnn-log1p"]], "ttnn.log1p_bw": [[203, "ttnn-log1p-bw"]], "ttnn.log2": [[204, "ttnn-log2"]], "ttnn.log2_bw": [[205, "ttnn-log2-bw"]], "ttnn.log_bw": [[206, "ttnn-log-bw"]], "ttnn.log_sigmoid": [[207, "ttnn-log-sigmoid"]], "ttnn.log_sigmoid_bw": [[208, "ttnn-log-sigmoid-bw"]], "ttnn.logaddexp": [[209, "ttnn-logaddexp"]], "ttnn.logaddexp2": [[210, "ttnn-logaddexp2"]], "ttnn.logaddexp2_bw": [[211, "ttnn-logaddexp2-bw"]], "ttnn.logaddexp_bw": [[212, "ttnn-logaddexp-bw"]], "ttnn.logical_and": [[213, "ttnn-logical-and"]], "ttnn.logical_and_": [[214, "ttnn-logical-and"]], "ttnn.logical_not": [[215, "ttnn-logical-not"]], "ttnn.logical_not_": [[216, "ttnn-logical-not"]], "ttnn.logical_or": [[217, "ttnn-logical-or"]], "ttnn.logical_or_": [[218, "ttnn-logical-or"]], "ttnn.logical_xor": [[219, "ttnn-logical-xor"]], "ttnn.logical_xor_": [[220, "ttnn-logical-xor"]], "ttnn.logit": [[221, "ttnn-logit"]], "ttnn.logit_bw": [[222, "ttnn-logit-bw"]], "ttnn.logiteps_bw": [[223, "ttnn-logiteps-bw"]], "ttnn.lt": [[224, "ttnn-lt"]], "ttnn.lt_": [[225, "ttnn-lt"]], "ttnn.ltz": [[226, "ttnn-ltz"]], "ttnn.mac": [[227, "ttnn-mac"]], "ttnn.manage_device": [[228, "ttnn-manage-device"]], "ttnn.manual_seed": [[229, "ttnn-manual-seed"]], "ttnn.matmul": [[230, "ttnn-matmul"]], "Supported Memory Configurations": [[230, "id4"], [344, "id5"]], "ttnn.max": [[231, "ttnn-max"]], "ttnn.max_bw": [[232, "ttnn-max-bw"]], "ttnn.max_pool2d": [[233, "ttnn-max-pool2d"]], "ttnn.maximum": [[234, "ttnn-maximum"]], "ttnn.mean": [[235, "ttnn-mean"]], "ttnn.min": [[236, "ttnn-min"]], "ttnn.min_bw": [[237, "ttnn-min-bw"]], "ttnn.minimum": [[238, "ttnn-minimum"]], "ttnn.mish": [[239, "ttnn-mish"]], "ttnn.model_preprocessing.preprocess_model": [[240, "ttnn-model-preprocessing-preprocess-model"]], "ttnn.model_preprocessing.preprocess_model_parameters": [[241, "ttnn-model-preprocessing-preprocess-model-parameters"]], "ttnn.moe": [[242, "ttnn-moe"]], "ttnn.mse_loss": [[243, "ttnn-mse-loss"]], "ttnn.mul_bw": [[244, "ttnn-mul-bw"]], "ttnn.multigammaln": [[245, "ttnn-multigammaln"]], "ttnn.multigammaln_bw": [[246, "ttnn-multigammaln-bw"]], "ttnn.multiply": [[247, "ttnn-multiply"]], "ttnn.ne": [[248, "ttnn-ne"]], "ttnn.ne_": [[249, "ttnn-ne"]], "ttnn.neg": [[250, "ttnn-neg"]], "ttnn.neg_bw": [[251, "ttnn-neg-bw"]], "ttnn.nextafter": [[252, "ttnn-nextafter"]], "ttnn.nez": [[253, "ttnn-nez"]], "ttnn.nonzero": [[254, "ttnn-nonzero"]], "ttnn.normalize_global": [[255, "ttnn-normalize-global"]], "ttnn.normalize_hw": [[256, "ttnn-normalize-hw"]], "ttnn.ones": [[257, "ttnn-ones"]], "ttnn.ones_like": [[258, "ttnn-ones-like"]], "ttnn.open_device": [[259, "ttnn-open-device"]], "ttnn.outer": [[260, "ttnn-outer"]], "ttnn.pad": [[261, "ttnn-pad"]], "ttnn.pad_to_tile_shape": [[262, "ttnn-pad-to-tile-shape"]], "ttnn.permute": [[263, "ttnn-permute"]], "ttnn.polar": [[264, "ttnn-polar"]], "ttnn.polar_bw": [[265, "ttnn-polar-bw"]], "ttnn.polygamma": [[266, "ttnn-polygamma"]], "ttnn.polygamma_bw": [[267, "ttnn-polygamma-bw"]], "ttnn.polyval": [[268, "ttnn-polyval"]], "ttnn.pow": [[269, "ttnn-pow"]], "ttnn.pow_bw": [[270, "ttnn-pow-bw"]], "ttnn.prelu": [[271, "ttnn-prelu"]], "ttnn.prepare_conv_bias": [[272, "ttnn-prepare-conv-bias"]], "ttnn.prepare_conv_transpose2d_bias": [[273, "ttnn-prepare-conv-transpose2d-bias"]], "ttnn.prepare_conv_transpose2d_weights": [[274, "ttnn-prepare-conv-transpose2d-weights"]], "ttnn.prepare_conv_weights": [[275, "ttnn-prepare-conv-weights"]], "ttnn.prod": [[276, "ttnn-prod"]], "ttnn.prod_bw": [[277, "ttnn-prod-bw"]], "ttnn.rad2deg": [[278, "ttnn-rad2deg"]], "ttnn.rad2deg_bw": [[279, "ttnn-rad2deg-bw"]], "ttnn.rand": [[280, "ttnn-rand"]], "ttnn.rdiv": [[281, "ttnn-rdiv"]], "ttnn.rdiv_bw": [[282, "ttnn-rdiv-bw"]], "ttnn.real": [[283, "ttnn-real"]], "ttnn.real_bw": [[284, "ttnn-real-bw"]], "ttnn.reallocate": [[285, "ttnn-reallocate"]], "ttnn.reciprocal": [[286, "ttnn-reciprocal"]], "ttnn.reciprocal_bw": [[287, "ttnn-reciprocal-bw"]], "ttnn.reduce_scatter": [[288, "ttnn-reduce-scatter"]], "ttnn.register_post_operation_hook": [[289, "ttnn-register-post-operation-hook"]], "ttnn.register_pre_operation_hook": [[290, "ttnn-register-pre-operation-hook"]], "ttnn.reglu": [[291, "ttnn-reglu"]], "ttnn.relu": [[292, "ttnn-relu"]], "ttnn.relu6": [[293, "ttnn-relu6"]], "ttnn.relu6_bw": [[294, "ttnn-relu6-bw"]], "ttnn.relu_bw": [[295, "ttnn-relu-bw"]], "ttnn.relu_max": [[296, "ttnn-relu-max"]], "ttnn.relu_min": [[297, "ttnn-relu-min"]], "ttnn.remainder": [[298, "ttnn-remainder"]], "ttnn.remainder_bw": [[299, "ttnn-remainder-bw"]], "ttnn.repeat": [[300, "ttnn-repeat"]], "ttnn.repeat_bw": [[301, "ttnn-repeat-bw"]], "ttnn.repeat_interleave": [[302, "ttnn-repeat-interleave"]], "ttnn.reshape": [[303, "ttnn-reshape"]], "ttnn.rms_norm": [[304, "ttnn-rms-norm"]], "ttnn.rms_norm_post_all_gather": [[305, "ttnn-rms-norm-post-all-gather"]], "ttnn.rms_norm_pre_all_gather": [[306, "ttnn-rms-norm-pre-all-gather"]], "ttnn.round": [[307, "ttnn-round"]], "ttnn.round_bw": [[308, "ttnn-round-bw"]], "ttnn.rpow": [[309, "ttnn-rpow"]], "ttnn.rpow_bw": [[310, "ttnn-rpow-bw"]], "ttnn.rsqrt": [[311, "ttnn-rsqrt"]], "ttnn.rsqrt_bw": [[312, "ttnn-rsqrt-bw"]], "ttnn.rsub": [[313, "ttnn-rsub"]], "ttnn.rsub_bw": [[314, "ttnn-rsub-bw"]], "ttnn.scale_causal_mask_hw_dims_softmax_in_place": [[315, "ttnn-scale-causal-mask-hw-dims-softmax-in-place"]], "Input Tensor (Sharded)": [[315, "id2"]], "Mask Tensor [1, 1, H, W]": [[315, "id3"]], "ttnn.scale_mask_softmax": [[316, "ttnn-scale-mask-softmax"]], "Mask Tensor (optional)": [[316, "id3"], [317, "id3"]], "ttnn.scale_mask_softmax_in_place": [[317, "ttnn-scale-mask-softmax-in-place"]], "ttnn.scatter": [[318, "ttnn-scatter"]], "ttnn.selu": [[319, "ttnn-selu"]], "ttnn.selu_bw": [[320, "ttnn-selu-bw"]], "ttnn.set_printoptions": [[321, "ttnn-set-printoptions"]], "Examples": [[321, null]], "ttnn.sigmoid": [[322, "ttnn-sigmoid"]], "ttnn.sigmoid_accurate": [[323, "ttnn-sigmoid-accurate"]], "ttnn.sigmoid_bw": [[324, "ttnn-sigmoid-bw"]], "ttnn.sign": [[325, "ttnn-sign"]], "ttnn.sign_bw": [[326, "ttnn-sign-bw"]], "ttnn.signbit": [[327, "ttnn-signbit"]], "ttnn.silu": [[328, "ttnn-silu"]], "ttnn.silu_bw": [[329, "ttnn-silu-bw"]], "ttnn.sin": [[330, "ttnn-sin"]], "ttnn.sin_bw": [[331, "ttnn-sin-bw"]], "ttnn.sinh": [[332, "ttnn-sinh"]], "ttnn.sinh_bw": [[333, "ttnn-sinh-bw"]], "ttnn.slice": [[334, "ttnn-slice"]], "ttnn.softmax": [[335, "ttnn-softmax"]], "ttnn.softmax_in_place": [[336, "ttnn-softmax-in-place"]], "ttnn.softplus": [[337, "ttnn-softplus"]], "ttnn.softplus_bw": [[338, "ttnn-softplus-bw"]], "ttnn.softshrink": [[339, "ttnn-softshrink"]], "ttnn.softshrink_bw": [[340, "ttnn-softshrink-bw"]], "ttnn.softsign": [[341, "ttnn-softsign"]], "ttnn.softsign_bw": [[342, "ttnn-softsign-bw"]], "ttnn.sort": [[343, "ttnn-sort"]], "ttnn.sparse_matmul": [[344, "ttnn-sparse-matmul"]], "sparsity": [[344, "id4"]], "ttnn.split_work_to_cores": [[345, "ttnn-split-work-to-cores"]], "ttnn.sqrt": [[346, "ttnn-sqrt"]], "ttnn.sqrt_bw": [[347, "ttnn-sqrt-bw"]], "ttnn.square": [[348, "ttnn-square"]], "ttnn.square_bw": [[349, "ttnn-square-bw"]], "ttnn.squared_difference": [[350, "ttnn-squared-difference"]], "ttnn.squared_difference_bw": [[351, "ttnn-squared-difference-bw"]], "ttnn.std": [[352, "ttnn-std"]], "ttnn.sub_bw": [[353, "ttnn-sub-bw"]], "ttnn.subalpha": [[354, "ttnn-subalpha"]], "ttnn.subalpha_bw": [[355, "ttnn-subalpha-bw"]], "ttnn.subtract": [[356, "ttnn-subtract"]], "ttnn.sum": [[357, "ttnn-sum"]], "ttnn.swiglu": [[358, "ttnn-swiglu"]], "ttnn.swish": [[359, "ttnn-swish"]], "ttnn.synchronize_device": [[360, "ttnn-synchronize-device"]], "ttnn.tan": [[361, "ttnn-tan"]], "ttnn.tan_bw": [[362, "ttnn-tan-bw"]], "ttnn.tanh": [[363, "ttnn-tanh"]], "ttnn.tanh_bw": [[364, "ttnn-tanh-bw"]], "ttnn.tanhshrink": [[365, "ttnn-tanhshrink"]], "ttnn.tanhshrink_bw": [[366, "ttnn-tanhshrink-bw"]], "ttnn.threshold": [[367, "ttnn-threshold"]], "ttnn.threshold_bw": [[368, "ttnn-threshold-bw"]], "ttnn.tilize": [[369, "ttnn-tilize"]], "ttnn.tilize_with_val_padding": [[370, "ttnn-tilize-with-val-padding"]], "ttnn.to_device": [[371, "ttnn-to-device"]], "ttnn.to_layout": [[372, "ttnn-to-layout"]], "ttnn.to_memory_config": [[373, "ttnn-to-memory-config"]], "ttnn.to_torch": [[374, "ttnn-to-torch"]], "ttnn.topk": [[375, "ttnn-topk"]], "index_tensor": [[375, "id3"]], "ttnn.transformer.attention_softmax": [[376, "ttnn-transformer-attention-softmax"]], "ttnn.transformer.attention_softmax_": [[377, "ttnn-transformer-attention-softmax"]], "ttnn.transformer.concatenate_heads": [[378, "ttnn-transformer-concatenate-heads"]], "ttnn.transformer.scaled_dot_product_attention": [[379, "ttnn-transformer-scaled-dot-product-attention"]], "ttnn.transformer.scaled_dot_product_attention_decode": [[380, "ttnn-transformer-scaled-dot-product-attention-decode"]], "ttnn.transformer.split_query_key_value_and_split_heads": [[381, "ttnn-transformer-split-query-key-value-and-split-heads"]], "ttnn.tril": [[382, "ttnn-tril"]], "ttnn.triu": [[383, "ttnn-triu"]], "ttnn.trunc": [[384, "ttnn-trunc"]], "ttnn.trunc_bw": [[385, "ttnn-trunc-bw"]], "ttnn.unary_chain": [[386, "ttnn-unary-chain"]], "ttnn.untilize": [[387, "ttnn-untilize"]], "ttnn.untilize_with_unpadding": [[388, "ttnn-untilize-with-unpadding"]], "ttnn.upsample": [[389, "ttnn-upsample"]], "ttnn.var": [[390, "ttnn-var"]], "ttnn.where": [[391, "ttnn-where"]], "ttnn.where_bw": [[392, "ttnn-where-bw"]], "ttnn.xlogy": [[393, "ttnn-xlogy"]], "ttnn.xlogy_bw": [[394, "ttnn-xlogy-bw"]], "ttnn.zeros": [[395, "ttnn-zeros"]], "ttnn.zeros_like": [[396, "ttnn-zeros-like"]], "Converting PyTorch Model to TT-NN": [[397, "converting-pytorch-model-to-tt-nn"]], "Step 1 - Rewriting the Model": [[397, "step-1-rewriting-the-model"]], "Step 2 - Switching to ttnn Operations": [[397, "step-2-switching-to-ttnn-operations"]], "Step 3 - Optimizing the Model": [[397, "step-3-optimizing-the-model"]], "More examples": [[397, "more-examples"]], "Building and Uplifting Demos": [[398, "building-and-uplifting-demos"]], "Getting Started": [[399, "getting-started"]], "1. Install and Build": [[399, "install-and-build"]], "2. Explore Our Model Demos": [[399, "explore-our-model-demos"]], "Where To Go From Here": [[399, "where-to-go-from-here"]], "Install": [[400, "install"]], "Prerequisites:": [[400, "prerequisites"]], "1: Set Up the Hardware": [[400, "set-up-the-hardware"]], "2: Install Software Dependencies": [[400, "install-software-dependencies"]], "Option 1: TT-Installer Script (recommended)": [[400, "option-1-tt-installer-script-recommended"]], "Option 2: Manual Installation": [[400, "option-2-manual-installation"]], "TT-NN / TT-Metalium Installation": [[400, "tt-nn-tt-metalium-installation"]], "There are four options for installing TT-Metalium:": [[400, "there-are-four-options-for-installing-tt-metalium"]], "Binaries": [[400, "binaries"]], "Step 1. Install the Latest Wheel:": [[400, "step-1-install-the-latest-wheel"]], "Step 2. (For models users only) Set Up Environment for Models:": [[400, "step-2-for-models-users-only-set-up-environment-for-models"]], "Docker Release Image": [[400, "docker-release-image"]], "Source": [[400, "source"]], "Step 1. Clone the Repository:": [[400, "step-1-clone-the-repository"]], "Step 2. Build the Library:": [[400, "step-2-build-the-library"]], "Step 3. Virtual Environment Setup": [[400, "step-3-virtual-environment-setup"]], "Anaconda": [[400, "anaconda"]], "Step 1. Install the Latest Package:": [[400, "step-1-install-the-latest-package"]], "You are All Set!": [[400, "you-are-all-set"]], "To verify your installation (for source or wheel installation only), try executing a programming example:": [[400, "to-verify-your-installation-for-source-or-wheel-installation-only-try-executing-a-programming-example"]], "Interested in Contributing?": [[400, "interested-in-contributing"]], "Multi-Card Configuration (TT-Topology)": [[400, "multi-card-configuration-tt-topology"]], "Virtual Machine Requirements": [[400, "virtual-machine-requirements"]], "Overview": [[400, "overview"]], "Why It Matters": [[400, "why-it-matters"]], "Requirements for VMs": [[400, "requirements-for-vms"]], "Onboarding New Functionality": [[401, "onboarding-new-functionality"]], "Profiling TT-NN Operations": [[402, "profiling-tt-nn-operations"]], "Perf Report Headers": [[402, "perf-report-headers"]], "profile_this description": [[402, "profile-this-description"]], "Using the Performance Report with TT-NN Visualizer": [[402, "using-the-performance-report-with-tt-nn-visualizer"]], "Tensor": [[403, "tensor"]], "Shape": [[403, "shape"]], "Layout": [[403, "layout"]], "Data Type": [[403, "data-type"]], "Required Width Multiples for Data Types": [[403, "id5"]], "Limitation of BFLOAT8_B": [[403, "limitation-of-bfloat8-b"]], "Storage": [[403, "storage"]], "Tensor Sharding": [[403, "tensor-sharding"]], "Tutorials": [[404, "tutorials"]], "Add Tensors": [[405, "Add-Tensors"]], "Import Libraries": [[405, "Import-Libraries"], [406, "Import-Libraries"], [407, "Import-Libraries"], [408, "Import-Libraries"], [410, "Import-Libraries"], [413, "Import-Libraries"]], "Open Tenstorrent device": [[405, "Open-Tenstorrent-device"]], "Addition Operation and Conversion": [[405, "Addition-Operation-and-Conversion"]], "Close the Device": [[405, "Close-the-Device"], [406, "Close-the-Device"], [408, "Close-the-Device"], [410, "Close-the-Device"], [412, "Close-the-Device"]], "Full Example and Output": [[405, "Full-Example-and-Output"], [406, "Full-Example-and-Output"], [407, "Full-Example-and-Output"], [408, "Full-Example-and-Output"], [410, "Full-Example-and-Output"], [411, "Full-Example-and-Output"], [412, "Full-Example-and-Output"]], "Basic Convolution": [[406, "Basic-Convolution"]], "Set Manual Seed": [[406, "Set-Manual-Seed"]], "Open the Device": [[406, "Open-the-Device"], [407, "Open-the-Device"], [408, "Open-the-Device"], [410, "Open-the-Device"], [412, "Open-the-Device"]], "Create Forward Method": [[406, "Create-Forward-Method"]], "Set Input and Convolution Parameters": [[406, "Set-Input-and-Convolution-Parameters"]], "Create Tensors": [[406, "Create-Tensors"]], "Run the Convolution Operation": [[406, "Run-the-Convolution-Operation"]], "Tensor Configuration": [[407, "Tensor-Configuration"]], "Initialize tensors a and b with random values": [[407, "Initialize-tensors-a-and-b-with-random-values"]], "Matrix multiply tensor a and b": [[407, "Matrix-multiply-tensor-a-and-b"]], "Inspect the layout of matrix multiplication output": [[407, "Inspect-the-layout-of-matrix-multiplication-output"]], "Inspect the result of the matrix multiplication": [[407, "Inspect-the-result-of-the-matrix-multiplication"]], "Matrix multiply tensor a and b by using more performant config": [[407, "Matrix-multiply-tensor-a-and-b-by-using-more-performant-config"]], "Close the device": [[407, "Close-the-device"], [413, "Close-the-device"]], "Basic Tensor Operations": [[408, "Basic-Tensor-Operations"]], "Host Tensor Creation": [[408, "Host-Tensor-Creation"]], "Host Tensor Conversion and Creation": [[408, "Host-Tensor-Conversion-and-Creation"]], "Tile-Based Arithmetic Operations": [[408, "Tile-Based-Arithmetic-Operations"]], "Simulated Broadcasting - Row Vector Expansion": [[408, "Simulated-Broadcasting---Row-Vector-Expansion"]], "Building CLIP Model for Zero-Shot Image Classification with TT-NN": [[409, "Building-CLIP-Model-for-Zero-Shot-Image-Classification-with-TT-NN"]], "What CLIP Does": [[409, "What-CLIP-Does"]], "Imports and Dependencies": [[409, "Imports-and-Dependencies"]], "TT-NN Device Management and Utility Functions": [[409, "TT-NN-Device-Management-and-Utility-Functions"]], "Model Weight Conversion": [[409, "Model-Weight-Conversion"]], "Generic Transformer Implementation": [[409, "Generic-Transformer-Implementation"]], "Transformer Architecture": [[409, "Transformer-Architecture"]], "Vision Transformer Implementation": [[409, "Vision-Transformer-Implementation"]], "Vision Processing Pipeline": [[409, "Vision-Processing-Pipeline"]], "Complete CLIP Model Implementation": [[409, "Complete-CLIP-Model-Implementation"]], "CLIP Architecture Components": [[409, "CLIP-Architecture-Components"]], "Key Methods": [[409, "Key-Methods"]], "Image Preprocessing": [[409, "Image-Preprocessing"]], "Preprocessing Pipeline": [[409, "Preprocessing-Pipeline"]], "Image Download Utility": [[409, "Image-Download-Utility"]], "Model and Tokenizer downloading": [[409, "Model-and-Tokenizer-downloading"]], "Running CLIP Inference": [[409, "Running-CLIP-Inference"]], "Inference Pipeline:": [[409, "Inference-Pipeline:"]], "Text Tokenization": [[409, "Text-Tokenization"]], "MLP Inference": [[410, "MLP-Inference"]], "Load MNIST Test Data": [[410, "Load-MNIST-Test-Data"]], "Load Pretrained MLP Weights": [[410, "Load-Pretrained-MLP-Weights"]], "Accuracy Tracking, Inference, Loop, and Image Flattening": [[410, "Accuracy-Tracking,-Inference,-Loop,-and-Image-Flattening"]], "Multi-Head Attention": [[411, "Multi-Head-Attention"]], "Write Multi-Head Attention with TT-NN": [[411, "Write-Multi-Head-Attention-with-TT-NN"]], "Configuration": [[411, "Configuration"]], "Running a Simple CNN Inference on CIFAR-10": [[412, "Running-a-Simple-CNN-Inference-on-CIFAR-10"]], "Setup and Imports": [[412, "Setup-and-Imports"]], "Load the CIFAR-10 Dataset": [[412, "Load-the-CIFAR-10-Dataset"]], "Load or Initialize Weights": [[412, "Load-or-Initialize-Weights"]], "Define Convolution and Pooling Stage": [[412, "Define-Convolution-and-Pooling-Stage"]], "Run Inference on Test Samples": [[412, "Run-Inference-on-Test-Samples"]], "TT-NN Tracer and BERT Model Visualization Tutorial": [[413, "TT-NN-Tracer-and-BERT-Model-Visualization-Tutorial"]], "Set program config": [[413, "Set-program-config"]], "Example 1: Tracing PyTorch Operations": [[413, "Example-1:-Tracing-PyTorch-Operations"]], "Example 2: Tracing TT-NN Tensor Operations": [[413, "Example-2:-Tracing-TT-NN-Tensor-Operations"]], "Model and Config downloading": [[413, "Model-and-Config-downloading"]], "Example 3: Tracing a BERT Layer": [[413, "Example-3:-Tracing-a-BERT-Layer"]], "Example 4: Trace models written using ttnn": [[413, "Example-4:-Trace-models-written-using-ttnn"]], "TT-NN Visualizer": [[414, "tt-nn-visualizer"]], "Prerequisites": [[414, "prerequisites"]], "Running TT-NN Visualizer": [[414, "running-tt-nn-visualizer"]], "Model Profiling": [[414, "model-profiling"]], "Generating the Memory Report": [[414, "generating-the-memory-report"]], "Generating Performance Reports": [[414, "generating-performance-reports"]], "Result Analysis": [[414, "result-analysis"]], "Uploading Reports": [[414, "uploading-reports"]], "Operations Tab": [[414, "operations-tab"]], "Tensors Tab": [[414, "tensors-tab"]], "Buffers Tab": [[414, "buffers-tab"]], "Graph Tab": [[414, "graph-tab"]], "Performance Tab": [[414, "performance-tab"]], "Recap": [[414, "recap"]], "Using TT-NN": [[415, "using-tt-nn"]], "Basic Examples": [[415, "basic-examples"]], "1. Converting from and to torch tensor": [[415, "converting-from-and-to-torch-tensor"]], "2. Running an operation on the device": [[415, "running-an-operation-on-the-device"]], "3. Using __getitem__ to slice the tensor": [[415, "using-getitem-to-slice-the-tensor"]], "4. Enabling program cache": [[415, "enabling-program-cache"]], "5. Debugging intermediate tensors": [[415, "debugging-intermediate-tensors"]], "6. Tracing the graph of operations": [[415, "tracing-the-graph-of-operations"]], "7. Using tt_lib operation in TT-NN": [[415, "using-tt-lib-operation-in-tt-nn"]], "8. Enabling Logging": [[415, "enabling-logging"]], "9. Supported Python Operators": [[415, "supported-python-operators"]], "10. Changing the string representation of the tensor": [[415, "changing-the-string-representation-of-the-tensor"]], "11. Visualize using Web Browser": [[415, "visualize-using-web-browser"]], "12. Register pre- and/or post-operation hooks": [[415, "register-pre-and-or-post-operation-hooks"]], "13. Query all operations": [[415, "query-all-operations"]], "14. Falling back to torch": [[415, "falling-back-to-torch"]], "15. Capturing graph of C++ functions, buffer allocations, etc": [[415, "capturing-graph-of-c-functions-buffer-allocations-etc"]]}, "indexentries": {"conv2dconfig (class in ttnn)": [[7, "ttnn.Conv2dConfig"]], "act_block_h_override (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.act_block_h_override"]], "act_block_w_div (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.act_block_w_div"]], "activation (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.activation"]], "config_tensors_in_dram (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.config_tensors_in_dram"]], "core_grid (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.core_grid"]], "deallocate_activation (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.deallocate_activation"]], "enable_act_double_buffer (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.enable_act_double_buffer"]], "enable_activation_reuse (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.enable_activation_reuse"]], "enable_kernel_stride_folding (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.enable_kernel_stride_folding"]], "enable_weights_double_buffer (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.enable_weights_double_buffer"]], "force_split_reader (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.force_split_reader"]], "full_inner_dim (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.full_inner_dim"]], "output_layout (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.output_layout"]], "override_output_sharding_config (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.override_output_sharding_config"]], "override_sharding_config (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.override_sharding_config"]], "reallocate_halo_output (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.reallocate_halo_output"]], "reshard_if_not_optimal (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.reshard_if_not_optimal"]], "shard_layout (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.shard_layout"]], "transpose_shards (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.transpose_shards"]], "weights_dtype (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.weights_dtype"]], "conv2dsliceconfig (class in ttnn)": [[8, "ttnn.Conv2dSliceConfig"]], "conv2dsliceconfig.slicetypeenum (class in ttnn)": [[8, "ttnn.Conv2dSliceConfig.SliceTypeEnum"]], "dramsliceheight (ttnn.conv2dsliceconfig.slicetypeenum attribute)": [[8, "ttnn.Conv2dSliceConfig.SliceTypeEnum.DRAMSliceHeight"]], "dramslicewidth (ttnn.conv2dsliceconfig.slicetypeenum attribute)": [[8, "ttnn.Conv2dSliceConfig.SliceTypeEnum.DRAMSliceWidth"]], "l1full (ttnn.conv2dsliceconfig.slicetypeenum attribute)": [[8, "ttnn.Conv2dSliceConfig.SliceTypeEnum.L1Full"]], "name (ttnn.conv2dsliceconfig.slicetypeenum property)": [[8, "ttnn.Conv2dSliceConfig.SliceTypeEnum.name"]], "num_slices (ttnn.conv2dsliceconfig property)": [[8, "ttnn.Conv2dSliceConfig.num_slices"]], "slice_type (ttnn.conv2dsliceconfig property)": [[8, "ttnn.Conv2dSliceConfig.slice_type"]], "value (ttnn.conv2dsliceconfig.slicetypeenum property)": [[8, "ttnn.Conv2dSliceConfig.SliceTypeEnum.value"]], "getdefaultdevice() (in module ttnn)": [[9, "ttnn.GetDefaultDevice"]], "matmulmulticorereusemulticast1dprogramconfig (class in ttnn)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig"]], "compute_with_storage_grid_size (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.compute_with_storage_grid_size"]], "from_json() (ttnn.matmulmulticorereusemulticast1dprogramconfig method)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.from_json"]], "fuse_batch (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.fuse_batch"]], "fused_activation (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.fused_activation"]], "gather_in0 (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.gather_in0"]], "hop_cores (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.hop_cores"]], "in0_block_w (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.in0_block_w"]], "mcast_in0 (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.mcast_in0"]], "num_global_cb_receivers (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.num_global_cb_receivers"]], "out_block_h (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.out_block_h"]], "out_block_w (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.out_block_w"]], "out_subblock_h (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.out_subblock_h"]], "out_subblock_w (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.out_subblock_w"]], "per_core_m (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.per_core_M"]], "per_core_n (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.per_core_N"]], "to_json() (ttnn.matmulmulticorereusemulticast1dprogramconfig method)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.to_json"]], "untilize_out (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.untilize_out"]], "matmulmulticorereusemulticastdramshardedprogramconfig (class in ttnn)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig"]], "from_json() (ttnn.matmulmulticorereusemulticastdramshardedprogramconfig method)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig.from_json"]], "fused_activation (ttnn.matmulmulticorereusemulticastdramshardedprogramconfig property)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig.fused_activation"]], "in0_block_w (ttnn.matmulmulticorereusemulticastdramshardedprogramconfig property)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig.in0_block_w"]], "per_core_m (ttnn.matmulmulticorereusemulticastdramshardedprogramconfig property)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig.per_core_M"]], "per_core_n (ttnn.matmulmulticorereusemulticastdramshardedprogramconfig property)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig.per_core_N"]], "to_json() (ttnn.matmulmulticorereusemulticastdramshardedprogramconfig method)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig.to_json"]], "matmulmulticorereusemulticastprogramconfig (class in ttnn)": [[12, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig"]], "compute_with_storage_grid_size (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[12, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.compute_with_storage_grid_size"]], "from_json() (ttnn.matmulmulticorereusemulticastprogramconfig method)": [[12, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.from_json"]], "fuse_batch (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[12, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.fuse_batch"]], "fused_activation (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[12, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.fused_activation"]], "in0_block_w (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[12, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.in0_block_w"]], "out_block_h (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[12, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.out_block_h"]], "out_block_w (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[12, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.out_block_w"]], "out_subblock_h (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[12, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.out_subblock_h"]], "out_subblock_w (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[12, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.out_subblock_w"]], "per_core_m (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[12, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.per_core_M"]], "per_core_n (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[12, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.per_core_N"]], "to_json() (ttnn.matmulmulticorereusemulticastprogramconfig method)": [[12, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.to_json"]], "transpose_mcast (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[12, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.transpose_mcast"]], "matmulmulticorereuseprogramconfig (class in ttnn)": [[13, "ttnn.MatmulMultiCoreReuseProgramConfig"]], "compute_with_storage_grid_size (ttnn.matmulmulticorereuseprogramconfig property)": [[13, "ttnn.MatmulMultiCoreReuseProgramConfig.compute_with_storage_grid_size"]], "from_json() (ttnn.matmulmulticorereuseprogramconfig method)": [[13, "ttnn.MatmulMultiCoreReuseProgramConfig.from_json"]], "in0_block_w (ttnn.matmulmulticorereuseprogramconfig property)": [[13, "ttnn.MatmulMultiCoreReuseProgramConfig.in0_block_w"]], "out_subblock_h (ttnn.matmulmulticorereuseprogramconfig property)": [[13, "ttnn.MatmulMultiCoreReuseProgramConfig.out_subblock_h"]], "out_subblock_w (ttnn.matmulmulticorereuseprogramconfig property)": [[13, "ttnn.MatmulMultiCoreReuseProgramConfig.out_subblock_w"]], "per_core_m (ttnn.matmulmulticorereuseprogramconfig property)": [[13, "ttnn.MatmulMultiCoreReuseProgramConfig.per_core_M"]], "per_core_n (ttnn.matmulmulticorereuseprogramconfig property)": [[13, "ttnn.MatmulMultiCoreReuseProgramConfig.per_core_N"]], "to_json() (ttnn.matmulmulticorereuseprogramconfig method)": [[13, "ttnn.MatmulMultiCoreReuseProgramConfig.to_json"]], "setdefaultdevice() (in module ttnn)": [[14, "ttnn.SetDefaultDevice"]], "softmaxdefaultprogramconfig (class in ttnn)": [[15, "ttnn.SoftmaxDefaultProgramConfig"]], "softmaxprogramconfig (class in ttnn)": [[16, "ttnn.SoftmaxProgramConfig"]], "softmaxshardedmulticoreprogramconfig (class in ttnn)": [[17, "ttnn.SoftmaxShardedMultiCoreProgramConfig"]], "block_w (ttnn.softmaxshardedmulticoreprogramconfig property)": [[17, "ttnn.SoftmaxShardedMultiCoreProgramConfig.block_w"]], "abs() (in module ttnn)": [[18, "ttnn.abs"]], "abs_bw() (in module ttnn)": [[19, "ttnn.abs_bw"]], "acos() (in module ttnn)": [[20, "ttnn.acos"]], "acos_bw() (in module ttnn)": [[21, "ttnn.acos_bw"]], "acosh() (in module ttnn)": [[22, "ttnn.acosh"]], "acosh_bw() (in module ttnn)": [[23, "ttnn.acosh_bw"]], "add() (in module ttnn)": [[24, "ttnn.add"]], "add_bw() (in module ttnn)": [[25, "ttnn.add_bw"]], "addalpha() (in module ttnn)": [[26, "ttnn.addalpha"]], "addalpha_bw() (in module ttnn)": [[27, "ttnn.addalpha_bw"]], "addcdiv() (in module ttnn)": [[28, "ttnn.addcdiv"]], "addcdiv_bw() (in module ttnn)": [[29, "ttnn.addcdiv_bw"]], "addcmul() (in module ttnn)": [[30, "ttnn.addcmul"]], "addcmul_bw() (in module ttnn)": [[31, "ttnn.addcmul_bw"]], "addmm() (in module ttnn)": [[32, "ttnn.addmm"]], "all_gather() (in module ttnn)": [[33, "ttnn.all_gather"]], "all_reduce() (in module ttnn)": [[34, "ttnn.all_reduce"]], "alt_complex_rotate90() (in module ttnn)": [[35, "ttnn.alt_complex_rotate90"]], "angle() (in module ttnn)": [[36, "ttnn.angle"]], "angle_bw() (in module ttnn)": [[37, "ttnn.angle_bw"]], "arange() (in module ttnn)": [[38, "ttnn.arange"]], "argmax() (in module ttnn)": [[39, "ttnn.argmax"]], "as_tensor() (in module ttnn)": [[40, "ttnn.as_tensor"]], "asin() (in module ttnn)": [[41, "ttnn.asin"]], "asin_bw() (in module ttnn)": [[42, "ttnn.asin_bw"]], "asinh() (in module ttnn)": [[43, "ttnn.asinh"]], "asinh_bw() (in module ttnn)": [[44, "ttnn.asinh_bw"]], "assign_bw() (in module ttnn)": [[45, "ttnn.assign_bw"]], "atan() (in module ttnn)": [[46, "ttnn.atan"]], "atan2() (in module ttnn)": [[47, "ttnn.atan2"]], "atan2_bw() (in module ttnn)": [[48, "ttnn.atan2_bw"]], "atan_bw() (in module ttnn)": [[49, "ttnn.atan_bw"]], "atanh() (in module ttnn)": [[50, "ttnn.atanh"]], "atanh_bw() (in module ttnn)": [[51, "ttnn.atanh_bw"]], "avg_pool2d() (in module ttnn)": [[52, "ttnn.avg_pool2d"]], "batch_norm() (in module ttnn)": [[53, "ttnn.batch_norm"]], "bias_gelu_bw() (in module ttnn)": [[54, "ttnn.bias_gelu_bw"]], "bitcast() (in module ttnn)": [[55, "ttnn.bitcast"]], "bitwise_and() (in module ttnn)": [[56, "ttnn.bitwise_and"]], "bitwise_left_shift() (in module ttnn)": [[57, "ttnn.bitwise_left_shift"]], "bitwise_not() (in module ttnn)": [[58, "ttnn.bitwise_not"]], "bitwise_or() (in module ttnn)": [[59, "ttnn.bitwise_or"]], "bitwise_right_shift() (in module ttnn)": [[60, "ttnn.bitwise_right_shift"]], "bitwise_xor() (in module ttnn)": [[61, "ttnn.bitwise_xor"]], "cbrt() (in module ttnn)": [[62, "ttnn.cbrt"]], "ceil() (in module ttnn)": [[63, "ttnn.ceil"]], "ceil_bw() (in module ttnn)": [[64, "ttnn.ceil_bw"]], "celu() (in module ttnn)": [[65, "ttnn.celu"]], "celu_bw() (in module ttnn)": [[66, "ttnn.celu_bw"]], "clamp() (in module ttnn)": [[67, "ttnn.clamp"]], "clamp_bw() (in module ttnn)": [[68, "ttnn.clamp_bw"]], "clip() (in module ttnn)": [[69, "ttnn.clip"]], "clip_bw() (in module ttnn)": [[70, "ttnn.clip_bw"]], "clone() (in module ttnn)": [[71, "ttnn.clone"]], "close_device() (in module ttnn)": [[72, "ttnn.close_device"]], "concat() (in module ttnn)": [[73, "ttnn.concat"]], "concat_bw() (in module ttnn)": [[74, "ttnn.concat_bw"]], "conj() (in module ttnn)": [[75, "ttnn.conj"]], "conj_bw() (in module ttnn)": [[76, "ttnn.conj_bw"]], "conv1d() (in module ttnn)": [[77, "ttnn.conv1d"]], "conv2d() (in module ttnn)": [[78, "ttnn.conv2d"]], "conv_transpose2d() (in module ttnn)": [[79, "ttnn.conv_transpose2d"]], "cos() (in module ttnn)": [[80, "ttnn.cos"]], "cos_bw() (in module ttnn)": [[81, "ttnn.cos_bw"]], "cosh() (in module ttnn)": [[82, "ttnn.cosh"]], "cosh_bw() (in module ttnn)": [[83, "ttnn.cosh_bw"]], "create_sharded_memory_config() (in module ttnn)": [[84, "ttnn.create_sharded_memory_config"]], "cumprod() (in module ttnn)": [[85, "ttnn.cumprod"]], "cumsum() (in module ttnn)": [[86, "ttnn.cumsum"]], "deallocate() (in module ttnn)": [[87, "ttnn.deallocate"]], "deg2rad() (in module ttnn)": [[88, "ttnn.deg2rad"]], "deg2rad_bw() (in module ttnn)": [[89, "ttnn.deg2rad_bw"]], "digamma() (in module ttnn)": [[90, "ttnn.digamma"]], "digamma_bw() (in module ttnn)": [[91, "ttnn.digamma_bw"]], "div() (in module ttnn)": [[92, "ttnn.div"]], "div_bw() (in module ttnn)": [[93, "ttnn.div_bw"]], "div_no_nan() (in module ttnn)": [[94, "ttnn.div_no_nan"]], "div_no_nan_bw() (in module ttnn)": [[95, "ttnn.div_no_nan_bw"]], "dump_tensor() (in module ttnn)": [[96, "ttnn.dump_tensor"]], "elu() (in module ttnn)": [[97, "ttnn.elu"]], "elu_bw() (in module ttnn)": [[98, "ttnn.elu_bw"]], "ema() (in module ttnn)": [[99, "ttnn.ema"]], "embedding() (in module ttnn)": [[100, "ttnn.embedding"]], "embedding_bw() (in module ttnn)": [[101, "ttnn.embedding_bw"]], "empty() (in module ttnn)": [[102, "ttnn.empty"]], "empty_like() (in module ttnn)": [[103, "ttnn.empty_like"]], "eq() (in module ttnn)": [[104, "ttnn.eq"]], "eq_() (in module ttnn)": [[105, "ttnn.eq_"]], "eqz() (in module ttnn)": [[106, "ttnn.eqz"]], "erf() (in module ttnn)": [[107, "ttnn.erf"]], "erf_bw() (in module ttnn)": [[108, "ttnn.erf_bw"]], "erfc() (in module ttnn)": [[109, "ttnn.erfc"]], "erfc_bw() (in module ttnn)": [[110, "ttnn.erfc_bw"]], "erfinv() (in module ttnn)": [[111, "ttnn.erfinv"]], "erfinv_bw() (in module ttnn)": [[112, "ttnn.erfinv_bw"]], "exp() (in module ttnn)": [[113, "ttnn.exp"]], "exp2() (in module ttnn)": [[114, "ttnn.exp2"]], "exp2_bw() (in module ttnn)": [[115, "ttnn.exp2_bw"]], "exp_bw() (in module ttnn)": [[116, "ttnn.exp_bw"]], "conv3d() (in module ttnn.experimental)": [[117, "ttnn.experimental.conv3d"]], "dropout() (in module ttnn.experimental)": [[118, "ttnn.experimental.dropout"]], "gelu_bw() (in module ttnn.experimental)": [[119, "ttnn.experimental.gelu_bw"]], "rotary_embedding() (in module ttnn.experimental)": [[120, "ttnn.experimental.rotary_embedding"]], "expm1() (in module ttnn)": [[121, "ttnn.expm1"]], "expm1_bw() (in module ttnn)": [[122, "ttnn.expm1_bw"]], "fill() (in module ttnn)": [[123, "ttnn.fill"]], "fill_bw() (in module ttnn)": [[124, "ttnn.fill_bw"]], "fill_ones_rm() (in module ttnn)": [[125, "ttnn.fill_ones_rm"]], "fill_rm() (in module ttnn)": [[126, "ttnn.fill_rm"]], "fill_zero_bw() (in module ttnn)": [[127, "ttnn.fill_zero_bw"]], "floor() (in module ttnn)": [[128, "ttnn.floor"]], "floor_bw() (in module ttnn)": [[129, "ttnn.floor_bw"]], "floor_div() (in module ttnn)": [[130, "ttnn.floor_div"]], "fmod() (in module ttnn)": [[131, "ttnn.fmod"]], "fmod_bw() (in module ttnn)": [[132, "ttnn.fmod_bw"]], "frac() (in module ttnn)": [[133, "ttnn.frac"]], "frac_bw() (in module ttnn)": [[134, "ttnn.frac_bw"]], "from_buffer() (in module ttnn)": [[135, "ttnn.from_buffer"]], "from_device() (in module ttnn)": [[136, "ttnn.from_device"]], "from_torch() (in module ttnn)": [[137, "ttnn.from_torch"]], "full() (in module ttnn)": [[138, "ttnn.full"]], "full_like() (in module ttnn)": [[139, "ttnn.full_like"]], "gather() (in module ttnn)": [[140, "ttnn.gather"]], "gcd() (in module ttnn)": [[141, "ttnn.gcd"]], "ge() (in module ttnn)": [[142, "ttnn.ge"]], "ge_() (in module ttnn)": [[143, "ttnn.ge_"]], "geglu() (in module ttnn)": [[144, "ttnn.geglu"]], "gelu() (in module ttnn)": [[145, "ttnn.gelu"]], "gelu_bw() (in module ttnn)": [[146, "ttnn.gelu_bw"]], "gez() (in module ttnn)": [[147, "ttnn.gez"]], "global_avg_pool2d() (in module ttnn)": [[148, "ttnn.global_avg_pool2d"]], "glu() (in module ttnn)": [[149, "ttnn.glu"]], "group_norm() (in module ttnn)": [[150, "ttnn.group_norm"]], "gt() (in module ttnn)": [[151, "ttnn.gt"]], "gt_() (in module ttnn)": [[152, "ttnn.gt_"]], "gtz() (in module ttnn)": [[153, "ttnn.gtz"]], "hardshrink() (in module ttnn)": [[154, "ttnn.hardshrink"]], "hardshrink_bw() (in module ttnn)": [[155, "ttnn.hardshrink_bw"]], "hardsigmoid() (in module ttnn)": [[156, "ttnn.hardsigmoid"]], "hardsigmoid_bw() (in module ttnn)": [[157, "ttnn.hardsigmoid_bw"]], "hardswish() (in module ttnn)": [[158, "ttnn.hardswish"]], "hardswish_bw() (in module ttnn)": [[159, "ttnn.hardswish_bw"]], "hardtanh() (in module ttnn)": [[160, "ttnn.hardtanh"]], "hardtanh_bw() (in module ttnn)": [[161, "ttnn.hardtanh_bw"]], "heaviside() (in module ttnn)": [[162, "ttnn.heaviside"]], "hypot() (in module ttnn)": [[163, "ttnn.hypot"]], "hypot_bw() (in module ttnn)": [[164, "ttnn.hypot_bw"]], "i0() (in module ttnn)": [[165, "ttnn.i0"]], "i0_bw() (in module ttnn)": [[166, "ttnn.i0_bw"]], "identity() (in module ttnn)": [[167, "ttnn.identity"]], "imag() (in module ttnn)": [[168, "ttnn.imag"]], "imag_bw() (in module ttnn)": [[169, "ttnn.imag_bw"]], "indexed_fill() (in module ttnn)": [[170, "ttnn.indexed_fill"]], "is_imag() (in module ttnn)": [[171, "ttnn.is_imag"]], "is_real() (in module ttnn)": [[172, "ttnn.is_real"]], "isclose() (in module ttnn)": [[173, "ttnn.isclose"]], "isfinite() (in module ttnn)": [[174, "ttnn.isfinite"]], "isinf() (in module ttnn)": [[175, "ttnn.isinf"]], "isnan() (in module ttnn)": [[176, "ttnn.isnan"]], "isneginf() (in module ttnn)": [[177, "ttnn.isneginf"]], "isposinf() (in module ttnn)": [[178, "ttnn.isposinf"]], "fill_cache_for_user_() (in module ttnn.kv_cache)": [[179, "ttnn.kv_cache.fill_cache_for_user_"]], "update_cache_for_token_() (in module ttnn.kv_cache)": [[180, "ttnn.kv_cache.update_cache_for_token_"]], "l1_loss() (in module ttnn)": [[181, "ttnn.l1_loss"]], "layer_norm() (in module ttnn)": [[182, "ttnn.layer_norm"]], "layer_norm_post_all_gather() (in module ttnn)": [[183, "ttnn.layer_norm_post_all_gather"]], "layer_norm_pre_all_gather() (in module ttnn)": [[184, "ttnn.layer_norm_pre_all_gather"]], "lcm() (in module ttnn)": [[185, "ttnn.lcm"]], "ldexp() (in module ttnn)": [[186, "ttnn.ldexp"]], "ldexp_bw() (in module ttnn)": [[187, "ttnn.ldexp_bw"]], "le() (in module ttnn)": [[188, "ttnn.le"]], "le_() (in module ttnn)": [[189, "ttnn.le_"]], "leaky_relu() (in module ttnn)": [[190, "ttnn.leaky_relu"]], "leaky_relu_bw() (in module ttnn)": [[191, "ttnn.leaky_relu_bw"]], "lerp() (in module ttnn)": [[192, "ttnn.lerp"]], "lerp_bw() (in module ttnn)": [[193, "ttnn.lerp_bw"]], "lez() (in module ttnn)": [[194, "ttnn.lez"]], "lgamma() (in module ttnn)": [[195, "ttnn.lgamma"]], "lgamma_bw() (in module ttnn)": [[196, "ttnn.lgamma_bw"]], "linear() (in module ttnn)": [[197, "ttnn.linear"]], "load_tensor() (in module ttnn)": [[198, "ttnn.load_tensor"]], "log() (in module ttnn)": [[199, "ttnn.log"]], "log10() (in module ttnn)": [[200, "ttnn.log10"]], "log10_bw() (in module ttnn)": [[201, "ttnn.log10_bw"]], "log1p() (in module ttnn)": [[202, "ttnn.log1p"]], "log1p_bw() (in module ttnn)": [[203, "ttnn.log1p_bw"]], "log2() (in module ttnn)": [[204, "ttnn.log2"]], "log2_bw() (in module ttnn)": [[205, "ttnn.log2_bw"]], "log_bw() (in module ttnn)": [[206, "ttnn.log_bw"]], "log_sigmoid() (in module ttnn)": [[207, "ttnn.log_sigmoid"]], "log_sigmoid_bw() (in module ttnn)": [[208, "ttnn.log_sigmoid_bw"]], "logaddexp() (in module ttnn)": [[209, "ttnn.logaddexp"]], "logaddexp2() (in module ttnn)": [[210, "ttnn.logaddexp2"]], "logaddexp2_bw() (in module ttnn)": [[211, "ttnn.logaddexp2_bw"]], "logaddexp_bw() (in module ttnn)": [[212, "ttnn.logaddexp_bw"]], "logical_and() (in module ttnn)": [[213, "ttnn.logical_and"]], "logical_and_() (in module ttnn)": [[214, "ttnn.logical_and_"]], "logical_not() (in module ttnn)": [[215, "ttnn.logical_not"]], "logical_not_() (in module ttnn)": [[216, "ttnn.logical_not_"]], "logical_or() (in module ttnn)": [[217, "ttnn.logical_or"]], "logical_or_() (in module ttnn)": [[218, "ttnn.logical_or_"]], "logical_xor() (in module ttnn)": [[219, "ttnn.logical_xor"]], "logical_xor_() (in module ttnn)": [[220, "ttnn.logical_xor_"]], "logit() (in module ttnn)": [[221, "ttnn.logit"]], "logit_bw() (in module ttnn)": [[222, "ttnn.logit_bw"]], "logiteps_bw() (in module ttnn)": [[223, "ttnn.logiteps_bw"]], "lt() (in module ttnn)": [[224, "ttnn.lt"]], "lt_() (in module ttnn)": [[225, "ttnn.lt_"]], "ltz() (in module ttnn)": [[226, "ttnn.ltz"]], "mac() (in module ttnn)": [[227, "ttnn.mac"]], "manage_device() (in module ttnn)": [[228, "ttnn.manage_device"]], "manual_seed() (in module ttnn)": [[229, "ttnn.manual_seed"]], "matmul() (in module ttnn)": [[230, "ttnn.matmul"]], "max() (in module ttnn)": [[231, "ttnn.max"]], "max_bw() (in module ttnn)": [[232, "ttnn.max_bw"]], "max_pool2d() (in module ttnn)": [[233, "ttnn.max_pool2d"]], "maximum() (in module ttnn)": [[234, "ttnn.maximum"]], "mean() (in module ttnn)": [[235, "ttnn.mean"]], "min() (in module ttnn)": [[236, "ttnn.min"]], "min_bw() (in module ttnn)": [[237, "ttnn.min_bw"]], "minimum() (in module ttnn)": [[238, "ttnn.minimum"]], "mish() (in module ttnn)": [[239, "ttnn.mish"]], "preprocess_model() (in module ttnn.model_preprocessing)": [[240, "ttnn.model_preprocessing.preprocess_model"]], "preprocess_model_parameters() (in module ttnn.model_preprocessing)": [[241, "ttnn.model_preprocessing.preprocess_model_parameters"]], "moe() (in module ttnn)": [[242, "ttnn.moe"]], "mse_loss() (in module ttnn)": [[243, "ttnn.mse_loss"]], "mul_bw() (in module ttnn)": [[244, "ttnn.mul_bw"]], "multigammaln() (in module ttnn)": [[245, "ttnn.multigammaln"]], "multigammaln_bw() (in module ttnn)": [[246, "ttnn.multigammaln_bw"]], "multiply() (in module ttnn)": [[247, "ttnn.multiply"]], "ne() (in module ttnn)": [[248, "ttnn.ne"]], "ne_() (in module ttnn)": [[249, "ttnn.ne_"]], "neg() (in module ttnn)": [[250, "ttnn.neg"]], "neg_bw() (in module ttnn)": [[251, "ttnn.neg_bw"]], "nextafter() (in module ttnn)": [[252, "ttnn.nextafter"]], "nez() (in module ttnn)": [[253, "ttnn.nez"]], "nonzero() (in module ttnn)": [[254, "ttnn.nonzero"]], "normalize_global() (in module ttnn)": [[255, "ttnn.normalize_global"]], "normalize_hw() (in module ttnn)": [[256, "ttnn.normalize_hw"]], "ones() (in module ttnn)": [[257, "ttnn.ones"]], "ones_like() (in module ttnn)": [[258, "ttnn.ones_like"]], "open_device() (in module ttnn)": [[259, "ttnn.open_device"]], "outer() (in module ttnn)": [[260, "ttnn.outer"]], "pad() (in module ttnn)": [[261, "ttnn.pad"]], "pad_to_tile_shape() (in module ttnn)": [[262, "ttnn.pad_to_tile_shape"]], "permute() (in module ttnn)": [[263, "ttnn.permute"]], "polar() (in module ttnn)": [[264, "ttnn.polar"]], "polar_bw() (in module ttnn)": [[265, "ttnn.polar_bw"]], "polygamma() (in module ttnn)": [[266, "ttnn.polygamma"]], "polygamma_bw() (in module ttnn)": [[267, "ttnn.polygamma_bw"]], "polyval() (in module ttnn)": [[268, "ttnn.polyval"]], "pow() (in module ttnn)": [[269, "ttnn.pow"]], "pow_bw() (in module ttnn)": [[270, "ttnn.pow_bw"]], "prelu() (in module ttnn)": [[271, "ttnn.prelu"]], "prepare_conv_bias() (in module ttnn)": [[272, "ttnn.prepare_conv_bias"]], "prepare_conv_transpose2d_bias() (in module ttnn)": [[273, "ttnn.prepare_conv_transpose2d_bias"]], "prepare_conv_transpose2d_weights() (in module ttnn)": [[274, "ttnn.prepare_conv_transpose2d_weights"]], "prepare_conv_weights() (in module ttnn)": [[275, "ttnn.prepare_conv_weights"]], "prod() (in module ttnn)": [[276, "ttnn.prod"]], "prod_bw() (in module ttnn)": [[277, "ttnn.prod_bw"]], "rad2deg() (in module ttnn)": [[278, "ttnn.rad2deg"]], "rad2deg_bw() (in module ttnn)": [[279, "ttnn.rad2deg_bw"]], "rand() (in module ttnn)": [[280, "ttnn.rand"]], "rdiv() (in module ttnn)": [[281, "ttnn.rdiv"]], "rdiv_bw() (in module ttnn)": [[282, "ttnn.rdiv_bw"]], "real() (in module ttnn)": [[283, "ttnn.real"]], "real_bw() (in module ttnn)": [[284, "ttnn.real_bw"]], "reallocate() (in module ttnn)": [[285, "ttnn.reallocate"]], "reciprocal() (in module ttnn)": [[286, "ttnn.reciprocal"]], "reciprocal_bw() (in module ttnn)": [[287, "ttnn.reciprocal_bw"]], "reduce_scatter() (in module ttnn)": [[288, "ttnn.reduce_scatter"]], "register_post_operation_hook() (in module ttnn)": [[289, "ttnn.register_post_operation_hook"]], "register_pre_operation_hook() (in module ttnn)": [[290, "ttnn.register_pre_operation_hook"]], "reglu() (in module ttnn)": [[291, "ttnn.reglu"]], "relu() (in module ttnn)": [[292, "ttnn.relu"]], "relu6() (in module ttnn)": [[293, "ttnn.relu6"]], "relu6_bw() (in module ttnn)": [[294, "ttnn.relu6_bw"]], "relu_bw() (in module ttnn)": [[295, "ttnn.relu_bw"]], "relu_max() (in module ttnn)": [[296, "ttnn.relu_max"]], "relu_min() (in module ttnn)": [[297, "ttnn.relu_min"]], "remainder() (in module ttnn)": [[298, "ttnn.remainder"]], "remainder_bw() (in module ttnn)": [[299, "ttnn.remainder_bw"]], "repeat() (in module ttnn)": [[300, "ttnn.repeat"]], "repeat_bw() (in module ttnn)": [[301, "ttnn.repeat_bw"]], "repeat_interleave() (in module ttnn)": [[302, "ttnn.repeat_interleave"]], "reshape() (in module ttnn)": [[303, "ttnn.reshape"]], "rms_norm() (in module ttnn)": [[304, "ttnn.rms_norm"]], "rms_norm_post_all_gather() (in module ttnn)": [[305, "ttnn.rms_norm_post_all_gather"]], "rms_norm_pre_all_gather() (in module ttnn)": [[306, "ttnn.rms_norm_pre_all_gather"]], "round() (in module ttnn)": [[307, "ttnn.round"]], "round_bw() (in module ttnn)": [[308, "ttnn.round_bw"]], "rpow() (in module ttnn)": [[309, "ttnn.rpow"]], "rpow_bw() (in module ttnn)": [[310, "ttnn.rpow_bw"]], "rsqrt() (in module ttnn)": [[311, "ttnn.rsqrt"]], "rsqrt_bw() (in module ttnn)": [[312, "ttnn.rsqrt_bw"]], "rsub() (in module ttnn)": [[313, "ttnn.rsub"]], "rsub_bw() (in module ttnn)": [[314, "ttnn.rsub_bw"]], "scale_causal_mask_hw_dims_softmax_in_place() (in module ttnn)": [[315, "ttnn.scale_causal_mask_hw_dims_softmax_in_place"]], "scale_mask_softmax() (in module ttnn)": [[316, "ttnn.scale_mask_softmax"]], "scale_mask_softmax_in_place() (in module ttnn)": [[317, "ttnn.scale_mask_softmax_in_place"]], "scatter() (in module ttnn)": [[318, "ttnn.scatter"]], "selu() (in module ttnn)": [[319, "ttnn.selu"]], "selu_bw() (in module ttnn)": [[320, "ttnn.selu_bw"]], "set_printoptions() (in module ttnn)": [[321, "ttnn.set_printoptions"]], "sigmoid() (in module ttnn)": [[322, "ttnn.sigmoid"]], "sigmoid_accurate() (in module ttnn)": [[323, "ttnn.sigmoid_accurate"]], "sigmoid_bw() (in module ttnn)": [[324, "ttnn.sigmoid_bw"]], "sign() (in module ttnn)": [[325, "ttnn.sign"]], "sign_bw() (in module ttnn)": [[326, "ttnn.sign_bw"]], "signbit() (in module ttnn)": [[327, "ttnn.signbit"]], "silu() (in module ttnn)": [[328, "ttnn.silu"]], "silu_bw() (in module ttnn)": [[329, "ttnn.silu_bw"]], "sin() (in module ttnn)": [[330, "ttnn.sin"]], "sin_bw() (in module ttnn)": [[331, "ttnn.sin_bw"]], "sinh() (in module ttnn)": [[332, "ttnn.sinh"]], "sinh_bw() (in module ttnn)": [[333, "ttnn.sinh_bw"]], "slice() (in module ttnn)": [[334, "ttnn.slice"]], "softmax() (in module ttnn)": [[335, "ttnn.softmax"]], "softmax_in_place() (in module ttnn)": [[336, "ttnn.softmax_in_place"]], "softplus() (in module ttnn)": [[337, "ttnn.softplus"]], "softplus_bw() (in module ttnn)": [[338, "ttnn.softplus_bw"]], "softshrink() (in module ttnn)": [[339, "ttnn.softshrink"]], "softshrink_bw() (in module ttnn)": [[340, "ttnn.softshrink_bw"]], "softsign() (in module ttnn)": [[341, "ttnn.softsign"]], "softsign_bw() (in module ttnn)": [[342, "ttnn.softsign_bw"]], "sort() (in module ttnn)": [[343, "ttnn.sort"]], "sparse_matmul() (in module ttnn)": [[344, "ttnn.sparse_matmul"]], "split_work_to_cores() (in module ttnn)": [[345, "ttnn.split_work_to_cores"]], "sqrt() (in module ttnn)": [[346, "ttnn.sqrt"]], "sqrt_bw() (in module ttnn)": [[347, "ttnn.sqrt_bw"]], "square() (in module ttnn)": [[348, "ttnn.square"]], "square_bw() (in module ttnn)": [[349, "ttnn.square_bw"]], "squared_difference() (in module ttnn)": [[350, "ttnn.squared_difference"]], "squared_difference_bw() (in module ttnn)": [[351, "ttnn.squared_difference_bw"]], "std() (in module ttnn)": [[352, "ttnn.std"]], "sub_bw() (in module ttnn)": [[353, "ttnn.sub_bw"]], "subalpha() (in module ttnn)": [[354, "ttnn.subalpha"]], "subalpha_bw() (in module ttnn)": [[355, "ttnn.subalpha_bw"]], "subtract() (in module ttnn)": [[356, "ttnn.subtract"]], "sum() (in module ttnn)": [[357, "ttnn.sum"]], "swiglu() (in module ttnn)": [[358, "ttnn.swiglu"]], "swish() (in module ttnn)": [[359, "ttnn.swish"]], "synchronize_device() (in module ttnn)": [[360, "ttnn.synchronize_device"]], "tan() (in module ttnn)": [[361, "ttnn.tan"]], "tan_bw() (in module ttnn)": [[362, "ttnn.tan_bw"]], "tanh() (in module ttnn)": [[363, "ttnn.tanh"]], "tanh_bw() (in module ttnn)": [[364, "ttnn.tanh_bw"]], "tanhshrink() (in module ttnn)": [[365, "ttnn.tanhshrink"]], "tanhshrink_bw() (in module ttnn)": [[366, "ttnn.tanhshrink_bw"]], "threshold() (in module ttnn)": [[367, "ttnn.threshold"]], "threshold_bw() (in module ttnn)": [[368, "ttnn.threshold_bw"]], "tilize() (in module ttnn)": [[369, "ttnn.tilize"]], "tilize_with_val_padding() (in module ttnn)": [[370, "ttnn.tilize_with_val_padding"]], "to_device() (in module ttnn)": [[371, "ttnn.to_device"]], "to_layout() (in module ttnn)": [[372, "ttnn.to_layout"]], "to_memory_config() (in module ttnn)": [[373, "ttnn.to_memory_config"]], "to_torch() (in module ttnn)": [[374, "ttnn.to_torch"]], "topk() (in module ttnn)": [[375, "ttnn.topk"]], "attention_softmax() (in module ttnn.transformer)": [[376, "ttnn.transformer.attention_softmax"]], "attention_softmax_() (in module ttnn.transformer)": [[377, "ttnn.transformer.attention_softmax_"]], "concatenate_heads() (in module ttnn.transformer)": [[378, "ttnn.transformer.concatenate_heads"]], "scaled_dot_product_attention() (in module ttnn.transformer)": [[379, "ttnn.transformer.scaled_dot_product_attention"]], "scaled_dot_product_attention_decode() (in module ttnn.transformer)": [[380, "ttnn.transformer.scaled_dot_product_attention_decode"]], "split_query_key_value_and_split_heads() (in module ttnn.transformer)": [[381, "ttnn.transformer.split_query_key_value_and_split_heads"]], "tril() (in module ttnn)": [[382, "ttnn.tril"]], "triu() (in module ttnn)": [[383, "ttnn.triu"]], "trunc() (in module ttnn)": [[384, "ttnn.trunc"]], "trunc_bw() (in module ttnn)": [[385, "ttnn.trunc_bw"]], "unary_chain() (in module ttnn)": [[386, "ttnn.unary_chain"]], "untilize() (in module ttnn)": [[387, "ttnn.untilize"]], "untilize_with_unpadding() (in module ttnn)": [[388, "ttnn.untilize_with_unpadding"]], "upsample() (in module ttnn)": [[389, "ttnn.upsample"]], "var() (in module ttnn)": [[390, "ttnn.var"]], "where() (in module ttnn)": [[391, "ttnn.where"]], "where_bw() (in module ttnn)": [[392, "ttnn.where_bw"]], "xlogy() (in module ttnn)": [[393, "ttnn.xlogy"]], "xlogy_bw() (in module ttnn)": [[394, "ttnn.xlogy_bw"]], "zeros() (in module ttnn)": [[395, "ttnn.zeros"]], "zeros_like() (in module ttnn)": [[396, "ttnn.zeros_like"]], "shape (class in ttnn)": [[403, "ttnn.Shape"]], "rank (ttnn.shape property)": [[403, "ttnn.Shape.rank"]], "to_rank() (ttnn.shape method)": [[403, "ttnn.Shape.to_rank"]]}})