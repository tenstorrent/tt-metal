Search.setIndex({"docnames": ["index", "resources/contributing", "resources/support", "ttnn/about", "ttnn/adding_new_ttnn_operation", "ttnn/api", "ttnn/api/ttnn.Conv2dConfig", "ttnn/api/ttnn.Conv2dSliceConfig", "ttnn/api/ttnn.GetDefaultDevice", "ttnn/api/ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig", "ttnn/api/ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig", "ttnn/api/ttnn.MatmulMultiCoreReuseMultiCastProgramConfig", "ttnn/api/ttnn.MatmulMultiCoreReuseProgramConfig", "ttnn/api/ttnn.SetDefaultDevice", "ttnn/api/ttnn.SoftmaxDefaultProgramConfig", "ttnn/api/ttnn.SoftmaxProgramConfig", "ttnn/api/ttnn.SoftmaxShardedMultiCoreProgramConfig", "ttnn/api/ttnn.abs", "ttnn/api/ttnn.abs_bw", "ttnn/api/ttnn.acos", "ttnn/api/ttnn.acos_bw", "ttnn/api/ttnn.acosh", "ttnn/api/ttnn.acosh_bw", "ttnn/api/ttnn.add", "ttnn/api/ttnn.add_bw", "ttnn/api/ttnn.addalpha", "ttnn/api/ttnn.addalpha_bw", "ttnn/api/ttnn.addcdiv", "ttnn/api/ttnn.addcdiv_bw", "ttnn/api/ttnn.addcmul", "ttnn/api/ttnn.addcmul_bw", "ttnn/api/ttnn.addmm", "ttnn/api/ttnn.all_gather", "ttnn/api/ttnn.all_reduce", "ttnn/api/ttnn.alt_complex_rotate90", "ttnn/api/ttnn.angle", "ttnn/api/ttnn.angle_bw", "ttnn/api/ttnn.arange", "ttnn/api/ttnn.argmax", "ttnn/api/ttnn.as_tensor", "ttnn/api/ttnn.asin", "ttnn/api/ttnn.asin_bw", "ttnn/api/ttnn.asinh", "ttnn/api/ttnn.asinh_bw", "ttnn/api/ttnn.assign_bw", "ttnn/api/ttnn.atan", "ttnn/api/ttnn.atan2", "ttnn/api/ttnn.atan2_bw", "ttnn/api/ttnn.atan_bw", "ttnn/api/ttnn.atanh", "ttnn/api/ttnn.atanh_bw", "ttnn/api/ttnn.batch_norm", "ttnn/api/ttnn.bias_gelu_bw", "ttnn/api/ttnn.bitwise_and", "ttnn/api/ttnn.bitwise_left_shift", "ttnn/api/ttnn.bitwise_not", "ttnn/api/ttnn.bitwise_or", "ttnn/api/ttnn.bitwise_right_shift", "ttnn/api/ttnn.bitwise_xor", "ttnn/api/ttnn.cbrt", "ttnn/api/ttnn.ceil", "ttnn/api/ttnn.ceil_bw", "ttnn/api/ttnn.celu", "ttnn/api/ttnn.celu_bw", "ttnn/api/ttnn.clamp", "ttnn/api/ttnn.clamp_bw", "ttnn/api/ttnn.clip", "ttnn/api/ttnn.clip_bw", "ttnn/api/ttnn.clone", "ttnn/api/ttnn.close_device", "ttnn/api/ttnn.concat", "ttnn/api/ttnn.concat_bw", "ttnn/api/ttnn.conj", "ttnn/api/ttnn.conj_bw", "ttnn/api/ttnn.conv1d", "ttnn/api/ttnn.conv2d", "ttnn/api/ttnn.conv_transpose2d", "ttnn/api/ttnn.cos", "ttnn/api/ttnn.cos_bw", "ttnn/api/ttnn.cosh", "ttnn/api/ttnn.cosh_bw", "ttnn/api/ttnn.create_sharded_memory_config", "ttnn/api/ttnn.cumprod", "ttnn/api/ttnn.cumsum", "ttnn/api/ttnn.deallocate", "ttnn/api/ttnn.deg2rad", "ttnn/api/ttnn.deg2rad_bw", "ttnn/api/ttnn.digamma", "ttnn/api/ttnn.digamma_bw", "ttnn/api/ttnn.div", "ttnn/api/ttnn.div_bw", "ttnn/api/ttnn.div_no_nan", "ttnn/api/ttnn.div_no_nan_bw", "ttnn/api/ttnn.dump_tensor", "ttnn/api/ttnn.elu", "ttnn/api/ttnn.elu_bw", "ttnn/api/ttnn.embedding", "ttnn/api/ttnn.embedding_bw", "ttnn/api/ttnn.empty", "ttnn/api/ttnn.empty_like", "ttnn/api/ttnn.eq", "ttnn/api/ttnn.eq_", "ttnn/api/ttnn.eqz", "ttnn/api/ttnn.erf", "ttnn/api/ttnn.erf_bw", "ttnn/api/ttnn.erfc", "ttnn/api/ttnn.erfc_bw", "ttnn/api/ttnn.erfinv", "ttnn/api/ttnn.erfinv_bw", "ttnn/api/ttnn.exp", "ttnn/api/ttnn.exp2", "ttnn/api/ttnn.exp2_bw", "ttnn/api/ttnn.exp_bw", "ttnn/api/ttnn.experimental.conv3d", "ttnn/api/ttnn.experimental.dropout", "ttnn/api/ttnn.experimental.gelu_bw", "ttnn/api/ttnn.experimental.rotary_embedding", "ttnn/api/ttnn.expm1", "ttnn/api/ttnn.expm1_bw", "ttnn/api/ttnn.fill", "ttnn/api/ttnn.fill_bw", "ttnn/api/ttnn.fill_ones_rm", "ttnn/api/ttnn.fill_rm", "ttnn/api/ttnn.fill_zero_bw", "ttnn/api/ttnn.floor", "ttnn/api/ttnn.floor_bw", "ttnn/api/ttnn.floor_div", "ttnn/api/ttnn.fmod", "ttnn/api/ttnn.fmod_bw", "ttnn/api/ttnn.format_input_tensor", "ttnn/api/ttnn.format_output_tensor", "ttnn/api/ttnn.frac", "ttnn/api/ttnn.frac_bw", "ttnn/api/ttnn.from_device", "ttnn/api/ttnn.from_torch", "ttnn/api/ttnn.full", "ttnn/api/ttnn.full_like", "ttnn/api/ttnn.gather", "ttnn/api/ttnn.gcd", "ttnn/api/ttnn.ge", "ttnn/api/ttnn.ge_", "ttnn/api/ttnn.geglu", "ttnn/api/ttnn.gelu", "ttnn/api/ttnn.gelu_bw", "ttnn/api/ttnn.gez", "ttnn/api/ttnn.global_avg_pool2d", "ttnn/api/ttnn.glu", "ttnn/api/ttnn.group_norm", "ttnn/api/ttnn.gt", "ttnn/api/ttnn.gt_", "ttnn/api/ttnn.gtz", "ttnn/api/ttnn.hardshrink", "ttnn/api/ttnn.hardshrink_bw", "ttnn/api/ttnn.hardsigmoid", "ttnn/api/ttnn.hardsigmoid_bw", "ttnn/api/ttnn.hardswish", "ttnn/api/ttnn.hardswish_bw", "ttnn/api/ttnn.hardtanh", "ttnn/api/ttnn.hardtanh_bw", "ttnn/api/ttnn.heaviside", "ttnn/api/ttnn.hypot", "ttnn/api/ttnn.hypot_bw", "ttnn/api/ttnn.i0", "ttnn/api/ttnn.i0_bw", "ttnn/api/ttnn.identity", "ttnn/api/ttnn.imag", "ttnn/api/ttnn.imag_bw", "ttnn/api/ttnn.indexed_fill", "ttnn/api/ttnn.is_imag", "ttnn/api/ttnn.is_real", "ttnn/api/ttnn.isclose", "ttnn/api/ttnn.isfinite", "ttnn/api/ttnn.isinf", "ttnn/api/ttnn.isnan", "ttnn/api/ttnn.isneginf", "ttnn/api/ttnn.isposinf", "ttnn/api/ttnn.kv_cache.fill_cache_for_user_", "ttnn/api/ttnn.kv_cache.update_cache_for_token_", "ttnn/api/ttnn.l1_loss", "ttnn/api/ttnn.layer_norm", "ttnn/api/ttnn.lcm", "ttnn/api/ttnn.ldexp", "ttnn/api/ttnn.ldexp_bw", "ttnn/api/ttnn.le", "ttnn/api/ttnn.le_", "ttnn/api/ttnn.leaky_relu", "ttnn/api/ttnn.leaky_relu_bw", "ttnn/api/ttnn.lerp", "ttnn/api/ttnn.lerp_bw", "ttnn/api/ttnn.lez", "ttnn/api/ttnn.lgamma", "ttnn/api/ttnn.lgamma_bw", "ttnn/api/ttnn.linear", "ttnn/api/ttnn.load_tensor", "ttnn/api/ttnn.log", "ttnn/api/ttnn.log10", "ttnn/api/ttnn.log10_bw", "ttnn/api/ttnn.log1p", "ttnn/api/ttnn.log1p_bw", "ttnn/api/ttnn.log2", "ttnn/api/ttnn.log2_bw", "ttnn/api/ttnn.log_bw", "ttnn/api/ttnn.log_sigmoid", "ttnn/api/ttnn.log_sigmoid_bw", "ttnn/api/ttnn.logaddexp", "ttnn/api/ttnn.logaddexp2", "ttnn/api/ttnn.logaddexp2_bw", "ttnn/api/ttnn.logaddexp_bw", "ttnn/api/ttnn.logical_and", "ttnn/api/ttnn.logical_and_", "ttnn/api/ttnn.logical_not", "ttnn/api/ttnn.logical_not_", "ttnn/api/ttnn.logical_or", "ttnn/api/ttnn.logical_or_", "ttnn/api/ttnn.logical_xor", "ttnn/api/ttnn.logical_xor_", "ttnn/api/ttnn.logit", "ttnn/api/ttnn.logit_bw", "ttnn/api/ttnn.logiteps_bw", "ttnn/api/ttnn.lt", "ttnn/api/ttnn.lt_", "ttnn/api/ttnn.ltz", "ttnn/api/ttnn.mac", "ttnn/api/ttnn.manage_device", "ttnn/api/ttnn.matmul", "ttnn/api/ttnn.matmul_batched_weights", "ttnn/api/ttnn.max", "ttnn/api/ttnn.max_bw", "ttnn/api/ttnn.max_pool2d", "ttnn/api/ttnn.maximum", "ttnn/api/ttnn.mean", "ttnn/api/ttnn.min", "ttnn/api/ttnn.min_bw", "ttnn/api/ttnn.minimum", "ttnn/api/ttnn.mish", "ttnn/api/ttnn.model_preprocessing.preprocess_model", "ttnn/api/ttnn.model_preprocessing.preprocess_model_parameters", "ttnn/api/ttnn.moreh_sum", "ttnn/api/ttnn.mse_loss", "ttnn/api/ttnn.mul_bw", "ttnn/api/ttnn.multigammaln", "ttnn/api/ttnn.multigammaln_bw", "ttnn/api/ttnn.multiply", "ttnn/api/ttnn.ne", "ttnn/api/ttnn.ne_", "ttnn/api/ttnn.neg", "ttnn/api/ttnn.neg_bw", "ttnn/api/ttnn.nextafter", "ttnn/api/ttnn.nez", "ttnn/api/ttnn.nonzero", "ttnn/api/ttnn.normalize_global", "ttnn/api/ttnn.normalize_hw", "ttnn/api/ttnn.ones", "ttnn/api/ttnn.ones_like", "ttnn/api/ttnn.open_device", "ttnn/api/ttnn.outer", "ttnn/api/ttnn.pad", "ttnn/api/ttnn.pad_to_tile_shape", "ttnn/api/ttnn.permute", "ttnn/api/ttnn.polar", "ttnn/api/ttnn.polar_bw", "ttnn/api/ttnn.polygamma", "ttnn/api/ttnn.polygamma_bw", "ttnn/api/ttnn.polyval", "ttnn/api/ttnn.pow", "ttnn/api/ttnn.pow_bw", "ttnn/api/ttnn.prelu", "ttnn/api/ttnn.prepare_conv_bias", "ttnn/api/ttnn.prepare_conv_transpose2d_bias", "ttnn/api/ttnn.prepare_conv_transpose2d_weights", "ttnn/api/ttnn.prepare_conv_weights", "ttnn/api/ttnn.prod", "ttnn/api/ttnn.prod_bw", "ttnn/api/ttnn.rad2deg", "ttnn/api/ttnn.rad2deg_bw", "ttnn/api/ttnn.rand", "ttnn/api/ttnn.rdiv", "ttnn/api/ttnn.rdiv_bw", "ttnn/api/ttnn.real", "ttnn/api/ttnn.real_bw", "ttnn/api/ttnn.reallocate", "ttnn/api/ttnn.reciprocal", "ttnn/api/ttnn.reciprocal_bw", "ttnn/api/ttnn.reduce_scatter", "ttnn/api/ttnn.register_post_operation_hook", "ttnn/api/ttnn.register_pre_operation_hook", "ttnn/api/ttnn.reglu", "ttnn/api/ttnn.relu", "ttnn/api/ttnn.relu6", "ttnn/api/ttnn.relu6_bw", "ttnn/api/ttnn.relu_bw", "ttnn/api/ttnn.relu_max", "ttnn/api/ttnn.relu_min", "ttnn/api/ttnn.remainder", "ttnn/api/ttnn.remainder_bw", "ttnn/api/ttnn.repeat", "ttnn/api/ttnn.repeat_bw", "ttnn/api/ttnn.repeat_interleave", "ttnn/api/ttnn.reshape", "ttnn/api/ttnn.rms_norm", "ttnn/api/ttnn.round", "ttnn/api/ttnn.round_bw", "ttnn/api/ttnn.rpow", "ttnn/api/ttnn.rpow_bw", "ttnn/api/ttnn.rsqrt", "ttnn/api/ttnn.rsqrt_bw", "ttnn/api/ttnn.rsub", "ttnn/api/ttnn.rsub_bw", "ttnn/api/ttnn.scale_causal_mask_hw_dims_softmax_in_place", "ttnn/api/ttnn.scale_mask_softmax", "ttnn/api/ttnn.scale_mask_softmax_in_place", "ttnn/api/ttnn.scatter", "ttnn/api/ttnn.selu", "ttnn/api/ttnn.selu_bw", "ttnn/api/ttnn.set_printoptions", "ttnn/api/ttnn.sigmoid", "ttnn/api/ttnn.sigmoid_accurate", "ttnn/api/ttnn.sigmoid_bw", "ttnn/api/ttnn.sign", "ttnn/api/ttnn.sign_bw", "ttnn/api/ttnn.signbit", "ttnn/api/ttnn.silu", "ttnn/api/ttnn.silu_bw", "ttnn/api/ttnn.sin", "ttnn/api/ttnn.sin_bw", "ttnn/api/ttnn.sinh", "ttnn/api/ttnn.sinh_bw", "ttnn/api/ttnn.slice", "ttnn/api/ttnn.softmax", "ttnn/api/ttnn.softmax_in_place", "ttnn/api/ttnn.softplus", "ttnn/api/ttnn.softplus_bw", "ttnn/api/ttnn.softshrink", "ttnn/api/ttnn.softshrink_bw", "ttnn/api/ttnn.softsign", "ttnn/api/ttnn.softsign_bw", "ttnn/api/ttnn.sort", "ttnn/api/ttnn.sparse_matmul", "ttnn/api/ttnn.sqrt", "ttnn/api/ttnn.sqrt_bw", "ttnn/api/ttnn.square", "ttnn/api/ttnn.square_bw", "ttnn/api/ttnn.squared_difference", "ttnn/api/ttnn.squared_difference_bw", "ttnn/api/ttnn.std", "ttnn/api/ttnn.sub_bw", "ttnn/api/ttnn.subalpha", "ttnn/api/ttnn.subalpha_bw", "ttnn/api/ttnn.subtract", "ttnn/api/ttnn.sum", "ttnn/api/ttnn.swiglu", "ttnn/api/ttnn.swish", "ttnn/api/ttnn.synchronize_device", "ttnn/api/ttnn.tan", "ttnn/api/ttnn.tan_bw", "ttnn/api/ttnn.tanh", "ttnn/api/ttnn.tanh_bw", "ttnn/api/ttnn.tanhshrink", "ttnn/api/ttnn.tanhshrink_bw", "ttnn/api/ttnn.threshold", "ttnn/api/ttnn.threshold_bw", "ttnn/api/ttnn.tilize", "ttnn/api/ttnn.tilize_with_val_padding", "ttnn/api/ttnn.to_device", "ttnn/api/ttnn.to_layout", "ttnn/api/ttnn.to_memory_config", "ttnn/api/ttnn.to_torch", "ttnn/api/ttnn.topk", "ttnn/api/ttnn.transformer.attention_softmax", "ttnn/api/ttnn.transformer.attention_softmax_", "ttnn/api/ttnn.transformer.concatenate_heads", "ttnn/api/ttnn.transformer.scaled_dot_product_attention", "ttnn/api/ttnn.transformer.scaled_dot_product_attention_decode", "ttnn/api/ttnn.transformer.split_query_key_value_and_split_heads", "ttnn/api/ttnn.tril", "ttnn/api/ttnn.triu", "ttnn/api/ttnn.trunc", "ttnn/api/ttnn.trunc_bw", "ttnn/api/ttnn.unary_chain", "ttnn/api/ttnn.untilize", "ttnn/api/ttnn.untilize_with_unpadding", "ttnn/api/ttnn.upsample", "ttnn/api/ttnn.var", "ttnn/api/ttnn.where", "ttnn/api/ttnn.where_bw", "ttnn/api/ttnn.xlogy", "ttnn/api/ttnn.xlogy_bw", "ttnn/api/ttnn.zeros", "ttnn/api/ttnn.zeros_like", "ttnn/converting_torch_model_to_ttnn", "ttnn/demos", "ttnn/get_started", "ttnn/installing", "ttnn/onboarding", "ttnn/profiling_ttnn_operations", "ttnn/tensor", "ttnn/tutorials", "ttnn/tutorials/2025_dx_rework/ttnn_add_tensors", "ttnn/tutorials/2025_dx_rework/ttnn_basic_conv", "ttnn/tutorials/2025_dx_rework/ttnn_basic_operations", "ttnn/tutorials/2025_dx_rework/ttnn_mlp_inference_mnist", "ttnn/tutorials/2025_dx_rework/ttnn_multihead_attention", "ttnn/tutorials/2025_dx_rework/ttnn_simplecnn_inference", "ttnn/tutorials/2025_dx_rework/ttnn_visualizer", "ttnn/usage"], "filenames": ["index.rst", "resources/contributing.rst", "resources/support.rst", "ttnn/about.rst", "ttnn/adding_new_ttnn_operation.rst", "ttnn/api.rst", "ttnn/api/ttnn.Conv2dConfig.rst", "ttnn/api/ttnn.Conv2dSliceConfig.rst", "ttnn/api/ttnn.GetDefaultDevice.rst", "ttnn/api/ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.rst", "ttnn/api/ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig.rst", "ttnn/api/ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.rst", "ttnn/api/ttnn.MatmulMultiCoreReuseProgramConfig.rst", "ttnn/api/ttnn.SetDefaultDevice.rst", "ttnn/api/ttnn.SoftmaxDefaultProgramConfig.rst", "ttnn/api/ttnn.SoftmaxProgramConfig.rst", "ttnn/api/ttnn.SoftmaxShardedMultiCoreProgramConfig.rst", "ttnn/api/ttnn.abs.rst", "ttnn/api/ttnn.abs_bw.rst", "ttnn/api/ttnn.acos.rst", "ttnn/api/ttnn.acos_bw.rst", "ttnn/api/ttnn.acosh.rst", "ttnn/api/ttnn.acosh_bw.rst", "ttnn/api/ttnn.add.rst", "ttnn/api/ttnn.add_bw.rst", "ttnn/api/ttnn.addalpha.rst", "ttnn/api/ttnn.addalpha_bw.rst", "ttnn/api/ttnn.addcdiv.rst", "ttnn/api/ttnn.addcdiv_bw.rst", "ttnn/api/ttnn.addcmul.rst", "ttnn/api/ttnn.addcmul_bw.rst", "ttnn/api/ttnn.addmm.rst", "ttnn/api/ttnn.all_gather.rst", "ttnn/api/ttnn.all_reduce.rst", "ttnn/api/ttnn.alt_complex_rotate90.rst", "ttnn/api/ttnn.angle.rst", "ttnn/api/ttnn.angle_bw.rst", "ttnn/api/ttnn.arange.rst", "ttnn/api/ttnn.argmax.rst", "ttnn/api/ttnn.as_tensor.rst", "ttnn/api/ttnn.asin.rst", "ttnn/api/ttnn.asin_bw.rst", "ttnn/api/ttnn.asinh.rst", "ttnn/api/ttnn.asinh_bw.rst", "ttnn/api/ttnn.assign_bw.rst", "ttnn/api/ttnn.atan.rst", "ttnn/api/ttnn.atan2.rst", "ttnn/api/ttnn.atan2_bw.rst", "ttnn/api/ttnn.atan_bw.rst", "ttnn/api/ttnn.atanh.rst", "ttnn/api/ttnn.atanh_bw.rst", "ttnn/api/ttnn.batch_norm.rst", "ttnn/api/ttnn.bias_gelu_bw.rst", "ttnn/api/ttnn.bitwise_and.rst", "ttnn/api/ttnn.bitwise_left_shift.rst", "ttnn/api/ttnn.bitwise_not.rst", "ttnn/api/ttnn.bitwise_or.rst", "ttnn/api/ttnn.bitwise_right_shift.rst", "ttnn/api/ttnn.bitwise_xor.rst", "ttnn/api/ttnn.cbrt.rst", "ttnn/api/ttnn.ceil.rst", "ttnn/api/ttnn.ceil_bw.rst", "ttnn/api/ttnn.celu.rst", "ttnn/api/ttnn.celu_bw.rst", "ttnn/api/ttnn.clamp.rst", "ttnn/api/ttnn.clamp_bw.rst", "ttnn/api/ttnn.clip.rst", "ttnn/api/ttnn.clip_bw.rst", "ttnn/api/ttnn.clone.rst", "ttnn/api/ttnn.close_device.rst", "ttnn/api/ttnn.concat.rst", "ttnn/api/ttnn.concat_bw.rst", "ttnn/api/ttnn.conj.rst", "ttnn/api/ttnn.conj_bw.rst", "ttnn/api/ttnn.conv1d.rst", "ttnn/api/ttnn.conv2d.rst", "ttnn/api/ttnn.conv_transpose2d.rst", "ttnn/api/ttnn.cos.rst", "ttnn/api/ttnn.cos_bw.rst", "ttnn/api/ttnn.cosh.rst", "ttnn/api/ttnn.cosh_bw.rst", "ttnn/api/ttnn.create_sharded_memory_config.rst", "ttnn/api/ttnn.cumprod.rst", "ttnn/api/ttnn.cumsum.rst", "ttnn/api/ttnn.deallocate.rst", "ttnn/api/ttnn.deg2rad.rst", "ttnn/api/ttnn.deg2rad_bw.rst", "ttnn/api/ttnn.digamma.rst", "ttnn/api/ttnn.digamma_bw.rst", "ttnn/api/ttnn.div.rst", "ttnn/api/ttnn.div_bw.rst", "ttnn/api/ttnn.div_no_nan.rst", "ttnn/api/ttnn.div_no_nan_bw.rst", "ttnn/api/ttnn.dump_tensor.rst", "ttnn/api/ttnn.elu.rst", "ttnn/api/ttnn.elu_bw.rst", "ttnn/api/ttnn.embedding.rst", "ttnn/api/ttnn.embedding_bw.rst", "ttnn/api/ttnn.empty.rst", "ttnn/api/ttnn.empty_like.rst", "ttnn/api/ttnn.eq.rst", "ttnn/api/ttnn.eq_.rst", "ttnn/api/ttnn.eqz.rst", "ttnn/api/ttnn.erf.rst", "ttnn/api/ttnn.erf_bw.rst", "ttnn/api/ttnn.erfc.rst", "ttnn/api/ttnn.erfc_bw.rst", "ttnn/api/ttnn.erfinv.rst", "ttnn/api/ttnn.erfinv_bw.rst", "ttnn/api/ttnn.exp.rst", "ttnn/api/ttnn.exp2.rst", "ttnn/api/ttnn.exp2_bw.rst", "ttnn/api/ttnn.exp_bw.rst", "ttnn/api/ttnn.experimental.conv3d.rst", "ttnn/api/ttnn.experimental.dropout.rst", "ttnn/api/ttnn.experimental.gelu_bw.rst", "ttnn/api/ttnn.experimental.rotary_embedding.rst", "ttnn/api/ttnn.expm1.rst", "ttnn/api/ttnn.expm1_bw.rst", "ttnn/api/ttnn.fill.rst", "ttnn/api/ttnn.fill_bw.rst", "ttnn/api/ttnn.fill_ones_rm.rst", "ttnn/api/ttnn.fill_rm.rst", "ttnn/api/ttnn.fill_zero_bw.rst", "ttnn/api/ttnn.floor.rst", "ttnn/api/ttnn.floor_bw.rst", "ttnn/api/ttnn.floor_div.rst", "ttnn/api/ttnn.fmod.rst", "ttnn/api/ttnn.fmod_bw.rst", "ttnn/api/ttnn.format_input_tensor.rst", "ttnn/api/ttnn.format_output_tensor.rst", "ttnn/api/ttnn.frac.rst", "ttnn/api/ttnn.frac_bw.rst", "ttnn/api/ttnn.from_device.rst", "ttnn/api/ttnn.from_torch.rst", "ttnn/api/ttnn.full.rst", "ttnn/api/ttnn.full_like.rst", "ttnn/api/ttnn.gather.rst", "ttnn/api/ttnn.gcd.rst", "ttnn/api/ttnn.ge.rst", "ttnn/api/ttnn.ge_.rst", "ttnn/api/ttnn.geglu.rst", "ttnn/api/ttnn.gelu.rst", "ttnn/api/ttnn.gelu_bw.rst", "ttnn/api/ttnn.gez.rst", "ttnn/api/ttnn.global_avg_pool2d.rst", "ttnn/api/ttnn.glu.rst", "ttnn/api/ttnn.group_norm.rst", "ttnn/api/ttnn.gt.rst", "ttnn/api/ttnn.gt_.rst", "ttnn/api/ttnn.gtz.rst", "ttnn/api/ttnn.hardshrink.rst", "ttnn/api/ttnn.hardshrink_bw.rst", "ttnn/api/ttnn.hardsigmoid.rst", "ttnn/api/ttnn.hardsigmoid_bw.rst", "ttnn/api/ttnn.hardswish.rst", "ttnn/api/ttnn.hardswish_bw.rst", "ttnn/api/ttnn.hardtanh.rst", "ttnn/api/ttnn.hardtanh_bw.rst", "ttnn/api/ttnn.heaviside.rst", "ttnn/api/ttnn.hypot.rst", "ttnn/api/ttnn.hypot_bw.rst", "ttnn/api/ttnn.i0.rst", "ttnn/api/ttnn.i0_bw.rst", "ttnn/api/ttnn.identity.rst", "ttnn/api/ttnn.imag.rst", "ttnn/api/ttnn.imag_bw.rst", "ttnn/api/ttnn.indexed_fill.rst", "ttnn/api/ttnn.is_imag.rst", "ttnn/api/ttnn.is_real.rst", "ttnn/api/ttnn.isclose.rst", "ttnn/api/ttnn.isfinite.rst", "ttnn/api/ttnn.isinf.rst", "ttnn/api/ttnn.isnan.rst", "ttnn/api/ttnn.isneginf.rst", "ttnn/api/ttnn.isposinf.rst", "ttnn/api/ttnn.kv_cache.fill_cache_for_user_.rst", "ttnn/api/ttnn.kv_cache.update_cache_for_token_.rst", "ttnn/api/ttnn.l1_loss.rst", "ttnn/api/ttnn.layer_norm.rst", "ttnn/api/ttnn.lcm.rst", "ttnn/api/ttnn.ldexp.rst", "ttnn/api/ttnn.ldexp_bw.rst", "ttnn/api/ttnn.le.rst", "ttnn/api/ttnn.le_.rst", "ttnn/api/ttnn.leaky_relu.rst", "ttnn/api/ttnn.leaky_relu_bw.rst", "ttnn/api/ttnn.lerp.rst", "ttnn/api/ttnn.lerp_bw.rst", "ttnn/api/ttnn.lez.rst", "ttnn/api/ttnn.lgamma.rst", "ttnn/api/ttnn.lgamma_bw.rst", "ttnn/api/ttnn.linear.rst", "ttnn/api/ttnn.load_tensor.rst", "ttnn/api/ttnn.log.rst", "ttnn/api/ttnn.log10.rst", "ttnn/api/ttnn.log10_bw.rst", "ttnn/api/ttnn.log1p.rst", "ttnn/api/ttnn.log1p_bw.rst", "ttnn/api/ttnn.log2.rst", "ttnn/api/ttnn.log2_bw.rst", "ttnn/api/ttnn.log_bw.rst", "ttnn/api/ttnn.log_sigmoid.rst", "ttnn/api/ttnn.log_sigmoid_bw.rst", "ttnn/api/ttnn.logaddexp.rst", "ttnn/api/ttnn.logaddexp2.rst", "ttnn/api/ttnn.logaddexp2_bw.rst", "ttnn/api/ttnn.logaddexp_bw.rst", "ttnn/api/ttnn.logical_and.rst", "ttnn/api/ttnn.logical_and_.rst", "ttnn/api/ttnn.logical_not.rst", "ttnn/api/ttnn.logical_not_.rst", "ttnn/api/ttnn.logical_or.rst", "ttnn/api/ttnn.logical_or_.rst", "ttnn/api/ttnn.logical_xor.rst", "ttnn/api/ttnn.logical_xor_.rst", "ttnn/api/ttnn.logit.rst", "ttnn/api/ttnn.logit_bw.rst", "ttnn/api/ttnn.logiteps_bw.rst", "ttnn/api/ttnn.lt.rst", "ttnn/api/ttnn.lt_.rst", "ttnn/api/ttnn.ltz.rst", "ttnn/api/ttnn.mac.rst", "ttnn/api/ttnn.manage_device.rst", "ttnn/api/ttnn.matmul.rst", "ttnn/api/ttnn.matmul_batched_weights.rst", "ttnn/api/ttnn.max.rst", "ttnn/api/ttnn.max_bw.rst", "ttnn/api/ttnn.max_pool2d.rst", "ttnn/api/ttnn.maximum.rst", "ttnn/api/ttnn.mean.rst", "ttnn/api/ttnn.min.rst", "ttnn/api/ttnn.min_bw.rst", "ttnn/api/ttnn.minimum.rst", "ttnn/api/ttnn.mish.rst", "ttnn/api/ttnn.model_preprocessing.preprocess_model.rst", "ttnn/api/ttnn.model_preprocessing.preprocess_model_parameters.rst", "ttnn/api/ttnn.moreh_sum.rst", "ttnn/api/ttnn.mse_loss.rst", "ttnn/api/ttnn.mul_bw.rst", "ttnn/api/ttnn.multigammaln.rst", "ttnn/api/ttnn.multigammaln_bw.rst", "ttnn/api/ttnn.multiply.rst", "ttnn/api/ttnn.ne.rst", "ttnn/api/ttnn.ne_.rst", "ttnn/api/ttnn.neg.rst", "ttnn/api/ttnn.neg_bw.rst", "ttnn/api/ttnn.nextafter.rst", "ttnn/api/ttnn.nez.rst", "ttnn/api/ttnn.nonzero.rst", "ttnn/api/ttnn.normalize_global.rst", "ttnn/api/ttnn.normalize_hw.rst", "ttnn/api/ttnn.ones.rst", "ttnn/api/ttnn.ones_like.rst", "ttnn/api/ttnn.open_device.rst", "ttnn/api/ttnn.outer.rst", "ttnn/api/ttnn.pad.rst", "ttnn/api/ttnn.pad_to_tile_shape.rst", "ttnn/api/ttnn.permute.rst", "ttnn/api/ttnn.polar.rst", "ttnn/api/ttnn.polar_bw.rst", "ttnn/api/ttnn.polygamma.rst", "ttnn/api/ttnn.polygamma_bw.rst", "ttnn/api/ttnn.polyval.rst", "ttnn/api/ttnn.pow.rst", "ttnn/api/ttnn.pow_bw.rst", "ttnn/api/ttnn.prelu.rst", "ttnn/api/ttnn.prepare_conv_bias.rst", "ttnn/api/ttnn.prepare_conv_transpose2d_bias.rst", "ttnn/api/ttnn.prepare_conv_transpose2d_weights.rst", "ttnn/api/ttnn.prepare_conv_weights.rst", "ttnn/api/ttnn.prod.rst", "ttnn/api/ttnn.prod_bw.rst", "ttnn/api/ttnn.rad2deg.rst", "ttnn/api/ttnn.rad2deg_bw.rst", "ttnn/api/ttnn.rand.rst", "ttnn/api/ttnn.rdiv.rst", "ttnn/api/ttnn.rdiv_bw.rst", "ttnn/api/ttnn.real.rst", "ttnn/api/ttnn.real_bw.rst", "ttnn/api/ttnn.reallocate.rst", "ttnn/api/ttnn.reciprocal.rst", "ttnn/api/ttnn.reciprocal_bw.rst", "ttnn/api/ttnn.reduce_scatter.rst", "ttnn/api/ttnn.register_post_operation_hook.rst", "ttnn/api/ttnn.register_pre_operation_hook.rst", "ttnn/api/ttnn.reglu.rst", "ttnn/api/ttnn.relu.rst", "ttnn/api/ttnn.relu6.rst", "ttnn/api/ttnn.relu6_bw.rst", "ttnn/api/ttnn.relu_bw.rst", "ttnn/api/ttnn.relu_max.rst", "ttnn/api/ttnn.relu_min.rst", "ttnn/api/ttnn.remainder.rst", "ttnn/api/ttnn.remainder_bw.rst", "ttnn/api/ttnn.repeat.rst", "ttnn/api/ttnn.repeat_bw.rst", "ttnn/api/ttnn.repeat_interleave.rst", "ttnn/api/ttnn.reshape.rst", "ttnn/api/ttnn.rms_norm.rst", "ttnn/api/ttnn.round.rst", "ttnn/api/ttnn.round_bw.rst", "ttnn/api/ttnn.rpow.rst", "ttnn/api/ttnn.rpow_bw.rst", "ttnn/api/ttnn.rsqrt.rst", "ttnn/api/ttnn.rsqrt_bw.rst", "ttnn/api/ttnn.rsub.rst", "ttnn/api/ttnn.rsub_bw.rst", "ttnn/api/ttnn.scale_causal_mask_hw_dims_softmax_in_place.rst", "ttnn/api/ttnn.scale_mask_softmax.rst", "ttnn/api/ttnn.scale_mask_softmax_in_place.rst", "ttnn/api/ttnn.scatter.rst", "ttnn/api/ttnn.selu.rst", "ttnn/api/ttnn.selu_bw.rst", "ttnn/api/ttnn.set_printoptions.rst", "ttnn/api/ttnn.sigmoid.rst", "ttnn/api/ttnn.sigmoid_accurate.rst", "ttnn/api/ttnn.sigmoid_bw.rst", "ttnn/api/ttnn.sign.rst", "ttnn/api/ttnn.sign_bw.rst", "ttnn/api/ttnn.signbit.rst", "ttnn/api/ttnn.silu.rst", "ttnn/api/ttnn.silu_bw.rst", "ttnn/api/ttnn.sin.rst", "ttnn/api/ttnn.sin_bw.rst", "ttnn/api/ttnn.sinh.rst", "ttnn/api/ttnn.sinh_bw.rst", "ttnn/api/ttnn.slice.rst", "ttnn/api/ttnn.softmax.rst", "ttnn/api/ttnn.softmax_in_place.rst", "ttnn/api/ttnn.softplus.rst", "ttnn/api/ttnn.softplus_bw.rst", "ttnn/api/ttnn.softshrink.rst", "ttnn/api/ttnn.softshrink_bw.rst", "ttnn/api/ttnn.softsign.rst", "ttnn/api/ttnn.softsign_bw.rst", "ttnn/api/ttnn.sort.rst", "ttnn/api/ttnn.sparse_matmul.rst", "ttnn/api/ttnn.sqrt.rst", "ttnn/api/ttnn.sqrt_bw.rst", "ttnn/api/ttnn.square.rst", "ttnn/api/ttnn.square_bw.rst", "ttnn/api/ttnn.squared_difference.rst", "ttnn/api/ttnn.squared_difference_bw.rst", "ttnn/api/ttnn.std.rst", "ttnn/api/ttnn.sub_bw.rst", "ttnn/api/ttnn.subalpha.rst", "ttnn/api/ttnn.subalpha_bw.rst", "ttnn/api/ttnn.subtract.rst", "ttnn/api/ttnn.sum.rst", "ttnn/api/ttnn.swiglu.rst", "ttnn/api/ttnn.swish.rst", "ttnn/api/ttnn.synchronize_device.rst", "ttnn/api/ttnn.tan.rst", "ttnn/api/ttnn.tan_bw.rst", "ttnn/api/ttnn.tanh.rst", "ttnn/api/ttnn.tanh_bw.rst", "ttnn/api/ttnn.tanhshrink.rst", "ttnn/api/ttnn.tanhshrink_bw.rst", "ttnn/api/ttnn.threshold.rst", "ttnn/api/ttnn.threshold_bw.rst", "ttnn/api/ttnn.tilize.rst", "ttnn/api/ttnn.tilize_with_val_padding.rst", "ttnn/api/ttnn.to_device.rst", "ttnn/api/ttnn.to_layout.rst", "ttnn/api/ttnn.to_memory_config.rst", "ttnn/api/ttnn.to_torch.rst", "ttnn/api/ttnn.topk.rst", "ttnn/api/ttnn.transformer.attention_softmax.rst", "ttnn/api/ttnn.transformer.attention_softmax_.rst", "ttnn/api/ttnn.transformer.concatenate_heads.rst", "ttnn/api/ttnn.transformer.scaled_dot_product_attention.rst", "ttnn/api/ttnn.transformer.scaled_dot_product_attention_decode.rst", "ttnn/api/ttnn.transformer.split_query_key_value_and_split_heads.rst", "ttnn/api/ttnn.tril.rst", "ttnn/api/ttnn.triu.rst", "ttnn/api/ttnn.trunc.rst", "ttnn/api/ttnn.trunc_bw.rst", "ttnn/api/ttnn.unary_chain.rst", "ttnn/api/ttnn.untilize.rst", "ttnn/api/ttnn.untilize_with_unpadding.rst", "ttnn/api/ttnn.upsample.rst", "ttnn/api/ttnn.var.rst", "ttnn/api/ttnn.where.rst", "ttnn/api/ttnn.where_bw.rst", "ttnn/api/ttnn.xlogy.rst", "ttnn/api/ttnn.xlogy_bw.rst", "ttnn/api/ttnn.zeros.rst", "ttnn/api/ttnn.zeros_like.rst", "ttnn/converting_torch_model_to_ttnn.rst", "ttnn/demos.rst", "ttnn/get_started.rst", "ttnn/installing.md", "ttnn/onboarding.rst", "ttnn/profiling_ttnn_operations.rst", "ttnn/tensor.rst", "ttnn/tutorials.rst", "ttnn/tutorials/2025_dx_rework/ttnn_add_tensors.ipynb", "ttnn/tutorials/2025_dx_rework/ttnn_basic_conv.ipynb", "ttnn/tutorials/2025_dx_rework/ttnn_basic_operations.ipynb", "ttnn/tutorials/2025_dx_rework/ttnn_mlp_inference_mnist.ipynb", "ttnn/tutorials/2025_dx_rework/ttnn_multihead_attention.ipynb", "ttnn/tutorials/2025_dx_rework/ttnn_simplecnn_inference.ipynb", "ttnn/tutorials/2025_dx_rework/ttnn_visualizer.md", "ttnn/usage.rst"], "titles": ["Welcome to TT-NN documentation!", "Contributing as a developer", "Support", "What is TT-NN?", "Adding New TT-NN Operation", "APIs", "ttnn.Conv2dConfig", "ttnn.Conv2dSliceConfig", "ttnn.GetDefaultDevice", "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig", "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig", "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig", "ttnn.MatmulMultiCoreReuseProgramConfig", "ttnn.SetDefaultDevice", "ttnn.SoftmaxDefaultProgramConfig", "ttnn.SoftmaxProgramConfig", "ttnn.SoftmaxShardedMultiCoreProgramConfig", "ttnn.abs", "ttnn.abs_bw", "ttnn.acos", "ttnn.acos_bw", "ttnn.acosh", "ttnn.acosh_bw", "ttnn.add", "ttnn.add_bw", "ttnn.addalpha", "ttnn.addalpha_bw", "ttnn.addcdiv", "ttnn.addcdiv_bw", "ttnn.addcmul", "ttnn.addcmul_bw", "ttnn.addmm", "ttnn.all_gather", "ttnn.all_reduce", "ttnn.alt_complex_rotate90", "ttnn.angle", "ttnn.angle_bw", "ttnn.arange", "ttnn.argmax", "ttnn.as_tensor", "ttnn.asin", "ttnn.asin_bw", "ttnn.asinh", "ttnn.asinh_bw", "ttnn.assign_bw", "ttnn.atan", "ttnn.atan2", "ttnn.atan2_bw", "ttnn.atan_bw", "ttnn.atanh", "ttnn.atanh_bw", "ttnn.batch_norm", "ttnn.bias_gelu_bw", "ttnn.bitwise_and", "ttnn.bitwise_left_shift", "ttnn.bitwise_not", "ttnn.bitwise_or", "ttnn.bitwise_right_shift", "ttnn.bitwise_xor", "ttnn.cbrt", "ttnn.ceil", "ttnn.ceil_bw", "ttnn.celu", "ttnn.celu_bw", "ttnn.clamp", "ttnn.clamp_bw", "ttnn.clip", "ttnn.clip_bw", "ttnn.clone", "ttnn.close_device", "ttnn.concat", "ttnn.concat_bw", "ttnn.conj", "ttnn.conj_bw", "ttnn.conv1d", "ttnn.conv2d", "ttnn.conv_transpose2d", "ttnn.cos", "ttnn.cos_bw", "ttnn.cosh", "ttnn.cosh_bw", "ttnn.create_sharded_memory_config", "ttnn.cumprod", "ttnn.cumsum", "ttnn.deallocate", "ttnn.deg2rad", "ttnn.deg2rad_bw", "ttnn.digamma", "ttnn.digamma_bw", "ttnn.div", "ttnn.div_bw", "ttnn.div_no_nan", "ttnn.div_no_nan_bw", "ttnn.dump_tensor", "ttnn.elu", "ttnn.elu_bw", "ttnn.embedding", "ttnn.embedding_bw", "ttnn.empty", "ttnn.empty_like", "ttnn.eq", "ttnn.eq_", "ttnn.eqz", "ttnn.erf", "ttnn.erf_bw", "ttnn.erfc", "ttnn.erfc_bw", "ttnn.erfinv", "ttnn.erfinv_bw", "ttnn.exp", "ttnn.exp2", "ttnn.exp2_bw", "ttnn.exp_bw", "ttnn.experimental.conv3d", "ttnn.experimental.dropout", "ttnn.experimental.gelu_bw", "ttnn.experimental.rotary_embedding", "ttnn.expm1", "ttnn.expm1_bw", "ttnn.fill", "ttnn.fill_bw", "ttnn.fill_ones_rm", "ttnn.fill_rm", "ttnn.fill_zero_bw", "ttnn.floor", "ttnn.floor_bw", "ttnn.floor_div", "ttnn.fmod", "ttnn.fmod_bw", "ttnn.format_input_tensor", "ttnn.format_output_tensor", "ttnn.frac", "ttnn.frac_bw", "ttnn.from_device", "ttnn.from_torch", "ttnn.full", "ttnn.full_like", "ttnn.gather", "ttnn.gcd", "ttnn.ge", "ttnn.ge_", "ttnn.geglu", "ttnn.gelu", "ttnn.gelu_bw", "ttnn.gez", "ttnn.global_avg_pool2d", "ttnn.glu", "ttnn.group_norm", "ttnn.gt", "ttnn.gt_", "ttnn.gtz", "ttnn.hardshrink", "ttnn.hardshrink_bw", "ttnn.hardsigmoid", "ttnn.hardsigmoid_bw", "ttnn.hardswish", "ttnn.hardswish_bw", "ttnn.hardtanh", "ttnn.hardtanh_bw", "ttnn.heaviside", "ttnn.hypot", "ttnn.hypot_bw", "ttnn.i0", "ttnn.i0_bw", "ttnn.identity", "ttnn.imag", "ttnn.imag_bw", "ttnn.indexed_fill", "ttnn.is_imag", "ttnn.is_real", "ttnn.isclose", "ttnn.isfinite", "ttnn.isinf", "ttnn.isnan", "ttnn.isneginf", "ttnn.isposinf", "ttnn.kv_cache.fill_cache_for_user_", "ttnn.kv_cache.update_cache_for_token_", "ttnn.l1_loss", "ttnn.layer_norm", "ttnn.lcm", "ttnn.ldexp", "ttnn.ldexp_bw", "ttnn.le", "ttnn.le_", "ttnn.leaky_relu", "ttnn.leaky_relu_bw", "ttnn.lerp", "ttnn.lerp_bw", "ttnn.lez", "ttnn.lgamma", "ttnn.lgamma_bw", "ttnn.linear", "ttnn.load_tensor", "ttnn.log", "ttnn.log10", "ttnn.log10_bw", "ttnn.log1p", "ttnn.log1p_bw", "ttnn.log2", "ttnn.log2_bw", "ttnn.log_bw", "ttnn.log_sigmoid", "ttnn.log_sigmoid_bw", "ttnn.logaddexp", "ttnn.logaddexp2", "ttnn.logaddexp2_bw", "ttnn.logaddexp_bw", "ttnn.logical_and", "ttnn.logical_and_", "ttnn.logical_not", "ttnn.logical_not_", "ttnn.logical_or", "ttnn.logical_or_", "ttnn.logical_xor", "ttnn.logical_xor_", "ttnn.logit", "ttnn.logit_bw", "ttnn.logiteps_bw", "ttnn.lt", "ttnn.lt_", "ttnn.ltz", "ttnn.mac", "ttnn.manage_device", "ttnn.matmul", "ttnn.matmul_batched_weights", "ttnn.max", "ttnn.max_bw", "ttnn.max_pool2d", "ttnn.maximum", "ttnn.mean", "ttnn.min", "ttnn.min_bw", "ttnn.minimum", "ttnn.mish", "ttnn.model_preprocessing.preprocess_model", "ttnn.model_preprocessing.preprocess_model_parameters", "ttnn.moreh_sum", "ttnn.mse_loss", "ttnn.mul_bw", "ttnn.multigammaln", "ttnn.multigammaln_bw", "ttnn.multiply", "ttnn.ne", "ttnn.ne_", "ttnn.neg", "ttnn.neg_bw", "ttnn.nextafter", "ttnn.nez", "ttnn.nonzero", "ttnn.normalize_global", "ttnn.normalize_hw", "ttnn.ones", "ttnn.ones_like", "ttnn.open_device", "ttnn.outer", "ttnn.pad", "ttnn.pad_to_tile_shape", "ttnn.permute", "ttnn.polar", "ttnn.polar_bw", "ttnn.polygamma", "ttnn.polygamma_bw", "ttnn.polyval", "ttnn.pow", "ttnn.pow_bw", "ttnn.prelu", "ttnn.prepare_conv_bias", "ttnn.prepare_conv_transpose2d_bias", "ttnn.prepare_conv_transpose2d_weights", "ttnn.prepare_conv_weights", "ttnn.prod", "ttnn.prod_bw", "ttnn.rad2deg", "ttnn.rad2deg_bw", "ttnn.rand", "ttnn.rdiv", "ttnn.rdiv_bw", "ttnn.real", "ttnn.real_bw", "ttnn.reallocate", "ttnn.reciprocal", "ttnn.reciprocal_bw", "ttnn.reduce_scatter", "ttnn.register_post_operation_hook", "ttnn.register_pre_operation_hook", "ttnn.reglu", "ttnn.relu", "ttnn.relu6", "ttnn.relu6_bw", "ttnn.relu_bw", "ttnn.relu_max", "ttnn.relu_min", "ttnn.remainder", "ttnn.remainder_bw", "ttnn.repeat", "ttnn.repeat_bw", "ttnn.repeat_interleave", "ttnn.reshape", "ttnn.rms_norm", "ttnn.round", "ttnn.round_bw", "ttnn.rpow", "ttnn.rpow_bw", "ttnn.rsqrt", "ttnn.rsqrt_bw", "ttnn.rsub", "ttnn.rsub_bw", "ttnn.scale_causal_mask_hw_dims_softmax_in_place", "ttnn.scale_mask_softmax", "ttnn.scale_mask_softmax_in_place", "ttnn.scatter", "ttnn.selu", "ttnn.selu_bw", "ttnn.set_printoptions", "ttnn.sigmoid", "ttnn.sigmoid_accurate", "ttnn.sigmoid_bw", "ttnn.sign", "ttnn.sign_bw", "ttnn.signbit", "ttnn.silu", "ttnn.silu_bw", "ttnn.sin", "ttnn.sin_bw", "ttnn.sinh", "ttnn.sinh_bw", "ttnn.slice", "ttnn.softmax", "ttnn.softmax_in_place", "ttnn.softplus", "ttnn.softplus_bw", "ttnn.softshrink", "ttnn.softshrink_bw", "ttnn.softsign", "ttnn.softsign_bw", "ttnn.sort", "ttnn.sparse_matmul", "ttnn.sqrt", "ttnn.sqrt_bw", "ttnn.square", "ttnn.square_bw", "ttnn.squared_difference", "ttnn.squared_difference_bw", "ttnn.std", "ttnn.sub_bw", "ttnn.subalpha", "ttnn.subalpha_bw", "ttnn.subtract", "ttnn.sum", "ttnn.swiglu", "ttnn.swish", "ttnn.synchronize_device", "ttnn.tan", "ttnn.tan_bw", "ttnn.tanh", "ttnn.tanh_bw", "ttnn.tanhshrink", "ttnn.tanhshrink_bw", "ttnn.threshold", "ttnn.threshold_bw", "ttnn.tilize", "ttnn.tilize_with_val_padding", "ttnn.to_device", "ttnn.to_layout", "ttnn.to_memory_config", "ttnn.to_torch", "ttnn.topk", "ttnn.transformer.attention_softmax", "ttnn.transformer.attention_softmax_", "ttnn.transformer.concatenate_heads", "ttnn.transformer.scaled_dot_product_attention", "ttnn.transformer.scaled_dot_product_attention_decode", "ttnn.transformer.split_query_key_value_and_split_heads", "ttnn.tril", "ttnn.triu", "ttnn.trunc", "ttnn.trunc_bw", "ttnn.unary_chain", "ttnn.untilize", "ttnn.untilize_with_unpadding", "ttnn.upsample", "ttnn.var", "ttnn.where", "ttnn.where_bw", "ttnn.xlogy", "ttnn.xlogy_bw", "ttnn.zeros", "ttnn.zeros_like", "Converting PyTorch Model to TT-NN", "Building and Uplifting Demos", "Getting Started", "Install", "Onboarding New Functionality", "Profiling TT-NN Operations", "Tensor", "Tutorials", "Add Tensors", "Basic Convolution", "Basic Tensor Operations", "MLP Inference", "Multi-Head Attention", "Running a Simple CNN Inference on CIFAR-10", "TT-NN Visualizer", "Using TT-NN"], "terms": {"what": [0, 390, 394], "i": [0, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 24, 26, 31, 32, 33, 38, 47, 51, 52, 55, 64, 69, 70, 71, 75, 76, 81, 82, 83, 89, 90, 96, 97, 100, 116, 119, 121, 122, 129, 130, 132, 134, 137, 139, 141, 145, 146, 147, 148, 161, 163, 168, 169, 179, 182, 183, 188, 192, 206, 207, 208, 219, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 235, 236, 239, 241, 243, 254, 256, 257, 259, 261, 263, 264, 266, 271, 272, 276, 281, 283, 284, 285, 286, 291, 292, 297, 298, 299, 307, 308, 309, 310, 311, 327, 328, 329, 336, 337, 343, 344, 345, 347, 349, 350, 352, 353, 354, 355, 357, 364, 366, 367, 368, 369, 371, 372, 373, 381, 382, 383, 386, 389, 390, 391, 392, 393, 394, 395, 397, 398, 399, 400, 401, 402, 403, 404], "get": [0, 6, 8, 315, 370, 389, 392, 396, 398, 403], "start": [0, 37, 187, 327, 389, 392, 394, 396, 401, 403], "1": [0, 6, 7, 9, 17, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 43, 44, 46, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 74, 76, 78, 80, 82, 83, 86, 88, 89, 90, 91, 92, 94, 95, 96, 97, 100, 101, 103, 104, 105, 106, 108, 109, 111, 112, 113, 114, 115, 116, 118, 119, 120, 121, 122, 123, 125, 126, 127, 128, 129, 130, 132, 134, 135, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 151, 152, 154, 156, 157, 158, 159, 160, 161, 163, 164, 165, 166, 167, 168, 169, 170, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 191, 194, 195, 196, 197, 198, 199, 200, 201, 203, 204, 205, 206, 207, 208, 209, 212, 213, 214, 215, 216, 217, 218, 219, 220, 222, 224, 226, 227, 228, 229, 230, 231, 232, 233, 238, 239, 240, 241, 242, 243, 244, 246, 247, 249, 250, 251, 252, 253, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 271, 272, 274, 275, 276, 277, 278, 279, 281, 282, 283, 286, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 309, 310, 311, 312, 313, 315, 316, 317, 319, 322, 324, 326, 327, 328, 329, 330, 331, 332, 333, 335, 336, 337, 339, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 353, 354, 355, 356, 357, 358, 359, 360, 364, 367, 371, 372, 373, 377, 382, 383, 384, 385, 386, 387, 388, 393, 395, 397, 398, 399, 400, 401, 402, 403], "instal": [0, 390, 394, 396, 403, 404], "build": [0, 398, 403], "2": [0, 6, 7, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 71, 73, 76, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 140, 142, 143, 144, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 166, 167, 170, 171, 172, 173, 174, 175, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 252, 253, 256, 257, 258, 260, 261, 262, 263, 264, 265, 266, 271, 273, 274, 276, 277, 279, 281, 282, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 351, 353, 354, 355, 356, 357, 358, 359, 360, 366, 371, 373, 374, 375, 376, 377, 378, 382, 383, 384, 385, 386, 387, 388, 395, 397, 398, 399, 400, 401, 402], "explor": [0, 403], "our": [0, 390, 392, 393, 395, 399], "model": [0, 3, 235, 236, 390, 393, 394, 396, 397, 398, 400, 401, 402], "demo": [0, 392, 394, 403], "where": [0, 2, 6, 31, 32, 37, 51, 76, 81, 98, 99, 129, 130, 147, 179, 192, 224, 225, 249, 266, 283, 299, 308, 309, 311, 337, 364, 384, 390, 394, 395, 402, 403], "To": [0, 6, 367, 393, 395, 396, 398, 399, 403, 404], "go": [0, 311], "from": [0, 2, 4, 6, 9, 32, 33, 37, 39, 69, 70, 74, 75, 76, 82, 83, 97, 134, 137, 176, 177, 193, 267, 268, 269, 270, 275, 306, 307, 308, 309, 311, 328, 337, 348, 380, 383, 389, 390, 392, 394, 395, 397, 398, 399, 400, 401, 402, 403], "here": [0, 2, 4, 392, 394, 397, 398, 399, 400, 403], "prerequisit": [0, 396], "set": [0, 6, 9, 13, 14, 70, 75, 76, 121, 224, 241, 258, 267, 268, 271, 298, 314, 337, 352, 367, 370, 373, 389, 394, 396, 400, 402, 403, 404], "up": [0, 3, 122, 147, 311, 394, 396, 398, 402], "hardwar": [0, 2, 4, 254, 389, 390, 391, 395, 397, 400, 402, 403, 404], "softwar": [0, 398, 399, 400, 401, 402], "depend": [0, 11, 224, 267, 268, 269, 270, 364, 383, 390, 394, 395, 396], "option": [0, 6, 9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 178, 179, 180, 181, 182, 183, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 214, 216, 217, 218, 219, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 238, 239, 240, 241, 242, 243, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 261, 262, 263, 264, 265, 266, 271, 272, 273, 274, 275, 276, 277, 278, 280, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 394, 398, 402, 403, 404], "script": [0, 390, 394, 396, 397, 398, 399, 400, 401, 402], "recommend": [0, 6, 182, 389, 396, 403, 404], "manual": [0, 4, 389, 396, 404], "metalium": [0, 4], "There": [0, 4, 224, 389, 395], "ar": [0, 6, 7, 9, 11, 12, 13, 24, 31, 32, 46, 51, 54, 57, 68, 70, 75, 82, 83, 90, 122, 137, 147, 179, 192, 224, 228, 249, 258, 264, 284, 285, 299, 302, 309, 327, 337, 352, 367, 373, 381, 389, 390, 391, 394, 395, 396, 398, 400, 401, 402, 403, 404], "four": 0, "binari": [0, 23, 25, 46, 53, 54, 56, 57, 58, 89, 91, 100, 101, 126, 127, 138, 139, 140, 148, 149, 160, 170, 180, 181, 183, 184, 204, 205, 208, 209, 212, 213, 214, 215, 219, 220, 229, 233, 242, 243, 244, 247, 255, 263, 264, 266, 293, 306, 342, 346, 348, 385], "step": [0, 37, 327, 390, 393, 403], "latest": [0, 394], "wheel": [0, 404], "For": [0, 4, 6, 18, 28, 39, 52, 75, 82, 83, 90, 128, 134, 137, 143, 147, 161, 188, 196, 198, 200, 201, 203, 206, 207, 217, 218, 224, 265, 272, 277, 282, 294, 309, 310, 329, 331, 336, 339, 367, 373, 383, 386, 389, 390, 393, 394, 395, 397, 400, 402, 403], "user": [0, 4, 6, 31, 164, 192, 224, 225, 235, 236, 254, 337, 362, 390, 391, 393, 398, 399, 400, 401, 402, 403], "onli": [0, 4, 6, 7, 24, 26, 27, 29, 38, 47, 52, 64, 65, 66, 67, 70, 71, 81, 89, 90, 97, 122, 134, 139, 141, 146, 147, 148, 161, 182, 183, 187, 188, 206, 207, 208, 219, 222, 224, 226, 227, 230, 231, 232, 239, 256, 261, 269, 270, 271, 281, 286, 294, 307, 308, 337, 343, 344, 345, 347, 349, 350, 352, 355, 357, 367, 371, 372, 373, 382, 383, 386, 389, 390, 394, 395, 402, 403, 404], "environ": [0, 6, 391, 396, 403, 404], "docker": [0, 404], "releas": [0, 84, 404], "imag": [0, 6, 76, 168, 259, 390, 394, 395, 396, 398, 402, 404], "sourc": [0, 3, 137, 176, 311, 403], "clone": [0, 164, 373, 403], "repositori": [0, 1, 391, 396, 403], "librari": [0, 3, 396, 402], "3": [0, 6, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 34, 36, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 70, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 170, 171, 172, 173, 174, 175, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 227, 228, 229, 232, 233, 234, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 256, 258, 260, 261, 262, 263, 264, 265, 266, 271, 273, 274, 276, 277, 281, 282, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 345, 346, 347, 348, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 363, 364, 366, 373, 374, 375, 376, 377, 378, 383, 384, 385, 386, 387, 388, 395, 396, 397, 398, 400, 401, 402], "virtual": [0, 396], "setup": [0, 11, 390, 391, 396, 403], "anaconda": 0, "packag": 0, "you": [0, 1, 2, 4, 330, 390, 391, 394, 395, 396, 403, 404], "all": [0, 4, 6, 9, 15, 31, 32, 33, 38, 51, 137, 145, 179, 224, 226, 230, 231, 235, 236, 256, 263, 271, 272, 283, 299, 309, 310, 328, 344, 349, 352, 367, 382, 389, 390, 393, 394, 395, 397, 400, 401, 402, 403], "verifi": [0, 393, 402], "your": [0, 390, 391, 394, 396, 403], "try": [0, 6, 390, 398], "execut": [0, 6, 16, 284, 285, 367, 390, 394, 398, 400, 401, 402, 403, 404], "program": [0, 3, 4, 9, 10, 11, 12, 14, 15, 16, 31, 192, 224, 225, 308, 310, 329, 337, 368, 369, 394, 397, 398, 399, 400, 401, 402], "exampl": [0, 32, 33, 133, 137, 258, 283, 297, 311, 336, 363, 390, 391, 393, 394, 395, 396, 403], "interest": 0, "contribut": [0, 2, 391], "multi": [0, 12, 16, 39, 329, 381, 395, 396, 400], "card": [0, 390], "configur": [0, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 131, 132, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 178, 179, 180, 181, 182, 183, 185, 186, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 214, 216, 217, 218, 219, 221, 222, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 238, 239, 240, 241, 242, 243, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 258, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 367, 368, 369, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391, 396, 398, 402, 403], "topologi": [0, 32, 33, 283], "machin": [0, 394, 403], "requir": [0, 4, 9, 24, 26, 44, 51, 71, 81, 90, 121, 122, 135, 147, 235, 236, 239, 252, 271, 307, 328, 345, 347, 384, 387, 390, 394, 398, 399, 401, 402, 403], "overview": [0, 403], "why": 0, "It": [0, 3, 4, 6, 7, 12, 14, 15, 16, 74, 75, 76, 235, 236, 271, 283, 366, 400, 402, 403], "matter": 0, "vm": 0, "us": [0, 3, 4, 6, 7, 8, 9, 11, 12, 13, 14, 15, 31, 32, 33, 52, 70, 74, 75, 76, 81, 90, 96, 97, 99, 103, 105, 109, 114, 115, 116, 122, 134, 136, 142, 143, 147, 161, 164, 192, 194, 195, 197, 199, 224, 225, 228, 235, 236, 253, 254, 256, 267, 268, 269, 270, 271, 276, 277, 283, 284, 285, 296, 308, 309, 310, 314, 315, 316, 328, 329, 330, 336, 337, 361, 362, 365, 366, 367, 372, 373, 379, 380, 388, 389, 390, 391, 392, 393, 395, 396, 398, 399, 400, 401, 402, 403], "basic": [0, 12, 392, 396], "convert": [0, 3, 4, 6, 9, 39, 68, 134, 137, 147, 235, 236, 267, 268, 269, 270, 336, 365, 366, 395, 398, 399, 400, 401, 402], "torch": [0, 4, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 77, 78, 79, 80, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 131, 132, 133, 134, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253, 255, 258, 259, 260, 261, 262, 263, 264, 265, 266, 272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 300, 301, 302, 303, 304, 305, 306, 307, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 363, 364, 365, 366, 367, 373, 374, 375, 376, 377, 378, 382, 383, 384, 385, 386, 388, 389, 395, 398, 399, 400, 401, 402], "tensor": [0, 3, 4, 6, 7, 9, 10, 11, 12, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 227, 228, 229, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 345, 346, 347, 348, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 383, 384, 385, 386, 387, 388, 389, 394, 396, 400, 401, 402], "run": [0, 3, 4, 51, 134, 235, 284, 285, 355, 357, 367, 390, 391, 392, 393, 394, 396, 397, 399, 400, 401], "an": [0, 2, 3, 4, 70, 74, 75, 76, 96, 113, 122, 123, 127, 145, 223, 224, 259, 266, 271, 275, 293, 308, 363, 366, 390, 391, 392, 393, 394, 395, 396, 398, 401, 402, 403], "oper": [0, 3, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 392, 393, 395, 396, 400, 401, 402], "devic": [0, 3, 6, 8, 13, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 374, 375, 376, 377, 378, 379, 380, 382, 383, 384, 385, 386, 387, 388, 389, 390, 392, 394, 395, 396, 401, 403], "__getitem__": 0, "slice": [0, 6, 7, 75], "4": [0, 6, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 34, 36, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 71, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 138, 139, 140, 141, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 170, 171, 172, 173, 174, 175, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 227, 228, 229, 232, 233, 234, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 250, 251, 255, 256, 257, 260, 261, 262, 263, 264, 265, 266, 271, 272, 273, 274, 276, 277, 281, 282, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 298, 300, 301, 302, 303, 304, 305, 306, 307, 310, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 345, 346, 347, 348, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 374, 375, 376, 377, 378, 383, 384, 385, 386, 387, 388, 392, 395, 398, 399, 400, 401, 402, 403], "enabl": [0, 3, 6, 16, 75, 314, 316, 355, 357, 367, 392, 393, 398, 399, 400, 401, 402, 403], "cach": [0, 3, 39, 69, 116, 176, 177, 235, 236, 394, 395, 398, 399, 400, 401, 402, 403], "5": [0, 6, 37, 51, 65, 67, 81, 82, 83, 87, 94, 96, 98, 119, 136, 151, 152, 170, 216, 240, 266, 271, 277, 311, 332, 333, 336, 383, 399, 400, 401, 402], "debug": [0, 3, 4, 393, 402, 403], "intermedi": [0, 12, 309, 310], "6": [0, 37, 119, 240, 300, 314, 336, 383, 395, 398, 399, 400, 401, 402], "trace": [0, 3, 254, 394], "graph": [0, 3, 235, 396], "7": [0, 96, 135, 300, 337, 398, 399, 400, 401, 402], "tt_lib": [0, 121, 122, 363], "8": [0, 16, 32, 33, 37, 81, 96, 147, 256, 283, 308, 310, 329, 337, 383, 394, 395, 399, 402], "log": [0, 203, 385, 394, 397, 400, 402, 403], "9": [0, 64, 66, 79, 96, 98, 325, 383, 400, 401], "support": [0, 1, 3, 6, 7, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 34, 36, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 131, 132, 137, 138, 139, 140, 141, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 170, 171, 172, 173, 174, 175, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 250, 251, 255, 256, 260, 261, 262, 263, 264, 265, 266, 269, 270, 271, 272, 273, 274, 275, 276, 277, 281, 282, 286, 287, 288, 289, 290, 291, 292, 293, 294, 296, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 367, 372, 374, 375, 376, 377, 378, 382, 383, 384, 385, 386, 387, 388, 392, 395, 402, 403], "python": [0, 3, 392, 393, 394, 395, 396, 397, 399, 403], "10": [0, 37, 84, 96, 133, 137, 145, 163, 192, 224, 261, 280, 311, 337, 359, 363, 364, 365, 392, 396, 398, 400, 403], "chang": [0, 6, 298, 361, 362, 379, 380, 395], "string": [0, 52, 89, 143, 235, 236, 276, 277, 314], "represent": [0, 330, 395, 403], "11": [0, 98, 392], "visual": [0, 3, 396], "web": 0, "browser": [0, 403], "12": [0, 71, 147, 179, 256, 299, 308, 310, 389, 392, 396, 401], "regist": [0, 3, 4, 284, 285], "pre": [0, 4, 147, 228, 285, 392, 401, 402], "post": [0, 284, 394], "hook": [0, 284, 285], "13": [0, 398, 399, 400, 401, 402], "queri": [0, 373, 401], "14": [0, 71, 398, 399, 400, 401, 402], "fall": [0, 32, 182, 283, 329], "back": [0, 4, 32, 283, 329, 370, 394, 398, 400, 402], "15": [0, 147, 398, 399, 402], "captur": 0, "c": [0, 3, 7, 51, 74, 75, 76, 113, 121, 122, 147, 228, 315, 367, 381, 392, 394, 395, 398, 402], "function": [0, 6, 8, 9, 10, 11, 13, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 395, 398, 399, 400, 402], "buffer": [0, 4, 6, 84, 224, 254, 299, 329, 394, 395, 396], "alloc": [0, 4, 37, 98, 99, 135, 136, 147, 252, 253, 275, 310, 329, 387, 388, 395, 398, 402, 403], "etc": [0, 3, 4, 6, 311, 395, 402], "shape": [0, 9, 11, 31, 32, 33, 37, 38, 51, 70, 76, 81, 82, 83, 98, 99, 119, 129, 130, 135, 136, 137, 145, 147, 179, 192, 224, 228, 249, 252, 253, 256, 257, 258, 271, 275, 283, 295, 296, 297, 298, 299, 308, 310, 327, 329, 337, 362, 367, 370, 373, 380, 387, 388, 389, 394, 397, 398, 399, 400, 401, 402, 403], "layout": [0, 4, 6, 9, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 71, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 170, 171, 172, 173, 174, 175, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 293, 294, 296, 297, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 366, 367, 374, 375, 376, 377, 378, 379, 380, 382, 383, 384, 385, 386, 387, 388, 389, 394, 397, 398, 399, 400, 401, 402, 403, 404], "data": [0, 3, 6, 9, 10, 11, 12, 16, 23, 31, 32, 33, 37, 38, 39, 68, 74, 75, 76, 96, 97, 98, 99, 100, 121, 122, 134, 135, 136, 138, 139, 145, 147, 148, 179, 180, 181, 183, 192, 204, 205, 208, 212, 214, 219, 224, 225, 226, 228, 229, 230, 231, 233, 242, 243, 252, 253, 267, 268, 269, 270, 271, 275, 276, 299, 306, 308, 309, 310, 328, 329, 337, 342, 344, 348, 349, 361, 362, 364, 365, 366, 367, 379, 380, 381, 382, 385, 387, 388, 389, 394, 396, 399, 402, 403, 404], "type": [0, 3, 4, 6, 7, 15, 23, 31, 32, 33, 37, 38, 39, 52, 68, 74, 75, 76, 82, 83, 90, 96, 97, 98, 99, 100, 113, 121, 122, 129, 134, 135, 136, 137, 138, 139, 143, 145, 147, 148, 179, 180, 181, 183, 192, 204, 205, 208, 212, 214, 219, 224, 225, 226, 229, 230, 231, 233, 242, 243, 252, 253, 254, 258, 267, 268, 269, 270, 271, 275, 276, 280, 283, 298, 299, 306, 308, 309, 310, 328, 329, 336, 337, 342, 344, 348, 349, 352, 361, 362, 364, 365, 366, 367, 379, 380, 382, 385, 387, 388, 389, 394, 400, 402, 403], "limit": [0, 18, 28, 38, 51, 52, 82, 83, 90, 128, 143, 147, 161, 179, 192, 196, 198, 200, 201, 203, 206, 207, 217, 218, 265, 271, 272, 277, 282, 294, 299, 308, 309, 310, 328, 329, 331, 339, 367, 386, 390], "bfloat8_b": [0, 6, 17, 18, 19, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 34, 40, 42, 43, 44, 45, 46, 47, 48, 52, 59, 60, 62, 64, 71, 77, 78, 79, 85, 86, 87, 88, 90, 92, 94, 100, 101, 102, 103, 105, 107, 109, 110, 111, 112, 117, 118, 119, 124, 128, 131, 132, 134, 139, 140, 141, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153, 155, 157, 158, 159, 160, 161, 162, 163, 164, 171, 172, 173, 174, 175, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 230, 231, 232, 234, 239, 242, 243, 244, 245, 246, 247, 248, 263, 264, 265, 266, 272, 273, 274, 275, 277, 281, 282, 286, 287, 288, 290, 291, 294, 299, 300, 303, 304, 305, 306, 307, 308, 309, 310, 312, 313, 316, 317, 318, 320, 321, 322, 323, 324, 325, 328, 329, 330, 331, 332, 333, 334, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 357, 358, 366, 374, 375, 376, 378, 382, 383, 384, 386, 389, 394, 401], "storag": [0, 9, 11, 12, 16], "shard": [0, 6, 9, 10, 11, 16, 24, 31, 38, 70, 81, 147, 179, 192, 224, 225, 226, 228, 230, 231, 267, 268, 269, 270, 271, 299, 310, 328, 329, 337, 344, 349, 363, 365, 367, 373, 382, 403], "memori": [0, 6, 9, 10, 11, 12, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 178, 179, 180, 181, 182, 183, 185, 186, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 214, 216, 217, 218, 219, 221, 222, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 238, 239, 240, 241, 242, 243, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 392, 394, 396, 398, 401, 402], "config": [0, 6, 9, 10, 11, 12, 14, 16, 72, 96, 97, 113, 129, 130, 179, 224, 259, 267, 268, 298, 308, 310, 327, 367, 368, 369, 370, 389, 398, 402, 403, 404], "api": [0, 4, 371, 389, 391, 392, 393, 397, 398, 399, 400, 401, 402, 403, 404], "rank": [0, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 34, 36, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 71, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 131, 132, 138, 139, 140, 141, 142, 143, 144, 146, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 170, 171, 172, 173, 174, 175, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 227, 229, 232, 233, 234, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 250, 251, 255, 256, 260, 261, 262, 263, 264, 265, 266, 271, 272, 273, 274, 276, 277, 281, 282, 286, 287, 288, 289, 290, 291, 292, 293, 294, 296, 299, 300, 301, 302, 303, 304, 305, 306, 307, 310, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 330, 331, 332, 333, 334, 335, 338, 339, 340, 341, 342, 343, 345, 346, 347, 348, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 366, 374, 375, 376, 377, 378, 383, 384, 385, 386, 387, 388, 395], "to_rank": [0, 395], "open_devic": [0, 13, 69, 84, 96, 97, 133, 193, 280, 311, 352, 363, 364, 365, 397, 398, 399, 400, 401, 402, 404], "close_devic": [0, 397, 398, 399, 400, 401, 402, 404], "manage_devic": [0, 404], "synchronize_devic": 0, "setdefaultdevic": 0, "getdefaultdevic": 0, "format_input_tensor": 0, "format_output_tensor": 0, "pad_to_tile_shap": 0, "create_sharded_memory_config": [0, 395], "core": [0, 4, 6, 9, 10, 11, 12, 16, 31, 32, 33, 81, 147, 192, 224, 225, 254, 283, 329, 337, 364, 365, 367, 379, 389, 394, 395, 398, 399, 400, 401, 402, 403], "as_tensor": 0, "from_torch": [0, 4, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 77, 78, 79, 80, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 131, 132, 133, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 227, 228, 229, 232, 233, 234, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253, 255, 258, 259, 260, 261, 262, 263, 264, 265, 266, 272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 300, 301, 302, 303, 304, 305, 306, 307, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 330, 331, 332, 333, 334, 335, 336, 338, 339, 340, 341, 342, 343, 345, 346, 347, 348, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 363, 364, 365, 366, 374, 375, 376, 377, 378, 383, 384, 385, 386, 388, 389, 395, 399, 400, 401, 402, 404], "to_torch": [0, 4, 147, 256, 389, 398, 399, 400, 401, 402, 404], "to_devic": [0, 35, 36, 72, 73, 84, 92, 96, 133, 165, 166, 167, 168, 169, 178, 192, 224, 228, 238, 249, 258, 259, 260, 262, 278, 279, 280, 303, 337, 364, 365, 401], "from_devic": 0, "to_layout": [0, 84, 134, 366, 395, 401, 402], "dump_tensor": 0, "load_tensor": 0, "dealloc": [0, 6, 228, 280, 395, 401, 404], "realloc": [0, 228, 298], "to_memory_config": [0, 308, 310, 395], "creation": [0, 16, 37, 98, 99, 134, 135, 136, 252, 253, 387, 388, 395, 396, 398, 400], "arang": [0, 298, 399], "empti": [0, 4, 9, 404], "empty_lik": 0, "zero": [0, 6, 70, 74, 75, 76, 89, 114, 123, 136, 147, 223, 249, 258, 267, 268, 269, 270, 276, 281, 327, 337, 395, 398, 399, 404], "zeros_lik": 0, "ones": [0, 93, 129, 337, 395, 399], "ones_lik": 0, "full": [0, 6, 271, 390, 394, 395, 396, 403, 404], "full_lik": 0, "rand": [0, 19, 21, 34, 38, 40, 42, 45, 49, 51, 55, 59, 60, 71, 77, 79, 82, 83, 85, 96, 102, 107, 110, 117, 124, 131, 141, 144, 146, 147, 150, 153, 155, 162, 171, 172, 173, 174, 175, 179, 189, 190, 202, 210, 211, 221, 226, 230, 231, 234, 245, 248, 250, 251, 255, 266, 271, 272, 273, 286, 287, 288, 296, 297, 299, 304, 308, 309, 310, 318, 320, 321, 323, 325, 328, 329, 334, 338, 340, 344, 349, 350, 351, 353, 367, 374, 375, 376, 382, 398, 399, 400, 402, 404], "matrix": [0, 3, 6, 9, 11, 31, 96, 192, 224, 225, 337, 395, 397, 399, 403, 404], "multipl": [0, 3, 4, 6, 11, 12, 31, 68, 84, 122, 135, 141, 146, 147, 180, 224, 225, 235, 252, 267, 268, 269, 270, 286, 337, 350, 364, 367, 387, 392, 394, 399, 403], "matmul": [0, 9, 10, 11, 12, 31, 192, 225, 337, 399, 401, 403], "linear": [0, 225, 330, 389, 400, 401, 402], "matmul_batched_weight": 0, "addmm": 0, "sparse_matmul": 0, "matmulmulticorereuseprogramconfig": [0, 224], "compute_with_storage_grid_s": [0, 4, 9, 11, 12, 16, 308, 309, 310, 329], "from_json": [0, 9, 10, 11, 12], "in0_block_w": [0, 9, 10, 11, 12], "out_subblock_h": [0, 9, 11, 12], "out_subblock_w": [0, 9, 11, 12], "per_core_m": [0, 9, 10, 11, 12], "per_core_n": [0, 9, 10, 11, 12], "to_json": [0, 9, 10, 11, 12], "matmulmulticorereusemulticastprogramconfig": [0, 224], "fuse_batch": [0, 9, 11], "fused_activ": [0, 9, 10, 11], "out_block_h": [0, 9, 11], "out_block_w": [0, 9, 11], "transpose_mcast": [0, 11], "matmulmulticorereusemulticast1dprogramconfig": [0, 224], "gather_in0": [0, 9], "hop_cor": [0, 9], "mcast_in0": [0, 9, 224], "num_global_cb_receiv": [0, 9], "untilize_out": [0, 9], "matmulmulticorereusemulticastdramshardedprogramconfig": [0, 224], "pointwis": 0, "unari": [0, 4, 17, 19, 21, 34, 40, 42, 45, 49, 55, 59, 60, 62, 64, 66, 77, 79, 85, 87, 94, 102, 103, 105, 107, 109, 110, 117, 119, 124, 131, 141, 142, 144, 146, 150, 151, 153, 155, 157, 159, 162, 164, 171, 172, 173, 174, 175, 185, 189, 190, 194, 195, 197, 199, 202, 210, 211, 216, 221, 234, 240, 245, 248, 250, 251, 261, 273, 276, 277, 281, 286, 287, 288, 291, 292, 300, 302, 304, 312, 315, 316, 318, 320, 321, 323, 325, 330, 332, 334, 338, 340, 350, 351, 353, 355, 357, 359, 374, 375, 376, 378], "ab": [0, 18], "aco": [0, 20], "acosh": [0, 22], "asin": [0, 41], "asinh": [0, 43], "atan": [0, 48], "atanh": [0, 50], "bitwise_not": 0, "bitwise_left_shift": 0, "bitwise_right_shift": 0, "cbrt": 0, "ceil": [0, 61, 147, 228], "celu": [0, 63], "clamp": [0, 65], "clip": [0, 67], "co": 0, "cosh": [0, 80], "deg2rad": [0, 86], "digamma": [0, 88], "experiment": [0, 6, 308, 390], "dropout": 0, "gelu_bw": 0, "elu": [0, 95], "eqz": 0, "erf": [0, 104], "erfc": [0, 106], "erfinv": [0, 108], "exp": [0, 378, 404], "exp2": [0, 111], "expm1": [0, 118], "fill": [0, 120, 122, 123, 135, 136, 252, 253, 275, 295, 327, 387, 388, 394, 397], "floor": [0, 89, 90, 125, 126, 276, 277], "frac": [0, 46, 51, 89, 132, 147, 179, 299, 328, 329], "geglu": 0, "gelu": [0, 6, 11, 115, 141, 143, 389], "glu": 0, "gez": 0, "gtz": 0, "hardshrink": [0, 152, 186], "hardsigmoid": [0, 154], "hardswish": [0, 156], "hardtanh": [0, 158], "heavisid": 0, "i0": [0, 163], "ident": [0, 147, 179, 310], "isfinit": 0, "isinf": 0, "isnan": 0, "isneginf": 0, "isposinf": 0, "leaky_relu": [0, 186], "lez": 0, "lgamma": [0, 191], "log10": [0, 196], "log1p": [0, 198], "log2": [0, 200], "log_sigmoid": 0, "logical_not": [0, 211], "logical_not_": 0, "logit": [0, 217, 400, 402], "ltz": 0, "mish": 0, "multigammaln": 0, "neg": [0, 246, 265], "nez": 0, "normalize_glob": 0, "normalize_hw": 0, "polygamma": [0, 262], "prelu": 0, "rad2deg": [0, 274], "rdiv": [0, 277], "reciproc": [0, 147, 282, 305, 395], "reglu": 0, "relu": [0, 6, 11, 23, 185, 242, 286, 290, 291, 292, 330, 348, 378, 400, 402], "relu_max": 0, "relu_min": 0, "relu6": [0, 289], "remaind": [0, 294], "round": [0, 10, 44, 89, 90, 276, 277, 301, 395], "rsqrt": 0, "selu": [0, 313], "sigmoid": [0, 6, 203, 317], "sigmoid_accur": 0, "sign": [0, 319], "signbit": 0, "silu": [0, 6, 322, 350, 404], "sin": [0, 324], "sinh": [0, 326], "softplu": [0, 331], "softshrink": [0, 333], "softsign": [0, 335], "sqrt": [0, 51, 147, 160, 179, 299], "squar": [0, 6, 238, 299, 305, 339, 341, 342, 368, 369, 395], "swiglu": 0, "swish": 0, "tan": [0, 354], "tanh": [0, 6, 52, 115, 143, 356], "tanhshrink": [0, 358], "threshold": [0, 330, 331, 360], "tril": 0, "triu": 0, "trunc": [0, 89, 90, 276, 277], "unary_chain": 0, "clamp_bw": 0, "clip_bw": 0, "hardtanh_bw": 0, "threshold_bw": 0, "softplus_bw": 0, "rdiv_bw": 0, "pow_bw": 0, "exp_bw": 0, "tanh_bw": 0, "sqrt_bw": 0, "multigammaln_bw": 0, "lgamma_bw": 0, "fill_bw": 0, "hardsigmoid_bw": 0, "cos_bw": 0, "acosh_bw": 0, "acos_bw": 0, "atan_bw": 0, "rad2deg_bw": 0, "frac_bw": 0, "trunc_bw": 0, "log_sigmoid_bw": 0, "fill_zero_bw": 0, "i0_bw": 0, "tan_bw": 0, "sigmoid_bw": 0, "rsqrt_bw": 0, "neg_bw": 0, "relu_bw": 0, "logit_bw": 0, "hardshrink_bw": 0, "softshrink_bw": 0, "leaky_relu_bw": 0, "elu_bw": 0, "celu_bw": 0, "rpow_bw": 0, "floor_bw": 0, "round_bw": 0, "log_bw": 0, "relu6_bw": 0, "abs_bw": 0, "silu_bw": 0, "selu_bw": 0, "square_bw": 0, "prod_bw": 0, "hardswish_bw": 0, "tanhshrink_bw": 0, "atanh_bw": 0, "asin_bw": 0, "asinh_bw": 0, "sin_bw": 0, "sinh_bw": 0, "log10_bw": 0, "log1p_bw": 0, "erfc_bw": 0, "ceil_bw": 0, "softsign_bw": 0, "cosh_bw": 0, "logiteps_bw": 0, "log2_bw": 0, "sign_bw": 0, "div_no_nan_bw": 0, "exp2_bw": 0, "expm1_bw": 0, "reciprocal_bw": 0, "digamma_bw": 0, "erfinv_bw": 0, "erf_bw": 0, "deg2rad_bw": 0, "polygamma_bw": 0, "repeat_bw": 0, "real": [0, 31, 169, 259, 279, 390], "angl": [0, 36], "is_imag": 0, "is_real": 0, "polar_bw": 0, "imag_bw": 0, "real_bw": 0, "angle_bw": 0, "conj_bw": 0, "conj": [0, 73], "polar": [0, 260], "alt_complex_rotate90": 0, "add": [0, 24, 256, 309, 310, 368, 369, 390, 392, 393, 396, 399, 401, 404], "addalpha": [0, 26], "subalpha": [0, 347], "multipli": [0, 6, 25, 27, 29, 31, 192, 224, 225, 239, 308, 309, 310, 337, 346, 381, 395, 404], "subtract": [0, 306, 307, 345, 373, 404], "div": 0, "div_no_nan": [0, 92], "floor_div": 0, "fmod": [0, 128], "gcd": 0, "lcm": 0, "logical_and_": 0, "logical_or_": 0, "logical_xor_": 0, "rpow": [0, 303], "rsub": 0, "ldexp": [0, 182], "logical_and": 0, "logical_or": 0, "logical_xor": 0, "bitwise_and": [0, 54, 57], "bitwise_or": 0, "bitwise_xor": 0, "logaddexp": [0, 207], "logaddexp2": [0, 206], "hypot": [0, 161], "xlogi": [0, 386], "squared_differ": [0, 343], "gt": 0, "gt_": 0, "lt_": 0, "ge_": 0, "le_": 0, "eq_": 0, "ne_": 0, "ge": 0, "lt": 0, "le": 0, "eq": 0, "ne": 0, "isclos": 0, "nextaft": 0, "maximum": [0, 6, 38, 64, 65, 66, 67, 158, 227, 228], "minimum": [0, 4, 64, 65, 66, 67, 158, 232, 395], "outer": 0, "pow": 0, "polyv": 0, "scatter": [0, 283], "atan2": [0, 47], "add_bw": 0, "assign_bw": 0, "atan2_bw": 0, "bias_gelu_bw": 0, "div_bw": 0, "embedding_bw": 0, "fmod_bw": 0, "remainder_bw": 0, "addalpha_bw": 0, "subalpha_bw": 0, "xlogy_bw": 0, "hypot_bw": 0, "ldexp_bw": 0, "logaddexp_bw": 0, "logaddexp2_bw": 0, "mul_bw": 0, "sub_bw": 0, "squared_difference_bw": 0, "concat_bw": 0, "rsub_bw": 0, "min_bw": 0, "max_bw": 0, "ternari": [0, 27, 29, 187, 222, 383], "addcdiv": [0, 28], "addcmul": [0, 30], "mac": 0, "lerp": [0, 188], "addcmul_bw": 0, "addcdiv_bw": 0, "where_bw": 0, "lerp_bw": 0, "loss": [0, 178, 238], "l1_loss": 0, "mse_loss": 0, "reduct": [0, 3, 6, 33, 38, 82, 83, 178, 226, 230, 231, 238, 271, 311, 344, 349, 367, 382], "cumprod": 0, "max": [0, 64, 65, 66, 67, 157, 158, 228, 291, 402], "mean": [0, 6, 12, 31, 51, 147, 178, 179, 238, 299, 395, 401], "min": [0, 64, 65, 66, 67, 157, 158, 291, 292], "std": [0, 4, 226, 230, 231, 258, 314, 349, 382, 402], "sum": [0, 33, 83, 237, 311], "var": 0, "argmax": [0, 400, 402], "prod": [0, 81, 272], "topk": 0, "cumsum": 0, "movement": [0, 403], "concat": [0, 71, 393], "nonzero": 0, "pad": [0, 6, 32, 68, 74, 75, 76, 96, 122, 129, 130, 134, 147, 179, 228, 257, 258, 267, 268, 269, 270, 298, 299, 310, 327, 362, 364, 367, 395, 398, 402], "permut": [0, 147, 228, 373, 398, 401, 402], "reshap": [0, 6, 147, 228, 373, 398, 400, 401, 402], "repeat": [0, 3, 296, 297, 399], "repeat_interleav": 0, "tiliz": [0, 39, 51, 147, 401], "tilize_with_val_pad": 0, "fill_rm": [0, 121], "fill_ones_rm": 0, "until": [0, 9, 311, 366, 380], "untilize_with_unpad": 0, "indexed_fil": 0, "gather": [0, 32], "sort": [0, 367], "normal": [0, 51, 147, 164, 179, 299, 308, 309, 310, 328, 329, 400, 402], "group_norm": 0, "layer_norm": 0, "rms_norm": 0, "batch_norm": 0, "softmax": [0, 14, 15, 16, 308, 309, 310, 329, 368, 369, 401], "scale_mask_softmax": 0, "softmax_in_plac": [0, 14, 16], "scale_mask_softmax_in_plac": [0, 308], "scale_causal_mask_hw_dims_softmax_in_plac": 0, "softmaxprogramconfig": [0, 308, 310, 329, 368, 369], "softmaxdefaultprogramconfig": [0, 308, 310, 329, 368, 369], "softmaxshardedmulticoreprogramconfig": [0, 308, 310, 329], "block_w": [0, 16, 308, 310, 329], "moreh": [0, 237], "moreh_sum": 0, "transform": [0, 3, 192, 259, 308, 389, 400, 401, 402], "split_query_key_value_and_split_head": [0, 401], "concatenate_head": [0, 401], "attention_softmax": 0, "attention_softmax_": [0, 401], "rotary_embed": 0, "scaled_dot_product_attent": 0, "scaled_dot_product_attention_decod": 0, "ccl": [0, 3, 32, 33, 283], "all_gath": 0, "reduce_scatt": 0, "all_reduc": 0, "embed": [0, 97, 116], "convolut": [0, 3, 6, 70, 74, 75, 76, 113, 228, 267, 268, 269, 270, 396], "conv1d": [0, 6], "conv2d": [0, 6, 7, 76, 267, 268, 269, 270, 398, 402], "conv3d": 0, "conv_transpose2d": [0, 6], "prepare_conv_weight": 0, "prepare_conv_bia": 0, "prepare_conv_transpose2d_weight": 0, "prepare_conv_transpose2d_bia": 0, "conv2dconfig": [0, 74, 75, 76, 267, 268, 269, 270, 398, 402], "act_block_h_overrid": [0, 6, 402], "act_block_w_div": [0, 6, 402], "activ": [0, 6, 9, 10, 11, 23, 100, 138, 139, 148, 158, 180, 181, 183, 192, 204, 205, 208, 212, 214, 219, 224, 225, 229, 233, 242, 243, 306, 342, 348, 385, 389, 392, 396, 400, 401, 402, 403], "config_tensors_in_dram": [0, 6], "core_grid": [0, 6, 31, 81, 147, 192, 224, 225, 337, 389, 401, 402], "deallocate_activ": [0, 6, 228, 402], "enable_act_double_buff": [0, 6, 402], "enable_activation_reus": [0, 6], "enable_kernel_stride_fold": [0, 6, 402], "enable_weights_double_buff": [0, 6, 402], "force_split_read": [0, 6], "full_inner_dim": [0, 6], "in_plac": [0, 6, 228, 402], "output_layout": [0, 6, 147, 228, 402], "override_sharding_config": [0, 6, 402], "reallocate_halo_output": [0, 6, 228, 402], "reshard_if_not_optim": [0, 6, 402], "shard_layout": [0, 6, 402], "transpose_shard": [0, 6, 402], "weights_dtyp": [0, 6, 267, 268, 398, 402], "conv2dsliceconfig": [0, 75], "slicetypeenum": [0, 7], "dramsliceheight": [0, 7], "dramslicewidth": [0, 7], "l1full": [0, 7], "name": [0, 4, 7, 39, 93, 193, 235, 236, 371, 389, 393, 394, 402, 403, 404], "valu": [0, 6, 7, 9, 11, 12, 25, 26, 27, 28, 29, 30, 37, 38, 51, 63, 64, 65, 66, 67, 70, 72, 82, 83, 87, 92, 95, 98, 99, 119, 121, 122, 129, 134, 135, 136, 137, 145, 147, 151, 152, 157, 158, 159, 167, 168, 169, 170, 176, 177, 186, 190, 218, 226, 228, 230, 231, 241, 252, 253, 254, 256, 258, 261, 264, 265, 275, 276, 291, 292, 298, 302, 303, 311, 312, 314, 327, 330, 331, 332, 333, 336, 337, 344, 346, 347, 349, 359, 360, 362, 373, 374, 375, 382, 383, 387, 388, 390, 395, 397, 399, 400, 401, 402, 403, 404], "num_slic": [0, 7], "slice_typ": [0, 7], "pool": [0, 6, 145, 228, 381, 396], "global_avg_pool2d": 0, "max_pool2d": [0, 402], "vision": 0, "upsampl": 0, "kv": 0, "kv_cach": 0, "fill_cache_for_user_": 0, "update_cache_for_token_": 0, "convers": [0, 86, 274, 396, 400, 402], "model_preprocess": [0, 389, 401], "preprocess_model": 0, "preprocess_model_paramet": [0, 389], "report": [0, 75, 224, 390, 396, 400, 404], "set_printopt": [0, 404], "register_pre_operation_hook": [0, 404], "register_post_operation_hook": [0, 404], "tutori": [0, 391, 397, 398, 399, 400, 401, 402, 403], "import": [0, 4, 9, 137, 228, 311, 336, 389, 390, 392, 394, 395, 396, 401, 404], "open": [0, 3, 223, 254, 396, 401, 403, 404], "tenstorr": [0, 2, 4, 6, 389, 390, 391, 392, 396, 400, 402, 403, 404], "addit": [0, 10, 137, 224, 310, 311, 329, 336, 389, 392, 396, 399, 402], "close": [0, 69, 223, 392, 396, 401], "output": [0, 4, 6, 7, 9, 10, 11, 12, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 278, 279, 281, 282, 283, 284, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 388, 389, 390, 392, 394, 395, 396, 403, 404], "host": [0, 74, 75, 119, 133, 256, 327, 352, 392, 394, 395, 396, 398], "tile": [0, 6, 9, 10, 11, 12, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 36, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 71, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128, 131, 132, 134, 137, 138, 139, 140, 141, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 170, 171, 172, 173, 174, 175, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 229, 230, 231, 232, 233, 234, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 250, 251, 255, 257, 258, 260, 261, 262, 263, 264, 265, 266, 271, 272, 273, 274, 275, 276, 277, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 293, 294, 296, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 366, 367, 371, 372, 374, 375, 376, 377, 378, 379, 380, 382, 383, 384, 385, 386, 387, 388, 394, 395, 396, 397, 400, 402, 403], "base": [0, 4, 6, 7, 9, 10, 11, 12, 14, 15, 16, 51, 70, 81, 137, 147, 224, 257, 275, 337, 395, 396, 401], "arithmet": [0, 396], "simul": [0, 396, 398, 400, 401, 402, 403], "broadcast": [0, 9, 11, 23, 24, 25, 32, 90, 100, 137, 139, 148, 181, 183, 192, 204, 205, 208, 212, 214, 219, 224, 242, 243, 283, 297, 308, 309, 310, 342, 345, 346, 348, 371, 385, 396, 398, 400, 402, 404], "row": [0, 6, 9, 32, 61, 113, 120, 122, 123, 125, 132, 147, 224, 249, 283, 301, 319, 377, 394, 395, 396, 403], "vector": [0, 4, 224, 225, 263, 315, 396, 400], "expans": [0, 396], "mlp": [0, 396], "infer": [0, 51, 75, 337, 390, 394, 396], "load": [0, 3, 6, 9, 11, 193, 396, 403], "mnist": [0, 396], "test": [0, 4, 389, 390, 393, 394, 396, 399, 401, 403, 404], "pretrain": [0, 396, 402], "weight": [0, 6, 51, 74, 75, 76, 96, 97, 113, 187, 192, 266, 267, 268, 269, 270, 337, 389, 396, 398, 401], "accuraci": [0, 390, 396, 402], "track": [0, 396, 402], "loop": [0, 396], "flatten": [0, 396, 398, 402], "head": [0, 368, 369, 370, 371, 373, 396], "attent": [0, 308, 309, 310, 328, 368, 369, 371, 372, 373, 396], "write": [0, 1, 4, 31, 81, 192, 224, 225, 337, 389, 395, 396], "seed": [0, 114, 275, 396], "creat": [0, 4, 6, 37, 68, 81, 82, 83, 98, 99, 119, 135, 136, 137, 147, 252, 253, 310, 336, 337, 387, 388, 392, 393, 394, 395, 396, 397, 399, 400, 401, 402, 403], "forward": [0, 389, 396], "method": [0, 4, 280, 355, 357, 396], "input": [0, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 131, 132, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 227, 228, 229, 232, 233, 234, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 345, 346, 347, 348, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 384, 385, 386, 388, 390, 392, 394, 395, 396, 397, 400, 401, 402, 403, 404], "paramet": [0, 3, 9, 10, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 257, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 392, 396, 401, 402], "simpl": [0, 392, 396, 397, 398, 399, 400], "cnn": [0, 396, 398], "cifar": [0, 396], "dataset": [0, 395, 396, 400, 403], "initi": [0, 99, 235, 236, 275, 389, 396, 397, 398, 399, 400, 401, 403], "defin": [0, 4, 9, 39, 328, 393, 395, 396], "stage": [0, 396, 403], "sampl": [0, 396, 400, 403], "profil": [0, 164, 314, 396, 404], "gener": [0, 6, 11, 96, 114, 122, 147, 224, 259, 275, 308, 390, 394, 396, 398, 399, 400, 401, 402], "perform": [0, 6, 9, 10, 11, 12, 16, 18, 20, 22, 24, 26, 28, 30, 32, 33, 35, 36, 41, 43, 44, 47, 48, 50, 52, 53, 54, 56, 57, 58, 61, 63, 65, 66, 67, 71, 73, 78, 79, 80, 82, 83, 86, 87, 88, 90, 92, 94, 95, 101, 104, 106, 108, 111, 112, 118, 120, 123, 125, 127, 128, 132, 137, 140, 141, 143, 145, 146, 149, 151, 152, 154, 156, 157, 158, 161, 163, 164, 165, 166, 182, 184, 186, 188, 190, 191, 196, 198, 200, 201, 203, 206, 207, 211, 216, 217, 218, 220, 223, 224, 225, 227, 228, 232, 239, 240, 241, 244, 246, 250, 251, 256, 259, 260, 261, 262, 264, 265, 266, 267, 268, 269, 270, 271, 272, 274, 276, 277, 278, 279, 282, 286, 289, 290, 293, 294, 296, 301, 302, 303, 305, 307, 308, 309, 310, 312, 313, 315, 317, 319, 322, 324, 325, 326, 327, 331, 332, 333, 335, 337, 339, 341, 343, 345, 347, 350, 351, 354, 355, 356, 357, 358, 359, 360, 367, 374, 375, 377, 384, 386, 389, 390, 392, 393, 396, 397, 398, 399, 400, 401, 402], "result": [0, 12, 14, 16, 31, 33, 37, 39, 51, 122, 134, 136, 137, 147, 164, 192, 224, 225, 228, 253, 267, 268, 269, 270, 271, 275, 276, 281, 328, 329, 337, 355, 357, 388, 394, 395, 396, 397, 398, 399, 400, 402], "analysi": [0, 394, 396], "upload": [0, 394, 396], "tab": [0, 394, 396], "recap": [0, 396], "onboard": 0, "new": [0, 32, 99, 136, 253, 295, 298, 388, 390, 392, 394], "pytorch": [0, 3, 74, 75, 76, 224, 267, 268, 269, 270, 336, 367, 371, 373, 398, 399, 400, 402, 404], "rewrit": 0, "switch": [0, 256, 330], "optim": [0, 6, 9, 10, 11, 16, 308, 328, 355, 357, 395, 401, 402, 403], "more": [0, 1, 3, 4, 6, 9, 11, 12, 18, 28, 51, 52, 75, 90, 128, 143, 147, 161, 179, 196, 198, 200, 201, 203, 206, 207, 217, 218, 265, 272, 277, 281, 282, 294, 299, 329, 331, 339, 386, 391, 392, 394, 395, 397, 398, 399, 401, 402, 403], "ad": [0, 31, 74, 75, 76, 192, 225, 267, 268, 269, 270, 393], "faq": 0, "need": [0, 1, 2, 7, 9, 11, 16, 31, 147, 192, 224, 225, 337, 367, 390, 394, 395, 402, 403, 404], "implement": [0, 6, 9, 11, 32, 74, 147, 224, 256, 283, 311, 371, 372, 373, 389, 393, 394, 401, 403], "bind": [0, 398, 399, 400, 401, 402], "golden": [0, 389, 403, 404], "perf": [0, 6, 390, 403], "header": 0, "profile_thi": 0, "descript": [0, 121, 122, 393], "uplift": 0, "bug": 0, "featur": [0, 3, 6, 9, 308, 392, 393, 403, 404], "propos": [0, 393], "request": [0, 364, 393], "troubleshoot": [0, 390], "tip": 0, "commun": [0, 9, 11, 392], "develop": [0, 3, 6, 391, 392, 393, 394, 396, 397, 403], "index": [0, 137, 176, 177, 311, 336, 372, 394, 402], "modul": [0, 4, 76, 235, 236, 389, 400, 402, 403], "search": [0, 403], "page": [0, 392, 403], "If": [1, 2, 4, 6, 7, 9, 10, 11, 31, 32, 33, 38, 68, 74, 75, 76, 81, 82, 83, 137, 147, 179, 192, 224, 225, 226, 230, 231, 235, 236, 254, 256, 258, 271, 272, 299, 309, 327, 328, 336, 337, 344, 349, 352, 366, 367, 371, 372, 373, 382, 391, 392, 393, 394, 395, 402, 403, 404], "would": [1, 147, 329, 393, 394, 395], "like": [1, 31, 123, 311, 330, 389, 394, 395, 397, 398, 402, 404], "thi": [1, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 31, 38, 51, 68, 70, 74, 75, 76, 97, 115, 116, 119, 129, 130, 145, 147, 164, 179, 182, 228, 257, 267, 268, 269, 270, 271, 283, 291, 292, 298, 308, 309, 310, 311, 329, 330, 336, 367, 371, 389, 390, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404], "project": [1, 2, 3, 391, 403], "pleas": [1, 2, 224, 390, 391, 393, 404], "review": [1, 391, 393], "standard": [1, 2, 14, 292, 329, 390, 391, 392, 402], "gain": 1, "access": [1, 2, 3, 9, 10, 11, 12, 392], "read": [1, 81, 391, 395], "section": [1, 2, 390, 395, 396, 403], "detail": [1, 18, 28, 51, 52, 90, 128, 143, 147, 161, 179, 196, 198, 200, 201, 203, 206, 207, 217, 218, 265, 272, 277, 282, 294, 299, 331, 339, 386, 391, 398, 402, 403, 404], "contact": 1, "u": [1, 393, 400], "have": [2, 4, 11, 12, 32, 38, 82, 83, 84, 97, 130, 137, 147, 179, 187, 224, 267, 268, 271, 276, 299, 308, 311, 361, 362, 367, 379, 380, 390, 392, 394, 395, 402, 403], "formal": 2, "permiss": 2, "cloud": 2, "issu": [2, 6, 224, 330, 390, 393, 394, 398], "file": [2, 4, 39, 93, 193, 390, 392, 394, 397, 398, 399, 400, 402, 403, 404], "github": [2, 390, 391, 392, 403], "can": [2, 3, 4, 6, 7, 9, 10, 11, 12, 16, 37, 52, 65, 66, 67, 74, 75, 76, 89, 90, 99, 136, 143, 147, 224, 253, 256, 267, 268, 269, 270, 276, 277, 284, 285, 314, 330, 352, 388, 389, 392, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404], "check": [2, 4, 224, 390, 392, 400, 401, 402], "out": [2, 4, 6, 31, 82, 83, 114, 137, 292, 327, 336, 367, 397, 398, 401, 402], "relev": [2, 390], "ever": 2, "help": [2, 147, 393, 396, 403], "we": [2, 6, 32, 130, 134, 147, 224, 283, 308, 366, 390, 393, 395, 396, 397, 398, 399, 400, 402, 403, 404], "offici": [2, 403], "discord": 2, "channel": [2, 6, 51, 74, 75, 76, 121, 122, 145, 147, 228, 267, 268, 269, 270, 381, 394, 398, 402], "repres": [2, 3, 394, 395, 402], "both": [2, 6, 9, 10, 11, 12, 24, 74, 75, 76, 82, 83, 224, 228, 264, 267, 268, 269, 270, 329, 337, 389, 390, 394, 395, 403], "metal": [2, 3, 391, 392, 396, 398, 399, 400, 401, 402, 403], "join": 2, "discuss": [2, 390], "board": 2, "member": [2, 7], "bounc": 2, "idea": [2, 390], "off": [2, 291, 300, 389, 395], "each": [2, 7, 9, 10, 11, 12, 32, 51, 70, 122, 137, 145, 147, 228, 256, 295, 297, 327, 337, 383, 392, 394, 395, 396, 400, 402, 403], "other": [2, 4, 6, 9, 137, 224, 389, 390, 391, 395, 402, 403, 404], "refer": [2, 3, 18, 28, 31, 52, 75, 84, 90, 99, 119, 128, 143, 147, 161, 196, 198, 200, 201, 203, 206, 207, 217, 218, 224, 241, 265, 272, 277, 282, 294, 331, 339, 386, 392, 393, 395, 402, 403, 404], "code": [2, 4, 284, 285, 336, 367, 373, 391, 392, 393, 394, 395, 398, 399, 400, 401, 402, 404], "conduct": 2, "when": [2, 4, 6, 7, 8, 9, 11, 13, 32, 38, 51, 70, 74, 75, 76, 81, 90, 116, 147, 161, 208, 223, 224, 228, 229, 235, 258, 264, 271, 277, 283, 316, 355, 357, 364, 367, 390, 393, 395, 398, 401, 404], "interact": [2, 403], "neural": [3, 398, 400, 402, 403], "network": [3, 400, 402, 403], "built": [3, 280, 392, 402], "design": [3, 16, 308, 396, 397, 403], "feel": [3, 403], "familiar": 3, "experienc": 3, "kei": [3, 373, 389, 395, 401, 402], "includ": [3, 4, 6, 51, 68, 390, 395, 399, 400, 402, 403], "than": [3, 6, 7, 70, 87, 121, 122, 139, 140, 147, 148, 149, 183, 184, 190, 219, 220, 241, 271, 308, 329, 394, 401, 404], "200": 3, "fuse": [3, 6, 9, 10, 11, 309, 310, 389, 401], "A": [3, 4, 6, 37, 135, 136, 137, 224, 226, 230, 231, 252, 253, 275, 330, 344, 349, 372, 382, 387, 388, 390, 392, 393, 395, 403], "differ": [3, 6, 15, 76, 147, 224, 315, 337, 342, 355, 357, 392, 395, 399, 400, 402, 404], "wai": [3, 235, 236, 389, 392, 396], "distribut": [3, 9, 10, 11, 12, 31, 81, 192, 224, 225, 275, 337, 395, 403], "The": [3, 4, 6, 7, 8, 11, 13, 16, 31, 32, 33, 34, 37, 38, 39, 62, 68, 69, 74, 75, 76, 84, 93, 94, 96, 97, 98, 99, 115, 119, 122, 134, 135, 136, 137, 145, 147, 159, 185, 192, 223, 224, 225, 226, 228, 230, 231, 241, 252, 253, 254, 257, 267, 268, 269, 270, 271, 275, 283, 284, 285, 291, 292, 295, 308, 309, 310, 311, 328, 329, 336, 337, 344, 349, 352, 363, 366, 367, 371, 372, 381, 382, 383, 387, 388, 389, 390, 391, 392, 393, 394, 395, 397, 398, 399, 400, 401, 402, 403, 404], "abil": [3, 403], "custom": [3, 4, 284, 285, 392, 398, 400, 401, 402, 403], "nativ": [3, 395, 399], "mesh": [3, 32, 33, 134, 283, 366, 398, 399, 400, 401, 402], "tool": [3, 392, 394, 403], "comput": [3, 4, 6, 9, 10, 11, 12, 16, 25, 27, 29, 31, 46, 51, 68, 74, 75, 76, 82, 83, 89, 91, 113, 126, 137, 138, 145, 147, 160, 170, 179, 180, 181, 187, 192, 204, 205, 208, 209, 212, 213, 214, 215, 222, 224, 225, 226, 229, 230, 231, 233, 247, 255, 263, 267, 268, 269, 270, 271, 299, 308, 309, 310, 328, 329, 337, 342, 344, 346, 349, 368, 369, 372, 373, 382, 385, 394, 399, 400, 402, 403], "util": [3, 10, 147, 395, 400, 402, 403], "significantli": [3, 6, 9, 16, 401], "speed": 3, "comparison": [3, 170, 393], "mode": [3, 44, 51, 52, 90, 103, 105, 109, 142, 143, 178, 194, 195, 197, 199, 228, 238, 277, 314, 315, 316, 355, 357, 398, 399, 400, 401, 402, 403], "long": [3, 393], "sequenc": [3, 4, 309, 310, 337, 371, 372], "against": [3, 137, 390, 403, 404], "known": [3, 76], "document": [4, 147, 390, 392, 393, 403], "meant": [4, 6], "contributor": 4, "Not": [4, 244, 275, 389, 404], "mai": [4, 6, 11, 12, 84, 90, 161, 182, 224, 277, 281, 298, 310, 329, 355, 357, 363, 389, 395, 398, 399, 400, 401, 402, 404], "grayskul": [4, 39, 389, 404], "wormhol": [4, 39, 389, 392, 404], "take": [4, 6, 370, 390, 391, 395, 401, 402], "one": [4, 9, 65, 66, 67, 137, 147, 271, 391, 395, 396, 399], "produc": [4, 179, 281, 355, 357, 390, 400], "call": [4, 6, 9, 134, 284, 285, 366, 393, 394, 395, 401, 403, 404], "optiona": 4, "b": [4, 116, 224, 225, 297, 337, 371, 372, 392, 398, 402], "composit": [4, 32, 283], "struct": 4, "specifi": [4, 6, 9, 10, 11, 12, 31, 32, 33, 37, 68, 82, 83, 93, 98, 99, 135, 136, 137, 192, 224, 225, 226, 230, 231, 235, 236, 252, 253, 256, 257, 258, 271, 275, 283, 295, 311, 328, 336, 337, 344, 349, 352, 362, 371, 372, 382, 387, 388, 389, 392, 394, 395, 401], "how": [4, 6, 7, 9, 10, 11, 12, 147, 390, 394, 395, 397, 399, 400, 401, 402, 403, 404], "simpli": [4, 364, 403], "ttnn": [4, 392, 393, 395, 396, 397, 398, 399, 400, 401, 402, 404], "register_oper": 4, "exist": [4, 254, 392, 394, 400, 402], "bind_registered_oper": 4, "auto": [4, 314], "attach": [4, 235, 236], "attach_golden_funct": 4, "let": [4, 395, 397, 398, 399, 400, 401, 402], "": [4, 6, 16, 68, 74, 75, 76, 147, 179, 224, 226, 230, 231, 235, 236, 299, 309, 311, 344, 349, 371, 372, 382, 383, 390, 392, 395, 399, 400, 402, 403], "just": [4, 147, 267, 268, 269, 270], "copi": [4, 68, 133, 164, 363, 403], "In": [4, 9, 114, 147, 224, 298, 299, 364, 369, 389, 394, 395, 400, 402, 403], "order": [4, 81, 224, 262, 307, 336, 367, 394, 395, 401, 403, 404], "follow": [4, 6, 31, 38, 76, 122, 192, 224, 225, 226, 230, 231, 271, 298, 308, 309, 310, 328, 329, 336, 337, 344, 349, 367, 382, 389, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404], "directori": [4, 392, 396, 403], "structur": [4, 6, 7, 389, 400, 403], "shown": [4, 395, 397, 398], "below": [4, 224, 390, 394, 395, 397, 398], "cpp": [4, 398, 399, 400, 401, 402, 403], "categori": [4, 403], "operation_nam": 4, "_device_oper": 4, "hpp": 4, "program_factory_0": 4, "_program_factori": 4, "mani": [4, 9, 10, 11, 12, 389], "factori": 4, "But": 4, "concret": [4, 147, 224], "found": [4, 389, 400, 402, 403], "example_device_oper": 4, "spdx": [4, 404], "filecopyrighttext": [4, 404], "2023": 4, "inc": [4, 404], "licens": [4, 404], "identifi": [4, 403, 404], "apach": [4, 404], "0": [4, 6, 7, 13, 23, 25, 27, 28, 29, 30, 31, 32, 35, 37, 39, 51, 53, 54, 56, 57, 58, 64, 65, 66, 67, 69, 71, 72, 76, 82, 83, 84, 87, 96, 97, 98, 99, 102, 114, 121, 122, 129, 133, 134, 135, 136, 137, 144, 147, 150, 151, 152, 165, 167, 168, 169, 170, 186, 187, 189, 190, 193, 210, 216, 218, 221, 223, 226, 228, 230, 231, 242, 248, 252, 253, 254, 256, 258, 259, 263, 271, 272, 275, 277, 278, 280, 291, 292, 297, 298, 300, 308, 309, 310, 311, 327, 330, 332, 333, 337, 340, 344, 346, 348, 349, 352, 359, 360, 363, 364, 365, 366, 373, 374, 375, 382, 383, 384, 387, 388, 389, 392, 394, 395, 397, 398, 399, 400, 401, 402, 403, 404], "pragma": 4, "onc": [4, 267, 268, 269, 270, 400, 403], "variant": [4, 147, 224], "device_oper": 4, "decor": [4, 393], "namespac": 4, "exampledeviceoper": 4, "attribut": [4, 389, 394, 395], "store": [4, 6, 10, 31, 51, 137, 394, 395, 401], "variabl": [4, 392, 403, 404], "aren": [4, 8], "t": [4, 8, 147, 164, 224, 235, 236, 394, 395, 401], "operation_attributes_t": 4, "bool": [4, 24, 26, 38, 44, 51, 71, 74, 75, 76, 81, 82, 83, 84, 89, 90, 103, 105, 109, 137, 142, 147, 170, 178, 192, 194, 195, 197, 199, 224, 225, 226, 228, 230, 231, 235, 236, 238, 239, 256, 269, 270, 271, 298, 307, 308, 309, 310, 315, 316, 328, 329, 336, 337, 344, 345, 347, 349, 361, 362, 367, 368, 369, 371, 372, 373, 379, 380, 382, 384, 402], "int": [4, 16, 31, 32, 33, 37, 38, 70, 71, 74, 75, 76, 81, 82, 83, 96, 98, 116, 119, 121, 122, 130, 134, 136, 137, 141, 146, 147, 167, 176, 177, 192, 223, 224, 225, 226, 228, 230, 231, 254, 256, 257, 258, 261, 264, 267, 268, 269, 270, 271, 272, 275, 283, 286, 296, 300, 314, 315, 327, 328, 329, 336, 337, 344, 349, 350, 352, 366, 367, 368, 369, 371, 372, 373, 381, 382, 395, 398, 402], "some_other_attribut": 4, "argument": [4, 6, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 99, 100, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 131, 132, 134, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 178, 179, 180, 181, 182, 183, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 214, 216, 217, 218, 219, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 238, 239, 240, 241, 242, 243, 245, 246, 247, 248, 249, 250, 251, 254, 255, 259, 260, 261, 262, 263, 264, 265, 266, 271, 272, 273, 274, 275, 276, 277, 278, 279, 281, 282, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 395], "pass": [4, 6, 9, 74, 75, 115, 116, 122, 284, 285, 373, 389, 393, 395, 398, 400, 402], "don": [4, 224], "thei": [4, 7, 75, 147, 224, 390, 395, 401], "tensor_args_t": 4, "const": [4, 314], "input_tensor": [4, 17, 18, 19, 20, 21, 22, 32, 33, 34, 35, 36, 38, 40, 41, 42, 43, 44, 45, 48, 49, 50, 51, 52, 55, 59, 60, 61, 62, 63, 64, 65, 66, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 90, 92, 94, 95, 96, 97, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 123, 124, 125, 129, 131, 132, 137, 140, 141, 142, 143, 144, 145, 146, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 162, 163, 164, 165, 166, 168, 169, 171, 172, 173, 174, 175, 176, 184, 185, 186, 189, 190, 191, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 210, 211, 216, 217, 218, 220, 221, 226, 228, 230, 231, 234, 240, 241, 244, 245, 246, 248, 249, 250, 251, 253, 256, 258, 259, 260, 261, 262, 263, 264, 265, 272, 273, 274, 276, 277, 278, 279, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 295, 296, 297, 298, 300, 301, 302, 303, 304, 305, 308, 309, 310, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 338, 339, 340, 341, 344, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 368, 369, 370, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 388, 398, 402, 404], "howev": [4, 256], "show": [4, 224, 395, 401, 403], "els": [4, 32, 89, 283, 400, 402], "done": [4, 6, 390, 392, 394], "io_tensor": 4, "optional_output_tensor": [4, 31, 192, 224, 225, 337], "vector_of_tensor": 4, "tupl": [4, 74, 75, 76, 81, 228, 256, 267, 268, 269, 270, 336, 367, 373, 398, 402], "tuple_of_tensor": 4, "vector_of_optional_tensor": 4, "some_crazy_tuple_of_tensor": 4, "return": [4, 8, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 308, 309, 310, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 346, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 384, 385, 386, 387, 388, 389, 398, 399, 401, 402], "spec": [4, 81, 134, 179], "singl": [4, 225, 226, 230, 231, 344, 349, 372, 382, 389, 390, 393, 394, 395], "tensorspec": [4, 134], "spec_return_value_t": 4, "tensor_return_value_t": 4, "note": [4, 6, 73, 137, 147, 166, 224, 252, 253, 279, 298, 392, 394, 395, 401, 403, 404], "should": [4, 6, 9, 11, 31, 76, 141, 146, 147, 164, 170, 187, 224, 228, 249, 256, 264, 276, 286, 350, 367, 390, 393, 394, 397, 402, 403], "same": [4, 6, 23, 25, 27, 29, 33, 46, 53, 54, 56, 57, 58, 74, 75, 76, 82, 83, 89, 91, 97, 99, 100, 101, 119, 121, 126, 136, 137, 138, 139, 140, 148, 149, 160, 164, 170, 180, 181, 183, 184, 187, 192, 204, 205, 208, 209, 212, 213, 214, 215, 219, 220, 222, 224, 229, 233, 235, 236, 242, 243, 244, 247, 249, 253, 255, 263, 264, 267, 268, 269, 270, 271, 283, 306, 308, 310, 311, 329, 342, 346, 348, 364, 367, 371, 385, 388, 394, 395, 399, 400, 401, 403], "pattern": [4, 9, 10, 11, 70, 235, 308, 337, 393, 400, 402], "e": [4, 11, 38, 147, 179, 224, 226, 230, 231, 271, 308, 328, 329, 344, 349, 382, 392, 394, 395, 400, 402, 403, 404], "singlecor": 4, "share": [4, 9, 10, 11, 12, 395], "between": [4, 6, 37, 51, 74, 75, 76, 224, 267, 268, 269, 270, 365, 402, 403], "override_runtime_argu": 4, "shared_variables_t": 4, "tt_metal": [4, 381, 392, 403], "kernelhandl": 4, "unary_reader_kernel_id": 4, "unary_writer_kernel_id": 4, "cached_program_t": 4, "cachedprogram": 4, "static": 4, "operation_attribut": 4, "tensor_arg": 4, "tensor_return_valu": 4, "void": 4, "cached_program": 4, "multicor": [4, 256, 361, 362, 367, 379, 380], "size_t": 4, "num_cor": [4, 6], "num_cores_i": 4, "program_factory_t": 4, "mandatori": 4, "select": [4, 6, 14, 32, 51, 147, 283, 383, 393, 400], "arg": [4, 32, 33, 122, 258, 267, 268, 269, 270, 283, 284, 285, 402, 404], "select_program_factori": 4, "valid": [4, 6, 121, 122, 147, 224, 235, 236, 367, 389, 390, 394, 395], "usual": 4, "validate_on_program_cache_miss": 4, "reus": [4, 6, 12, 267, 268, 269, 270, 310, 403], "less": [4, 6, 183, 184, 219, 220, 271, 394, 404], "validate_on_program_cache_hit": 4, "compute_output_spec": 4, "create_output_tensor": 4, "map": [4, 39, 298], "abl": [4, 147], "prim": 4, "after": [4, 6, 16, 113, 228, 256, 284, 314, 393, 394, 395, 398, 400, 402, 403, 404], "op": [4, 6, 7, 8, 9, 38, 74, 75, 76, 81, 97, 147, 228, 352, 367, 371, 372, 378, 393, 394, 398, 403, 404], "invok": 4, "case": [4, 14, 89, 134, 147, 170, 224, 247, 266, 299, 364, 367, 389, 390, 395, 402, 403, 404], "hash": [4, 235, 236, 394], "stl": 4, "hash_t": 4, "compute_program_hash": 4, "create_op_performance_model": 4, "opperformancemodel": 4, "make": [4, 235, 236, 329, 330, 367, 373, 389, 395, 403, 404], "avail": [4, 6, 12, 381, 396, 403, 404], "constexpr": 4, "some_condition_based_on_operation_attributes_and_or_tensor_arg": 4, "true": [4, 6, 9, 11, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 131, 132, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 271, 272, 273, 274, 275, 276, 277, 278, 279, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 395, 400, 402, 403, 404], "logical_shap": 4, "tensorlayout": 4, "dtype": [4, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128, 129, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 286, 287, 288, 289, 290, 291, 292, 293, 294, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 374, 375, 376, 377, 378, 382, 383, 384, 385, 386, 387, 388, 389, 395, 397, 398, 399, 400, 401, 402, 404], "pageconfig": 4, "memoryconfig": [4, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 178, 179, 180, 181, 182, 183, 185, 186, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 214, 216, 217, 218, 219, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 238, 239, 240, 241, 242, 243, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 328, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 367, 368, 369, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388], "output_spec": 4, "create_device_tensor": 4, "42": [4, 400], "single_core_program_factori": 4, "work_split": 4, "tensor_accessor_arg": 4, "output_tensor": [4, 17, 19, 21, 23, 25, 32, 34, 38, 40, 42, 45, 49, 53, 54, 55, 56, 57, 58, 59, 60, 62, 64, 70, 77, 85, 89, 96, 97, 100, 102, 103, 105, 107, 109, 110, 112, 114, 117, 119, 120, 124, 130, 131, 135, 136, 138, 139, 142, 143, 144, 148, 150, 153, 155, 159, 162, 164, 171, 172, 173, 174, 175, 178, 180, 181, 183, 185, 189, 194, 195, 197, 199, 202, 204, 205, 208, 210, 212, 214, 219, 221, 229, 233, 234, 238, 242, 243, 245, 246, 248, 253, 264, 265, 273, 276, 281, 283, 287, 288, 291, 292, 300, 302, 304, 305, 306, 315, 316, 318, 320, 321, 322, 323, 329, 330, 334, 338, 339, 340, 342, 346, 348, 353, 355, 356, 357, 365, 367, 376, 378, 383, 384, 385, 388, 404], "src_buffer": 4, "dst_buffer": 4, "dataformat": 4, "cb_data_format": 4, "datatype_to_dataformat_convert": 4, "uint32_t": [4, 114], "single_tile_s": 4, "tile_s": [4, 147], "cb_data_format_output": 4, "single_tile_size_output": 4, "num_til": 4, "physical_volum": 4, "constant": [4, 51, 147, 179, 299, 395], "tile_hw": 4, "corecoord": [4, 16, 308, 310], "y": [4, 11, 12, 46, 147, 259, 308, 309, 310, 394, 395, 401], "all_cor": 4, "core_group_1": 4, "core_group_2": 4, "num_tiles_per_core_group_1": 4, "num_tiles_per_core_group_2": 4, "split_work_to_cor": 4, "src0_cb_index": 4, "cbindex": 4, "c_0": 4, "num_input_til": 4, "circularbufferconfig": 4, "cb_src0_config": 4, "set_page_s": 4, "createcircularbuff": 4, "output_cb_index": 4, "c_2": 4, "num_output_til": 4, "cb_output_config": 4, "reader_compile_time_arg": 4, "tensoraccessorarg": 4, "append_to": 4, "writer_compile_time_arg": 4, "createkernel": 4, "eltwis": [4, 127, 266, 293, 397], "kernel": [4, 6, 31, 51, 68, 74, 75, 76, 113, 115, 147, 192, 224, 225, 226, 228, 230, 231, 267, 268, 269, 270, 308, 309, 310, 328, 329, 337, 344, 349, 382, 392, 394, 398, 402, 403], "dataflow": 4, "reader_unary_interleaved_start_id": 4, "readerdatamovementconfig": 4, "writer_unary_interleaved_start_id": 4, "writerdatamovementconfig": 4, "compute_kernel_args_group_1": 4, "per_core_block_cnt": 4, "per_core_block_s": 4, "math_approx_mod": 4, "fals": [4, 6, 9, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 400, 401, 402, 403, 404], "eltwise_sfpu": 4, "computeconfig": 4, "math_fidel": 4, "mathfidel": 4, "hifi4": [4, 394], "compile_arg": 4, "rang": [4, 23, 37, 53, 54, 55, 56, 57, 58, 79, 121, 122, 138, 163, 180, 182, 197, 210, 229, 234, 240, 242, 261, 275, 300, 325, 348, 353, 354, 367, 395], "compute_kernel_args_group_2": 4, "num_tiles_written": 4, "num_tiles_per_cor": 4, "contain": [4, 6, 37, 38, 82, 83, 96, 137, 147, 228, 275, 281, 311, 337, 383, 392, 395, 399, 402, 403], "tt_assert": 4, "setruntimearg": 4, "address": [4, 403], "move": [4, 6, 129, 130, 337, 390, 401], "shared_vari": 4, "runtime_arg": 4, "getruntimearg": 4, "multi_core_program_factori": 4, "idevic": [4, 267, 268], "written": [4, 31, 176, 177, 192, 224, 225, 337, 401], "primit": 4, "compositeexampleoper": 4, "composite_exampl": 4, "another_copi": 4, "_pybind": 4, "example_pybind": 4, "pybind": 4, "pybind_fwd": 4, "py": [4, 389, 390, 394, 396, 397, 398, 399, 400, 401, 402, 403, 404], "pybind11": 4, "bind_example_oper": 4, "r": [4, 259, 392, 403], "doc": [4, 403], "overload": [4, 271], "expos": 4, "logic": [4, 208, 209, 212, 213, 215, 224, 235, 236], "self": [4, 9, 10, 11, 12, 389, 395], "correct": [4, 147, 226, 230, 231, 344, 349, 382, 400, 402], "specif": [4, 6, 16, 38, 224, 254, 308, 336, 372, 390, 395, 396, 402, 403], "pybind_overload_t": 4, "decltyp": 4, "examples_pybind": 4, "py_modul": 4, "final": [4, 31, 224, 389, 390, 393, 400, 402, 403], "wherev": 4, "want": [4, 392, 404], "compar": [4, 100, 139, 147, 148, 183, 219, 224, 243, 400, 402], "its": [4, 6, 68, 76, 99, 224, 229, 256, 327, 389, 390, 393, 394, 395, 400, 402, 404], "equival": [4, 336, 367, 373, 395, 396], "signatur": 4, "keep": [4, 38, 226, 230, 231, 271, 344, 349, 382, 389, 395], "mind": [4, 401], "And": [4, 389, 395], "ignor": [4, 31], "kwarg": [4, 267, 268, 269, 270, 284, 285, 404], "def": [4, 389, 398, 399, 401, 402, 404], "golden_funct": [4, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388], "befor": [4, 6, 39, 82, 83, 256, 267, 268, 269, 270, 285, 309, 330, 390, 395], "automat": [4, 14, 32, 75, 223, 283, 364, 390, 392, 394, 395], "some": [4, 337, 352, 392, 404], "preprocess": [4, 6, 39, 74, 75, 76, 235, 236, 267, 268, 269, 270, 395, 402], "postprocess": 4, "pack": [4, 379, 380], "preprocess_golden_function_input": [4, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388], "ttnn_input_tensor": 4, "postprocess_golden_function_output": [4, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388], "torch_output_tensor": [4, 147, 404], "becaus": [4, 395, 401], "wa": [4, 147, 390, 394, 395], "class": [6, 7, 9, 10, 11, 12, 14, 15, 16, 389, 390, 394, 395, 400], "pybind11_object": [6, 7, 9, 10, 11, 12, 14, 15, 16, 395], "flag": [6, 392], "properti": [6, 7, 9, 10, 11, 12, 16, 395], "control": [6, 9, 11, 12, 16, 392], "size": [6, 7, 9, 11, 12, 16, 37, 74, 75, 76, 82, 83, 96, 97, 137, 147, 224, 226, 228, 230, 231, 254, 266, 267, 268, 269, 270, 271, 310, 327, 344, 349, 371, 372, 373, 381, 382, 395, 397, 404], "block": [6, 9, 10, 11, 12, 16, 74, 75, 76, 81, 147, 179, 223, 224, 228, 267, 268, 269, 270, 281, 310, 395], "height": [6, 7, 9, 11, 12, 16, 68, 70, 74, 75, 76, 81, 121, 122, 135, 145, 147, 179, 224, 226, 228, 230, 231, 252, 267, 268, 269, 270, 299, 308, 328, 344, 349, 364, 382, 387, 394, 395, 398, 402], "chunk": [6, 9, 12, 371, 372], "l1": [6, 7, 31, 38, 51, 81, 82, 83, 137, 147, 179, 192, 224, 225, 226, 230, 231, 254, 271, 299, 308, 310, 328, 329, 337, 344, 349, 363, 365, 367, 382, 395, 398, 401, 402, 403], "divid": [6, 7, 11, 12, 90, 147, 368, 369, 403], "among": 6, "also": [6, 31, 74, 75, 76, 82, 83, 241, 271, 299, 308, 390, 394, 395, 396, 403], "further": 6, "subdivid": 6, "within": [6, 9, 11, 12, 37, 54, 57, 228, 390, 395, 403], "possibl": [6, 224, 366, 389], "which": [6, 9, 11, 31, 32, 54, 57, 81, 82, 83, 135, 136, 137, 147, 192, 224, 225, 228, 235, 236, 252, 253, 275, 276, 283, 327, 328, 336, 337, 371, 372, 387, 388, 389, 394, 395, 397, 398, 402, 403], "equal": [6, 100, 101, 139, 140, 170, 179, 183, 184, 243, 244, 309, 310, 336, 404], "output_matrix_height_per_cor": 6, "lead": 6, "larg": [6, 7, 11, 310, 330, 389, 395], "temporari": 6, "circular": [6, 329, 403], "oom": 6, "must": [6, 10, 11, 12, 16, 34, 38, 51, 74, 75, 82, 83, 97, 113, 137, 147, 179, 224, 226, 230, 231, 241, 265, 267, 268, 271, 276, 298, 299, 308, 309, 310, 311, 327, 328, 329, 336, 344, 349, 361, 362, 363, 367, 379, 380, 382, 383, 392, 395, 403], "32": [6, 12, 16, 32, 33, 38, 68, 70, 71, 84, 113, 122, 133, 135, 141, 145, 146, 147, 179, 192, 224, 249, 250, 251, 252, 255, 256, 258, 266, 272, 280, 283, 286, 296, 297, 299, 310, 327, 328, 329, 350, 363, 364, 365, 367, 378, 387, 395, 397, 399, 400, 402, 404], "evenli": [6, 11, 12, 37, 147], "reduc": [6, 7, 9, 12, 33, 38, 147, 226, 230, 231, 271, 283, 311, 328, 344, 349, 367, 382], "width": [6, 7, 9, 10, 11, 12, 16, 68, 74, 75, 76, 81, 121, 122, 135, 145, 147, 179, 224, 226, 228, 230, 231, 252, 267, 268, 269, 270, 308, 309, 310, 344, 349, 364, 367, 370, 382, 387, 394, 398, 402], "prevent": [6, 392], "greater": [6, 70, 87, 121, 122, 139, 140, 148, 149, 190, 241, 404], "n150": 6, "thats": 6, "64": [6, 38, 70, 81, 84, 133, 141, 146, 147, 179, 192, 224, 258, 280, 286, 299, 327, 328, 337, 350, 363, 364, 365, 367, 395, 400, 401, 402, 404], "2048": [6, 402, 404], "divisor": [6, 11, 12, 138, 277], "halv": 6, "appli": [6, 7, 9, 10, 11, 17, 19, 21, 23, 34, 40, 42, 45, 51, 55, 59, 60, 62, 64, 68, 74, 75, 76, 77, 82, 83, 85, 100, 102, 103, 105, 107, 109, 110, 113, 114, 115, 116, 117, 119, 124, 131, 138, 139, 141, 142, 144, 145, 146, 147, 148, 150, 153, 155, 159, 162, 171, 172, 173, 174, 175, 180, 181, 183, 185, 189, 192, 194, 195, 197, 199, 202, 204, 205, 208, 210, 212, 214, 219, 221, 224, 225, 226, 228, 229, 230, 231, 233, 234, 242, 243, 245, 248, 256, 267, 268, 269, 270, 273, 281, 286, 287, 288, 291, 292, 300, 302, 304, 306, 308, 309, 310, 315, 316, 318, 320, 321, 323, 328, 329, 330, 334, 338, 340, 342, 344, 348, 349, 350, 353, 355, 357, 376, 378, 382, 385, 390, 400, 402], "none": [6, 13, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 214, 216, 217, 218, 219, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 258, 259, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388], "unarywithparam": [6, 402], "unaryoptyp": [6, 378, 402], "boolean": [6, 168, 169, 337, 355, 357, 367], "determin": [6, 7, 9, 10, 11, 12, 32, 76, 147, 224, 235, 236, 283, 395, 402], "them": [6, 373, 392, 394, 397], "dram": [6, 7, 10, 38, 51, 75, 82, 83, 137, 147, 179, 224, 226, 230, 231, 271, 298, 299, 328, 344, 349, 363, 365, 367, 382, 395, 403], "l1_small": 6, "2d": [6, 9, 11, 12, 74, 75, 76, 137, 145, 224, 381, 395, 398, 402], "instead": [6, 147, 164, 271, 292, 401], "risc": [6, 394], "grid": [6, 9, 11, 12, 16, 31, 147, 192, 224, 225, 337, 371, 372, 379, 389, 395, 402], "indic": [6, 38, 96, 97, 137, 228, 249, 311, 327, 336, 337, 367, 380, 403], "whether": [6, 9, 11, 38, 82, 83, 84, 192, 224, 225, 226, 228, 230, 231, 235, 236, 269, 270, 308, 309, 310, 328, 329, 337, 344, 349, 361, 362, 367, 372, 373, 379, 380, 382, 402], "conv": [6, 74, 75, 76, 398, 402], "halo": [6, 7, 228], "micro": 6, "anoth": [6, 337, 392, 395], "doubl": 6, "allow": [6, 9, 11, 224, 271, 390, 393, 400], "stall": 6, "reader": [6, 235], "improv": [6, 9, 11, 330, 390, 403], "increas": [6, 11, 403], "usag": [6, 9, 11, 12, 390, 392, 403], "consecut": [6, 37, 395], "boost": 6, "image2column": 6, "so": [6, 122, 147, 389], "bound": [6, 275], "fold": 6, "stride": [6, 74, 75, 76, 228, 267, 268, 269, 270, 398, 402], "match": [6, 68, 82, 83, 147, 179, 224, 226, 230, 231, 235, 236, 298, 299, 344, 349, 382, 395, 397, 399, 400, 401], "dimens": [6, 7, 9, 10, 11, 12, 16, 32, 34, 38, 68, 70, 71, 82, 83, 135, 137, 141, 145, 146, 147, 167, 179, 224, 226, 230, 231, 252, 256, 258, 266, 271, 272, 283, 286, 295, 297, 298, 308, 309, 310, 311, 327, 328, 329, 336, 337, 344, 349, 350, 364, 366, 367, 370, 371, 372, 373, 382, 387, 395, 398, 399, 400], "under": [6, 390, 393, 394, 403, 404], "without": [6, 99, 224], "notic": 6, "caution": 6, "product": [6, 31, 82, 224, 271, 371, 372], "22378": 6, "nhwc": [6, 398, 402], "format": [6, 7, 39, 46, 74, 75, 76, 113, 129, 130, 134, 137, 147, 228, 267, 268, 269, 270, 281, 336, 393, 395, 398, 400, 402], "n": [6, 7, 9, 10, 11, 12, 31, 51, 70, 74, 75, 76, 82, 83, 113, 121, 122, 147, 226, 228, 230, 231, 249, 262, 263, 275, 299, 344, 349, 367, 381, 382, 392, 394, 399, 402], "h": [6, 7, 51, 74, 75, 76, 113, 121, 122, 147, 228, 299, 367, 381, 395, 398, 402], "w": [6, 7, 51, 74, 75, 76, 113, 121, 122, 147, 179, 228, 299, 367, 381, 394, 398, 402], "ic": 6, "oc": 6, "pad_h": [6, 228], "pad_w": [6, 228], "implicit": 6, "via": [6, 392, 400, 402, 403], "current": [6, 38, 68, 70, 81, 137, 147, 235, 236, 269, 270, 311, 363, 367, 371, 372, 395, 404], "condit": [6, 298, 383, 390], "met": [6, 298], "divis": [6, 7, 11, 16, 126, 141, 146, 276, 286, 350, 395], "writer": 6, "carri": [6, 292], "bottleneck": [6, 403], "overrid": [6, 403, 404], "split": [6, 70, 141, 146, 147, 286, 350, 373, 395], "heurist": 6, "By": [6, 330, 393, 395], "default": [6, 8, 9, 11, 13, 14, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 178, 179, 180, 181, 182, 183, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 214, 216, 217, 218, 219, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 280, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 392, 394, 395, 402, 404], "inner": [6, 9, 10, 11, 12], "dim": [6, 32, 33, 38, 70, 71, 81, 82, 83, 137, 141, 146, 167, 179, 226, 230, 231, 258, 271, 272, 283, 286, 297, 299, 311, 327, 328, 329, 336, 344, 349, 350, 367, 373, 382, 395, 400, 401, 402], "kernel_h": [6, 228], "constraint": [6, 224, 308, 310, 367, 395], "space": [6, 37, 74, 75, 76, 267, 268, 269, 270, 395], "re": [6, 389, 391, 403], "overwrit": 6, "ani": [6, 121, 122, 256, 271, 390, 395, 403], "either": [6, 7, 81, 147, 224, 364, 392, 395, 403], "row_major": [6, 37, 38, 81, 98, 99, 134, 135, 136, 147, 179, 226, 230, 231, 250, 251, 252, 271, 299, 308, 309, 310, 337, 344, 349, 361, 362, 379, 380, 382, 387, 388, 394, 395], "expect": [6, 9, 76, 113, 122, 147, 228, 373, 390, 393, 398, 400, 403], "next": [6, 147, 247, 392, 395, 402], "fragment": [6, 298], "ideal": [6, 228, 367, 395], "face": [6, 395], "reshard": 6, "alreadi": [6, 76, 235, 236, 254, 329, 402], "anywai": 6, "previou": [6, 31, 97, 397, 400, 403], "due": [6, 281, 394, 395], "v": [6, 122, 372, 402, 403], "dilat": [6, 74, 75, 76, 228, 267, 268, 269, 270, 402], "tensormemorylayout": [6, 228, 308, 310], "own": [6, 395], "height_shard": [6, 308, 310], "block_shard": [6, 228], "width_shard": 6, "orient": [6, 81, 224, 308, 395], "major": [6, 9, 32, 61, 113, 120, 122, 123, 125, 132, 147, 224, 249, 283, 301, 319, 377, 395, 403], "column": [6, 147, 394, 395, 399], "bia": [6, 51, 52, 74, 75, 76, 113, 225, 267, 268, 269, 270, 389, 398, 400, 401, 402], "respons": [6, 402], "prepar": [6, 147, 390, 398, 402], "bfloat16": [6, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 129, 131, 132, 133, 134, 135, 136, 137, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 178, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 258, 259, 260, 261, 262, 263, 264, 265, 266, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 286, 287, 288, 289, 290, 291, 292, 293, 294, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 374, 375, 376, 377, 378, 379, 380, 382, 383, 384, 385, 386, 387, 388, 389, 394, 395, 398, 399, 400, 401, 402, 404], "unspecifi": [6, 32, 283], "float32": [6, 19, 21, 31, 34, 37, 38, 40, 42, 49, 51, 59, 62, 64, 82, 83, 85, 98, 99, 100, 119, 127, 131, 137, 138, 139, 148, 151, 153, 155, 157, 160, 164, 179, 180, 183, 185, 192, 194, 195, 197, 199, 219, 224, 225, 226, 229, 230, 231, 233, 243, 264, 273, 275, 299, 300, 308, 309, 310, 311, 320, 328, 329, 332, 334, 337, 344, 349, 355, 357, 367, 376, 382, 383, 387, 388, 389, 394, 395, 397, 399, 400, 404], "place": [7, 51, 101, 140, 149, 176, 177, 184, 220, 228, 244, 300, 308, 310, 329, 363, 369, 395], "too": [7, 404], "fit": [7, 297], "conv2d_dram": 7, "version": [7, 64, 147, 235, 236, 271, 308, 316, 372, 392, 394, 396, 398, 399, 400, 401, 402, 403], "happen": 7, "number": [7, 9, 10, 11, 12, 23, 24, 27, 29, 31, 32, 33, 52, 64, 70, 74, 75, 76, 89, 90, 91, 100, 101, 121, 122, 126, 127, 128, 137, 139, 140, 147, 148, 149, 181, 183, 184, 188, 204, 205, 208, 212, 214, 219, 220, 222, 224, 226, 228, 229, 230, 231, 233, 239, 242, 243, 244, 249, 256, 258, 265, 266, 267, 268, 269, 270, 275, 283, 293, 294, 295, 297, 298, 306, 311, 314, 337, 342, 344, 345, 348, 349, 362, 367, 368, 369, 382, 383, 385, 393, 394, 395, 398, 400, 401, 402, 403], "along": [7, 9, 10, 11, 12, 32, 33, 82, 83, 137, 147, 226, 230, 231, 272, 283, 309, 310, 311, 327, 328, 329, 336, 344, 349, 367, 370, 382, 395, 402], "correspond": [7, 96, 97, 337, 372, 383, 403], "calcul": [7, 31, 81, 394], "last": [7, 34, 38, 68, 135, 141, 146, 147, 179, 252, 286, 298, 299, 309, 310, 328, 329, 336, 350, 364, 367, 371, 373, 387, 394, 395], "smaller": [7, 11, 12, 395, 399], "rest": [7, 122], "sliceheight": 7, "slicewidth": 7, "prefer": [7, 392, 397, 403], "much": [7, 394], "larger": [7, 12, 399], "_ttnn": [8, 9, 10, 11, 12, 13, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 395], "multi_devic": [8, 13, 129, 130, 254, 352], "meshdevic": [8, 13, 39, 69, 74, 75, 76, 98, 99, 129, 130, 134, 135, 136, 193, 223, 252, 253, 254, 275, 352, 363, 366, 387, 388, 402], "plan": [8, 13, 129, 130, 257], "deprec": [8, 13, 129, 130, 257], "futur": [8, 13, 129, 130, 257], "1d": [9, 74, 147, 224, 249, 266, 400], "multicast": [9, 10, 11, 224], "advanc": [9, 298, 392, 403], "veri": [9, 10, 310, 329, 330, 394], "narrow": [9, 10], "interleav": [9, 11, 38, 51, 70, 82, 83, 113, 137, 147, 179, 224, 226, 230, 231, 271, 299, 308, 311, 328, 344, 349, 363, 365, 367, 373, 382, 395], "capabl": [9, 11, 12, 16], "while": [9, 147, 393, 400], "str": [9, 10, 11, 12, 23, 39, 44, 90, 93, 100, 115, 138, 139, 148, 180, 181, 183, 192, 193, 204, 205, 208, 212, 214, 219, 224, 225, 229, 233, 235, 236, 242, 243, 306, 314, 342, 348, 385, 402], "batch": [9, 11, 51, 74, 75, 76, 121, 122, 167, 192, 224, 228, 267, 268, 269, 270, 308, 309, 310, 337, 372, 394, 398, 402], "incorpor": [9, 390], "effici": [9, 10, 11, 12, 309, 310, 329, 392, 399, 400], "process": [9, 10, 11, 12, 16, 70, 82, 83, 147, 267, 268, 308, 309, 310, 337, 394, 400, 401, 402], "dure": [9, 10, 11, 70, 134, 170, 366, 395, 403], "directli": [9, 10, 11, 310, 329, 396, 397, 398, 399, 400, 401, 402], "elimin": 9, "separ": [9, 11], "overal": [9, 400, 402, 403], "scenario": [9, 10, 11, 224, 395], "intern": [9, 76, 147, 310, 391], "left": [9, 46, 89, 138, 147, 180, 393], "k": [9, 10, 11, 12, 261, 328, 329, 367, 371, 372, 402], "granular": [9, 10, 11, 12, 16], "wide": [9, 10, 11, 12, 147, 329], "input_tensor_a": [9, 10, 11, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 41, 43, 44, 46, 47, 48, 50, 52, 53, 54, 56, 57, 58, 61, 63, 71, 78, 80, 86, 88, 89, 90, 91, 95, 100, 101, 104, 106, 108, 111, 118, 123, 125, 126, 127, 128, 132, 138, 139, 140, 148, 149, 152, 154, 156, 160, 161, 163, 167, 170, 180, 181, 182, 183, 184, 186, 188, 191, 196, 198, 200, 201, 203, 204, 205, 206, 207, 208, 209, 212, 213, 214, 215, 217, 218, 219, 220, 222, 227, 228, 229, 232, 233, 239, 241, 242, 243, 244, 247, 255, 263, 265, 266, 274, 275, 277, 282, 289, 290, 293, 294, 301, 306, 307, 313, 317, 319, 324, 326, 333, 335, 341, 342, 343, 345, 346, 347, 348, 354, 358, 377, 384, 385, 386, 404], "input_tensor_b": [9, 10, 11, 23, 24, 25, 26, 27, 28, 29, 30, 44, 46, 47, 52, 53, 54, 56, 57, 58, 71, 89, 90, 91, 100, 101, 126, 127, 128, 138, 139, 140, 148, 149, 160, 161, 167, 170, 180, 181, 182, 183, 184, 188, 204, 205, 206, 207, 208, 209, 212, 213, 214, 215, 219, 220, 222, 225, 227, 229, 232, 233, 239, 242, 243, 244, 247, 255, 266, 275, 293, 294, 306, 307, 342, 343, 345, 346, 347, 348, 384, 385, 386, 404], "impact": [9, 11, 12, 16, 390], "affect": [9, 10, 11, 12, 308, 394], "across": [9, 10, 11, 12, 32, 33, 145, 283, 394, 395, 399], "first": [9, 31, 134, 141, 146, 147, 192, 224, 225, 249, 256, 286, 337, 350, 389, 392, 394, 400, 401, 402, 404], "bandwidth": [9, 10], "certain": [9, 11], "m": [9, 10, 11, 12, 31, 392, 403], "subblock": [9, 11, 12], "schedul": [9, 11, 390], "particip": 9, "workload": [9, 391], "balanc": [9, 11], "crucial": 9, "achiev": [9, 390, 393], "subsequ": [9, 400, 401, 404], "special": [10, 308, 390, 395], "provid": [10, 11, 14, 15, 16, 31, 38, 51, 137, 147, 179, 192, 224, 225, 226, 230, 231, 235, 236, 271, 272, 299, 308, 309, 310, 328, 337, 344, 349, 352, 362, 367, 371, 382, 390, 392, 393, 394, 395, 396, 400, 401, 402, 403, 404], "signific": 10, "benefit": [10, 393], "avoid": [10, 11, 235, 310, 329, 330, 394, 395, 398, 399, 403], "trip": 10, "chosen": [10, 11, 12, 224, 394], "align": [10, 12, 179, 224, 309, 310, 394], "strategi": [10, 81, 224, 394, 395, 402], "ensur": [10, 68, 336, 352, 390, 392, 395, 398, 402, 403], "conflict": 10, "compat": [10, 16, 147, 402], "x": [11, 12, 46, 51, 76, 147, 179, 192, 224, 259, 299, 308, 309, 310, 371, 372, 392, 394, 395, 398, 401], "g": [11, 76, 147, 224, 392, 394, 395, 400, 402, 403], "overhead": [11, 399], "explicitli": [11, 84], "transpos": [11, 76, 116, 192, 224, 225, 258, 373, 395, 400, 402], "direct": 11, "benefici": 11, "reusabl": 12, "better": [12, 308, 315, 403], "togeth": [12, 397, 398, 399, 400, 401, 402], "suggest": 12, "decreas": 12, "fewer": 12, "doe": [12, 68, 134, 147, 271, 366, 390, 394], "work": [12, 224, 327, 389, 390, 392, 404], "total_m": 12, "total_n": 12, "arg0": [13, 395], "tt": [13, 361, 362, 379, 380, 381, 391, 396, 397, 399, 400, 402], "device_id": [13, 69, 84, 96, 97, 133, 223, 254, 280, 311, 352, 363, 364, 365, 397, 398, 399, 400, 401, 402, 404], "behavior": [14, 395], "suitabl": [14, 267, 268, 269, 270], "most": [14, 224, 395, 399, 401], "appropri": [14, 147, 224, 390, 392, 402], "characterist": [14, 281, 403], "program_config": [14, 16, 31, 179, 192, 224, 225, 299, 308, 310, 329, 337, 368, 369, 371, 372], "common": [15, 138, 180, 389, 390, 402], "interfac": [15, 400, 402], "customiz": 16, "fine": [16, 393], "grain": 16, "over": [16, 38, 51, 74, 75, 76, 113, 145, 147, 179, 226, 230, 231, 272, 299, 328, 344, 349, 371, 372, 382, 392, 403], "subblock_w": [16, 308, 310, 329], "sub": [16, 352, 379, 395], "block_h": [16, 308, 310, 329], "vertic": 16, "horizont": 16, "modifi": [16, 310, 314, 329, 330], "proper": [16, 392], "compute_grid": [16, 329], "24": [16, 308, 310, 402], "sharded_tensor": 16, "python_fully_qualified_nam": [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388], "abs_t": 17, "object": [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 131, 132, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 393], "default_preprocess_golden_function_input": [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 258, 259, 260, 261, 262, 263, 264, 265, 266, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388], "_golden_funct": [17, 20, 23, 28, 30, 31, 35, 36, 37, 42, 45, 59, 60, 68, 70, 72, 73, 75, 77, 79, 85, 87, 90, 96, 98, 99, 100, 102, 103, 105, 107, 109, 110, 116, 117, 120, 124, 133, 134, 136, 139, 142, 144, 147, 148, 150, 153, 155, 158, 162, 164, 165, 166, 168, 169, 171, 172, 173, 174, 175, 179, 181, 183, 188, 189, 190, 192, 194, 195, 197, 199, 202, 204, 205, 209, 210, 213, 215, 219, 221, 224, 227, 234, 239, 240, 242, 243, 245, 248, 252, 253, 256, 258, 259, 260, 262, 265, 273, 278, 279, 280, 281, 287, 288, 295, 296, 297, 298, 299, 304, 308, 309, 310, 315, 316, 318, 320, 321, 323, 325, 328, 329, 330, 331, 334, 338, 340, 348, 351, 353, 355, 363, 364, 365, 366, 368, 369, 370, 373, 374, 375, 381, 384, 387, 388], "default_postprocess_golden_function_output": [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 258, 259, 260, 261, 262, 263, 264, 265, 266, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388], "is_cpp_oper": [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388], "is_experiment": [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388], "element": [17, 19, 21, 34, 38, 40, 42, 45, 55, 59, 60, 62, 64, 70, 74, 75, 76, 77, 82, 83, 85, 102, 103, 105, 107, 109, 110, 114, 117, 119, 124, 131, 137, 141, 142, 144, 145, 146, 147, 150, 153, 155, 159, 162, 171, 172, 173, 174, 175, 185, 189, 194, 195, 197, 199, 202, 210, 221, 234, 245, 248, 249, 256, 263, 264, 267, 268, 269, 270, 271, 273, 276, 281, 286, 287, 288, 291, 292, 297, 300, 302, 304, 315, 316, 318, 320, 321, 323, 330, 334, 336, 338, 340, 350, 353, 355, 357, 367, 376, 378, 380, 383, 395, 399], "wise": [17, 19, 21, 34, 40, 42, 45, 55, 59, 60, 62, 64, 77, 85, 102, 103, 105, 107, 109, 110, 114, 117, 119, 124, 131, 141, 142, 144, 146, 150, 153, 155, 159, 162, 171, 172, 173, 174, 175, 185, 189, 194, 195, 197, 199, 202, 210, 221, 234, 245, 248, 264, 273, 276, 281, 286, 287, 288, 291, 292, 300, 302, 304, 315, 316, 318, 320, 321, 323, 330, 334, 338, 340, 350, 353, 355, 357, 376, 378, 399], "mathrm": [17, 19, 21, 23, 25, 34, 40, 42, 45, 46, 49, 53, 54, 55, 56, 57, 58, 59, 60, 62, 77, 82, 83, 85, 89, 91, 100, 101, 102, 103, 105, 107, 109, 110, 114, 117, 119, 124, 126, 127, 131, 138, 139, 140, 141, 142, 144, 145, 146, 148, 149, 150, 153, 155, 159, 160, 162, 164, 170, 171, 172, 173, 174, 175, 180, 181, 183, 184, 185, 187, 189, 194, 195, 197, 199, 202, 204, 205, 208, 209, 210, 212, 213, 214, 215, 219, 220, 221, 234, 242, 243, 244, 245, 247, 248, 255, 263, 264, 266, 273, 281, 286, 287, 288, 291, 292, 293, 300, 302, 304, 306, 315, 316, 318, 320, 321, 323, 330, 334, 338, 340, 342, 346, 348, 350, 353, 355, 357, 376, 378, 385], "_tensor": [17, 19, 21, 23, 25, 34, 40, 42, 45, 46, 53, 54, 55, 56, 57, 58, 59, 60, 62, 77, 85, 89, 91, 100, 101, 102, 103, 105, 107, 109, 110, 114, 117, 119, 124, 126, 127, 131, 138, 139, 140, 141, 142, 144, 145, 146, 148, 149, 150, 153, 155, 159, 160, 162, 164, 170, 171, 172, 173, 174, 175, 180, 181, 183, 184, 185, 187, 189, 194, 195, 197, 199, 202, 204, 205, 208, 209, 210, 212, 213, 214, 215, 219, 220, 221, 234, 242, 243, 244, 245, 247, 248, 255, 263, 264, 266, 273, 281, 286, 287, 288, 291, 292, 293, 300, 302, 304, 306, 315, 316, 318, 320, 321, 323, 330, 334, 338, 340, 342, 346, 348, 350, 353, 355, 357, 376, 378, 385], "_i": [17, 19, 21, 23, 40, 42, 45, 46, 49, 53, 54, 55, 56, 57, 58, 59, 60, 62, 77, 82, 83, 85, 89, 100, 102, 103, 105, 107, 109, 110, 114, 117, 119, 124, 131, 138, 139, 141, 142, 144, 145, 146, 148, 150, 153, 155, 159, 160, 162, 164, 171, 172, 173, 174, 175, 180, 183, 185, 189, 194, 195, 197, 199, 202, 208, 209, 210, 212, 213, 214, 215, 219, 221, 234, 242, 243, 245, 247, 248, 263, 264, 273, 281, 286, 287, 288, 291, 292, 300, 302, 304, 306, 315, 316, 318, 320, 321, 323, 330, 334, 338, 340, 348, 350, 353, 355, 357, 376, 378, 385], "verb": [17, 19, 21, 40, 42, 45, 49, 53, 54, 55, 56, 57, 58, 59, 60, 62, 77, 85, 91, 107, 110, 114, 117, 119, 124, 126, 127, 131, 138, 141, 146, 153, 155, 159, 162, 164, 171, 172, 173, 174, 175, 180, 181, 185, 187, 202, 204, 205, 234, 245, 266, 273, 281, 286, 287, 288, 291, 292, 293, 300, 302, 304, 315, 316, 318, 320, 321, 323, 334, 338, 340, 342, 350, 353, 376, 378], "complextensor": [17, 18, 24, 35, 36, 72, 73, 89, 90, 165, 166, 168, 169, 239, 259, 260, 278, 279, 281, 282, 345], "keyword": [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 99, 100, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 131, 132, 134, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 178, 179, 180, 181, 182, 183, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 214, 216, 217, 218, 219, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 238, 239, 240, 241, 242, 243, 245, 246, 247, 248, 249, 250, 251, 254, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 271, 272, 273, 274, 275, 276, 277, 278, 279, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386], "memory_config": [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 131, 132, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 178, 179, 180, 181, 182, 183, 185, 186, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 214, 216, 217, 218, 219, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 238, 239, 240, 241, 242, 243, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 309, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 399, 401, 404], "prealloc": [17, 19, 21, 23, 24, 25, 26, 32, 34, 38, 40, 42, 44, 45, 51, 53, 54, 55, 56, 57, 58, 59, 60, 62, 64, 70, 71, 77, 82, 83, 85, 89, 90, 96, 97, 100, 102, 103, 105, 107, 109, 110, 112, 114, 115, 117, 119, 120, 124, 131, 135, 136, 137, 138, 139, 142, 143, 144, 148, 150, 153, 155, 159, 162, 164, 171, 172, 173, 174, 175, 178, 180, 181, 183, 185, 189, 194, 195, 197, 199, 202, 204, 205, 208, 210, 212, 214, 219, 221, 229, 233, 234, 238, 239, 242, 243, 245, 246, 248, 253, 264, 265, 273, 276, 281, 283, 287, 288, 291, 292, 300, 302, 304, 305, 306, 307, 315, 316, 318, 320, 321, 322, 323, 330, 334, 336, 338, 339, 340, 342, 345, 346, 347, 348, 353, 355, 356, 357, 367, 376, 378, 383, 384, 385, 388], "tile_layout": [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 34, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 71, 77, 78, 79, 80, 84, 85, 86, 87, 88, 89, 90, 91, 94, 95, 97, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 135, 138, 139, 140, 141, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 170, 171, 172, 173, 174, 175, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 227, 229, 232, 233, 234, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 250, 251, 252, 253, 261, 263, 264, 265, 272, 273, 274, 275, 276, 277, 281, 282, 286, 287, 288, 289, 290, 291, 292, 293, 294, 296, 297, 299, 300, 301, 302, 304, 305, 306, 307, 308, 309, 310, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 338, 339, 340, 341, 342, 343, 345, 346, 347, 348, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 364, 367, 374, 375, 376, 377, 378, 383, 384, 385, 386, 387, 388, 389, 395, 397, 399, 400, 401, 402, 404], "unary_backward": [18, 20, 22, 41, 43, 48, 50, 61, 63, 65, 67, 78, 80, 86, 88, 92, 95, 104, 106, 108, 111, 112, 118, 120, 123, 125, 132, 143, 152, 154, 156, 158, 163, 186, 191, 196, 198, 200, 201, 203, 217, 218, 241, 246, 262, 265, 272, 274, 277, 282, 289, 290, 296, 301, 303, 305, 313, 317, 319, 322, 324, 326, 331, 333, 335, 339, 341, 354, 356, 358, 360, 377], "abs_bw_t": 18, "_golden_function_ab": 18, "backward": [18, 20, 22, 24, 26, 28, 30, 36, 41, 43, 44, 47, 48, 50, 52, 61, 63, 65, 67, 71, 73, 78, 80, 86, 88, 90, 92, 95, 97, 104, 106, 108, 111, 112, 115, 118, 120, 123, 125, 128, 132, 143, 152, 154, 156, 158, 161, 163, 166, 182, 186, 188, 191, 196, 198, 200, 201, 203, 206, 207, 217, 218, 227, 232, 239, 241, 246, 260, 262, 265, 272, 274, 277, 279, 282, 289, 290, 294, 296, 301, 303, 305, 307, 313, 317, 319, 322, 324, 326, 331, 333, 335, 339, 341, 343, 345, 347, 354, 356, 358, 360, 377, 384, 386], "given": [18, 20, 22, 24, 26, 28, 30, 36, 38, 41, 43, 44, 47, 48, 50, 52, 61, 63, 65, 67, 71, 73, 76, 78, 80, 82, 83, 86, 88, 90, 92, 95, 99, 104, 106, 108, 111, 112, 118, 120, 123, 125, 128, 132, 143, 147, 152, 154, 156, 158, 161, 163, 166, 182, 186, 188, 191, 196, 198, 200, 201, 203, 206, 207, 217, 218, 227, 232, 235, 236, 239, 241, 246, 254, 257, 260, 262, 265, 272, 274, 275, 277, 279, 282, 289, 290, 294, 296, 297, 301, 303, 305, 307, 311, 313, 317, 319, 322, 324, 326, 331, 333, 335, 339, 341, 343, 345, 347, 354, 356, 358, 360, 367, 372, 377, 381, 384, 386, 389, 399, 402], "grad_tensor": [18, 20, 22, 24, 26, 28, 30, 36, 41, 43, 44, 47, 48, 50, 52, 61, 63, 65, 67, 71, 73, 78, 80, 86, 88, 90, 92, 95, 97, 104, 106, 108, 111, 112, 115, 118, 120, 123, 125, 128, 132, 143, 152, 154, 156, 158, 161, 163, 166, 182, 186, 188, 191, 196, 198, 200, 201, 203, 206, 207, 217, 218, 227, 232, 239, 241, 246, 260, 262, 265, 272, 274, 277, 279, 282, 289, 290, 294, 296, 301, 303, 305, 307, 313, 317, 319, 322, 324, 326, 331, 333, 335, 339, 341, 343, 345, 347, 354, 356, 358, 360, 377, 384, 386], "gradient": [18, 20, 22, 24, 26, 28, 30, 41, 43, 44, 47, 48, 50, 52, 61, 63, 65, 67, 71, 76, 78, 80, 86, 88, 90, 92, 95, 97, 104, 106, 108, 111, 112, 115, 118, 120, 123, 125, 128, 132, 137, 143, 152, 154, 156, 158, 161, 163, 182, 186, 188, 191, 196, 198, 200, 201, 203, 206, 207, 217, 218, 227, 232, 239, 241, 246, 262, 265, 272, 274, 277, 282, 289, 290, 294, 296, 301, 303, 305, 307, 313, 317, 319, 322, 324, 326, 331, 333, 335, 339, 341, 343, 345, 347, 354, 356, 358, 360, 377, 384, 386], "list": [18, 20, 22, 23, 24, 26, 28, 30, 31, 36, 41, 43, 44, 47, 48, 50, 52, 61, 63, 65, 67, 70, 71, 73, 78, 80, 81, 86, 88, 90, 92, 95, 96, 98, 100, 104, 106, 108, 111, 112, 118, 120, 123, 125, 128, 130, 132, 138, 139, 143, 148, 152, 154, 156, 158, 161, 163, 166, 180, 181, 182, 183, 186, 188, 191, 192, 196, 198, 200, 201, 203, 204, 205, 206, 207, 208, 212, 214, 217, 218, 219, 224, 225, 227, 228, 229, 232, 233, 239, 241, 242, 243, 246, 249, 256, 257, 258, 260, 262, 265, 266, 271, 272, 274, 275, 277, 279, 282, 289, 290, 294, 296, 301, 303, 305, 306, 307, 313, 317, 319, 322, 324, 326, 327, 331, 333, 335, 337, 339, 341, 342, 343, 345, 347, 348, 352, 354, 356, 358, 360, 367, 372, 377, 378, 379, 380, 384, 385, 386, 391, 397, 403], "about": [18, 28, 52, 128, 143, 196, 198, 200, 201, 203, 206, 207, 217, 218, 265, 272, 281, 282, 294, 331, 339, 386, 395, 397, 399, 401, 403], "requires_grad": [18, 20, 22, 24, 26, 28, 30, 41, 43, 44, 47, 48, 50, 52, 61, 63, 65, 67, 71, 78, 80, 86, 88, 90, 95, 97, 104, 106, 108, 111, 112, 115, 118, 120, 123, 125, 128, 132, 143, 152, 154, 156, 158, 161, 163, 182, 186, 188, 191, 196, 198, 200, 201, 203, 206, 207, 217, 218, 227, 232, 239, 241, 246, 265, 272, 274, 277, 282, 289, 290, 294, 296, 301, 305, 307, 313, 317, 319, 322, 324, 326, 331, 333, 335, 339, 341, 343, 345, 347, 354, 356, 358, 360, 377, 384, 386], "acos_t": 19, "_golden_function_aco": 19, "acos_bw_t": 20, "invers": [20, 22, 41, 43, 48, 50], "cosin": [20, 22, 78, 80, 116], "acosh_t": 21, "_golden_function_acosh": [21, 22], "acosh_bw_t": 22, "hyperbol": [22, 43, 50, 80, 326, 356], "add_t": 23, "_a": [23, 25, 46, 53, 54, 56, 57, 58, 89, 91, 100, 101, 126, 127, 138, 139, 140, 148, 149, 160, 170, 180, 181, 183, 184, 204, 205, 208, 209, 212, 213, 214, 215, 219, 220, 242, 243, 244, 247, 255, 266, 293, 306, 342, 346, 348, 385], "_b": [23, 25, 46, 53, 54, 56, 57, 58, 89, 91, 100, 101, 126, 127, 138, 139, 140, 148, 149, 160, 170, 180, 181, 183, 184, 204, 205, 208, 209, 212, 213, 214, 215, 219, 220, 242, 243, 244, 247, 255, 266, 293, 306, 342, 346, 348, 385], "datatyp": [23, 31, 37, 39, 51, 68, 74, 75, 76, 82, 83, 96, 97, 98, 99, 100, 134, 135, 136, 138, 139, 145, 147, 148, 179, 180, 181, 183, 192, 204, 205, 208, 212, 214, 219, 224, 225, 228, 229, 233, 242, 243, 252, 253, 267, 268, 269, 270, 275, 299, 306, 329, 337, 342, 348, 364, 365, 385, 387, 388, 397, 399, 402, 403], "int32": [23, 38, 53, 54, 55, 56, 57, 58, 64, 82, 83, 100, 102, 119, 137, 138, 139, 144, 148, 150, 180, 183, 189, 208, 209, 210, 211, 212, 213, 214, 215, 219, 221, 226, 229, 230, 231, 233, 242, 243, 248, 311, 320, 340, 342, 344, 348, 349, 367, 382, 383], "uint32": [23, 38, 53, 54, 56, 57, 58, 82, 83, 96, 97, 102, 119, 137, 164, 167, 210, 211, 226, 230, 231, 248, 336, 344, 348, 349, 367, 382, 394, 395], "4294967295": [23, 210, 348], "uint16": [23, 38, 53, 56, 58, 100, 102, 135, 137, 164, 208, 209, 210, 212, 213, 214, 215, 242, 243, 248, 252, 336, 340, 342, 348, 367, 387, 395], "65535": [23, 53, 56, 58, 210, 242, 348], "tensor1": [23, 24, 25, 26, 27, 28, 29, 30, 44, 46, 47, 52, 53, 54, 56, 57, 58, 70, 71, 89, 90, 91, 100, 101, 126, 127, 128, 138, 139, 140, 148, 149, 160, 161, 167, 170, 180, 181, 182, 183, 184, 187, 188, 204, 205, 206, 207, 208, 209, 212, 213, 214, 215, 219, 220, 222, 224, 227, 229, 232, 233, 239, 242, 243, 244, 247, 255, 266, 293, 294, 306, 307, 342, 343, 345, 346, 347, 348, 383, 384, 385, 386], "tensor2": [23, 24, 25, 26, 27, 28, 29, 30, 44, 46, 47, 52, 53, 54, 56, 57, 58, 70, 71, 89, 90, 91, 100, 101, 126, 127, 128, 138, 139, 140, 148, 149, 160, 161, 167, 170, 180, 181, 182, 183, 184, 187, 188, 204, 205, 206, 207, 208, 209, 212, 213, 214, 215, 219, 220, 222, 224, 227, 229, 232, 233, 239, 242, 243, 244, 247, 255, 266, 293, 294, 306, 307, 342, 343, 345, 346, 347, 348, 383, 384, 385, 386], "scalar": [23, 24, 27, 29, 53, 54, 56, 57, 58, 64, 89, 90, 91, 92, 100, 101, 126, 127, 128, 135, 136, 139, 140, 148, 149, 181, 183, 184, 187, 188, 204, 205, 208, 212, 214, 219, 220, 222, 226, 229, 230, 231, 233, 239, 242, 243, 244, 262, 266, 271, 276, 277, 293, 294, 306, 342, 344, 345, 348, 349, 382, 385, 404], "binary_backward": [24, 26, 44, 47, 52, 71, 90, 128, 161, 182, 206, 207, 227, 232, 239, 294, 307, 343, 345, 347, 386], "add_bw_t": 24, "_golden_function_bw": [24, 26, 44, 47, 52, 71, 128, 161, 182, 206, 207, 232, 294, 307, 343, 345, 347, 386], "are_required_output": [24, 26, 44, 71, 90, 239, 307, 345, 347, 384], "input_grad": [24, 26, 44, 71, 90, 115, 239, 307, 345, 347], "other_grad": [24, 26, 44, 71, 90, 239, 307, 345, 347], "bfloat4_b": [24, 26, 27, 29, 31, 34, 47, 52, 71, 90, 134, 161, 182, 187, 188, 192, 206, 207, 222, 224, 225, 227, 232, 239, 275, 307, 337, 343, 345, 347, 355, 357, 366, 383, 386], "addalpha_t": 25, "_golden_function_addalpha": 25, "alpha": [25, 26, 28, 30, 31, 62, 63, 90, 94, 95, 312, 346, 347], "float": [25, 26, 27, 28, 29, 30, 31, 51, 62, 63, 65, 66, 67, 92, 94, 95, 114, 119, 122, 129, 134, 135, 136, 147, 151, 152, 157, 158, 159, 170, 179, 185, 186, 187, 216, 218, 226, 230, 231, 256, 258, 262, 263, 264, 266, 275, 276, 277, 291, 292, 299, 302, 303, 308, 309, 310, 312, 314, 330, 331, 332, 333, 344, 346, 347, 349, 359, 360, 371, 372, 374, 375, 382, 395], "addalpha_bw_t": 26, "addcdiv_t": 27, "_golden_function_addcdiv": 27, "input_tensor_c": [27, 28, 29, 30, 188, 222, 384], "tensor3": [27, 28, 29, 30, 187, 188, 222, 383, 384], "ternary_backward": [28, 30, 188, 384], "addcdiv_bw_t": 28, "addcmul_t": 29, "_golden_function_addcmul": 29, "addcmul_bw_t": 30, "addmm_t": 31, "beta": [31, 51, 330, 331], "matmulprogramconfig": [31, 192, 224, 225, 337], "compute_kernel_config": [31, 51, 68, 113, 116, 147, 179, 192, 224, 225, 226, 230, 231, 299, 308, 309, 310, 328, 329, 337, 344, 349, 371, 372, 382], "devicecomputekernelconfig": [31, 51, 74, 75, 76, 113, 116, 147, 179, 192, 224, 225, 267, 268, 269, 270, 299, 308, 309, 310, 328, 329, 337, 371, 372], "coregrid": [31, 81, 147, 192, 224, 225, 337, 401], "output_til": [31, 192, 224, 225, 337], "ha": [31, 54, 57, 76, 224, 235, 236, 269, 270, 337, 352, 373, 389, 390, 394, 395, 396, 403, 404], "p": [31, 224, 395, 403], "content": 31, "overwritten": 31, "scale": [31, 51, 114, 147, 179, 226, 230, 231, 299, 308, 309, 310, 312, 330, 337, 344, 349, 371, 372, 382, 402], "factor": [31, 226, 230, 231, 308, 309, 310, 337, 344, 349, 382], "look": [31, 147, 224, 367, 390, 391, 394, 395, 403], "second": [31, 134, 141, 146, 147, 192, 224, 225, 256, 266, 286, 298, 337, 350, 394, 401, 402, 404], "dram_memory_config": [31, 37, 68, 98, 99, 147, 192, 224, 225, 275, 337, 363, 395, 399], "highest": [31, 400, 402], "precis": [31, 314, 355, 357, 395, 400], "global_cb": [31, 192, 224, 225, 337], "globalcircularbuff": [31, 192, 224, 225, 337], "tbd": 31, "sub_device_id": [31, 192, 224, 225, 337, 352], "subdeviceid": [31, 32, 33, 192, 224, 225, 283, 337, 352], "all_gather_t": 32, "cluster_axi": [32, 33, 283], "subdevice_id": [32, 33, 283], "num_link": [32, 33, 283], "cluster": [32, 33, 283, 398, 399, 400, 401, 402, 403], "axi": [32, 33, 82, 83, 283, 297, 399], "collect": [32, 33, 390, 394, 396, 403], "concaten": [32, 70, 71, 370, 373], "non": [32, 89, 224, 228, 249, 265, 276, 281, 302, 337], "independ": [32, 70], "subdevic": [32, 33, 283], "id": [32, 33, 134, 223, 254, 283, 352, 366, 394, 398, 399, 400, 401, 402, 403], "worker": [32, 33, 254, 283, 398, 399, 400, 401, 402], "link": [32, 33, 283], "fabric": [32, 33, 283], "output_shap": [32, 271, 283], "input_shap": [32, 97, 228, 271, 283, 308, 309, 310], "num_devic": [32, 283], "total": [32, 283, 394, 400, 402], "full_tensor": [32, 33], "randn": [32, 33, 39, 84, 97, 99, 133, 134, 145, 192, 224, 228, 253, 280, 311, 363, 364, 365, 366, 378, 388, 401], "256": [32, 33, 228, 283, 402], "mesh_devic": [32, 33, 398, 399, 400, 401, 402], "open_mesh_devic": [32, 33], "meshshap": [32, 33], "ttnn_tensor": [32, 33, 283, 366], "input_dtyp": [32, 33, 267, 268, 269, 270], "mem_config": [32, 33], "mesh_mapp": [32, 33, 39, 134], "shardtensor2dmesh": [32, 33], "mesh_shap": [32, 33], "print": [32, 33, 37, 39, 70, 98, 99, 133, 134, 135, 136, 192, 223, 224, 252, 253, 254, 258, 283, 295, 297, 314, 327, 337, 363, 364, 366, 387, 388, 394, 395, 398, 402, 404], "all_reduce_t": 33, "alt_complex_rotate90_t": 34, "_golden_function_alt_complex_rotate90": 34, "_": [34, 389, 394, 401], "2i": 34, "even": [34, 84, 223, 224], "complex_unari": [35, 72, 165, 168, 169, 259, 278], "angle_t": 35, "complex": [35, 36, 72, 73, 165, 166, 259, 260, 278, 279, 398, 402], "complex_unary_backward": [36, 73, 166, 260, 279], "angle_bw_t": 36, "arange_t": 37, "inclus": [37, 275, 395], "end": [37, 82, 83, 89, 170, 187, 247, 327, 380, 390, 394, 399, 401, 403], "exclus": [37, 256, 271, 275], "00000": [37, 397], "argmax_t": 38, "_create_golden_funct": [38, 226, 230, 231, 344, 349, 382], "local": [38, 42, 45, 59, 60, 77, 79, 85, 87, 102, 103, 105, 107, 109, 110, 117, 124, 142, 144, 150, 153, 155, 162, 164, 171, 172, 173, 174, 175, 189, 190, 194, 195, 197, 199, 202, 210, 221, 226, 230, 231, 234, 240, 245, 248, 273, 287, 288, 304, 315, 316, 318, 320, 321, 323, 325, 330, 334, 338, 340, 344, 349, 351, 353, 355, 367, 374, 375, 382, 389, 390, 395, 396, 398, 399, 400, 401, 402, 403], "keepdim": [38, 226, 230, 231, 271, 344, 349, 382], "row_major_layout": [38, 39, 68, 93, 96, 129, 130, 134, 135, 147, 228, 252, 364, 387, 395, 398, 401, 402], "yield": [38, 226, 230, 231, 344, 349, 382, 402], "output_onedim": 38, "output_alldim": 38, "cache_file_nam": 39, "pathlib": [39, 93, 193], "path": [39, 93, 193, 392, 394, 400, 402, 403, 404], "callabl": [39, 235, 236], "serial": 39, "cpptensortomesh": 39, "tensortomesh": [39, 134], "truncat": [39, 377], "mantissa": 39, "bit": 39, "bfp": [39, 395], "rais": [39, 366], "runtim": [39, 337, 403], "error": [39, 178, 223, 224, 238, 366, 390, 398, 399, 400, 401, 402], "rte": 39, "bfp8": 39, "bfp4": 39, "375": [39, 134], "30469": [39, 134], "714844": [39, 134], "761719": [39, 134], "53125": [39, 134], "652344": [39, 134], "asin_t": 40, "_golden_function_asin": 40, "asin_bw_t": 41, "lambda": [41, 43, 50, 61, 63, 65, 67, 80, 92, 95, 106, 152, 186, 196, 198, 200, 218, 277, 303, 319, 324, 326, 333, 335, 389], "sine": [41, 43, 116, 326], "asinh_t": 42, "register_ttnn_cpp_unary_funct": [42, 45, 59, 60, 77, 79, 85, 87, 102, 103, 105, 107, 109, 110, 117, 124, 142, 144, 150, 153, 155, 162, 164, 171, 172, 173, 174, 175, 189, 190, 194, 195, 197, 199, 202, 210, 221, 234, 240, 245, 248, 273, 287, 288, 304, 315, 316, 318, 320, 321, 323, 325, 330, 334, 338, 340, 351, 353, 355, 374, 375], "asinh_bw_t": 43, "assign_bw_t": 44, "assign": 44, "other_tensor": [44, 90], "round_mod": [44, 89, 90, 276, 277], "atan_t": 45, "atan2_t": 46, "_golden_function_atan2": 46, "arctan": 46, "right": [46, 89, 138, 147, 180], "atan2_bw_t": 47, "atan_bw_t": 48, "_golden_function_atan": 48, "tangenr": 48, "atanh_t": 49, "_golden_function_atanh": 49, "bflaot8_b": 49, "atanh_bw_t": 50, "tangent": [50, 356], "batch_norm_t": 51, "running_mean": 51, "running_var": 51, "train": [51, 400, 402, 403], "ep": [51, 216, 218], "1e": [51, 147, 170, 179, 299], "05": [51, 399], "momentum": 51, "norm": [51, 179, 299], "see": [51, 147, 179, 299, 390, 391, 393, 394, 397, 398, 399, 400, 401, 402, 403], "spatial": [51, 145, 381, 398], "text": [51, 89, 147, 170, 179, 247, 255, 299, 328, 329, 390], "gamma": [51, 241], "epsilon": [51, 147, 179, 299], "mu": [51, 147, 179], "sigma": [51, 147, 179], "cdot": [51, 82, 83, 147, 179, 299, 385, 399], "varianc": [51, 147, 179, 395], "respect": [51, 76, 97, 147, 179, 224, 395, 397], "learnabl": [51, 147, 179], "shift": [51, 147, 179, 299], "small": [51, 147, 179, 254, 299, 398], "updat": [51, 177, 390, 393, 400, 402, 404], "evalu": 51, "These": [51, 224, 390, 394, 396, 399, 400, 403, 404], "bias_gelu_bw_t": 52, "bias_gelu": 52, "approxim": [52, 103, 105, 109, 115, 142, 143, 194, 195, 197, 199, 315, 316, 355, 357], "bitwise_and_t": 53, "_golden_function_bitwise_and": 53, "integ": [53, 54, 56, 57, 58, 82, 83, 256, 275, 372], "bitwise_left_shift_t": 54, "_golden_function_bitwise_left_shift": 54, "shift_bit": [54, 57], "31": [54, 57, 392, 397, 399], "bitwise_not_t": 55, "_golden_function_bitwise_not": 55, "2147483647": [55, 138], "bitwise_or_t": 56, "_golden_function_bitwise_or": 56, "bitwise_right_shift_t": 57, "_golden_function_bitwise_right_shift": 57, "bitwise_xor_t": 58, "_golden_function_bitwise_xor": 58, "cbrt_t": 59, "ceil_t": 60, "ceil_bw_t": 61, "celu_t": 62, "_golden_function_celu": 62, "celu_bw_t": 63, "formula": [63, 95, 152, 186, 187, 218, 331, 333], "clamp_t": 64, "_golden_function_clamp": 64, "min_tensor": [64, 66], "max_tensor": [64, 66], "clamp_bw_t": 65, "clip_t": 66, "_golden_function_clip": 66, "clip_bw_t": 67, "data_mov": [68, 70, 121, 122, 137, 167, 249, 256, 258, 295, 297, 298, 311, 327, 336, 361, 362, 379, 380], "clone_t": 68, "alter": 68, "unpad": [68, 130, 364, 380], "two": [68, 70, 141, 146, 224, 286, 298, 309, 310, 350, 364, 373, 389, 392, 395, 396, 397, 400, 401, 402, 403], "adjust": [68, 330, 403], "necessari": [68, 390, 396, 400], "target": [68, 129, 130, 271, 390, 402], "l1_memory_config": [68, 363, 389, 395, 401, 404], "remov": [69, 366, 380], "success": [69, 398, 403], "concat_t": 70, "group": [70, 74, 75, 76, 147, 267, 268, 269, 270, 395, 398, 402], "partit": 70, "altern": [70, 224, 399, 400, 404], "recombin": 70, "residu": 70, "concat_bw_t": 71, "30": [71, 98, 137, 311, 397, 399, 402], "conj_t": 72, "conjug": 72, "conj_bw_t": 73, "conv1d_t": 74, "signal": [74, 75, 113, 145], "compos": [74, 75, 76, 113, 145, 366, 400, 402], "sever": [74, 75, 76, 113, 145, 401, 402], "plane": [74, 75, 76, 113, 145], "input_length": 74, "weight_tensor": [74, 75, 76, 113, 269, 270, 398, 402], "out_channel": [74, 75, 76, 267, 268, 269, 270, 398, 402], "in_channel": [74, 75, 76, 267, 268, 269, 270, 398, 402], "kernel_height": [74, 75, 269, 270], "kernel_width": [74, 75, 269, 270], "bias_tensor": [74, 75, 76, 113, 398, 402], "batch_siz": [74, 75, 76, 97, 145, 228, 267, 268, 269, 270, 299, 370, 373, 389, 398, 400, 401, 402], "length": [74, 266, 371, 372], "kernel_s": [74, 75, 76, 228, 267, 268, 269, 270, 398, 402], "convolv": [74, 75, 76, 228, 267, 268, 269, 270], "cross": [74, 75, 76, 267, 268, 269, 270], "correl": [74, 75, 76, 267, 268, 269, 270, 390, 403], "side": [74, 75, 76, 267, 268, 269, 270, 394], "pad_length": 74, "pad_left": [74, 75, 76, 267, 268, 269, 270], "pad_right": [74, 75, 76, 267, 268, 269, 270], "connect": [74, 75, 76, 267, 268, 269, 270, 400, 402, 403], "conv_config": [74, 75, 76, 267, 268, 269, 270, 398, 402], "compute_config": [74, 75, 76, 267, 268, 269, 270], "return_output_dim": [74, 75, 76], "return_weights_and_bia": [74, 75, 76], "bias": [74, 179, 389, 400, 401, 402], "conv2d_t": 75, "inform": [75, 137, 281, 392, 402, 403], "tech": 75, "input_height": [75, 76, 267, 268, 269, 270, 398, 402], "input_width": [75, 76, 267, 268, 269, 270, 398, 402], "pad_height": [75, 76, 267, 268, 269, 270], "pad_width": [75, 76, 267, 268, 269, 270], "pad_top": [75, 76, 267, 268, 269, 270], "pad_bottom": [75, 76, 267, 268, 269, 270], "slice_config": 75, "conv_transpose2d_t": 76, "seen": [76, 81], "fraction": 76, "deconvolut": 76, "o": [76, 392, 394, 400, 401, 402], "k_h": 76, "k_w": 76, "equat": 76, "h_out": 76, "h_in": 76, "output_pad": 76, "w_out": 76, "w_in": 76, "mirror_kernel": 76, "mirror": [76, 226, 230, 231, 344, 349, 382], "been": [76, 235, 236, 311, 352, 390, 403], "cos_t": 77, "cos_bw_t": 78, "_golden_function_co": 78, "cosh_t": 79, "cosh_bw_t": 80, "corerangeset": [81, 308, 310, 367, 379], "shardstrategi": 81, "shardorient": [81, 308, 310], "use_height_and_width_as_shard_shap": 81, "travers": 81, "math": [81, 147, 394], "320": 81, "cumprod_t": 82, "reverse_ord": [82, 83], "cumul": [82, 83], "_1": [82, 83], "time": [82, 170, 235, 295, 311, 390, 394, 401, 403, 404], "_2": [82, 83], "desir": [82, 83, 99, 121, 122, 129, 130, 134, 364, 365, 366], "cast": [82, 83], "accumul": [82, 83], "begin": [82, 83, 89, 170, 247, 394, 399], "tensor_input": [82, 83], "tensor_output": [82, 83], "With": [82, 83, 396], "preallocated_output": [82, 83], "cumsum_t": 83, "pycapsul": [84, 133, 280, 363], "resourc": [84, 403], "whose": [84, 99], "forc": [84, 314], "deg2rad_t": 85, "deg2rad_bw_t": 86, "_golden_function_deg2rad": 86, "degre": [86, 274], "radian": [86, 274], "digamma_t": 87, "digamma_bw_t": 88, "_golden_function_digamma": 88, "div_t": 89, "_golden_function_div": 89, "_mode": 89, "accurate_mod": 89, "div_bw_t": 90, "pcc": [90, 161, 182, 277, 390, 393, 403], "degrad": [90, 161, 182, 277, 398, 399, 400, 401, 402], "div_no_nan_t": 91, "_golden_function_div_no_nan": 91, "div_no_nan_bw_t": 92, "denomin": [92, 276], "dump": [93, 235, 403, 404], "file_nam": [93, 193, 404], "save": [93, 329], "tensorbin": [93, 193], "elu_t": 94, "_golden_function_elu": 94, "elu_bw_t": 95, "embedding_t": 96, "retriev": 96, "word": 96, "padding_idx": 96, "token": [96, 177, 337, 371, 372], "embeddings_typ": 96, "embeddingstyp": 96, "106445": 96, "988281": 96, "59375": [96, 399], "212891": 96, "964844": 96, "199219": 96, "996094": 96, "78362e": 96, "38": [96, 395, 397, 401], "89785e": 96, "39": [96, 401], "04479e": 96, "25815e": 96, "71833e": 96, "59995e": 96, "60398e": 96, "83671e": 96, "22242e": 96, "88263e": 96, "35917e": 96, "49994e": 96, "embedding_backward": 97, "embedding_bw_t": 97, "extract": [97, 137, 402], "vocabulari": 97, "output_gradient_tensor": 97, "seq_len": [97, 116], "embedding_dim": 97, "num_embed": 97, "1024": [97, 402], "4096": 97, "3200": 97, "input_index": 97, "randint": [97, 311], "weights_shap": 97, "weights_ttnn": 97, "grad_shap": 97, "grad_data": 97, "empty_t": 98, "uniniti": [98, 99], "bfloat_8": 98, "21": [98, 402], "67": 98, "empty_like_t": 99, "87": [99, 400, 402], "45": [99, 353, 354], "22": [99, 392, 398, 399, 400, 401, 402], "60": [99, 137], "75": [99, 133], "25": [99, 364, 394, 402], "eq_t": 100, "eq__t": 101, "_golden_function_eq_": 101, "input_a": [101, 140, 149, 167, 184, 220, 226, 230, 231, 244, 344, 349, 382], "input_b": [101, 140, 149, 167, 184, 220, 244], "eqz_t": 102, "_tensor_i": [102, 144, 150, 189, 210, 221, 248], "erf_t": 103, "fast_and_approximate_mod": [103, 105, 109, 142, 194, 195, 197, 199, 315, 316, 355, 357], "fast": [103, 105, 109, 142, 194, 195, 197, 199, 315, 316], "erf_bw_t": 104, "_golden_function_erf": 104, "erfc_t": 105, "erfc_bw_t": 106, "erfinv_t": 107, "erfinv_bw_t": 108, "_golden_function_erfinv": 108, "exp_t": 109, "exp2_t": 110, "exp2_bw_t": 111, "_golden_function_exp2": 111, "exp_bw_t": 112, "_golden_function_exp": 112, "exponenti": [112, 316], "conv3d_t": 113, "3d": 113, "d": [113, 392], "kd": 113, "kh": [113, 398], "kw": [113, 398], "c_in": 113, "c_out": 113, "conv3dconfig": 113, "dropout_t": 114, "rng": 114, "probabl": 114, "averag": [114, 145, 394], "total_elem": 114, "124": 114, "prob": 114, "gelu_bw_t": [115, 143], "_golden_function_gelu": [115, 143], "algorithm": [115, 147, 316, 381], "rotary_embedding_t": 116, "rotari": 116, "cos_cach": 116, "sin_cach": 116, "token_idx": 116, "assum": [116, 130, 352, 381], "head_dim": 116, "cod_cach": 116, "token_index": 116, "expm1_t": 117, "expm1_bw_t": 118, "_golden_function_expm1": 118, "fill_t": 119, "_golden_function_fil": 119, "fill_valu": [119, 135, 136, 395, 397, 399], "fill_bw_t": 120, "fill_ones_rm_t": 121, "val_hi": [121, 122], "val_lo": [121, 122], "count": [121, 122, 147, 179, 394], "ye": [121, 122], "hone": [121, 122], "high": [121, 122, 275, 392, 395, 397, 399, 403, 404], "region": [121, 122, 254], "wone": [121, 122], "fill_rm_t": 122, "nchw": [122, 402], "hw": 122, "hfill": 122, "wfill": 122, "hi": 122, "lo": 122, "low": [122, 275], "fill_zero_bw_t": 123, "_golden_function_fill_zero": 123, "floor_t": 124, "floor_bw_t": 125, "_golden_function_floor": 125, "floor_div_t": 126, "_golden_function_floor_div": 126, "fmod_t": 127, "_golden_function_fmod": 127, "fmod_bw_t": 128, "padded_shap": [129, 257], "pad_valu": [129, 134, 258, 298, 327, 362], "target_layout": [129, 130], "target_mem_config": [129, 130], "padded_tensor": 129, "output_mem_config": [129, 130], "unpadded_tensor": 130, "frac_t": 131, "_golden_function_frac": [131, 132], "frac_bw_t": 132, "param": [133, 256, 363], "tensor_on_devic": [133, 363], "tensor_on_host": [133, 363], "365234": 133, "130859": 133, "itself": 134, "twice": [134, 394], "purpos": [134, 390, 393, 395, 403], "now": [134, 336, 366, 381, 395, 397, 401], "mapper": 134, "cq_id": [134, 352, 366], "command": [134, 352, 366, 394, 404], "queue": [134, 352, 366], "full_t": 135, "_golden_function_ful": 135, "filled_tensor": [135, 136], "full_like_t": 136, "templat": [136, 253, 388, 394], "gather_t": 137, "except": [137, 224, 309, 310, 401], "exce": 137, "do": [137, 390], "sparse_grad": 137, "spars": [137, 337], "20": [137, 234, 256, 311, 330, 331, 394, 402, 403], "40": [137, 228], "50": 137, "70": 137, "80": [137, 182, 402], "index_tensor": 137, "input_tensor_ttnn": [137, 336], "index_tensor_ttnn": 137, "gathered_tensor": 137, "gcd_t": 138, "_golden_function_gcd": 138, "greatest": 138, "2147483648": 138, "tensorint32default": [138, 180], "ge_t": 139, "ge__t": 140, "_golden_function_ge_": 140, "geglu_t": 141, "_golden_function_geglu": 141, "part": [141, 146, 286, 350, 390, 394, 401], "system": [141, 146, 276, 286, 291, 292, 350, 392, 394, 403], "gelu_t": 142, "gez_t": 144, "global_avg_pool2d_t": 145, "golden_global_avg_pool2d": 145, "adapt": 145, "entir": [145, 352, 403], "typic": [145, 179, 308, 395, 402], "glu_t": 146, "_golden_function_glu": 146, "group_norm_t": 147, "_postprocess_golden_function_output": [147, 256], "num_group": 147, "inplac": [147, 209, 211, 213, 215], "num_out_block": 147, "negative_mask": 147, "use_welford": 147, "groupnorm": 147, "tradition": 147, "slightli": 147, "form": [147, 381], "determine_expected_group_norm_sharded_config_and_grid_s": 147, "create_group_norm_input_mask": 147, "mask": [147, 368, 369, 372, 398, 399, 400, 401, 402, 403], "create_group_norm_weight_bias_rm": 147, "properli": [147, 394, 398], "cb": [147, 394], "overlap": 147, "fact": 147, "rm": [147, 299, 392], "welford": 147, "fp32": 147, "4d": [147, 328, 367], "draw": 147, "upon": [147, 390], "rather": 147, "random": [147, 258, 275, 398, 400, 402], "torch_input_tensor": [147, 297, 404], "torch_weight": 147, "torch_bia": 147, "nn": [147, 235, 236, 391, 396, 397, 399, 400, 402], "view": [147, 298, 394, 400, 402, 403, 404], "sharded_mem_config": [147, 308, 310], "grid_siz": 147, "num_channel": 147, "input_nhw": 147, "is_height_shard": 147, "is_row_major": 147, "block_wt": 147, "As": [147, 397, 401], "input_mask_tensor": 147, "everi": [147, 271, 390, 394, 399, 401, 403, 404], "16": [147, 327, 395, 399, 401, 402], "half": 147, "32x32": [147, 395, 399], "num_cores_across_channel": 147, "explain": [147, 394], "suppli": 147, "isn": [147, 395], "Then": [147, 373, 389, 392, 403], "tiles_per_core_tot": 147, "num_cores_x": [147, 389, 401], "gamma_t": 147, "beta_t": 147, "assert_with_pcc": [147, 389], "9999": [147, 389, 403, 404], "480": 147, "input_tensor_row_major": 147, "input_tensor_til": 147, "tilize_with_zero_pad": 147, "use_multicor": [147, 256, 361, 362, 379, 380], "width_per_group": 147, "max_tiles_group_can_span": 147, "values_per_chunk": 147, "values_per_chunk_per_til": 147, "gamma_beta": 147, "gt_t": 148, "gt__t": 149, "_golden_function_gt_": 149, "gtz_t": 150, "hardshrink_t": 151, "_golden_function_hardshrink": 151, "lambd": [151, 152, 332, 333], "hardshrink_bw_t": 152, "hardsigmoid_t": 153, "hardsigmoid_bw_t": 154, "_golden_function_hardsigmoid": 154, "hardswish_t": 155, "hardswish_bw_t": 156, "_golden_function_hardswish": 156, "hardtanh_t": 157, "_golden_function_hardtanh": 157, "min_val": 157, "max_val": 157, "hardtanh_bw_t": 158, "heaviside_t": 159, "_golden_function_heavisid": 159, "hypot_t": 160, "_golden_function_hypot": 160, "hypot_bw_t": 161, "i0_t": 162, "i0_bw_t": 163, "_golden_function_i0": 163, "identity_t": 164, "sfpu": 164, "shouldn": 164, "sinc": 164, "lower": [164, 256, 275], "uint8": 164, "float16": 164, "imag_t": 165, "imag_bw_t": 166, "imaginari": 166, "indexed_fill_t": 167, "replac": [167, 359], "denot": 167, "batch_id": 167, "is_imag_t": 168, "is_real_t": 169, "isclose_t": 170, "_golden_function_isclos": 170, "leq": 170, "atol": 170, "rtol": 170, "otherwis": [170, 271, 352, 367, 383, 401, 404], "rel": 170, "toler": 170, "05f": 170, "absolut": [170, 178], "08f": 170, "equal_nan": 170, "nan": 170, "treat": [170, 224, 308], "isfinite_t": 171, "isinf_t": 172, "isnan_t": 173, "isneginf_t": 174, "isposinf_t": 175, "fill_cache_for_user__t": 176, "popul": [176, 235, 394], "batch_index": 176, "update_cache_for_token__t": 177, "update_index": 177, "batch_offset": 177, "l1_loss_t": 178, "_golden_function_l1_loss": 178, "input_refer": [178, 238], "input_predict": [178, 238], "layer_norm_t": 179, "programconfig": [179, 299], "layer": [179, 299, 394, 398, 400, 402], "pre_all_gath": 179, "bf16": 179, "unshard": [179, 299, 310, 395], "cannot": [179, 299], "tile_height": 179, "tile_width": [179, 299, 309, 310], "stick": 179, "lcm_t": 180, "_golden_function_lcm": 180, "least": [180, 224, 394], "32767": 180, "32768": 180, "ldexp_t": 181, "ldexp_bw_t": 182, "outsid": 182, "le_t": 183, "le__t": 184, "_golden_function_le_": 184, "leaky_relu_t": 185, "_golden_function_leaky_relu": 185, "negative_slop": [185, 186], "slope": 185, "leaki": 185, "leaky_relu_bw_t": 186, "01": [186, 403], "lerp_t": 187, "_golden_function_lerp": 187, "point": [187, 275, 314, 330, 394, 395], "interpol": 187, "lerp_bw_t": 188, "lez_t": 189, "lgamma_t": 190, "lgamma_bw_t": 191, "_golden_function_lgamma": 191, "linear_t": 192, "transpose_a": [192, 224, 225], "transpose_b": [192, 224, 225], "behaviour": [192, 224], "128": [192, 224, 337, 398, 399, 400, 401, 402, 404], "log_t": 194, "log10_t": 195, "log10_bw_t": 196, "log1p_t": 197, "1e7": 197, "log1p_bw_t": 198, "log2_t": 199, "log2_bw_t": 200, "log_bw_t": 201, "_golden_function_log": 201, "logarithm": [201, 241], "log_sigmoid_t": 202, "log_sigmoid_bw_t": 203, "_golden_function_log_sigmoid": 203, "logaddexp_t": 204, "logaddexp2_t": 205, "logaddexp2_bw_t": 206, "logaddexp_bw_t": 207, "logical_and_t": 208, "_golden_function_logical_and": 208, "AND": [208, 209], "use_legaci": 208, "logical_and__t": 209, "logical_not_t": 210, "logical_not__t": 211, "_golden_function_logical_not_": 211, "logical_or_t": 212, "_golden_function_logical_or": 212, "OR": [212, 213, 298], "logical_or__t": 213, "logical_xor_t": 214, "_golden_function_logical_xor": 214, "land": [214, 215], "lnot": [214, 215], "lor": [214, 215], "logical_xor__t": 215, "xor": 215, "logit_t": 216, "_golden_function_logit": [216, 217], "logit_bw_t": 217, "logiteps_bw_t": 218, "logitep": 218, "lt_t": 219, "lt__t": 220, "_golden_function_lt_": 220, "ltz_t": 221, "mac_t": 222, "_golden_function_mac": 222, "context": [223, 284, 285], "manag": [223, 284, 285, 390, 392, 402], "exit": 223, "occur": 223, "matmul_t": 224, "dimension": [224, 241, 395], "dot": [224, 371, 372], "although": 224, "combin": [224, 397, 400], "variou": [224, 399], "abov": [224, 392, 395], "criteria": 224, "messag": [224, 394, 402, 403], "unexpect": 224, "obviou": 224, "relat": 224, "swap": 224, "j": [224, 328, 329], "implicitli": 224, "extend": 224, "patch": 224, "leverag": 224, "accord": [224, 258, 295, 311, 337], "those": [224, 394], "n_size": 224, "m_size": 224, "k_size": 224, "though": 224, "carefulli": 224, "fix": [224, 398, 403], "problem": 224, "describ": [224, 390], "matmul_batched_weights_t": 225, "max_t": 226, "computekernelconfig": [226, 230, 231, 344, 349, 382], "origin": [226, 230, 231, 235, 236, 257, 271, 336, 344, 349, 382, 390, 393, 401], "bessel": [226, 230, 231, 344, 349, 382], "nd": [226, 230, 231, 344, 349, 382], "max_bw_t": 227, "max_pool2d_t": 228, "golden_maxpool2d": 228, "window": [228, 371, 372, 398, 402], "nhw": 228, "scheme": [228, 267, 268, 269, 270], "input_h": [228, 402], "input_w": [228, 402], "ceil_mod": [228, 402], "applied_shard_schem": 228, "deallocate_input": 228, "return_indic": 228, "createdevic": 228, "l1_small_siz": [228, 254, 398, 402], "8192": [228, 367, 398, 402], "kernel_w": 228, "stride_h": 228, "stride_w": 228, "dilation_h": 228, "dilation_w": 228, "nchw_shape": 228, "in_n": 228, "in_c": 228, "in_h": 228, "in_w": 228, "input_perm": 228, "input_reshap": 228, "tt_input": 228, "tt_input_dev": 228, "tt_output": [228, 309, 310], "in_place_halo": 228, "maximum_t": 229, "_golden_function_maximum": 229, "tensorsupport": 229, "16777216": 229, "mean_t": 230, "min_t": 231, "min_bw_t": 232, "minimum_t": 233, "_golden_function_minimum": 233, "mish_t": 234, "inf": [234, 240], "initialize_model": [235, 236, 389], "model_nam": [235, 236, 389], "convert_to_ttnn": [235, 236, 389], "custom_preprocessor": [235, 236, 389], "dict": [235, 236, 402], "parameterdict": [235, 236], "prefix": [235, 236], "run_model": 235, "reader_patterns_cach": 235, "disabl": [235, 236, 314, 398, 399, 400, 401, 402, 403, 404], "git": [235, 236, 392], "doesn": [235, 236], "invalid": [235, 236], "preprocessor": [235, 236], "put": [235, 236, 389, 398, 399, 400, 401, 402], "submodul": [235, 236, 392, 400, 402], "appear": [235, 236, 403], "ttnn_module_arg": 235, "tmp": [235, 403], "model_graph": 235, "svg": [235, 404], "recomput": [235, 298], "moreh_sum_t": 237, "mse_loss_t": 238, "_golden_function_mse_loss": 238, "mul_bw_t": 239, "multigammaln_t": 240, "multigammaln_bw_t": 241, "_golden_function_mvlgamma": 241, "multivari": 241, "mvlgamma": 241, "5f": 241, "multiply_t": 242, "ne_t": 243, "ne__t": 244, "_golden_function_ne_": 244, "neg_t": 245, "neg_bw_t": 246, "_golden_function_neg": 246, "nextafter_t": 247, "_golden_function_nextaft": 247, "_float": 247, "neq": 247, "nez_t": 248, "nonzero_t": 249, "well": [249, 337, 392, 393], "normalize_global_t": 250, "_golden_function_normalize_glob": 250, "normalize_hw_t": 251, "_golden_function_normalize_hw": 251, "ones_t": 252, "ones_like_t": 253, "trace_region_s": 254, "dispatch_core_config": 254, "dispatchcoreconfig": 254, "0x7f52689675f0": 254, "worker_l1_s": 254, "default_l1_small_s": 254, "default_trace_region_s": 254, "allocat": 254, "dispatch_core_typ": 254, "dispatchcoretyp": 254, "dispatch": [254, 394], "0x7fbac5bfc1b0": 254, "outer_t": 255, "_golden_function_out": 255, "otim": 255, "pad_t": 256, "_preprocess_golden_function_input": 256, "locat": [256, 367, 391, 394, 396, 403, 404], "mutual": [256, 271], "output_tensor_shap": [256, 362], "input_tensor_start": 256, "union": 256, "pad_input": 256, "assert": [256, 401, 404], "unpadded_shap": 257, "annot": [257, 390], "fixeds": 257, "permute_t": 258, "nullopt": [258, 402], "tthe": 258, "broken": 258, "garbag": 258, "polar_t": 259, "cartesian": 259, "theta": 259, "polar_bw_t": 260, "polygamma_t": 261, "_golden_function_polygamma": 261, "decim": [261, 300, 314], "polygamma_bw_t": 262, "polyval_t": 263, "_golden_function_polyv": 263, "coeffici": [263, 390], "coeff": 263, "sum_": [263, 299, 328, 329], "polynomi": 263, "pow_t": 264, "_golden_function_pow": 264, "expon": [264, 265, 302, 303, 395], "pow_bw_t": 265, "power": [265, 378, 403], "prelu_t": 266, "_golden_function_prelu": 266, "arrai": [266, 399], "invoc": [267, 268, 269, 270], "exact": [267, 268, 269, 270, 395], "input_memory_config": [267, 268, 269, 270], "input_layout": [267, 268, 269, 270], "output_dtyp": [267, 268, 269, 270], "convtranspose2d": [268, 269], "conv_tranpose2d": 269, "weights_format": [269, 270], "iohw": 269, "has_bia": [269, 270], "term": [269, 270, 393], "oihw": 270, "prod_t": 271, "squeez": [271, 366], "nich": 271, "nc": 271, "definit": 271, "intend": [271, 393], "output_all_dim": 271, "being": [271, 284, 285, 394, 395], "prod_bw_t": 272, "particular": [272, 389, 404], "taken": [272, 383], "all_dims_output": 272, "rad2deg_t": 273, "rad2deg_bw_t": 274, "_golden_function_rad2deg": 274, "rand_t": 275, "uniform": [275, 395], "upper": [275, 395], "reproduc": [275, 398], "rdiv_t": 276, "_golden_function_rdiv": 276, "consid": 276, "numer": [276, 308, 309, 310, 328, 329, 330, 391, 395], "rounding_mod": 276, "rdiv_bw_t": 277, "real_t": 278, "real_bw_t": 279, "new_tensor": 280, "my_memory_config": 280, "reciprocal_t": 281, "inaccur": [281, 395], "fp": 281, "reciprocal_bw_t": 282, "_golden_function_reciproc": 282, "reduce_scatter_t": 283, "break": [283, 400, 402], "apart": 283, "reglu_t": 286, "_golden_function_reglu": 286, "relu_t": 287, "relu6_t": 288, "relu6_bw_t": 289, "_golden_function_relu6": 289, "relu_bw_t": 290, "_golden_function_relu": 290, "relu_max_t": 291, "_golden_function_relu_max": 291, "upper_limit": 291, "cap": 291, "relu_min_t": 292, "_golden_function_relu_min": 292, "lower_limit": 292, "remainder_t": 293, "_golden_function_remaind": 293, "modulu": 293, "remainder_bw_t": 294, "whb0": 294, "repeat_t": 295, "repetit": [295, 297], "repetition_vector": 295, "smallvector": 295, "repeat_bw_t": 296, "repeat_interleave_t": 297, "he": 297, "expand": [297, 395, 399], "torch_result": 297, "reshape_t": 298, "cost": 298, "new_shap": 298, "kwtype": 298, "recreate_mapping_tensor": 298, "allevi": 298, "slow": 298, "rms_norm_t": 299, "root": [299, 305, 339, 368, 369, 400, 402, 403], "round_t": 300, "_golden_function_round": [300, 301], "No": [300, 398, 400], "round_bw_t": 301, "rpow_t": 302, "_golden_function_rpow": 302, "posit": [302, 371, 372], "rpow_bw_t": 303, "rsqrt_t": 304, "rsqrt_bw_t": 305, "_golden_function_rsqrt": 305, "rsub_t": 306, "_golden_function_rsub": 306, "rsub_bw_t": 307, "subract": 307, "revers": 307, "scale_causal_mask_hw_dims_softmax_in_place_t": 308, "numeric_st": [308, 309, 310, 328, 329, 401], "causal": [308, 309, 310, 368, 369, 371], "d_k": 308, "stabl": [308, 309, 310, 328, 329, 336, 397], "hw_dims_onli": 308, "compute_grid_s": [308, 309, 310], "num_cores_r": [308, 309, 310], "384": [308, 309, 310, 389, 401], "768": [308, 309, 310, 401], "attention_mask_t": [308, 309, 310], "input_til": 308, "grid_coord": [308, 310], "shard_grid": [308, 310], "corerang": [308, 310], "shard_shap": [308, 310], "shard_spec": [308, 310], "shardspec": [308, 310], "buffertyp": [308, 310], "input_shard": [308, 310], "tt_output_shard": 308, "scale_mask_softmax_t": 309, "is_causal_mask": [309, 310], "commonli": [309, 310], "mechan": [309, 310], "inherit": [309, 328], "fuse_head": [309, 310], "scale_mask_softmax_in_place_t": 310, "restrict": [310, 392], "scatter_t": 311, "onto": 311, "src": 311, "destin": 311, "amax": 311, "amin": 311, "info": [311, 336, 397, 398, 399, 400, 401, 402, 403], "input_torch": 311, "index_torch": 311, "int64": 311, "source_torch": 311, "input_ttnn": 311, "index_ttnn": 311, "source_ttnn": 311, "selu_t": 312, "_golden_function_selu": [312, 313], "0507": 312, "67326": 312, "selu_bw_t": 313, "sci_mod": 314, "scientif": 314, "notat": 314, "detect": 314, "digit": [314, 400], "short": [314, 404], "sigmoid_t": 315, "vector_mod": 315, "rc": [315, 392], "sigmoid_accurate_t": 316, "accur": 316, "sigmoid_bw_t": 317, "_golden_function_sigmoid": 317, "sign_t": 318, "sign_bw_t": 319, "signbit_t": 320, "silu_t": 321, "silu_bw_t": 322, "_golden_function_silu": 322, "sin_t": 323, "sin_bw_t": 324, "sinh_t": 325, "sinh_bw_t": 326, "slice_t": 327, "slice_start": 327, "input_tensor_shap": 327, "slice_end": 327, "slice_step": 327, "unmodifi": 327, "undefin": 327, "softmax_t": 328, "x_i": [328, 329], "x_j": [328, 329], "softmax_in_place_t": 329, "consum": 329, "90": [329, 402], "softplus_t": 330, "steep": 330, "higher": [330, 394, 395], "steeper": 330, "approach": [330, 389, 393, 404], "hard": 330, "stabil": [330, 393], "softplus_bw_t": 331, "softshrink_t": 332, "_golden_function_softshrink": 332, "softshrink_bw_t": 333, "softsign_t": 334, "softsign_bw_t": 335, "sort_t": 336, "ascend": 336, "descend": 336, "preserv": 336, "sorted_tensor": 336, "sorted_tensor_desc": 336, "indices_desc": 336, "input_tensor_tnn": 336, "input_tensor_2d": 336, "input_tensor_2d_ttnn": 336, "sorted_tensor_dim": 336, "indices_dim": 336, "sparse_matmul_t": 337, "nnz": 337, "is_input_a_spars": 337, "is_input_b_spars": 337, "expert": 337, "skip": [337, 372], "512": 337, "hidden": 337, "expert_weight": 337, "bitmask": 337, "sparsity_bitmask": 337, "simplifi": 337, "goe": 337, "sqrt_t": 338, "sqrt_bw_t": 339, "_golden_function_sqrt": 339, "square_t": 340, "255": 340, "square_bw_t": 341, "_golden_function_squar": 341, "squared_difference_t": 342, "_golden_function_squared_differ": 342, "squared_difference_bw_t": 343, "std_t": 344, "sub_bw_t": 345, "subalpha_t": 346, "_golden_function_subalpha": 346, "subalpha_bw_t": 347, "subtract_t": 348, "sum_t": 349, "swiglu_t": 350, "_golden_function_swiglu": 350, "swish_t": 351, "queueid": 352, "synchron": [352, 403, 404], "wait": [352, 394], "complet": [352, 396, 397, 398, 399, 400, 401, 402, 403], "associ": [352, 403], "ran": [352, 394, 401], "chip": [352, 398, 399, 400, 401, 402, 403], "set_sub_device_stall_group": 352, "queu": 352, "tan_t": 353, "tan_bw_t": 354, "_golden_function_tan": 354, "tanh_t": 355, "faster": [355, 357, 394, 401], "minor": [355, 357], "approx": [355, 357], "tanh_bw_t": 356, "_golden_function_tanh": 356, "tanhshrink_t": 357, "_golden_function_tanhshrink": [357, 358], "tanhshrink_bw_t": 358, "threshold_t": 359, "_golden_function_threshold": [359, 360], "threshold_bw_t": 360, "tilize_t": 361, "_nop_golden_funct": 361, "acceler": [361, 362, 379, 380, 392, 400], "tilize_with_val_padding_t": 362, "800781": 363, "455078": 363, "585938": 363, "to_layout_t": 364, "organ": [364, 390, 395], "becom": [364, 403], "42188": 364, "398438": 364, "to_memory_config_t": 365, "torch_rank": [366, 404], "Will": 366, "reach": 366, "mesh_compos": 366, "cppmeshtotensor": 366, "torch_tensor": [366, 399], "3008": 366, "8438": [366, 399], "3242": 366, "9023": 366, "5820": 366, "5312": 366, "topk_t": 367, "_create_golden_function_topk": 367, "largest": [367, 395], "sub_core_grid": [367, 379], "indices_tensor": 367, "smallest": 367, "sure": [367, 404], "top": 367, "bfloat8": 367, "output_value_tensor": 367, "output_index_tensor": 367, "fundament": 367, "manipul": [367, 400], "restor": 367, "afterward": 367, "satisfi": 367, "nearest": [367, 381], "65536": 367, "topk_valu": 367, "topk_indic": 367, "attention_softmax_t": 368, "head_siz": [368, 369, 370, 373, 401], "attention_mask": [368, 369, 401], "causal_mask": [368, 369], "attention_softmax__t": 369, "concatenate_heads_t": 370, "num_head": [370, 373, 401], "sequence_s": [370, 373, 389, 401], "scaled_dot_product_attention_t": 371, "mimick": 371, "flashattent": 371, "accept": [371, 372, 390, 393, 402], "sdpaprogramconfig": [371, 372], "q": [371, 372], "parallel": [371, 372, 394, 399, 403], "nqh": 371, "input_tensor_q": [371, 372], "dh": [371, 372], "input_tensor_k": [371, 372], "nkv": [371, 372], "input_tensor_v": [371, 372], "attn_mask": [371, 372], "impli": 371, "is_caus": [371, 372], "sliding_window_s": [371, 372], "slide": [371, 372, 398, 402], "attend": 371, "center": 371, "scaled_dot_product_attention_decode_t": 372, "decod": 372, "flash": [372, 392], "mqa": 372, "sdpamulticoreprogramconfig": 372, "nh": 372, "cur_po": 372, "cur_pos_tensor": 372, "pnh": 372, "split_query_key_value_and_split_heads_t": 373, "hidden_s": [373, 389, 401], "readi": [373, 390], "score": 373, "kv_input_tensor": 373, "q1": 373, "k1": 373, "v1": 373, "qn": 373, "kn": 373, "vn": 373, "cat": [373, 401], "num_kv_head": 373, "contigu": [373, 395, 402], "transpose_kei": 373, "num": 373, "tril_t": 374, "diagon": [374, 375], "triu_t": 375, "trunc_t": 376, "_golden_function_trunc": [376, 377], "trunc_bw_t": 377, "unary_chain_t": 378, "ops_chain": 378, "eltwiseunarywithparam": 378, "chain": 378, "untilize_t": 379, "use_pack_until": [379, 380], "untilize_with_unpadding_t": 380, "output_tensor_end": 380, "upsample_t": 381, "scale_factor": 381, "array2d": 381, "var_t": 382, "where_t": 383, "_golden_function_wher": 383, "true_valu": 383, "false_valu": 383, "entri": 383, "where_bw_t": 384, "xlogy_t": 385, "_golden_function_xlogi": 385, "xlogy_bw_t": 386, "zeros_t": 387, "zeros_like_t": 388, "basi": 389, "rewritten": 389, "bert": 389, "modeling_bert": 389, "bertintermedi": 389, "__init__": 389, "super": 389, "dens": 389, "intermediate_s": 389, "hidden_st": [389, 401], "tdd": 389, "pytest": [389, 390, 394, 403, 404], "torch_bert": 389, "utility_funct": 389, "torch_random": 389, "utils_for_test": 389, "mark": [389, 390], "parametr": 389, "phiyodr": 389, "finetun": 389, "squad2": 389, "test_bert_intermedi": 389, "manual_se": [389, 398, 401, 402, 404], "bertconfig": 389, "from_pretrain": 389, "eval": 389, "torch_hidden_st": [389, 401], "torch_output": [389, 401], "bert_intermedi": 389, "dictionari": [389, 402], "turn": 389, "ttnn_bert": 389, "999": [389, 400], "someth": 389, "ttnn_optimized_bert": 389, "isinst": 389, "preprocess_linear_weight": [389, 401], "preprocess_linear_bia": [389, 401], "ff1_weight": 389, "ff1_bia": 389, "best": 389, "integr": [389, 390], "incredibli": 390, "excit": 390, "exploratori": 390, "folder": [390, 394], "freedom": 390, "showcas": 390, "few": [390, 395, 402], "question": 390, "answer": 390, "highlight": [390, 395], "successfulli": [390, 397], "migrat": [390, 404], "good": 390, "readm": [390, 392], "md": [390, 392], "credit": 390, "author": 390, "might": 390, "encount": 390, "demonstr": [390, 394, 397, 399, 400, 402], "adequ": 390, "pearson": 390, "ci": 390, "pipelin": [390, 394, 400], "unit": [390, 392], "metric": 390, "meet": 390, "continu": [390, 392, 393], "commit": 390, "ongo": 390, "complianc": 390, "catch": 390, "regress": 390, "earli": 390, "varieti": 390, "instruct": [390, 391, 392, 394, 396, 404], "measur": 390, "run_device_perf_model": 390, "run_perform": 390, "sh": [390, 392, 394, 403, 404], "models_device_performance_bare_met": 390, "clear": [390, 393, 398, 399, 400, 401, 402], "autom": 390, "extern": [390, 393, 395], "servic": 390, "workflow": [390, 392, 403], "impl": 390, "yaml": 390, "models_performance_bare_met": 390, "run_demos_single_card_n150_test": 390, "run_demos_single_card_n300_test": 390, "run_t3000_demo_test": 390, "test_ttnn_functional_resnet50": 390, "resnet50testinfra": 390, "friendli": 391, "ml": [391, 392], "http": [391, 392, 403], "com": [391, 392], "guid": [391, 392, 396, 398, 403], "choic": 391, "jupyt": 391, "notebook": 391, "comprehens": [392, 403], "stack": 392, "deploy": 392, "find": [392, 403], "asset": 392, "tag": 392, "quick": [392, 402], "download": [392, 400, 402], "curl": 392, "fssl": 392, "chmod": 392, "podman": 392, "warn": [392, 394, 398, 399, 400, 401, 402], "galaxi": 392, "6u": 392, "blackhol": 392, "driver": [392, 398, 399, 400, 401, 402, 403], "kmd": [392, 398, 399, 400, 401, 402, 403], "firmwar": 392, "smi": 392, "ubuntu": 392, "04": [392, 399], "v2": 392, "fw_pack": 392, "18": 392, "fwbundl": 392, "v18": 392, "v3": 392, "28": [392, 400], "17": [392, 402], "fw": [392, 394, 398, 399, 400, 401, 402], "34": [392, 398, 399, 400, 401, 402, 403], "visit": [392, 403], "compon": 392, "immedi": 392, "ai": [392, 398, 399, 400, 401, 402], "closer": 392, "conveni": 392, "who": [392, 395], "linux": 392, "distro": 392, "glibc": 392, "newer": 392, "pip": [392, 403, 404], "cpu": [392, 394, 398, 399, 400, 401, 402], "governor": 392, "export": [392, 403, 404], "pythonpath": 392, "pwd": 392, "python_env": [392, 403], "dev": 392, "txt": [392, 403], "sudo": [392, 394], "apt": 392, "cpufrequtil": 392, "cpupow": 392, "frequenc": 392, "registri": 392, "pull": [392, 393, 398], "ghcr": 392, "io": [392, 394], "amd64": 392, "bash": 392, "recurs": 392, "simplest": 392, "install_depend": 392, "build_met": [392, 394, 403], "cmake": 392, "mkdir": 392, "cd": [392, 394], "ninja": 392, "dcmake_build_typ": 392, "relwithdebuginfo": 392, "dcmake_cxx_compil": 392, "compil": [392, 400, 401, 404], "envirion": 392, "python_env_dir": 392, "path_to_your_env_directori": 392, "create_venv": 392, "bin": 392, "driven": [392, 393], "recip": 392, "conda": 392, "forg": 392, "python3": [392, 396, 397, 398, 399, 400, 401, 402], "run_op_on_devic": 392, "eth": 392, "rout": 392, "loudbox": 392, "quietbox": 392, "iommu": [392, 398, 399, 400, 401, 402, 403], "level": [392, 394, 397, 399, 403], "isol": 392, "passthrough": 392, "On": 392, "translat": 392, "viommu": 392, "hypervisor": 392, "secur": 392, "dma": 392, "pcie": 392, "guest": 392, "corrupt": 392, "reliabl": [392, 393], "intel_iommu": 392, "amd_iommu": 392, "provis": 392, "remap": 392, "intel": 392, "vt": 392, "amd": 392, "vi": [392, 403], "maintain": 393, "simultan": [393, 403], "tune": 393, "themselv": [393, 395], "goal": 393, "ask": 393, "popular": 393, "kent": 393, "beck": 393, "submit": 393, "label": [393, 395, 400, 402], "fulli": [393, 394, 400, 402], "fallback": 393, "branch": 393, "brief": 393, "4730": 393, "rst": 393, "referenc": 393, "sweep": 393, "codeown": 393, "pr": 393, "reflect": 393, "merg": 393, "main": [393, 397, 398, 399, 400, 401, 402, 403, 404], "comment": 393, "resnet": 394, "tt_metal_hom": [394, 397, 398, 399, 400, 401, 402], "traci": [394, 403], "test_perf_resnet": 394, "test_perf_bare_met": 394, "0185": 394, "finish": 394, "csv": [394, 403], "consol": 394, "similar": [394, 403], "give": [394, 403], "shorter": 394, "append": 394, "cli": 394, "reset": 394, "tt_smi": 394, "tensix_reset": 394, "tensix": [394, 399], "skew": 394, "timer": 394, "reboot": 394, "wh": 394, "analyz": [394, 403], "1000": [394, 398, 399, 400, 401, 402], "fixtur": 394, "ttl": 394, "readdeviceprofil": 394, "drop": 394, "around": 394, "120": [394, 402], "eighth": 394, "receiv": 394, "mention": 394, "flow": [394, 403], "come": 394, "python_fallback": 394, "tt_dnn_cpu": 394, "tt_dnn_devic": 394, "global": [394, 403], "fidel": 394, "field": 394, "lofi": 394, "hifi2": 394, "hifi3": 394, "clock": 394, "stamp": 394, "durat": [394, 401, 404], "nanosecond": 394, "end_t": 394, "start_t": 394, "cycl": 394, "earliest": 394, "core_frequ": 394, "marker": 394, "brisc": 394, "ncrisc": 394, "trisc0": 394, "trisc1": 394, "trisc2": 394, "front": 394, "spent": [394, 401], "cb_wait_front": 394, "reserv": 394, "cb_reserve_back": 394, "datamov": 394, "input_0_memori": 394, "z": 394, "channels_last": 394, "dev_0_dram": 394, "dec_0_l1": 394, "noc": 394, "timelin": 394, "npe": 394, "subdirectori": 394, "npe_viz": 394, "traffic": 394, "congest": 394, "item": [394, 400, 402], "aggreg": 394, "timestamp": [394, 403], "ops_perf_results_2025_06_25_14_04_34": 394, "2025_06_25_14_04_34": 394, "actual": [395, 400, 402], "4x4": 395, "still": 395, "transit": 395, "2x2": 395, "illustr": 395, "insid": [395, 404], "16x16": 395, "per": [395, 403], "li": 395, "fashion": 395, "face0": 395, "face1": 395, "face2": 395, "face3": 395, "pictur": 395, "reason": 395, "engin": 395, "matric": 395, "decompos": 395, "transpose_til": 395, "col": 395, "torch_t": 395, "byte": 395, "That": 395, "sizeof": 395, "introduc": 395, "observ": [395, 402], "magnitud": [395, 401], "flush": 395, "instabl": 395, "extrem": 395, "infin": 395, "domin": 395, "caus": 395, "lose": 395, "7014118346046923e": 395, "frequent": 395, "occurr": 395, "deal": 395, "critic": 395, "applic": 395, "homogen": 395, "unsuit": 395, "inher": 395, "owned_host_storag": 395, "borrowed_host_storag": 395, "borrow": 395, "numpi": [395, 399, 400], "device_storag": 395, "abstract": 395, "awai": 395, "compress": 395, "learn": [395, 397, 399], "remain": 395, "128x128": 395, "subset": 395, "know": 395, "understand": [395, 403], "coordin": 395, "physic": 395, "task": [396, 400], "smoothli": 396, "lightweight": 396, "minim": 396, "standalon": 396, "basic_python": [396, 397, 398, 399, 400, 401, 402], "llama": 397, "mistral": 397, "diffus": 397, "ttnn_add_tensor": 397, "loguru": [397, 398, 399, 400, 401, 402], "logger": [397, 398, 399, 400, 401, 402], "tt_tensor1": 397, "tt_tensor2": 397, "tt_result": 397, "2025": [397, 398, 399, 400, 401, 402, 403], "06": [397, 401], "23": 397, "09": [397, 398, 403], "36": 397, "58": 397, "211": 397, "__main__": [397, 398, 399, 400, 401, 402], "29": [397, 399], "37": 397, "00": [397, 400, 402], "524": 397, "525": 397, "ttnn_basic_conv": 398, "state": 398, "8kb": 398, "enough": [398, 402], "32kb": 398, "bchw": 398, "permuted_input": 398, "flat": 398, "reshaped_input": 398, "out_torch": 398, "f": [398, 399, 400, 401, 402, 404], "everyth": [398, 399, 400, 401, 402, 403], "07": [398, 399, 400, 401, 402], "02": [398, 403], "649": 398, "silicondriv": [398, 399, 400, 401, 402, 403], "pci": [398, 399, 400, 401, 402, 403], "pci_devic": [398, 399, 400, 401, 402, 403], "198": [398, 399, 400, 401, 402], "651": 398, "658": 398, "tt_cluster": [398, 399, 400, 401, 402, 403], "190": [398, 399, 400, 401, 402], "659": 398, "666": 398, "667": 398, "673": 398, "harvest": [398, 399, 400, 401, 402, 403], "0x100": [398, 399, 400, 401, 402], "noc0": [398, 399, 400, 401, 402, 403], "0x0": [398, 399, 400, 401, 402, 403], "282": [398, 399, 400, 401, 402], "772": 398, "817": 398, "remot": [398, 399, 400, 401, 402, 403], "147": [398, 399, 400, 401, 402], "828": 398, "ethernet": [398, 399, 400, 401, 402], "1039": [398, 399, 400, 401, 402], "915": 398, "clk": [398, 399, 400, 401, 402], "mhz": [398, 399, 400, 401, 402], "metal_context": [398, 399, 400, 401, 402], "487": 398, "428": [398, 399, 400, 401, 402], "489": 398, "unabl": [398, 399, 400, 401, 402], "thread": [398, 399, 400, 401, 402, 403, 404], "hardware_command_queu": [398, 399, 400, 401, 402], "74": [398, 399, 400, 401, 402], "921": 398, "reprocess": 398, "563": 398, "922": 398, "582": 398, "390": 398, "78": 398, "488": [398, 399, 400, 401, 402], "391": 398, "468": [398, 399, 400, 401, 402], "783": [398, 399, 400, 401, 402], "392": 398, "ttnn_basic_oper": 399, "np": [399, 400], "host_rand": 399, "helper": 399, "to_tt_til": 399, "tt_t1": 399, "transfer": 399, "tt_t2": 399, "tt_t3": 399, "tt_t4": 399, "t5": 399, "tt_t5": 399, "add_result": 399, "mul_result": 399, "mul": [399, 401], "matmul_result": 399, "bmatrix": 399, "rightarrow": 399, "broadcast_vector": 399, "broadcast_tt": 399, "broadcast_add_result": 399, "850": 399, "852": [399, 400], "859": 399, "860": 399, "866": 399, "867": 399, "873": 399, "970": 399, "015": 399, "025": 399, "111": 399, "678": 399, "680": 399, "537": 399, "564": 399, "47": 399, "08": [399, 403], "072": 399, "49": 399, "82812": 399, "04688": 399, "32812": 399, "00781": 399, "39844": 399, "03906": 399, "14844": 399, "24219": 399, "65625": 399, "31250": 399, "21094": 399, "21875": 399, "33594": 399, "37500": 399, "62500": 399, "670": 399, "52": 399, "12500": 399, "23438": 399, "96875": 399, "02600": 399, "97656": 399, "18164": 399, "87891": 399, "44531": 399, "48438": 399, "50781": 399, "35938": 399, "229": 399, "55": [399, 401], "50000": 399, "25000": 399, "56250": 399, "43750": 399, "57": 399, "231": 399, "59": 399, "233": 399, "63": 399, "8242": 399, "0469": 399, "2500": 399, "0000": 399, "3750": 399, "3945": 399, "0391": 399, "5625": 399, "1250": 399, "2188": 399, "8750": 399, "4375": 399, "7500": 399, "6250": 399, "7422": 399, "1484": 399, "9531": 399, "5000": 399, "6562": 399, "3281": 399, "0938": 399, "2158": 399, "3359": 399, "234": 399, "insight": [400, 402, 403], "ttnn_mlp_inference_mnist": 400, "essenti": 400, "classif": [400, 402], "perceptron": 400, "torchvis": [400, 402], "disk": [400, 402, 403, 404], "throughout": [400, 402, 403], "statu": 400, "predict": [400, 402], "backend": 400, "outcom": 400, "28x28": 400, "grayscal": 400, "dataload": [400, 402], "iter": [400, 401], "through": [400, 402, 403], "totensor": [400, 402], "testset": [400, 402], "testload": [400, 402], "shuffl": [400, 402], "train_and_export_mlp": 400, "poor": [400, 402], "mlp_mnist_weight": 400, "pt": [400, 402], "w1": 400, "b1": 400, "w2": 400, "b2": 400, "w3": [400, 402], "b3": [400, 402], "correctli": 400, "snippet": [400, 404], "five": [400, 402], "counter": [400, 402], "sequenti": 400, "three": 400, "raw": 400, "enumer": [400, 402], "_layout": 400, "image_tt": 400, "1x128": 400, "w1_final": 400, "b1_final": 400, "out1": 400, "w2_final": 400, "b2_final": 400, "out2": 400, "w3_final": 400, "b3_final": 400, "out3": 400, "predicted_label": [400, 402], "ntt": [400, 402], "100": [400, 402], "2f": [400, 402], "03": 400, "41": 400, "990": 400, "992": 400, "998": 400, "006": 400, "007": 400, "013": 400, "110": 400, "172": 400, "182": [400, 402], "268": 400, "886": 400, "888": 400, "44": 400, "48": 400, "677": 400, "682": 400, "686": 400, "690": 400, "695": 400, "89": [400, 402], "696": 400, "697": 400, "six": 401, "similarli": 401, "Be": 401, "multi_head_attent": 401, "query_weight": 401, "query_bia": 401, "key_weight": 401, "key_bia": 401, "value_weight": 401, "value_bia": 401, "output_weight": 401, "output_bia": 401, "fallback_reshap": 401, "get_fallback_funct": [401, 404], "attention_scor": 401, "attention_prob": 401, "context_lay": 401, "self_output": 401, "torch_attention_mask": 401, "torch_query_weight": 401, "torch_query_bia": 401, "torch_key_weight": 401, "torch_key_bia": 401, "torch_value_weight": 401, "torch_value_bia": 401, "torch_output_weight": 401, "torch_output_bia": 401, "fly": 401, "fortun": 401, "ahead": 401, "data_typ": 401, "optimized_multi_head_attent": 401, "fused_qkv_weight": 401, "fused_qkv_bia": 401, "self_output_weight": 401, "self_output_bia": 401, "fused_qkv_output": 401, "context_layer_after_concatenate_head": 401, "qkv": 401, "torch_qkv_weight": 401, "torch_qkv_bia": 401, "qkv_weight": 401, "qkv_bia": 401, "optimized_output": 401, "torch_optimized_output": 401, "allclos": 401, "ttnn_multihead_attent": 401, "769": 401, "776": [401, 403], "777": 401, "784": 401, "790": 401, "887": 401, "931": 401, "942": 401, "027": [401, 402], "603": 401, "605": 401, "51": [401, 403], "001": 401, "132": [401, 402], "265338897705078": 401, "056": 401, "151": [401, 402], "05480194091796875": 401, "363": 401, "259": 401, "2866740226745605": 401, "366": 401, "274": 401, "002416849136352539": 401, "417": 401, "418": 401, "460": 401, "pixel": 402, "scratchpad": 402, "kb": 402, "cifar10": 402, "train_and_export_cnn": 402, "simple_cnn_cifar10_weight": 402, "conv1": 402, "conv2": 402, "fc1": 402, "fc2": 402, "conv_pool_stag": 402, "encapsul": 402, "undergo": 402, "metadata": 402, "sizegur": 402, "again": 402, "record": 402, "modular": 402, "flexibl": [402, 403], "input_nhwc": 402, "conv_outchannel": 402, "weight_str": 402, "bias_str": 402, "log_first_sampl": 402, "conv_kernel_s": 402, "conv_strid": 402, "conv_pad": 402, "conv1_out": 402, "max_pool2d_kernel_s": 402, "max_pool2d_strid": 402, "max_pool2d_pad": 402, "max_pool2d_dil": 402, "max_pool2d_out": 402, "simplecnn": 402, "rearrang": 402, "obtain": 402, "ttnn_imag": 402, "ttnn_image_permu": 402, "log_thi": 402, "conv1_pool": 402, "conv2_pool": 402, "fc": 402, "out_flat": 402, "w4": 402, "b4": 402, "w3_tt": 402, "b3_tt": 402, "x_tt": 402, "w4_tt": 402, "b4_tt": 402, "ttnn_simplecnn_infer": 402, "041": 402, "043": 402, "050": 402, "051": 402, "057": 402, "058": 402, "064": 402, "161": 402, "224": 402, "235": 402, "321": 402, "889": 402, "891": 402, "19": 402, "734": 402, "471": 402, "075": 402, "86": 402, "88": 402, "076": 402, "91": 402, "92": 402, "93": 402, "1x1": 402, "94": 402, "95": 402, "96": [402, 404], "97": 402, "98": 402, "99": 402, "enable_split_read": 402, "enable_subblock_pad": 402, "101": 402, "960": 402, "129": 402, "130": 402, "131": 402, "961": 402, "133": 402, "134": 402, "135": 402, "136": 402, "137": 402, "138": 402, "139": 402, "026": 402, "157": [402, 403], "158": 402, "121": 402, "669": 402, "238": 402, "166": 402, "181": 402, "240": 402, "183": 402, "ll": 403, "offer": 403, "intuit": 403, "depth": 403, "searchabl": 403, "plot": 403, "placement": 403, "peak": 403, "hierarch": 403, "server": 403, "ssh": 403, "instanc": 403, "opportun": 403, "watch": 403, "walkthrough": 403, "video": 403, "offlin": 403, "launch": 403, "localhost": 403, "8000": 403, "chrome": 403, "greet": 403, "homepag": 403, "yolov4": 403, "320x320": 403, "coco": 403, "predefin": 403, "wrap": 403, "ttnn_config_path": [403, 404], "inlin": 403, "ttnn_config_overrid": [403, 404], "past": 403, "enable_fast_runtime_mod": [403, 404], "enable_log": [403, 404], "report_nam": [403, 404], "ttnn_visualizer_tutori": 403, "enable_graph_report": [403, 404], "enable_detailed_buffer_report": [403, 404], "enable_detailed_tensor_report": [403, 404], "enable_comparison_mod": [403, 404], "free": 403, "test_ttnn_yolov4": 403, "test_yolov4": 403, "pretrained_weight_tru": 403, "At": 403, "664": 403, "73": 403, "665": 403, "83": 403, "cache_path": 403, "model_cache_path": 403, "tmp_dir": 403, "enable_model_cach": 403, "throw_exception_on_fallback": 403, "comparison_mode_should_raise_except": 403, "comparison_mode_pcc": [403, 404], "root_report_path": 403, "4042956046390500517": 403, "754": 403, "197": 403, "758": 403, "192": 403, "761": 403, "764": 403, "0x80": 403, "295": 403, "836": 403, "navig": 403, "json": [403, 404], "db": 403, "sqlite": 403, "termin": 403, "session": 403, "unset": 403, "regener": 403, "Or": 403, "731": 403, "process_ops_log": 403, "generate_report": 403, "905": 403, "2025_08_01_10_51_02": 403, "ops_perf_results_2025_08_01_10_51_02": 403, "diredtori": 403, "ops_perf_results_": 403, "device_profile_log": 403, "bottom": 403, "filter": 403, "click": 403, "inspect": 403, "breakdown": 403, "relationship": 403, "easi": 403, "candid": 403, "tabl": 403, "chart": 403, "lifetim": 403, "estim": 403, "headroom": 403, "pinpoint": 403, "ineffici": 403, "node": 403, "edg": 403, "zoom": 403, "pan": 403, "subnetwork": 403, "flop": 403, "underutil": 403, "toggl": 403, "hint": 403, "suboptim": 403, "summar": 403, "deepli": 403, "architectur": 404, "2024": 404, "torch_input_tensor_a": 404, "torch_input_tensor_b": 404, "matmul_output_tensor": 404, "torch_matmul_output_tensor": 404, "unlik": 404, "start_tim": 404, "end_tim": 404, "stdout": 404, "6391518115997314": 404, "0007393360137939453": 404, "manage_config": 404, "9998": 404, "construct": 404, "tracer": 404, "exp_trac": 404, "miss": 404, "tt_logger_typ": 404, "tt_logger_level": 404, "substitut": 404, "implementaiton": 404, "unless": 404, "addition": 404, "app": 404, "pre_hook_to_print_args_and_kwarg": 404, "post_hook_to_print_output": 404, "query_registered_oper": 404, "begin_graph_captur": 404, "runmod": 404, "no_dispatch": 404, "captured_graph": 404, "end_graph_captur": 404, "pretty_print": 404}, "objects": {"ttnn": [[6, 0, 1, "", "Conv2dConfig"], [7, 0, 1, "", "Conv2dSliceConfig"], [8, 3, 1, "", "GetDefaultDevice"], [9, 0, 1, "", "MatmulMultiCoreReuseMultiCast1DProgramConfig"], [10, 0, 1, "", "MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig"], [11, 0, 1, "", "MatmulMultiCoreReuseMultiCastProgramConfig"], [12, 0, 1, "", "MatmulMultiCoreReuseProgramConfig"], [13, 3, 1, "", "SetDefaultDevice"], [395, 0, 1, "", "Shape"], [14, 0, 1, "", "SoftmaxDefaultProgramConfig"], [15, 0, 1, "", "SoftmaxProgramConfig"], [16, 0, 1, "", "SoftmaxShardedMultiCoreProgramConfig"], [17, 5, 1, "", "abs"], [18, 5, 1, "", "abs_bw"], [19, 5, 1, "", "acos"], [20, 5, 1, "", "acos_bw"], [21, 5, 1, "", "acosh"], [22, 5, 1, "", "acosh_bw"], [23, 5, 1, "", "add"], [24, 5, 1, "", "add_bw"], [25, 5, 1, "", "addalpha"], [26, 5, 1, "", "addalpha_bw"], [27, 5, 1, "", "addcdiv"], [28, 5, 1, "", "addcdiv_bw"], [29, 5, 1, "", "addcmul"], [30, 5, 1, "", "addcmul_bw"], [31, 5, 1, "", "addmm"], [32, 5, 1, "", "all_gather"], [33, 5, 1, "", "all_reduce"], [34, 5, 1, "", "alt_complex_rotate90"], [35, 5, 1, "", "angle"], [36, 5, 1, "", "angle_bw"], [37, 5, 1, "", "arange"], [38, 5, 1, "", "argmax"], [39, 5, 1, "", "as_tensor"], [40, 5, 1, "", "asin"], [41, 5, 1, "", "asin_bw"], [42, 5, 1, "", "asinh"], [43, 5, 1, "", "asinh_bw"], [44, 5, 1, "", "assign_bw"], [45, 5, 1, "", "atan"], [46, 5, 1, "", "atan2"], [47, 5, 1, "", "atan2_bw"], [48, 5, 1, "", "atan_bw"], [49, 5, 1, "", "atanh"], [50, 5, 1, "", "atanh_bw"], [51, 5, 1, "", "batch_norm"], [52, 5, 1, "", "bias_gelu_bw"], [53, 5, 1, "", "bitwise_and"], [54, 5, 1, "", "bitwise_left_shift"], [55, 5, 1, "", "bitwise_not"], [56, 5, 1, "", "bitwise_or"], [57, 5, 1, "", "bitwise_right_shift"], [58, 5, 1, "", "bitwise_xor"], [59, 5, 1, "", "cbrt"], [60, 5, 1, "", "ceil"], [61, 5, 1, "", "ceil_bw"], [62, 5, 1, "", "celu"], [63, 5, 1, "", "celu_bw"], [64, 5, 1, "", "clamp"], [65, 5, 1, "", "clamp_bw"], [66, 5, 1, "", "clip"], [67, 5, 1, "", "clip_bw"], [68, 5, 1, "", "clone"], [69, 3, 1, "", "close_device"], [70, 5, 1, "", "concat"], [71, 5, 1, "", "concat_bw"], [72, 5, 1, "", "conj"], [73, 5, 1, "", "conj_bw"], [74, 5, 1, "", "conv1d"], [75, 5, 1, "", "conv2d"], [76, 5, 1, "", "conv_transpose2d"], [77, 5, 1, "", "cos"], [78, 5, 1, "", "cos_bw"], [79, 5, 1, "", "cosh"], [80, 5, 1, "", "cosh_bw"], [81, 3, 1, "", "create_sharded_memory_config"], [82, 5, 1, "", "cumprod"], [83, 5, 1, "", "cumsum"], [84, 5, 1, "", "deallocate"], [85, 5, 1, "", "deg2rad"], [86, 5, 1, "", "deg2rad_bw"], [87, 5, 1, "", "digamma"], [88, 5, 1, "", "digamma_bw"], [89, 5, 1, "", "div"], [90, 5, 1, "", "div_bw"], [91, 5, 1, "", "div_no_nan"], [92, 5, 1, "", "div_no_nan_bw"], [93, 5, 1, "", "dump_tensor"], [94, 5, 1, "", "elu"], [95, 5, 1, "", "elu_bw"], [96, 5, 1, "", "embedding"], [97, 5, 1, "", "embedding_bw"], [98, 5, 1, "", "empty"], [99, 5, 1, "", "empty_like"], [100, 5, 1, "", "eq"], [101, 5, 1, "", "eq_"], [102, 5, 1, "", "eqz"], [103, 5, 1, "", "erf"], [104, 5, 1, "", "erf_bw"], [105, 5, 1, "", "erfc"], [106, 5, 1, "", "erfc_bw"], [107, 5, 1, "", "erfinv"], [108, 5, 1, "", "erfinv_bw"], [109, 5, 1, "", "exp"], [110, 5, 1, "", "exp2"], [111, 5, 1, "", "exp2_bw"], [112, 5, 1, "", "exp_bw"], [117, 5, 1, "", "expm1"], [118, 5, 1, "", "expm1_bw"], [119, 5, 1, "", "fill"], [120, 5, 1, "", "fill_bw"], [121, 5, 1, "", "fill_ones_rm"], [122, 5, 1, "", "fill_rm"], [123, 5, 1, "", "fill_zero_bw"], [124, 5, 1, "", "floor"], [125, 5, 1, "", "floor_bw"], [126, 5, 1, "", "floor_div"], [127, 5, 1, "", "fmod"], [128, 5, 1, "", "fmod_bw"], [129, 3, 1, "", "format_input_tensor"], [130, 3, 1, "", "format_output_tensor"], [131, 5, 1, "", "frac"], [132, 5, 1, "", "frac_bw"], [133, 5, 1, "", "from_device"], [134, 5, 1, "", "from_torch"], [135, 5, 1, "", "full"], [136, 5, 1, "", "full_like"], [137, 5, 1, "", "gather"], [138, 5, 1, "", "gcd"], [139, 5, 1, "", "ge"], [140, 5, 1, "", "ge_"], [141, 5, 1, "", "geglu"], [142, 5, 1, "", "gelu"], [143, 5, 1, "", "gelu_bw"], [144, 5, 1, "", "gez"], [145, 5, 1, "", "global_avg_pool2d"], [146, 5, 1, "", "glu"], [147, 5, 1, "", "group_norm"], [148, 5, 1, "", "gt"], [149, 5, 1, "", "gt_"], [150, 5, 1, "", "gtz"], [151, 5, 1, "", "hardshrink"], [152, 5, 1, "", "hardshrink_bw"], [153, 5, 1, "", "hardsigmoid"], [154, 5, 1, "", "hardsigmoid_bw"], [155, 5, 1, "", "hardswish"], [156, 5, 1, "", "hardswish_bw"], [157, 5, 1, "", "hardtanh"], [158, 5, 1, "", "hardtanh_bw"], [159, 5, 1, "", "heaviside"], [160, 5, 1, "", "hypot"], [161, 5, 1, "", "hypot_bw"], [162, 5, 1, "", "i0"], [163, 5, 1, "", "i0_bw"], [164, 5, 1, "", "identity"], [165, 5, 1, "", "imag"], [166, 5, 1, "", "imag_bw"], [167, 5, 1, "", "indexed_fill"], [168, 5, 1, "", "is_imag"], [169, 5, 1, "", "is_real"], [170, 5, 1, "", "isclose"], [171, 5, 1, "", "isfinite"], [172, 5, 1, "", "isinf"], [173, 5, 1, "", "isnan"], [174, 5, 1, "", "isneginf"], [175, 5, 1, "", "isposinf"], [178, 5, 1, "", "l1_loss"], [179, 5, 1, "", "layer_norm"], [180, 5, 1, "", "lcm"], [181, 5, 1, "", "ldexp"], [182, 5, 1, "", "ldexp_bw"], [183, 5, 1, "", "le"], [184, 5, 1, "", "le_"], [185, 5, 1, "", "leaky_relu"], [186, 5, 1, "", "leaky_relu_bw"], [187, 5, 1, "", "lerp"], [188, 5, 1, "", "lerp_bw"], [189, 5, 1, "", "lez"], [190, 5, 1, "", "lgamma"], [191, 5, 1, "", "lgamma_bw"], [192, 5, 1, "", "linear"], [193, 5, 1, "", "load_tensor"], [194, 5, 1, "", "log"], [195, 5, 1, "", "log10"], [196, 5, 1, "", "log10_bw"], [197, 5, 1, "", "log1p"], [198, 5, 1, "", "log1p_bw"], [199, 5, 1, "", "log2"], [200, 5, 1, "", "log2_bw"], [201, 5, 1, "", "log_bw"], [202, 5, 1, "", "log_sigmoid"], [203, 5, 1, "", "log_sigmoid_bw"], [204, 5, 1, "", "logaddexp"], [205, 5, 1, "", "logaddexp2"], [206, 5, 1, "", "logaddexp2_bw"], [207, 5, 1, "", "logaddexp_bw"], [208, 5, 1, "", "logical_and"], [209, 5, 1, "", "logical_and_"], [210, 5, 1, "", "logical_not"], [211, 5, 1, "", "logical_not_"], [212, 5, 1, "", "logical_or"], [213, 5, 1, "", "logical_or_"], [214, 5, 1, "", "logical_xor"], [215, 5, 1, "", "logical_xor_"], [216, 5, 1, "", "logit"], [217, 5, 1, "", "logit_bw"], [218, 5, 1, "", "logiteps_bw"], [219, 5, 1, "", "lt"], [220, 5, 1, "", "lt_"], [221, 5, 1, "", "ltz"], [222, 5, 1, "", "mac"], [223, 3, 1, "", "manage_device"], [224, 5, 1, "", "matmul"], [225, 5, 1, "", "matmul_batched_weights"], [226, 5, 1, "", "max"], [227, 5, 1, "", "max_bw"], [228, 5, 1, "", "max_pool2d"], [229, 5, 1, "", "maximum"], [230, 5, 1, "", "mean"], [231, 5, 1, "", "min"], [232, 5, 1, "", "min_bw"], [233, 5, 1, "", "minimum"], [234, 5, 1, "", "mish"], [237, 5, 1, "", "moreh_sum"], [238, 5, 1, "", "mse_loss"], [239, 5, 1, "", "mul_bw"], [240, 5, 1, "", "multigammaln"], [241, 5, 1, "", "multigammaln_bw"], [242, 5, 1, "", "multiply"], [243, 5, 1, "", "ne"], [244, 5, 1, "", "ne_"], [245, 5, 1, "", "neg"], [246, 5, 1, "", "neg_bw"], [247, 5, 1, "", "nextafter"], [248, 5, 1, "", "nez"], [249, 5, 1, "", "nonzero"], [250, 5, 1, "", "normalize_global"], [251, 5, 1, "", "normalize_hw"], [252, 5, 1, "", "ones"], [253, 5, 1, "", "ones_like"], [254, 3, 1, "", "open_device"], [255, 5, 1, "", "outer"], [256, 5, 1, "", "pad"], [257, 3, 1, "", "pad_to_tile_shape"], [258, 5, 1, "", "permute"], [259, 5, 1, "", "polar"], [260, 5, 1, "", "polar_bw"], [261, 5, 1, "", "polygamma"], [262, 5, 1, "", "polygamma_bw"], [263, 5, 1, "", "polyval"], [264, 5, 1, "", "pow"], [265, 5, 1, "", "pow_bw"], [266, 5, 1, "", "prelu"], [267, 3, 1, "", "prepare_conv_bias"], [268, 3, 1, "", "prepare_conv_transpose2d_bias"], [269, 3, 1, "", "prepare_conv_transpose2d_weights"], [270, 3, 1, "", "prepare_conv_weights"], [271, 5, 1, "", "prod"], [272, 5, 1, "", "prod_bw"], [273, 5, 1, "", "rad2deg"], [274, 5, 1, "", "rad2deg_bw"], [275, 5, 1, "", "rand"], [276, 5, 1, "", "rdiv"], [277, 5, 1, "", "rdiv_bw"], [278, 5, 1, "", "real"], [279, 5, 1, "", "real_bw"], [280, 5, 1, "", "reallocate"], [281, 5, 1, "", "reciprocal"], [282, 5, 1, "", "reciprocal_bw"], [283, 5, 1, "", "reduce_scatter"], [284, 3, 1, "", "register_post_operation_hook"], [285, 3, 1, "", "register_pre_operation_hook"], [286, 5, 1, "", "reglu"], [287, 5, 1, "", "relu"], [288, 5, 1, "", "relu6"], [289, 5, 1, "", "relu6_bw"], [290, 5, 1, "", "relu_bw"], [291, 5, 1, "", "relu_max"], [292, 5, 1, "", "relu_min"], [293, 5, 1, "", "remainder"], [294, 5, 1, "", "remainder_bw"], [295, 5, 1, "", "repeat"], [296, 5, 1, "", "repeat_bw"], [297, 5, 1, "", "repeat_interleave"], [298, 5, 1, "", "reshape"], [299, 5, 1, "", "rms_norm"], [300, 5, 1, "", "round"], [301, 5, 1, "", "round_bw"], [302, 5, 1, "", "rpow"], [303, 5, 1, "", "rpow_bw"], [304, 5, 1, "", "rsqrt"], [305, 5, 1, "", "rsqrt_bw"], [306, 5, 1, "", "rsub"], [307, 5, 1, "", "rsub_bw"], [308, 5, 1, "", "scale_causal_mask_hw_dims_softmax_in_place"], [309, 5, 1, "", "scale_mask_softmax"], [310, 5, 1, "", "scale_mask_softmax_in_place"], [311, 5, 1, "", "scatter"], [312, 5, 1, "", "selu"], [313, 5, 1, "", "selu_bw"], [314, 3, 1, "", "set_printoptions"], [315, 5, 1, "", "sigmoid"], [316, 5, 1, "", "sigmoid_accurate"], [317, 5, 1, "", "sigmoid_bw"], [318, 5, 1, "", "sign"], [319, 5, 1, "", "sign_bw"], [320, 5, 1, "", "signbit"], [321, 5, 1, "", "silu"], [322, 5, 1, "", "silu_bw"], [323, 5, 1, "", "sin"], [324, 5, 1, "", "sin_bw"], [325, 5, 1, "", "sinh"], [326, 5, 1, "", "sinh_bw"], [327, 5, 1, "", "slice"], [328, 5, 1, "", "softmax"], [329, 5, 1, "", "softmax_in_place"], [330, 5, 1, "", "softplus"], [331, 5, 1, "", "softplus_bw"], [332, 5, 1, "", "softshrink"], [333, 5, 1, "", "softshrink_bw"], [334, 5, 1, "", "softsign"], [335, 5, 1, "", "softsign_bw"], [336, 5, 1, "", "sort"], [337, 5, 1, "", "sparse_matmul"], [338, 5, 1, "", "sqrt"], [339, 5, 1, "", "sqrt_bw"], [340, 5, 1, "", "square"], [341, 5, 1, "", "square_bw"], [342, 5, 1, "", "squared_difference"], [343, 5, 1, "", "squared_difference_bw"], [344, 5, 1, "", "std"], [345, 5, 1, "", "sub_bw"], [346, 5, 1, "", "subalpha"], [347, 5, 1, "", "subalpha_bw"], [348, 5, 1, "", "subtract"], [349, 5, 1, "", "sum"], [350, 5, 1, "", "swiglu"], [351, 5, 1, "", "swish"], [352, 3, 1, "", "synchronize_device"], [353, 5, 1, "", "tan"], [354, 5, 1, "", "tan_bw"], [355, 5, 1, "", "tanh"], [356, 5, 1, "", "tanh_bw"], [357, 5, 1, "", "tanhshrink"], [358, 5, 1, "", "tanhshrink_bw"], [359, 5, 1, "", "threshold"], [360, 5, 1, "", "threshold_bw"], [361, 5, 1, "", "tilize"], [362, 5, 1, "", "tilize_with_val_padding"], [363, 5, 1, "", "to_device"], [364, 5, 1, "", "to_layout"], [365, 5, 1, "", "to_memory_config"], [366, 5, 1, "", "to_torch"], [367, 5, 1, "", "topk"], [374, 5, 1, "", "tril"], [375, 5, 1, "", "triu"], [376, 5, 1, "", "trunc"], [377, 5, 1, "", "trunc_bw"], [378, 5, 1, "", "unary_chain"], [379, 5, 1, "", "untilize"], [380, 5, 1, "", "untilize_with_unpadding"], [381, 5, 1, "", "upsample"], [382, 5, 1, "", "var"], [383, 5, 1, "", "where"], [384, 5, 1, "", "where_bw"], [385, 5, 1, "", "xlogy"], [386, 5, 1, "", "xlogy_bw"], [387, 5, 1, "", "zeros"], [388, 5, 1, "", "zeros_like"]], "ttnn.Conv2dConfig": [[6, 1, 1, "", "act_block_h_override"], [6, 1, 1, "", "act_block_w_div"], [6, 1, 1, "", "activation"], [6, 1, 1, "", "config_tensors_in_dram"], [6, 1, 1, "", "core_grid"], [6, 1, 1, "", "deallocate_activation"], [6, 1, 1, "", "enable_act_double_buffer"], [6, 1, 1, "", "enable_activation_reuse"], [6, 1, 1, "", "enable_kernel_stride_folding"], [6, 1, 1, "", "enable_weights_double_buffer"], [6, 1, 1, "", "force_split_reader"], [6, 1, 1, "", "full_inner_dim"], [6, 1, 1, "", "in_place"], [6, 1, 1, "", "output_layout"], [6, 1, 1, "", "override_sharding_config"], [6, 1, 1, "", "reallocate_halo_output"], [6, 1, 1, "", "reshard_if_not_optimal"], [6, 1, 1, "", "shard_layout"], [6, 1, 1, "", "transpose_shards"], [6, 1, 1, "", "weights_dtype"]], "ttnn.Conv2dSliceConfig": [[7, 0, 1, "", "SliceTypeEnum"], [7, 1, 1, "", "num_slices"], [7, 1, 1, "", "slice_type"]], "ttnn.Conv2dSliceConfig.SliceTypeEnum": [[7, 2, 1, "", "DRAMSliceHeight"], [7, 2, 1, "", "DRAMSliceWidth"], [7, 2, 1, "", "L1Full"], [7, 1, 1, "", "name"], [7, 1, 1, "", "value"]], "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig": [[9, 1, 1, "", "compute_with_storage_grid_size"], [9, 4, 1, "", "from_json"], [9, 1, 1, "", "fuse_batch"], [9, 1, 1, "", "fused_activation"], [9, 1, 1, "", "gather_in0"], [9, 1, 1, "", "hop_cores"], [9, 1, 1, "", "in0_block_w"], [9, 1, 1, "", "mcast_in0"], [9, 1, 1, "", "num_global_cb_receivers"], [9, 1, 1, "", "out_block_h"], [9, 1, 1, "", "out_block_w"], [9, 1, 1, "", "out_subblock_h"], [9, 1, 1, "", "out_subblock_w"], [9, 1, 1, "", "per_core_M"], [9, 1, 1, "", "per_core_N"], [9, 4, 1, "", "to_json"], [9, 1, 1, "", "untilize_out"]], "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig": [[10, 4, 1, "", "from_json"], [10, 1, 1, "", "fused_activation"], [10, 1, 1, "", "in0_block_w"], [10, 1, 1, "", "per_core_M"], [10, 1, 1, "", "per_core_N"], [10, 4, 1, "", "to_json"]], "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig": [[11, 1, 1, "", "compute_with_storage_grid_size"], [11, 4, 1, "", "from_json"], [11, 1, 1, "", "fuse_batch"], [11, 1, 1, "", "fused_activation"], [11, 1, 1, "", "in0_block_w"], [11, 1, 1, "", "out_block_h"], [11, 1, 1, "", "out_block_w"], [11, 1, 1, "", "out_subblock_h"], [11, 1, 1, "", "out_subblock_w"], [11, 1, 1, "", "per_core_M"], [11, 1, 1, "", "per_core_N"], [11, 4, 1, "", "to_json"], [11, 1, 1, "", "transpose_mcast"]], "ttnn.MatmulMultiCoreReuseProgramConfig": [[12, 1, 1, "", "compute_with_storage_grid_size"], [12, 4, 1, "", "from_json"], [12, 1, 1, "", "in0_block_w"], [12, 1, 1, "", "out_subblock_h"], [12, 1, 1, "", "out_subblock_w"], [12, 1, 1, "", "per_core_M"], [12, 1, 1, "", "per_core_N"], [12, 4, 1, "", "to_json"]], "ttnn.Shape": [[395, 1, 1, "", "rank"], [395, 4, 1, "", "to_rank"]], "ttnn.SoftmaxShardedMultiCoreProgramConfig": [[16, 1, 1, "", "block_w"]], "ttnn.experimental": [[113, 5, 1, "", "conv3d"], [114, 5, 1, "", "dropout"], [115, 5, 1, "", "gelu_bw"], [116, 5, 1, "", "rotary_embedding"]], "ttnn.kv_cache": [[176, 5, 1, "", "fill_cache_for_user_"], [177, 5, 1, "", "update_cache_for_token_"]], "ttnn.model_preprocessing": [[235, 3, 1, "", "preprocess_model"], [236, 3, 1, "", "preprocess_model_parameters"]], "ttnn.transformer": [[368, 5, 1, "", "attention_softmax"], [369, 5, 1, "", "attention_softmax_"], [370, 5, 1, "", "concatenate_heads"], [371, 5, 1, "", "scaled_dot_product_attention"], [372, 5, 1, "", "scaled_dot_product_attention_decode"], [373, 5, 1, "", "split_query_key_value_and_split_heads"]]}, "objtypes": {"0": "py:class", "1": "py:property", "2": "py:attribute", "3": "py:function", "4": "py:method", "5": "py:data"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "property", "Python property"], "2": ["py", "attribute", "Python attribute"], "3": ["py", "function", "Python function"], "4": ["py", "method", "Python method"], "5": ["py", "data", "Python data"]}, "titleterms": {"welcom": 0, "tt": [0, 3, 4, 389, 392, 394, 401, 403, 404], "nn": [0, 3, 4, 389, 392, 394, 401, 403, 404], "document": 0, "ttnn": [0, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 403], "resourc": 0, "indic": 0, "tabl": 0, "contribut": [1, 392], "develop": 1, "support": [2, 224, 404], "report": [2, 5, 394, 403], "bug": 2, "featur": 2, "propos": 2, "request": 2, "troubleshoot": 2, "debug": [2, 404], "tip": 2, "commun": 2, "what": [3, 4], "i": [3, 4], "ad": 4, "new": [4, 393], "oper": [4, 5, 389, 394, 397, 398, 399, 403, 404], "faq": 4, "step": [4, 389, 392], "ar": [4, 392], "need": 4, "add": [4, 23, 397], "c": [4, 404], "python": [4, 404], "exampl": [4, 8, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 259, 260, 261, 262, 263, 264, 265, 266, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 364, 365, 366, 367, 374, 375, 376, 377, 378, 382, 383, 384, 385, 386, 387, 388, 389, 392, 397, 398, 399, 400, 401, 402, 404], "devic": [4, 5, 397, 398, 399, 400, 402, 404], "implement": 4, "1": [4, 308, 389, 391, 392, 404], "2": [4, 389, 391, 392, 404], "bind": 4, "option": [4, 309, 310, 392], "golden": 4, "function": [4, 393, 404], "api": [5, 395], "memori": [5, 224, 395, 403], "config": [5, 395], "core": 5, "tensor": [5, 38, 226, 230, 231, 308, 309, 310, 344, 349, 382, 395, 397, 398, 399, 403, 404], "creation": [5, 397, 399], "matrix": 5, "multipl": [5, 395], "pointwis": 5, "unari": 5, "binari": [5, 392], "ternari": 5, "loss": 5, "reduct": 5, "data": [5, 395, 400], "movement": 5, "normal": 5, "program": [5, 392, 404], "moreh": 5, "transform": [5, 368, 369, 370, 371, 372, 373], "ccl": 5, "embed": [5, 96], "convolut": [5, 398, 402], "pool": [5, 402], "vision": 5, "kv": 5, "cach": [5, 404], "model": [5, 389, 391, 392, 403], "convers": [5, 397, 399], "hook": [5, 404], "conv2dconfig": 6, "conv2dsliceconfig": 7, "getdefaultdevic": 8, "matmulmulticorereusemulticast1dprogramconfig": 9, "matmulmulticorereusemulticastdramshardedprogramconfig": 10, "matmulmulticorereusemulticastprogramconfig": 11, "matmulmulticorereuseprogramconfig": 12, "setdefaultdevic": 13, "softmaxdefaultprogramconfig": 14, "softmaxprogramconfig": 15, "softmaxshardedmulticoreprogramconfig": 16, "ab": 17, "abs_bw": 18, "aco": 19, "acos_bw": 20, "acosh": 21, "acosh_bw": 22, "add_bw": 24, "addalpha": 25, "addalpha_bw": 26, "addcdiv": 27, "addcdiv_bw": 28, "addcmul": 29, "addcmul_bw": 30, "addmm": 31, "input_tensor": [31, 147, 179, 271, 299, 367], "mat1_tensor": 31, "mat2_tensor": 31, "all_gath": 32, "all_reduc": 33, "alt_complex_rotate90": 34, "angl": 35, "angle_bw": 36, "arang": 37, "argmax": 38, "input": [38, 226, 230, 231, 308, 309, 310, 344, 349, 382, 398], "output": [38, 397, 398, 399, 400, 401, 402], "as_tensor": 39, "asin": 40, "asin_bw": 41, "asinh": 42, "asinh_bw": 43, "assign_bw": 44, "atan": 45, "atan2": 46, "atan2_bw": 47, "atan_bw": 48, "atanh": 49, "atanh_bw": 50, "batch_norm": 51, "bias_gelu_bw": 52, "bitwise_and": 53, "bitwise_left_shift": 54, "bitwise_not": 55, "bitwise_or": 56, "bitwise_right_shift": 57, "bitwise_xor": 58, "cbrt": 59, "ceil": 60, "ceil_bw": 61, "celu": 62, "celu_bw": 63, "clamp": 64, "clamp_bw": 65, "clip": 66, "clip_bw": 67, "clone": [68, 392], "close_devic": 69, "concat": 70, "concat_bw": 71, "conj": 72, "conj_bw": 73, "conv1d": 74, "conv2d": 75, "conv_transpose2d": 76, "co": 77, "cos_bw": 78, "cosh": 79, "cosh_bw": 80, "create_sharded_memory_config": 81, "cumprod": 82, "cumsum": 83, "dealloc": 84, "deg2rad": 85, "deg2rad_bw": 86, "digamma": 87, "digamma_bw": 88, "div": 89, "div_bw": 90, "div_no_nan": 91, "div_no_nan_bw": 92, "dump_tensor": 93, "elu": 94, "elu_bw": 95, "embedding_bw": 97, "empti": 98, "empty_lik": 99, "eq": 100, "eq_": 101, "eqz": 102, "erf": 103, "erf_bw": 104, "erfc": 105, "erfc_bw": 106, "erfinv": 107, "erfinv_bw": 108, "exp": 109, "exp2": 110, "exp2_bw": 111, "exp_bw": 112, "experiment": [113, 114, 115, 116], "conv3d": 113, "dropout": 114, "gelu_bw": [115, 143], "rotary_embed": 116, "expm1": 117, "expm1_bw": 118, "fill": 119, "fill_bw": 120, "fill_ones_rm": 121, "fill_rm": 122, "fill_zero_bw": 123, "floor": 124, "floor_bw": 125, "floor_div": 126, "fmod": 127, "fmod_bw": 128, "format_input_tensor": 129, "format_output_tensor": 130, "frac": 131, "frac_bw": 132, "from_devic": 133, "from_torch": 134, "full": [135, 397, 398, 399, 400, 401, 402], "full_lik": 136, "gather": 137, "gcd": 138, "ge": 139, "ge_": 140, "geglu": 141, "gelu": 142, "gez": 144, "global_avg_pool2d": 145, "glu": 146, "group_norm": 147, "weight": [147, 179, 299, 400, 402], "gamma": [147, 179, 299], "bia": [147, 179, 192, 299], "beta": [147, 179, 299], "input_mask": 147, "output_tensor": [147, 179, 271, 299], "gt": 148, "gt_": 149, "gtz": 150, "hardshrink": 151, "hardshrink_bw": 152, "hardsigmoid": 153, "hardsigmoid_bw": 154, "hardswish": 155, "hardswish_bw": 156, "hardtanh": 157, "hardtanh_bw": 158, "heavisid": 159, "hypot": 160, "hypot_bw": 161, "i0": 162, "i0_bw": 163, "ident": 164, "imag": [165, 392, 400], "imag_bw": 166, "indexed_fil": 167, "is_imag": 168, "is_real": 169, "isclos": 170, "isfinit": 171, "isinf": 172, "isnan": 173, "isneginf": 174, "isposinf": 175, "kv_cach": [176, 177], "fill_cache_for_user_": 176, "update_cache_for_token_": 177, "l1_loss": 178, "layer_norm": 179, "residual_input_tensor": [179, 299], "stat": 179, "post_all_gath": 179, "onli": [179, 392], "lcm": 180, "ldexp": 181, "ldexp_bw": 182, "le": 183, "le_": 184, "leaky_relu": 185, "leaky_relu_bw": 186, "lerp": 187, "lerp_bw": 188, "lez": 189, "lgamma": 190, "lgamma_bw": 191, "linear": 192, "input_tensor_a": [192, 224, 225, 337], "input_tensor_b": [192, 224, 337], "load_tensor": 193, "log": [194, 404], "log10": 195, "log10_bw": 196, "log1p": 197, "log1p_bw": 198, "log2": 199, "log2_bw": 200, "log_bw": 201, "log_sigmoid": 202, "log_sigmoid_bw": 203, "logaddexp": 204, "logaddexp2": 205, "logaddexp2_bw": 206, "logaddexp_bw": 207, "logical_and": 208, "logical_and_": 209, "logical_not": 210, "logical_not_": 211, "logical_or": 212, "logical_or_": 213, "logical_xor": 214, "logical_xor_": 215, "logit": 216, "logit_bw": 217, "logiteps_bw": 218, "lt": 219, "lt_": 220, "ltz": 221, "mac": 222, "manage_devic": 223, "matmul": 224, "configur": [224, 392, 401], "matmul_batched_weight": 225, "input_tensors_b": 225, "max": 226, "max_bw": 227, "max_pool2d": 228, "maximum": 229, "mean": 230, "min": 231, "min_bw": 232, "minimum": 233, "mish": 234, "model_preprocess": [235, 236], "preprocess_model": 235, "preprocess_model_paramet": 236, "moreh_sum": 237, "mse_loss": 238, "mul_bw": 239, "multigammaln": 240, "multigammaln_bw": 241, "multipli": 242, "ne": 243, "ne_": 244, "neg": 245, "neg_bw": 246, "nextaft": 247, "nez": 248, "nonzero": 249, "normalize_glob": 250, "normalize_hw": 251, "ones": 252, "ones_lik": 253, "open_devic": 254, "outer": 255, "pad": 256, "pad_to_tile_shap": 257, "permut": 258, "polar": 259, "polar_bw": 260, "polygamma": 261, "polygamma_bw": 262, "polyv": 263, "pow": 264, "pow_bw": 265, "prelu": 266, "prepare_conv_bia": 267, "prepare_conv_transpose2d_bia": 268, "prepare_conv_transpose2d_weight": 269, "prepare_conv_weight": 270, "prod": 271, "prod_bw": 272, "rad2deg": 273, "rad2deg_bw": 274, "rand": 275, "rdiv": 276, "rdiv_bw": 277, "real": 278, "real_bw": 279, "realloc": 280, "reciproc": 281, "reciprocal_bw": 282, "reduce_scatt": 283, "register_post_operation_hook": 284, "register_pre_operation_hook": 285, "reglu": 286, "relu": 287, "relu6": 288, "relu6_bw": 289, "relu_bw": 290, "relu_max": 291, "relu_min": 292, "remaind": 293, "remainder_bw": 294, "repeat": 295, "repeat_bw": 296, "repeat_interleav": 297, "reshap": 298, "rms_norm": 299, "round": 300, "round_bw": 301, "rpow": 302, "rpow_bw": 303, "rsqrt": 304, "rsqrt_bw": 305, "rsub": 306, "rsub_bw": 307, "scale_causal_mask_hw_dims_softmax_in_plac": 308, "shard": [308, 395], "mask": [308, 309, 310], "h": 308, "w": 308, "scale_mask_softmax": 309, "scale_mask_softmax_in_plac": 310, "scatter": 311, "selu": 312, "selu_bw": 313, "set_printopt": 314, "sigmoid": 315, "sigmoid_accur": 316, "sigmoid_bw": 317, "sign": 318, "sign_bw": 319, "signbit": 320, "silu": 321, "silu_bw": 322, "sin": 323, "sin_bw": 324, "sinh": 325, "sinh_bw": 326, "slice": [327, 404], "softmax": 328, "softmax_in_plac": 329, "softplu": 330, "softplus_bw": 331, "softshrink": 332, "softshrink_bw": 333, "softsign": 334, "softsign_bw": 335, "sort": 336, "sparse_matmul": 337, "sparsiti": 337, "sqrt": 338, "sqrt_bw": 339, "squar": 340, "square_bw": 341, "squared_differ": 342, "squared_difference_bw": 343, "std": 344, "sub_bw": 345, "subalpha": 346, "subalpha_bw": 347, "subtract": 348, "sum": 349, "swiglu": 350, "swish": 351, "synchronize_devic": 352, "tan": 353, "tan_bw": 354, "tanh": 355, "tanh_bw": 356, "tanhshrink": 357, "tanhshrink_bw": 358, "threshold": 359, "threshold_bw": 360, "tiliz": 361, "tilize_with_val_pad": 362, "to_devic": 363, "to_layout": 364, "to_memory_config": 365, "to_torch": 366, "topk": 367, "index_tensor": 367, "attention_softmax": 368, "attention_softmax_": 369, "concatenate_head": 370, "scaled_dot_product_attent": 371, "scaled_dot_product_attention_decod": 372, "split_query_key_value_and_split_head": 373, "tril": 374, "triu": 375, "trunc": 376, "trunc_bw": 377, "unary_chain": 378, "until": 379, "untilize_with_unpad": 380, "upsampl": 381, "var": 382, "where": [383, 391], "where_bw": 384, "xlogi": 385, "xlogy_bw": 386, "zero": 387, "zeros_lik": 388, "convert": [389, 404], "pytorch": 389, "rewrit": 389, "switch": 389, "3": [389, 392, 404], "optim": 389, "more": 389, "build": [390, 391, 392], "uplift": 390, "demo": [390, 391], "get": 391, "start": 391, "instal": [391, 392], "explor": 391, "our": 391, "To": [391, 392], "go": 391, "from": [391, 404], "here": 391, "prerequisit": [392, 403], "set": [392, 398], "up": 392, "hardwar": 392, "softwar": 392, "depend": 392, "script": 392, "recommend": 392, "manual": [392, 398], "metalium": 392, "There": 392, "four": 392, "latest": 392, "wheel": 392, "For": 392, "user": 392, "environ": 392, "docker": 392, "releas": 392, "sourc": 392, "repositori": 392, "librari": [392, 397, 398, 399, 400], "virtual": 392, "setup": [392, 402], "anaconda": 392, "packag": 392, "you": 392, "all": [392, 404], "verifi": 392, "your": 392, "try": 392, "execut": 392, "interest": 392, "multi": [392, 401], "card": 392, "topologi": 392, "machin": 392, "requir": [392, 395], "overview": 392, "why": 392, "It": 392, "matter": 392, "vm": 392, "onboard": 393, "profil": [394, 403], "perf": 394, "header": 394, "profile_thi": 394, "descript": 394, "us": [394, 404], "perform": [394, 403], "visual": [394, 403, 404], "shape": 395, "layout": 395, "type": 395, "width": 395, "limit": 395, "bfloat8_b": 395, "storag": 395, "tutori": 396, "import": [397, 398, 399, 400, 402], "open": [397, 398, 399, 400, 402], "tenstorr": 397, "addit": 397, "close": [397, 398, 399, 400, 402], "basic": [398, 399, 404], "seed": 398, "creat": 398, "forward": 398, "method": 398, "paramet": 398, "run": [398, 402, 403, 404], "host": 399, "tile": 399, "base": 399, "arithmet": 399, "simul": 399, "broadcast": 399, "row": 399, "vector": 399, "expans": 399, "mlp": 400, "infer": [400, 402], "load": [400, 402], "mnist": 400, "test": [400, 402], "pretrain": 400, "accuraci": 400, "track": 400, "loop": 400, "flatten": 400, "head": 401, "attent": 401, "write": 401, "simpl": 402, "cnn": 402, "cifar": 402, "10": [402, 404], "dataset": 402, "initi": 402, "defin": 402, "stage": 402, "sampl": 402, "gener": 403, "result": 403, "analysi": 403, "upload": 403, "tab": 403, "buffer": [403, 404], "graph": [403, 404], "recap": 403, "torch": 404, "an": 404, "__getitem__": 404, "4": 404, "enabl": 404, "5": 404, "intermedi": 404, "6": 404, "trace": 404, "7": 404, "tt_lib": 404, "8": 404, "9": 404, "chang": 404, "string": 404, "represent": 404, "11": 404, "web": 404, "browser": 404, "12": 404, "regist": 404, "pre": 404, "post": 404, "13": 404, "queri": 404, "14": 404, "fall": 404, "back": 404, "15": 404, "captur": 404, "alloc": 404, "etc": 404}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "nbsphinx": 4, "sphinx": 58}, "alltitles": {"Welcome to TT-NN documentation!": [[0, "welcome-to-tt-nn-documentation"]], "TTNN": [[0, null]], "Resources": [[0, null]], "Indices and tables": [[0, "indices-and-tables"]], "Contributing as a developer": [[1, "contributing-as-a-developer"]], "Support": [[2, "support"]], "Reporting bugs, feature proposals, or support requests": [[2, "reporting-bugs-feature-proposals-or-support-requests"]], "Troubleshooting and debugging tips": [[2, "troubleshooting-and-debugging-tips"]], "Community": [[2, "community"]], "What is TT-NN?": [[3, "what-is-tt-nn"]], "Adding New TT-NN Operation": [[4, "adding-new-tt-nn-operation"]], "FAQ": [[4, "faq"]], "What is a TT-NN operation?": [[4, "what-is-a-tt-nn-operation"]], "What steps are needed to add TT-NN operation in C++?": [[4, "what-steps-are-needed-to-add-tt-nn-operation-in-c"]], "What steps are needed to add TT-NN operation in Python?": [[4, "what-steps-are-needed-to-add-tt-nn-operation-in-python"]], "Example of Adding a new Device Operation": [[4, "example-of-adding-a-new-device-operation"]], "C++ Implementation": [[4, "c-implementation"]], "Step 1: Implement device operation": [[4, "step-1-implement-device-operation"]], "Step 2: Implement the operation in C++": [[4, "step-2-implement-the-operation-in-c"]], "Python Implementation": [[4, "python-implementation"]], "Step 1: Add Python binding": [[4, "step-1-add-python-binding"]], "Step 2: (Optional) Add golden function for the operation in Python": [[4, "step-2-optional-add-golden-function-for-the-operation-in-python"]], "APIs": [[5, "apis"], [395, "apis"]], "Device": [[5, "device"]], "Memory Config": [[5, "memory-config"], [395, "memory-config"]], "Operations": [[5, "operations"]], "Core": [[5, "core"]], "Tensor Creation": [[5, "tensor-creation"], [397, "Tensor-Creation"]], "Matrix Multiplication": [[5, "matrix-multiplication"]], "Pointwise Unary": [[5, "pointwise-unary"]], "Pointwise Binary": [[5, "pointwise-binary"]], "Pointwise Ternary": [[5, "pointwise-ternary"]], "Losses": [[5, "losses"]], "Reduction": [[5, "reduction"]], "Data Movement": [[5, "data-movement"]], "Normalization": [[5, "normalization"]], "Normalization Program Configs": [[5, "normalization-program-configs"]], "Moreh Operations": [[5, "moreh-operations"]], "Transformer": [[5, "transformer"]], "CCL": [[5, "ccl"]], "Embedding": [[5, "embedding"]], "Convolution": [[5, "convolution"]], "Pooling": [[5, "pooling"]], "Vision": [[5, "vision"]], "KV Cache": [[5, "kv-cache"]], "Model Conversion": [[5, "model-conversion"]], "Reports": [[5, "reports"]], "Operation Hooks": [[5, "operation-hooks"]], "ttnn.Conv2dConfig": [[6, "ttnn-conv2dconfig"]], "ttnn.Conv2dSliceConfig": [[7, "ttnn-conv2dsliceconfig"]], "ttnn.GetDefaultDevice": [[8, "ttnn-getdefaultdevice"]], "Example": [[8, null], [13, null], [14, null], [16, null], [17, null], [18, null], [19, null], [20, null], [21, null], [22, null], [23, null], [24, null], [25, null], [26, null], [27, null], [28, null], [29, null], [30, null], [34, null], [35, null], [36, null], [37, null], [38, null], [40, null], [41, null], [42, null], [43, null], [44, null], [45, null], [46, null], [47, null], [48, null], [49, null], [50, null], [51, null], [52, null], [53, null], [54, null], [55, null], [56, null], [57, null], [58, null], [59, null], [60, null], [61, null], [62, null], [63, null], [64, null], [65, null], [66, null], [67, null], [69, null], [70, null], [71, null], [72, null], [73, null], [77, null], [78, null], [79, null], [80, null], [81, null], [82, null], [83, null], [84, null], [85, null], [86, null], [87, null], [88, null], [89, null], [90, null], [91, null], [92, null], [93, null], [94, null], [95, null], [96, null], [97, null], [98, null], [99, null], [100, null], [101, null], [102, null], [103, null], [104, null], [105, null], [106, null], [107, null], [108, null], [109, null], [110, null], [111, null], [112, null], [114, null], [115, null], [117, null], [118, null], [119, null], [120, null], [123, null], [124, null], [125, null], [126, null], [127, null], [128, null], [129, null], [130, null], [131, null], [132, null], [134, null], [135, null], [136, null], [138, null], [139, null], [140, null], [141, null], [142, null], [143, null], [144, null], [145, null], [146, null], [147, null], [148, null], [149, null], [150, null], [151, null], [152, null], [153, null], [154, null], [155, null], [156, null], [157, null], [158, null], [159, null], [160, null], [161, null], [162, null], [163, null], [164, null], [165, null], [166, null], [167, null], [168, null], [169, null], [170, null], [171, null], [172, null], [173, null], [174, null], [175, null], [178, null], [179, null], [180, null], [181, null], [182, null], [183, null], [184, null], [185, null], [186, null], [187, null], [188, null], [189, null], [190, null], [191, null], [192, null], [193, null], [194, null], [195, null], [196, null], [197, null], [198, null], [199, null], [200, null], [201, null], [202, null], [203, null], [204, null], [205, null], [206, null], [207, null], [208, null], [209, null], [210, null], [211, null], [212, null], [213, null], [214, null], [215, null], [216, null], [217, null], [218, null], [219, null], [220, null], [221, null], [222, null], [223, null], [224, null], [226, null], [227, null], [228, null], [229, null], [230, null], [231, null], [232, null], [233, null], [234, null], [238, null], [239, null], [240, null], [241, null], [242, null], [243, null], [244, null], [245, null], [246, null], [247, null], [248, null], [249, null], [250, null], [251, null], [252, null], [253, null], [254, null], [255, null], [256, null], [257, null], [259, null], [260, null], [261, null], [262, null], [263, null], [264, null], [265, null], [266, null], [271, null], [272, null], [273, null], [274, null], [275, null], [276, null], [277, null], [278, null], [279, null], [280, null], [281, null], [282, null], [286, null], [287, null], [288, null], [289, null], [290, null], [291, null], [292, null], [293, null], [294, null], [295, null], [296, null], [298, null], [299, null], [300, null], [301, null], [302, null], [303, null], [304, null], [305, null], [306, null], [307, null], [308, null], [309, null], [310, null], [312, null], [313, null], [315, null], [316, null], [317, null], [318, null], [319, null], [320, null], [321, null], [322, null], [323, null], [324, null], [325, null], [326, null], [327, null], [328, null], [329, null], [330, null], [331, null], [332, null], [333, null], [334, null], [335, null], [337, null], [338, null], [339, null], [340, null], [341, null], [342, null], [343, null], [344, null], [345, null], [346, null], [347, null], [348, null], [349, null], [350, null], [351, null], [352, null], [353, null], [354, null], [355, null], [356, null], [357, null], [358, null], [359, null], [360, null], [364, null], [365, null], [366, null], [367, null], [374, null], [375, null], [376, null], [377, null], [378, null], [382, null], [383, null], [384, null], [385, null], [386, null], [387, null], [388, null]], "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig": [[9, "ttnn-matmulmulticorereusemulticast1dprogramconfig"]], "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig": [[10, "ttnn-matmulmulticorereusemulticastdramshardedprogramconfig"]], "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig": [[11, "ttnn-matmulmulticorereusemulticastprogramconfig"]], "ttnn.MatmulMultiCoreReuseProgramConfig": [[12, "ttnn-matmulmulticorereuseprogramconfig"]], "ttnn.SetDefaultDevice": [[13, "ttnn-setdefaultdevice"]], "ttnn.SoftmaxDefaultProgramConfig": [[14, "ttnn-softmaxdefaultprogramconfig"]], "ttnn.SoftmaxProgramConfig": [[15, "ttnn-softmaxprogramconfig"]], "ttnn.SoftmaxShardedMultiCoreProgramConfig": [[16, "ttnn-softmaxshardedmulticoreprogramconfig"]], "ttnn.abs": [[17, "ttnn-abs"]], "ttnn.abs_bw": [[18, "ttnn-abs-bw"]], "ttnn.acos": [[19, "ttnn-acos"]], "ttnn.acos_bw": [[20, "ttnn-acos-bw"]], "ttnn.acosh": [[21, "ttnn-acosh"]], "ttnn.acosh_bw": [[22, "ttnn-acosh-bw"]], "ttnn.add": [[23, "ttnn-add"]], "ttnn.add_bw": [[24, "ttnn-add-bw"]], "ttnn.addalpha": [[25, "ttnn-addalpha"]], "ttnn.addalpha_bw": [[26, "ttnn-addalpha-bw"]], "ttnn.addcdiv": [[27, "ttnn-addcdiv"]], "ttnn.addcdiv_bw": [[28, "ttnn-addcdiv-bw"]], "ttnn.addcmul": [[29, "ttnn-addcmul"]], "ttnn.addcmul_bw": [[30, "ttnn-addcmul-bw"]], "ttnn.addmm": [[31, "ttnn-addmm"]], "input_tensor": [[31, "id2"], [147, "id2"], [179, "id2"], [271, "id2"], [299, "id2"], [367, "id2"]], "mat1_tensor": [[31, "id3"]], "mat2_tensor": [[31, "id4"]], "ttnn.all_gather": [[32, "ttnn-all-gather"]], "ttnn.all_reduce": [[33, "ttnn-all-reduce"]], "ttnn.alt_complex_rotate90": [[34, "ttnn-alt-complex-rotate90"]], "ttnn.angle": [[35, "ttnn-angle"]], "ttnn.angle_bw": [[36, "ttnn-angle-bw"]], "ttnn.arange": [[37, "ttnn-arange"]], "ttnn.argmax": [[38, "ttnn-argmax"]], "Input Tensor": [[38, "id2"], [226, "id2"], [230, "id2"], [231, "id2"], [309, "id2"], [310, "id2"], [344, "id2"], [349, "id2"], [382, "id2"]], "Output Tensor": [[38, "id3"]], "ttnn.as_tensor": [[39, "ttnn-as-tensor"]], "Examples": [[39, null], [314, null]], "ttnn.asin": [[40, "ttnn-asin"]], "ttnn.asin_bw": [[41, "ttnn-asin-bw"]], "ttnn.asinh": [[42, "ttnn-asinh"]], "ttnn.asinh_bw": [[43, "ttnn-asinh-bw"]], "ttnn.assign_bw": [[44, "ttnn-assign-bw"]], "ttnn.atan": [[45, "ttnn-atan"]], "ttnn.atan2": [[46, "ttnn-atan2"]], "ttnn.atan2_bw": [[47, "ttnn-atan2-bw"]], "ttnn.atan_bw": [[48, "ttnn-atan-bw"]], "ttnn.atanh": [[49, "ttnn-atanh"]], "ttnn.atanh_bw": [[50, "ttnn-atanh-bw"]], "ttnn.batch_norm": [[51, "ttnn-batch-norm"]], "ttnn.bias_gelu_bw": [[52, "ttnn-bias-gelu-bw"]], "ttnn.bitwise_and": [[53, "ttnn-bitwise-and"]], "ttnn.bitwise_left_shift": [[54, "ttnn-bitwise-left-shift"]], "ttnn.bitwise_not": [[55, "ttnn-bitwise-not"]], "ttnn.bitwise_or": [[56, "ttnn-bitwise-or"]], "ttnn.bitwise_right_shift": [[57, "ttnn-bitwise-right-shift"]], "ttnn.bitwise_xor": [[58, "ttnn-bitwise-xor"]], "ttnn.cbrt": [[59, "ttnn-cbrt"]], "ttnn.ceil": [[60, "ttnn-ceil"]], "ttnn.ceil_bw": [[61, "ttnn-ceil-bw"]], "ttnn.celu": [[62, "ttnn-celu"]], "ttnn.celu_bw": [[63, "ttnn-celu-bw"]], "ttnn.clamp": [[64, "ttnn-clamp"]], "ttnn.clamp_bw": [[65, "ttnn-clamp-bw"]], "ttnn.clip": [[66, "ttnn-clip"]], "ttnn.clip_bw": [[67, "ttnn-clip-bw"]], "ttnn.clone": [[68, "ttnn-clone"]], "ttnn.close_device": [[69, "ttnn-close-device"]], "ttnn.concat": [[70, "ttnn-concat"]], "ttnn.concat_bw": [[71, "ttnn-concat-bw"]], "ttnn.conj": [[72, "ttnn-conj"]], "ttnn.conj_bw": [[73, "ttnn-conj-bw"]], "ttnn.conv1d": [[74, "ttnn-conv1d"]], "ttnn.conv2d": [[75, "ttnn-conv2d"]], "ttnn.conv_transpose2d": [[76, "ttnn-conv-transpose2d"]], "ttnn.cos": [[77, "ttnn-cos"]], "ttnn.cos_bw": [[78, "ttnn-cos-bw"]], "ttnn.cosh": [[79, "ttnn-cosh"]], "ttnn.cosh_bw": [[80, "ttnn-cosh-bw"]], "ttnn.create_sharded_memory_config": [[81, "ttnn-create-sharded-memory-config"]], "ttnn.cumprod": [[82, "ttnn-cumprod"]], "ttnn.cumsum": [[83, "ttnn-cumsum"]], "ttnn.deallocate": [[84, "ttnn-deallocate"]], "ttnn.deg2rad": [[85, "ttnn-deg2rad"]], "ttnn.deg2rad_bw": [[86, "ttnn-deg2rad-bw"]], "ttnn.digamma": [[87, "ttnn-digamma"]], "ttnn.digamma_bw": [[88, "ttnn-digamma-bw"]], "ttnn.div": [[89, "ttnn-div"]], "ttnn.div_bw": [[90, "ttnn-div-bw"]], "ttnn.div_no_nan": [[91, "ttnn-div-no-nan"]], "ttnn.div_no_nan_bw": [[92, "ttnn-div-no-nan-bw"]], "ttnn.dump_tensor": [[93, "ttnn-dump-tensor"]], "ttnn.elu": [[94, "ttnn-elu"]], "ttnn.elu_bw": [[95, "ttnn-elu-bw"]], "ttnn.embedding": [[96, "ttnn-embedding"]], "ttnn.embedding_bw": [[97, "ttnn-embedding-bw"]], "ttnn.empty": [[98, "ttnn-empty"]], "ttnn.empty_like": [[99, "ttnn-empty-like"]], "ttnn.eq": [[100, "ttnn-eq"]], "ttnn.eq_": [[101, "ttnn-eq"]], "ttnn.eqz": [[102, "ttnn-eqz"]], "ttnn.erf": [[103, "ttnn-erf"]], "ttnn.erf_bw": [[104, "ttnn-erf-bw"]], "ttnn.erfc": [[105, "ttnn-erfc"]], "ttnn.erfc_bw": [[106, "ttnn-erfc-bw"]], "ttnn.erfinv": [[107, "ttnn-erfinv"]], "ttnn.erfinv_bw": [[108, "ttnn-erfinv-bw"]], "ttnn.exp": [[109, "ttnn-exp"]], "ttnn.exp2": [[110, "ttnn-exp2"]], "ttnn.exp2_bw": [[111, "ttnn-exp2-bw"]], "ttnn.exp_bw": [[112, "ttnn-exp-bw"]], "ttnn.experimental.conv3d": [[113, "ttnn-experimental-conv3d"]], "ttnn.experimental.dropout": [[114, "ttnn-experimental-dropout"]], "ttnn.experimental.gelu_bw": [[115, "ttnn-experimental-gelu-bw"]], "ttnn.experimental.rotary_embedding": [[116, "ttnn-experimental-rotary-embedding"]], "ttnn.expm1": [[117, "ttnn-expm1"]], "ttnn.expm1_bw": [[118, "ttnn-expm1-bw"]], "ttnn.fill": [[119, "ttnn-fill"]], "ttnn.fill_bw": [[120, "ttnn-fill-bw"]], "ttnn.fill_ones_rm": [[121, "ttnn-fill-ones-rm"]], "ttnn.fill_rm": [[122, "ttnn-fill-rm"]], "ttnn.fill_zero_bw": [[123, "ttnn-fill-zero-bw"]], "ttnn.floor": [[124, "ttnn-floor"]], "ttnn.floor_bw": [[125, "ttnn-floor-bw"]], "ttnn.floor_div": [[126, "ttnn-floor-div"]], "ttnn.fmod": [[127, "ttnn-fmod"]], "ttnn.fmod_bw": [[128, "ttnn-fmod-bw"]], "ttnn.format_input_tensor": [[129, "ttnn-format-input-tensor"]], "ttnn.format_output_tensor": [[130, "ttnn-format-output-tensor"]], "ttnn.frac": [[131, "ttnn-frac"]], "ttnn.frac_bw": [[132, "ttnn-frac-bw"]], "ttnn.from_device": [[133, "ttnn-from-device"]], "ttnn.from_torch": [[134, "ttnn-from-torch"]], "ttnn.full": [[135, "ttnn-full"]], "ttnn.full_like": [[136, "ttnn-full-like"]], "ttnn.gather": [[137, "ttnn-gather"]], "ttnn.gcd": [[138, "ttnn-gcd"]], "ttnn.ge": [[139, "ttnn-ge"]], "ttnn.ge_": [[140, "ttnn-ge"]], "ttnn.geglu": [[141, "ttnn-geglu"]], "ttnn.gelu": [[142, "ttnn-gelu"]], "ttnn.gelu_bw": [[143, "ttnn-gelu-bw"]], "ttnn.gez": [[144, "ttnn-gez"]], "ttnn.global_avg_pool2d": [[145, "ttnn-global-avg-pool2d"]], "ttnn.glu": [[146, "ttnn-glu"]], "ttnn.group_norm": [[147, "ttnn-group-norm"]], "weight (gamma) and bias (beta)": [[147, "id3"], [179, "id4"], [299, "id4"]], "input_mask": [[147, "id4"]], "output_tensor": [[147, "id5"], [179, "id6"], [271, "id3"], [299, "id5"]], "ttnn.gt": [[148, "ttnn-gt"]], "ttnn.gt_": [[149, "ttnn-gt"]], "ttnn.gtz": [[150, "ttnn-gtz"]], "ttnn.hardshrink": [[151, "ttnn-hardshrink"]], "ttnn.hardshrink_bw": [[152, "ttnn-hardshrink-bw"]], "ttnn.hardsigmoid": [[153, "ttnn-hardsigmoid"]], "ttnn.hardsigmoid_bw": [[154, "ttnn-hardsigmoid-bw"]], "ttnn.hardswish": [[155, "ttnn-hardswish"]], "ttnn.hardswish_bw": [[156, "ttnn-hardswish-bw"]], "ttnn.hardtanh": [[157, "ttnn-hardtanh"]], "ttnn.hardtanh_bw": [[158, "ttnn-hardtanh-bw"]], "ttnn.heaviside": [[159, "ttnn-heaviside"]], "ttnn.hypot": [[160, "ttnn-hypot"]], "ttnn.hypot_bw": [[161, "ttnn-hypot-bw"]], "ttnn.i0": [[162, "ttnn-i0"]], "ttnn.i0_bw": [[163, "ttnn-i0-bw"]], "ttnn.identity": [[164, "ttnn-identity"]], "ttnn.imag": [[165, "ttnn-imag"]], "ttnn.imag_bw": [[166, "ttnn-imag-bw"]], "ttnn.indexed_fill": [[167, "ttnn-indexed-fill"]], "ttnn.is_imag": [[168, "ttnn-is-imag"]], "ttnn.is_real": [[169, "ttnn-is-real"]], "ttnn.isclose": [[170, "ttnn-isclose"]], "ttnn.isfinite": [[171, "ttnn-isfinite"]], "ttnn.isinf": [[172, "ttnn-isinf"]], "ttnn.isnan": [[173, "ttnn-isnan"]], "ttnn.isneginf": [[174, "ttnn-isneginf"]], "ttnn.isposinf": [[175, "ttnn-isposinf"]], "ttnn.kv_cache.fill_cache_for_user_": [[176, "ttnn-kv-cache-fill-cache-for-user"]], "ttnn.kv_cache.update_cache_for_token_": [[177, "ttnn-kv-cache-update-cache-for-token"]], "ttnn.l1_loss": [[178, "ttnn-l1-loss"]], "ttnn.layer_norm": [[179, "ttnn-layer-norm"]], "residual_input_tensor": [[179, "id3"], [299, "id3"]], "stats (POST_ALL_GATHER only)": [[179, "id5"]], "ttnn.lcm": [[180, "ttnn-lcm"]], "ttnn.ldexp": [[181, "ttnn-ldexp"]], "ttnn.ldexp_bw": [[182, "ttnn-ldexp-bw"]], "ttnn.le": [[183, "ttnn-le"]], "ttnn.le_": [[184, "ttnn-le"]], "ttnn.leaky_relu": [[185, "ttnn-leaky-relu"]], "ttnn.leaky_relu_bw": [[186, "ttnn-leaky-relu-bw"]], "ttnn.lerp": [[187, "ttnn-lerp"]], "ttnn.lerp_bw": [[188, "ttnn-lerp-bw"]], "ttnn.lez": [[189, "ttnn-lez"]], "ttnn.lgamma": [[190, "ttnn-lgamma"]], "ttnn.lgamma_bw": [[191, "ttnn-lgamma-bw"]], "ttnn.linear": [[192, "ttnn-linear"]], "input_tensor_a": [[192, "id2"], [224, "id2"], [225, "id2"], [337, "id2"]], "input_tensor_b": [[192, "id3"], [224, "id3"], [337, "id3"]], "bias": [[192, "id4"]], "ttnn.load_tensor": [[193, "ttnn-load-tensor"]], "ttnn.log": [[194, "ttnn-log"]], "ttnn.log10": [[195, "ttnn-log10"]], "ttnn.log10_bw": [[196, "ttnn-log10-bw"]], "ttnn.log1p": [[197, "ttnn-log1p"]], "ttnn.log1p_bw": [[198, "ttnn-log1p-bw"]], "ttnn.log2": [[199, "ttnn-log2"]], "ttnn.log2_bw": [[200, "ttnn-log2-bw"]], "ttnn.log_bw": [[201, "ttnn-log-bw"]], "ttnn.log_sigmoid": [[202, "ttnn-log-sigmoid"]], "ttnn.log_sigmoid_bw": [[203, "ttnn-log-sigmoid-bw"]], "ttnn.logaddexp": [[204, "ttnn-logaddexp"]], "ttnn.logaddexp2": [[205, "ttnn-logaddexp2"]], "ttnn.logaddexp2_bw": [[206, "ttnn-logaddexp2-bw"]], "ttnn.logaddexp_bw": [[207, "ttnn-logaddexp-bw"]], "ttnn.logical_and": [[208, "ttnn-logical-and"]], "ttnn.logical_and_": [[209, "ttnn-logical-and"]], "ttnn.logical_not": [[210, "ttnn-logical-not"]], "ttnn.logical_not_": [[211, "ttnn-logical-not"]], "ttnn.logical_or": [[212, "ttnn-logical-or"]], "ttnn.logical_or_": [[213, "ttnn-logical-or"]], "ttnn.logical_xor": [[214, "ttnn-logical-xor"]], "ttnn.logical_xor_": [[215, "ttnn-logical-xor"]], "ttnn.logit": [[216, "ttnn-logit"]], "ttnn.logit_bw": [[217, "ttnn-logit-bw"]], "ttnn.logiteps_bw": [[218, "ttnn-logiteps-bw"]], "ttnn.lt": [[219, "ttnn-lt"]], "ttnn.lt_": [[220, "ttnn-lt"]], "ttnn.ltz": [[221, "ttnn-ltz"]], "ttnn.mac": [[222, "ttnn-mac"]], "ttnn.manage_device": [[223, "ttnn-manage-device"]], "ttnn.matmul": [[224, "ttnn-matmul"]], "Supported Memory Configurations": [[224, "id4"]], "ttnn.matmul_batched_weights": [[225, "ttnn-matmul-batched-weights"]], "input_tensors_b": [[225, "id3"]], "ttnn.max": [[226, "ttnn-max"]], "ttnn.max_bw": [[227, "ttnn-max-bw"]], "ttnn.max_pool2d": [[228, "ttnn-max-pool2d"]], "ttnn.maximum": [[229, "ttnn-maximum"]], "ttnn.mean": [[230, "ttnn-mean"]], "ttnn.min": [[231, "ttnn-min"]], "ttnn.min_bw": [[232, "ttnn-min-bw"]], "ttnn.minimum": [[233, "ttnn-minimum"]], "ttnn.mish": [[234, "ttnn-mish"]], "ttnn.model_preprocessing.preprocess_model": [[235, "ttnn-model-preprocessing-preprocess-model"]], "ttnn.model_preprocessing.preprocess_model_parameters": [[236, "ttnn-model-preprocessing-preprocess-model-parameters"]], "ttnn.moreh_sum": [[237, "ttnn-moreh-sum"]], "ttnn.mse_loss": [[238, "ttnn-mse-loss"]], "ttnn.mul_bw": [[239, "ttnn-mul-bw"]], "ttnn.multigammaln": [[240, "ttnn-multigammaln"]], "ttnn.multigammaln_bw": [[241, "ttnn-multigammaln-bw"]], "ttnn.multiply": [[242, "ttnn-multiply"]], "ttnn.ne": [[243, "ttnn-ne"]], "ttnn.ne_": [[244, "ttnn-ne"]], "ttnn.neg": [[245, "ttnn-neg"]], "ttnn.neg_bw": [[246, "ttnn-neg-bw"]], "ttnn.nextafter": [[247, "ttnn-nextafter"]], "ttnn.nez": [[248, "ttnn-nez"]], "ttnn.nonzero": [[249, "ttnn-nonzero"]], "ttnn.normalize_global": [[250, "ttnn-normalize-global"]], "ttnn.normalize_hw": [[251, "ttnn-normalize-hw"]], "ttnn.ones": [[252, "ttnn-ones"]], "ttnn.ones_like": [[253, "ttnn-ones-like"]], "ttnn.open_device": [[254, "ttnn-open-device"]], "ttnn.outer": [[255, "ttnn-outer"]], "ttnn.pad": [[256, "ttnn-pad"]], "ttnn.pad_to_tile_shape": [[257, "ttnn-pad-to-tile-shape"]], "ttnn.permute": [[258, "ttnn-permute"]], "ttnn.polar": [[259, "ttnn-polar"]], "ttnn.polar_bw": [[260, "ttnn-polar-bw"]], "ttnn.polygamma": [[261, "ttnn-polygamma"]], "ttnn.polygamma_bw": [[262, "ttnn-polygamma-bw"]], "ttnn.polyval": [[263, "ttnn-polyval"]], "ttnn.pow": [[264, "ttnn-pow"]], "ttnn.pow_bw": [[265, "ttnn-pow-bw"]], "ttnn.prelu": [[266, "ttnn-prelu"]], "ttnn.prepare_conv_bias": [[267, "ttnn-prepare-conv-bias"]], "ttnn.prepare_conv_transpose2d_bias": [[268, "ttnn-prepare-conv-transpose2d-bias"]], "ttnn.prepare_conv_transpose2d_weights": [[269, "ttnn-prepare-conv-transpose2d-weights"]], "ttnn.prepare_conv_weights": [[270, "ttnn-prepare-conv-weights"]], "ttnn.prod": [[271, "ttnn-prod"]], "ttnn.prod_bw": [[272, "ttnn-prod-bw"]], "ttnn.rad2deg": [[273, "ttnn-rad2deg"]], "ttnn.rad2deg_bw": [[274, "ttnn-rad2deg-bw"]], "ttnn.rand": [[275, "ttnn-rand"]], "ttnn.rdiv": [[276, "ttnn-rdiv"]], "ttnn.rdiv_bw": [[277, "ttnn-rdiv-bw"]], "ttnn.real": [[278, "ttnn-real"]], "ttnn.real_bw": [[279, "ttnn-real-bw"]], "ttnn.reallocate": [[280, "ttnn-reallocate"]], "ttnn.reciprocal": [[281, "ttnn-reciprocal"]], "ttnn.reciprocal_bw": [[282, "ttnn-reciprocal-bw"]], "ttnn.reduce_scatter": [[283, "ttnn-reduce-scatter"]], "ttnn.register_post_operation_hook": [[284, "ttnn-register-post-operation-hook"]], "ttnn.register_pre_operation_hook": [[285, "ttnn-register-pre-operation-hook"]], "ttnn.reglu": [[286, "ttnn-reglu"]], "ttnn.relu": [[287, "ttnn-relu"]], "ttnn.relu6": [[288, "ttnn-relu6"]], "ttnn.relu6_bw": [[289, "ttnn-relu6-bw"]], "ttnn.relu_bw": [[290, "ttnn-relu-bw"]], "ttnn.relu_max": [[291, "ttnn-relu-max"]], "ttnn.relu_min": [[292, "ttnn-relu-min"]], "ttnn.remainder": [[293, "ttnn-remainder"]], "ttnn.remainder_bw": [[294, "ttnn-remainder-bw"]], "ttnn.repeat": [[295, "ttnn-repeat"]], "ttnn.repeat_bw": [[296, "ttnn-repeat-bw"]], "ttnn.repeat_interleave": [[297, "ttnn-repeat-interleave"]], "ttnn.reshape": [[298, "ttnn-reshape"]], "ttnn.rms_norm": [[299, "ttnn-rms-norm"]], "ttnn.round": [[300, "ttnn-round"]], "ttnn.round_bw": [[301, "ttnn-round-bw"]], "ttnn.rpow": [[302, "ttnn-rpow"]], "ttnn.rpow_bw": [[303, "ttnn-rpow-bw"]], "ttnn.rsqrt": [[304, "ttnn-rsqrt"]], "ttnn.rsqrt_bw": [[305, "ttnn-rsqrt-bw"]], "ttnn.rsub": [[306, "ttnn-rsub"]], "ttnn.rsub_bw": [[307, "ttnn-rsub-bw"]], "ttnn.scale_causal_mask_hw_dims_softmax_in_place": [[308, "ttnn-scale-causal-mask-hw-dims-softmax-in-place"]], "Input Tensor (Sharded)": [[308, "id2"]], "Mask Tensor [1, 1, H, W]": [[308, "id3"]], "ttnn.scale_mask_softmax": [[309, "ttnn-scale-mask-softmax"]], "Mask Tensor (optional)": [[309, "id3"], [310, "id3"]], "ttnn.scale_mask_softmax_in_place": [[310, "ttnn-scale-mask-softmax-in-place"]], "ttnn.scatter": [[311, "ttnn-scatter"]], "ttnn.selu": [[312, "ttnn-selu"]], "ttnn.selu_bw": [[313, "ttnn-selu-bw"]], "ttnn.set_printoptions": [[314, "ttnn-set-printoptions"]], "ttnn.sigmoid": [[315, "ttnn-sigmoid"]], "ttnn.sigmoid_accurate": [[316, "ttnn-sigmoid-accurate"]], "ttnn.sigmoid_bw": [[317, "ttnn-sigmoid-bw"]], "ttnn.sign": [[318, "ttnn-sign"]], "ttnn.sign_bw": [[319, "ttnn-sign-bw"]], "ttnn.signbit": [[320, "ttnn-signbit"]], "ttnn.silu": [[321, "ttnn-silu"]], "ttnn.silu_bw": [[322, "ttnn-silu-bw"]], "ttnn.sin": [[323, "ttnn-sin"]], "ttnn.sin_bw": [[324, "ttnn-sin-bw"]], "ttnn.sinh": [[325, "ttnn-sinh"]], "ttnn.sinh_bw": [[326, "ttnn-sinh-bw"]], "ttnn.slice": [[327, "ttnn-slice"]], "ttnn.softmax": [[328, "ttnn-softmax"]], "ttnn.softmax_in_place": [[329, "ttnn-softmax-in-place"]], "ttnn.softplus": [[330, "ttnn-softplus"]], "ttnn.softplus_bw": [[331, "ttnn-softplus-bw"]], "ttnn.softshrink": [[332, "ttnn-softshrink"]], "ttnn.softshrink_bw": [[333, "ttnn-softshrink-bw"]], "ttnn.softsign": [[334, "ttnn-softsign"]], "ttnn.softsign_bw": [[335, "ttnn-softsign-bw"]], "ttnn.sort": [[336, "ttnn-sort"]], "ttnn.sparse_matmul": [[337, "ttnn-sparse-matmul"]], "sparsity": [[337, "id4"]], "ttnn.sqrt": [[338, "ttnn-sqrt"]], "ttnn.sqrt_bw": [[339, "ttnn-sqrt-bw"]], "ttnn.square": [[340, "ttnn-square"]], "ttnn.square_bw": [[341, "ttnn-square-bw"]], "ttnn.squared_difference": [[342, "ttnn-squared-difference"]], "ttnn.squared_difference_bw": [[343, "ttnn-squared-difference-bw"]], "ttnn.std": [[344, "ttnn-std"]], "ttnn.sub_bw": [[345, "ttnn-sub-bw"]], "ttnn.subalpha": [[346, "ttnn-subalpha"]], "ttnn.subalpha_bw": [[347, "ttnn-subalpha-bw"]], "ttnn.subtract": [[348, "ttnn-subtract"]], "ttnn.sum": [[349, "ttnn-sum"]], "ttnn.swiglu": [[350, "ttnn-swiglu"]], "ttnn.swish": [[351, "ttnn-swish"]], "ttnn.synchronize_device": [[352, "ttnn-synchronize-device"]], "ttnn.tan": [[353, "ttnn-tan"]], "ttnn.tan_bw": [[354, "ttnn-tan-bw"]], "ttnn.tanh": [[355, "ttnn-tanh"]], "ttnn.tanh_bw": [[356, "ttnn-tanh-bw"]], "ttnn.tanhshrink": [[357, "ttnn-tanhshrink"]], "ttnn.tanhshrink_bw": [[358, "ttnn-tanhshrink-bw"]], "ttnn.threshold": [[359, "ttnn-threshold"]], "ttnn.threshold_bw": [[360, "ttnn-threshold-bw"]], "ttnn.tilize": [[361, "ttnn-tilize"]], "ttnn.tilize_with_val_padding": [[362, "ttnn-tilize-with-val-padding"]], "ttnn.to_device": [[363, "ttnn-to-device"]], "ttnn.to_layout": [[364, "ttnn-to-layout"]], "ttnn.to_memory_config": [[365, "ttnn-to-memory-config"]], "ttnn.to_torch": [[366, "ttnn-to-torch"]], "ttnn.topk": [[367, "ttnn-topk"]], "index_tensor": [[367, "id3"]], "ttnn.transformer.attention_softmax": [[368, "ttnn-transformer-attention-softmax"]], "ttnn.transformer.attention_softmax_": [[369, "ttnn-transformer-attention-softmax"]], "ttnn.transformer.concatenate_heads": [[370, "ttnn-transformer-concatenate-heads"]], "ttnn.transformer.scaled_dot_product_attention": [[371, "ttnn-transformer-scaled-dot-product-attention"]], "ttnn.transformer.scaled_dot_product_attention_decode": [[372, "ttnn-transformer-scaled-dot-product-attention-decode"]], "ttnn.transformer.split_query_key_value_and_split_heads": [[373, "ttnn-transformer-split-query-key-value-and-split-heads"]], "ttnn.tril": [[374, "ttnn-tril"]], "ttnn.triu": [[375, "ttnn-triu"]], "ttnn.trunc": [[376, "ttnn-trunc"]], "ttnn.trunc_bw": [[377, "ttnn-trunc-bw"]], "ttnn.unary_chain": [[378, "ttnn-unary-chain"]], "ttnn.untilize": [[379, "ttnn-untilize"]], "ttnn.untilize_with_unpadding": [[380, "ttnn-untilize-with-unpadding"]], "ttnn.upsample": [[381, "ttnn-upsample"]], "ttnn.var": [[382, "ttnn-var"]], "ttnn.where": [[383, "ttnn-where"]], "ttnn.where_bw": [[384, "ttnn-where-bw"]], "ttnn.xlogy": [[385, "ttnn-xlogy"]], "ttnn.xlogy_bw": [[386, "ttnn-xlogy-bw"]], "ttnn.zeros": [[387, "ttnn-zeros"]], "ttnn.zeros_like": [[388, "ttnn-zeros-like"]], "Converting PyTorch Model to TT-NN": [[389, "converting-pytorch-model-to-tt-nn"]], "Step 1 - Rewriting the Model": [[389, "step-1-rewriting-the-model"]], "Step 2 - Switching to ttnn Operations": [[389, "step-2-switching-to-ttnn-operations"]], "Step 3 - Optimizing the Model": [[389, "step-3-optimizing-the-model"]], "More examples": [[389, "more-examples"]], "Building and Uplifting Demos": [[390, "building-and-uplifting-demos"]], "Getting Started": [[391, "getting-started"]], "1. Install and Build": [[391, "install-and-build"]], "2. Explore Our Model Demos": [[391, "explore-our-model-demos"]], "Where To Go From Here": [[391, "where-to-go-from-here"]], "Install": [[392, "install"]], "Prerequisites:": [[392, "prerequisites"]], "1: Set Up the Hardware": [[392, "set-up-the-hardware"]], "2: Install Software Dependencies": [[392, "install-software-dependencies"]], "Option 1: TT-Installer Script (recommended)": [[392, "option-1-tt-installer-script-recommended"]], "Option 2: Manual Installation": [[392, "option-2-manual-installation"]], "TT-NN / TT-Metalium Installation": [[392, "tt-nn-tt-metalium-installation"]], "There are four options for installing TT-Metalium:": [[392, "there-are-four-options-for-installing-tt-metalium"]], "Binaries": [[392, "binaries"]], "Step 1. Install the Latest Wheel:": [[392, "step-1-install-the-latest-wheel"]], "Step 2. (For models users only) Set Up Environment for Models:": [[392, "step-2-for-models-users-only-set-up-environment-for-models"]], "Docker Release Image": [[392, "docker-release-image"]], "Source": [[392, "source"]], "Step 1. Clone the Repository:": [[392, "step-1-clone-the-repository"]], "Step 2. Build the Library:": [[392, "step-2-build-the-library"]], "Step 3. Virtual Environment Setup": [[392, "step-3-virtual-environment-setup"]], "Anaconda": [[392, "anaconda"]], "Step 1. Install the Latest Package:": [[392, "step-1-install-the-latest-package"]], "You are All Set!": [[392, "you-are-all-set"]], "To verify your installation (for source or wheel installation only), try executing a programming example:": [[392, "to-verify-your-installation-for-source-or-wheel-installation-only-try-executing-a-programming-example"]], "Interested in Contributing?": [[392, "interested-in-contributing"]], "Multi-Card Configuration (TT-Topology)": [[392, "multi-card-configuration-tt-topology"]], "Virtual Machine Requirements": [[392, "virtual-machine-requirements"]], "Overview": [[392, "overview"]], "Why It Matters": [[392, "why-it-matters"]], "Requirements for VMs": [[392, "requirements-for-vms"]], "Onboarding New Functionality": [[393, "onboarding-new-functionality"]], "Profiling TT-NN Operations": [[394, "profiling-tt-nn-operations"]], "Perf Report Headers": [[394, "perf-report-headers"]], "profile_this description": [[394, "profile-this-description"]], "Using the Performance Report with TT-NN Visualizer": [[394, "using-the-performance-report-with-tt-nn-visualizer"]], "Tensor": [[395, "tensor"]], "Shape": [[395, "shape"]], "Layout": [[395, "layout"]], "Data Type": [[395, "data-type"]], "Required Width Multiples for Data Types": [[395, "id5"]], "Limitation of BFLOAT8_B": [[395, "limitation-of-bfloat8-b"]], "Storage": [[395, "storage"]], "Tensor Sharding": [[395, "tensor-sharding"]], "Tutorials": [[396, "tutorials"]], "Add Tensors": [[397, "Add-Tensors"]], "Import Libraries": [[397, "Import-Libraries"], [398, "Import-Libraries"], [399, "Import-Libraries"], [400, "Import-Libraries"]], "Open Tenstorrent device": [[397, "Open-Tenstorrent-device"]], "Addition Operation and Conversion": [[397, "Addition-Operation-and-Conversion"]], "Close the Device": [[397, "Close-the-Device"], [398, "Close-the-Device"], [399, "Close-the-Device"], [400, "Close-the-Device"], [402, "Close-the-Device"]], "Full Example and Output": [[397, "Full-Example-and-Output"], [398, "Full-Example-and-Output"], [399, "Full-Example-and-Output"], [400, "Full-Example-and-Output"], [401, "Full-Example-and-Output"], [402, "Full-Example-and-Output"]], "Basic Convolution": [[398, "Basic-Convolution"]], "Set Manual Seed": [[398, "Set-Manual-Seed"]], "Open the Device": [[398, "Open-the-Device"], [399, "Open-the-Device"], [400, "Open-the-Device"], [402, "Open-the-Device"]], "Create Forward Method": [[398, "Create-Forward-Method"]], "Set Input and Convolution Parameters": [[398, "Set-Input-and-Convolution-Parameters"]], "Create Tensors": [[398, "Create-Tensors"]], "Run the Convolution Operation": [[398, "Run-the-Convolution-Operation"]], "Basic Tensor Operations": [[399, "Basic-Tensor-Operations"]], "Host Tensor Creation": [[399, "Host-Tensor-Creation"]], "Host Tensor Conversion and Creation": [[399, "Host-Tensor-Conversion-and-Creation"]], "Tile-Based Arithmetic Operations": [[399, "Tile-Based-Arithmetic-Operations"]], "Simulated Broadcasting - Row Vector Expansion": [[399, "Simulated-Broadcasting---Row-Vector-Expansion"]], "MLP Inference": [[400, "MLP-Inference"]], "Load MNIST Test Data": [[400, "Load-MNIST-Test-Data"]], "Load Pretrained MLP Weights": [[400, "Load-Pretrained-MLP-Weights"]], "Accuracy Tracking, Inference, Loop, and Image Flattening": [[400, "Accuracy-Tracking,-Inference,-Loop,-and-Image-Flattening"]], "Multi-Head Attention": [[401, "Multi-Head-Attention"]], "Write Multi-Head Attention with TT-NN": [[401, "Write-Multi-Head-Attention-with-TT-NN"]], "Configuration": [[401, "Configuration"]], "Running a Simple CNN Inference on CIFAR-10": [[402, "Running-a-Simple-CNN-Inference-on-CIFAR-10"]], "Setup and Imports": [[402, "Setup-and-Imports"]], "Load the CIFAR-10 Dataset": [[402, "Load-the-CIFAR-10-Dataset"]], "Load or Initialize Weights": [[402, "Load-or-Initialize-Weights"]], "Define Convolution and Pooling Stage": [[402, "Define-Convolution-and-Pooling-Stage"]], "Run Inference on Test Samples": [[402, "Run-Inference-on-Test-Samples"]], "TT-NN Visualizer": [[403, "tt-nn-visualizer"]], "Prerequisites": [[403, "prerequisites"]], "Running TTNN Visualizer": [[403, "running-ttnn-visualizer"]], "Model Profiling": [[403, "model-profiling"]], "Generating the Memory Report": [[403, "generating-the-memory-report"]], "Generating Performance Reports": [[403, "generating-performance-reports"]], "Result Analysis": [[403, "result-analysis"]], "Uploading Reports": [[403, "uploading-reports"]], "Operations Tab": [[403, "operations-tab"]], "Tensors Tab": [[403, "tensors-tab"]], "Buffers Tab": [[403, "buffers-tab"]], "Graph Tab": [[403, "graph-tab"]], "Performance Tab": [[403, "performance-tab"]], "Recap": [[403, "recap"]], "Using TT-NN": [[404, "using-tt-nn"]], "Basic Examples": [[404, "basic-examples"]], "1. Converting from and to torch tensor": [[404, "converting-from-and-to-torch-tensor"]], "2. Running an operation on the device": [[404, "running-an-operation-on-the-device"]], "3. Using __getitem__ to slice the tensor": [[404, "using-getitem-to-slice-the-tensor"]], "4. Enabling program cache": [[404, "enabling-program-cache"]], "5. Debugging intermediate tensors": [[404, "debugging-intermediate-tensors"]], "6. Tracing the graph of operations": [[404, "tracing-the-graph-of-operations"]], "7. Using tt_lib operation in TT-NN": [[404, "using-tt-lib-operation-in-tt-nn"]], "8. Enabling Logging": [[404, "enabling-logging"]], "9. Supported Python Operators": [[404, "supported-python-operators"]], "10. Changing the string representation of the tensor": [[404, "changing-the-string-representation-of-the-tensor"]], "11. Visualize using Web Browser": [[404, "visualize-using-web-browser"]], "12. Register pre- and/or post-operation hooks": [[404, "register-pre-and-or-post-operation-hooks"]], "13. Query all operations": [[404, "query-all-operations"]], "14. Falling back to torch": [[404, "falling-back-to-torch"]], "15. Capturing graph of C++ functions, buffer allocations, etc": [[404, "capturing-graph-of-c-functions-buffer-allocations-etc"]]}, "indexentries": {"conv2dconfig (class in ttnn)": [[6, "ttnn.Conv2dConfig"]], "act_block_h_override (ttnn.conv2dconfig property)": [[6, "ttnn.Conv2dConfig.act_block_h_override"]], "act_block_w_div (ttnn.conv2dconfig property)": [[6, "ttnn.Conv2dConfig.act_block_w_div"]], "activation (ttnn.conv2dconfig property)": [[6, "ttnn.Conv2dConfig.activation"]], "config_tensors_in_dram (ttnn.conv2dconfig property)": [[6, "ttnn.Conv2dConfig.config_tensors_in_dram"]], "core_grid (ttnn.conv2dconfig property)": [[6, "ttnn.Conv2dConfig.core_grid"]], "deallocate_activation (ttnn.conv2dconfig property)": [[6, "ttnn.Conv2dConfig.deallocate_activation"]], "enable_act_double_buffer (ttnn.conv2dconfig property)": [[6, "ttnn.Conv2dConfig.enable_act_double_buffer"]], "enable_activation_reuse (ttnn.conv2dconfig property)": [[6, "ttnn.Conv2dConfig.enable_activation_reuse"]], "enable_kernel_stride_folding (ttnn.conv2dconfig property)": [[6, "ttnn.Conv2dConfig.enable_kernel_stride_folding"]], "enable_weights_double_buffer (ttnn.conv2dconfig property)": [[6, "ttnn.Conv2dConfig.enable_weights_double_buffer"]], "force_split_reader (ttnn.conv2dconfig property)": [[6, "ttnn.Conv2dConfig.force_split_reader"]], "full_inner_dim (ttnn.conv2dconfig property)": [[6, "ttnn.Conv2dConfig.full_inner_dim"]], "in_place (ttnn.conv2dconfig property)": [[6, "ttnn.Conv2dConfig.in_place"]], "output_layout (ttnn.conv2dconfig property)": [[6, "ttnn.Conv2dConfig.output_layout"]], "override_sharding_config (ttnn.conv2dconfig property)": [[6, "ttnn.Conv2dConfig.override_sharding_config"]], "reallocate_halo_output (ttnn.conv2dconfig property)": [[6, "ttnn.Conv2dConfig.reallocate_halo_output"]], "reshard_if_not_optimal (ttnn.conv2dconfig property)": [[6, "ttnn.Conv2dConfig.reshard_if_not_optimal"]], "shard_layout (ttnn.conv2dconfig property)": [[6, "ttnn.Conv2dConfig.shard_layout"]], "transpose_shards (ttnn.conv2dconfig property)": [[6, "ttnn.Conv2dConfig.transpose_shards"]], "weights_dtype (ttnn.conv2dconfig property)": [[6, "ttnn.Conv2dConfig.weights_dtype"]], "conv2dsliceconfig (class in ttnn)": [[7, "ttnn.Conv2dSliceConfig"]], "conv2dsliceconfig.slicetypeenum (class in ttnn)": [[7, "ttnn.Conv2dSliceConfig.SliceTypeEnum"]], "dramsliceheight (ttnn.conv2dsliceconfig.slicetypeenum attribute)": [[7, "ttnn.Conv2dSliceConfig.SliceTypeEnum.DRAMSliceHeight"]], "dramslicewidth (ttnn.conv2dsliceconfig.slicetypeenum attribute)": [[7, "ttnn.Conv2dSliceConfig.SliceTypeEnum.DRAMSliceWidth"]], "l1full (ttnn.conv2dsliceconfig.slicetypeenum attribute)": [[7, "ttnn.Conv2dSliceConfig.SliceTypeEnum.L1Full"]], "name (ttnn.conv2dsliceconfig.slicetypeenum property)": [[7, "ttnn.Conv2dSliceConfig.SliceTypeEnum.name"]], "num_slices (ttnn.conv2dsliceconfig property)": [[7, "ttnn.Conv2dSliceConfig.num_slices"]], "slice_type (ttnn.conv2dsliceconfig property)": [[7, "ttnn.Conv2dSliceConfig.slice_type"]], "value (ttnn.conv2dsliceconfig.slicetypeenum property)": [[7, "ttnn.Conv2dSliceConfig.SliceTypeEnum.value"]], "getdefaultdevice() (in module ttnn)": [[8, "ttnn.GetDefaultDevice"]], "matmulmulticorereusemulticast1dprogramconfig (class in ttnn)": [[9, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig"]], "compute_with_storage_grid_size (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[9, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.compute_with_storage_grid_size"]], "from_json() (ttnn.matmulmulticorereusemulticast1dprogramconfig method)": [[9, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.from_json"]], "fuse_batch (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[9, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.fuse_batch"]], "fused_activation (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[9, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.fused_activation"]], "gather_in0 (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[9, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.gather_in0"]], "hop_cores (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[9, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.hop_cores"]], "in0_block_w (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[9, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.in0_block_w"]], "mcast_in0 (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[9, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.mcast_in0"]], "num_global_cb_receivers (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[9, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.num_global_cb_receivers"]], "out_block_h (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[9, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.out_block_h"]], "out_block_w (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[9, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.out_block_w"]], "out_subblock_h (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[9, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.out_subblock_h"]], "out_subblock_w (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[9, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.out_subblock_w"]], "per_core_m (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[9, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.per_core_M"]], "per_core_n (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[9, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.per_core_N"]], "to_json() (ttnn.matmulmulticorereusemulticast1dprogramconfig method)": [[9, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.to_json"]], "untilize_out (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[9, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.untilize_out"]], "matmulmulticorereusemulticastdramshardedprogramconfig (class in ttnn)": [[10, "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig"]], "from_json() (ttnn.matmulmulticorereusemulticastdramshardedprogramconfig method)": [[10, "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig.from_json"]], "fused_activation (ttnn.matmulmulticorereusemulticastdramshardedprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig.fused_activation"]], "in0_block_w (ttnn.matmulmulticorereusemulticastdramshardedprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig.in0_block_w"]], "per_core_m (ttnn.matmulmulticorereusemulticastdramshardedprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig.per_core_M"]], "per_core_n (ttnn.matmulmulticorereusemulticastdramshardedprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig.per_core_N"]], "to_json() (ttnn.matmulmulticorereusemulticastdramshardedprogramconfig method)": [[10, "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig.to_json"]], "matmulmulticorereusemulticastprogramconfig (class in ttnn)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig"]], "compute_with_storage_grid_size (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.compute_with_storage_grid_size"]], "from_json() (ttnn.matmulmulticorereusemulticastprogramconfig method)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.from_json"]], "fuse_batch (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.fuse_batch"]], "fused_activation (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.fused_activation"]], "in0_block_w (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.in0_block_w"]], "out_block_h (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.out_block_h"]], "out_block_w (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.out_block_w"]], "out_subblock_h (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.out_subblock_h"]], "out_subblock_w (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.out_subblock_w"]], "per_core_m (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.per_core_M"]], "per_core_n (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.per_core_N"]], "to_json() (ttnn.matmulmulticorereusemulticastprogramconfig method)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.to_json"]], "transpose_mcast (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.transpose_mcast"]], "matmulmulticorereuseprogramconfig (class in ttnn)": [[12, "ttnn.MatmulMultiCoreReuseProgramConfig"]], "compute_with_storage_grid_size (ttnn.matmulmulticorereuseprogramconfig property)": [[12, "ttnn.MatmulMultiCoreReuseProgramConfig.compute_with_storage_grid_size"]], "from_json() (ttnn.matmulmulticorereuseprogramconfig method)": [[12, "ttnn.MatmulMultiCoreReuseProgramConfig.from_json"]], "in0_block_w (ttnn.matmulmulticorereuseprogramconfig property)": [[12, "ttnn.MatmulMultiCoreReuseProgramConfig.in0_block_w"]], "out_subblock_h (ttnn.matmulmulticorereuseprogramconfig property)": [[12, "ttnn.MatmulMultiCoreReuseProgramConfig.out_subblock_h"]], "out_subblock_w (ttnn.matmulmulticorereuseprogramconfig property)": [[12, "ttnn.MatmulMultiCoreReuseProgramConfig.out_subblock_w"]], "per_core_m (ttnn.matmulmulticorereuseprogramconfig property)": [[12, "ttnn.MatmulMultiCoreReuseProgramConfig.per_core_M"]], "per_core_n (ttnn.matmulmulticorereuseprogramconfig property)": [[12, "ttnn.MatmulMultiCoreReuseProgramConfig.per_core_N"]], "to_json() (ttnn.matmulmulticorereuseprogramconfig method)": [[12, "ttnn.MatmulMultiCoreReuseProgramConfig.to_json"]], "setdefaultdevice() (in module ttnn)": [[13, "ttnn.SetDefaultDevice"]], "softmaxdefaultprogramconfig (class in ttnn)": [[14, "ttnn.SoftmaxDefaultProgramConfig"]], "softmaxprogramconfig (class in ttnn)": [[15, "ttnn.SoftmaxProgramConfig"]], "softmaxshardedmulticoreprogramconfig (class in ttnn)": [[16, "ttnn.SoftmaxShardedMultiCoreProgramConfig"]], "block_w (ttnn.softmaxshardedmulticoreprogramconfig property)": [[16, "ttnn.SoftmaxShardedMultiCoreProgramConfig.block_w"]], "abs (in module ttnn)": [[17, "ttnn.abs"]], "abs_bw (in module ttnn)": [[18, "ttnn.abs_bw"]], "acos (in module ttnn)": [[19, "ttnn.acos"]], "acos_bw (in module ttnn)": [[20, "ttnn.acos_bw"]], "acosh (in module ttnn)": [[21, "ttnn.acosh"]], "acosh_bw (in module ttnn)": [[22, "ttnn.acosh_bw"]], "add (in module ttnn)": [[23, "ttnn.add"]], "add_bw (in module ttnn)": [[24, "ttnn.add_bw"]], "addalpha (in module ttnn)": [[25, "ttnn.addalpha"]], "addalpha_bw (in module ttnn)": [[26, "ttnn.addalpha_bw"]], "addcdiv (in module ttnn)": [[27, "ttnn.addcdiv"]], "addcdiv_bw (in module ttnn)": [[28, "ttnn.addcdiv_bw"]], "addcmul (in module ttnn)": [[29, "ttnn.addcmul"]], "addcmul_bw (in module ttnn)": [[30, "ttnn.addcmul_bw"]], "addmm (in module ttnn)": [[31, "ttnn.addmm"]], "all_gather (in module ttnn)": [[32, "ttnn.all_gather"]], "all_reduce (in module ttnn)": [[33, "ttnn.all_reduce"]], "alt_complex_rotate90 (in module ttnn)": [[34, "ttnn.alt_complex_rotate90"]], "angle (in module ttnn)": [[35, "ttnn.angle"]], "angle_bw (in module ttnn)": [[36, "ttnn.angle_bw"]], "arange (in module ttnn)": [[37, "ttnn.arange"]], "argmax (in module ttnn)": [[38, "ttnn.argmax"]], "as_tensor (in module ttnn)": [[39, "ttnn.as_tensor"]], "asin (in module ttnn)": [[40, "ttnn.asin"]], "asin_bw (in module ttnn)": [[41, "ttnn.asin_bw"]], "asinh (in module ttnn)": [[42, "ttnn.asinh"]], "asinh_bw (in module ttnn)": [[43, "ttnn.asinh_bw"]], "assign_bw (in module ttnn)": [[44, "ttnn.assign_bw"]], "atan (in module ttnn)": [[45, "ttnn.atan"]], "atan2 (in module ttnn)": [[46, "ttnn.atan2"]], "atan2_bw (in module ttnn)": [[47, "ttnn.atan2_bw"]], "atan_bw (in module ttnn)": [[48, "ttnn.atan_bw"]], "atanh (in module ttnn)": [[49, "ttnn.atanh"]], "atanh_bw (in module ttnn)": [[50, "ttnn.atanh_bw"]], "batch_norm (in module ttnn)": [[51, "ttnn.batch_norm"]], "bias_gelu_bw (in module ttnn)": [[52, "ttnn.bias_gelu_bw"]], "bitwise_and (in module ttnn)": [[53, "ttnn.bitwise_and"]], "bitwise_left_shift (in module ttnn)": [[54, "ttnn.bitwise_left_shift"]], "bitwise_not (in module ttnn)": [[55, "ttnn.bitwise_not"]], "bitwise_or (in module ttnn)": [[56, "ttnn.bitwise_or"]], "bitwise_right_shift (in module ttnn)": [[57, "ttnn.bitwise_right_shift"]], "bitwise_xor (in module ttnn)": [[58, "ttnn.bitwise_xor"]], "cbrt (in module ttnn)": [[59, "ttnn.cbrt"]], "ceil (in module ttnn)": [[60, "ttnn.ceil"]], "ceil_bw (in module ttnn)": [[61, "ttnn.ceil_bw"]], "celu (in module ttnn)": [[62, "ttnn.celu"]], "celu_bw (in module ttnn)": [[63, "ttnn.celu_bw"]], "clamp (in module ttnn)": [[64, "ttnn.clamp"]], "clamp_bw (in module ttnn)": [[65, "ttnn.clamp_bw"]], "clip (in module ttnn)": [[66, "ttnn.clip"]], "clip_bw (in module ttnn)": [[67, "ttnn.clip_bw"]], "clone (in module ttnn)": [[68, "ttnn.clone"]], "close_device() (in module ttnn)": [[69, "ttnn.close_device"]], "concat (in module ttnn)": [[70, "ttnn.concat"]], "concat_bw (in module ttnn)": [[71, "ttnn.concat_bw"]], "conj (in module ttnn)": [[72, "ttnn.conj"]], "conj_bw (in module ttnn)": [[73, "ttnn.conj_bw"]], "conv1d (in module ttnn)": [[74, "ttnn.conv1d"]], "conv2d (in module ttnn)": [[75, "ttnn.conv2d"]], "conv_transpose2d (in module ttnn)": [[76, "ttnn.conv_transpose2d"]], "cos (in module ttnn)": [[77, "ttnn.cos"]], "cos_bw (in module ttnn)": [[78, "ttnn.cos_bw"]], "cosh (in module ttnn)": [[79, "ttnn.cosh"]], "cosh_bw (in module ttnn)": [[80, "ttnn.cosh_bw"]], "create_sharded_memory_config() (in module ttnn)": [[81, "ttnn.create_sharded_memory_config"]], "cumprod (in module ttnn)": [[82, "ttnn.cumprod"]], "cumsum (in module ttnn)": [[83, "ttnn.cumsum"]], "deallocate (in module ttnn)": [[84, "ttnn.deallocate"]], "deg2rad (in module ttnn)": [[85, "ttnn.deg2rad"]], "deg2rad_bw (in module ttnn)": [[86, "ttnn.deg2rad_bw"]], "digamma (in module ttnn)": [[87, "ttnn.digamma"]], "digamma_bw (in module ttnn)": [[88, "ttnn.digamma_bw"]], "div (in module ttnn)": [[89, "ttnn.div"]], "div_bw (in module ttnn)": [[90, "ttnn.div_bw"]], "div_no_nan (in module ttnn)": [[91, "ttnn.div_no_nan"]], "div_no_nan_bw (in module ttnn)": [[92, "ttnn.div_no_nan_bw"]], "dump_tensor (in module ttnn)": [[93, "ttnn.dump_tensor"]], "elu (in module ttnn)": [[94, "ttnn.elu"]], "elu_bw (in module ttnn)": [[95, "ttnn.elu_bw"]], "embedding (in module ttnn)": [[96, "ttnn.embedding"]], "embedding_bw (in module ttnn)": [[97, "ttnn.embedding_bw"]], "empty (in module ttnn)": [[98, "ttnn.empty"]], "empty_like (in module ttnn)": [[99, "ttnn.empty_like"]], "eq (in module ttnn)": [[100, "ttnn.eq"]], "eq_ (in module ttnn)": [[101, "ttnn.eq_"]], "eqz (in module ttnn)": [[102, "ttnn.eqz"]], "erf (in module ttnn)": [[103, "ttnn.erf"]], "erf_bw (in module ttnn)": [[104, "ttnn.erf_bw"]], "erfc (in module ttnn)": [[105, "ttnn.erfc"]], "erfc_bw (in module ttnn)": [[106, "ttnn.erfc_bw"]], "erfinv (in module ttnn)": [[107, "ttnn.erfinv"]], "erfinv_bw (in module ttnn)": [[108, "ttnn.erfinv_bw"]], "exp (in module ttnn)": [[109, "ttnn.exp"]], "exp2 (in module ttnn)": [[110, "ttnn.exp2"]], "exp2_bw (in module ttnn)": [[111, "ttnn.exp2_bw"]], "exp_bw (in module ttnn)": [[112, "ttnn.exp_bw"]], "conv3d (in module ttnn.experimental)": [[113, "ttnn.experimental.conv3d"]], "dropout (in module ttnn.experimental)": [[114, "ttnn.experimental.dropout"]], "gelu_bw (in module ttnn.experimental)": [[115, "ttnn.experimental.gelu_bw"]], "rotary_embedding (in module ttnn.experimental)": [[116, "ttnn.experimental.rotary_embedding"]], "expm1 (in module ttnn)": [[117, "ttnn.expm1"]], "expm1_bw (in module ttnn)": [[118, "ttnn.expm1_bw"]], "fill (in module ttnn)": [[119, "ttnn.fill"]], "fill_bw (in module ttnn)": [[120, "ttnn.fill_bw"]], "fill_ones_rm (in module ttnn)": [[121, "ttnn.fill_ones_rm"]], "fill_rm (in module ttnn)": [[122, "ttnn.fill_rm"]], "fill_zero_bw (in module ttnn)": [[123, "ttnn.fill_zero_bw"]], "floor (in module ttnn)": [[124, "ttnn.floor"]], "floor_bw (in module ttnn)": [[125, "ttnn.floor_bw"]], "floor_div (in module ttnn)": [[126, "ttnn.floor_div"]], "fmod (in module ttnn)": [[127, "ttnn.fmod"]], "fmod_bw (in module ttnn)": [[128, "ttnn.fmod_bw"]], "format_input_tensor() (in module ttnn)": [[129, "ttnn.format_input_tensor"]], "format_output_tensor() (in module ttnn)": [[130, "ttnn.format_output_tensor"]], "frac (in module ttnn)": [[131, "ttnn.frac"]], "frac_bw (in module ttnn)": [[132, "ttnn.frac_bw"]], "from_device (in module ttnn)": [[133, "ttnn.from_device"]], "from_torch (in module ttnn)": [[134, "ttnn.from_torch"]], "full (in module ttnn)": [[135, "ttnn.full"]], "full_like (in module ttnn)": [[136, "ttnn.full_like"]], "gather (in module ttnn)": [[137, "ttnn.gather"]], "gcd (in module ttnn)": [[138, "ttnn.gcd"]], "ge (in module ttnn)": [[139, "ttnn.ge"]], "ge_ (in module ttnn)": [[140, "ttnn.ge_"]], "geglu (in module ttnn)": [[141, "ttnn.geglu"]], "gelu (in module ttnn)": [[142, "ttnn.gelu"]], "gelu_bw (in module ttnn)": [[143, "ttnn.gelu_bw"]], "gez (in module ttnn)": [[144, "ttnn.gez"]], "global_avg_pool2d (in module ttnn)": [[145, "ttnn.global_avg_pool2d"]], "glu (in module ttnn)": [[146, "ttnn.glu"]], "group_norm (in module ttnn)": [[147, "ttnn.group_norm"]], "gt (in module ttnn)": [[148, "ttnn.gt"]], "gt_ (in module ttnn)": [[149, "ttnn.gt_"]], "gtz (in module ttnn)": [[150, "ttnn.gtz"]], "hardshrink (in module ttnn)": [[151, "ttnn.hardshrink"]], "hardshrink_bw (in module ttnn)": [[152, "ttnn.hardshrink_bw"]], "hardsigmoid (in module ttnn)": [[153, "ttnn.hardsigmoid"]], "hardsigmoid_bw (in module ttnn)": [[154, "ttnn.hardsigmoid_bw"]], "hardswish (in module ttnn)": [[155, "ttnn.hardswish"]], "hardswish_bw (in module ttnn)": [[156, "ttnn.hardswish_bw"]], "hardtanh (in module ttnn)": [[157, "ttnn.hardtanh"]], "hardtanh_bw (in module ttnn)": [[158, "ttnn.hardtanh_bw"]], "heaviside (in module ttnn)": [[159, "ttnn.heaviside"]], "hypot (in module ttnn)": [[160, "ttnn.hypot"]], "hypot_bw (in module ttnn)": [[161, "ttnn.hypot_bw"]], "i0 (in module ttnn)": [[162, "ttnn.i0"]], "i0_bw (in module ttnn)": [[163, "ttnn.i0_bw"]], "identity (in module ttnn)": [[164, "ttnn.identity"]], "imag (in module ttnn)": [[165, "ttnn.imag"]], "imag_bw (in module ttnn)": [[166, "ttnn.imag_bw"]], "indexed_fill (in module ttnn)": [[167, "ttnn.indexed_fill"]], "is_imag (in module ttnn)": [[168, "ttnn.is_imag"]], "is_real (in module ttnn)": [[169, "ttnn.is_real"]], "isclose (in module ttnn)": [[170, "ttnn.isclose"]], "isfinite (in module ttnn)": [[171, "ttnn.isfinite"]], "isinf (in module ttnn)": [[172, "ttnn.isinf"]], "isnan (in module ttnn)": [[173, "ttnn.isnan"]], "isneginf (in module ttnn)": [[174, "ttnn.isneginf"]], "isposinf (in module ttnn)": [[175, "ttnn.isposinf"]], "fill_cache_for_user_ (in module ttnn.kv_cache)": [[176, "ttnn.kv_cache.fill_cache_for_user_"]], "update_cache_for_token_ (in module ttnn.kv_cache)": [[177, "ttnn.kv_cache.update_cache_for_token_"]], "l1_loss (in module ttnn)": [[178, "ttnn.l1_loss"]], "layer_norm (in module ttnn)": [[179, "ttnn.layer_norm"]], "lcm (in module ttnn)": [[180, "ttnn.lcm"]], "ldexp (in module ttnn)": [[181, "ttnn.ldexp"]], "ldexp_bw (in module ttnn)": [[182, "ttnn.ldexp_bw"]], "le (in module ttnn)": [[183, "ttnn.le"]], "le_ (in module ttnn)": [[184, "ttnn.le_"]], "leaky_relu (in module ttnn)": [[185, "ttnn.leaky_relu"]], "leaky_relu_bw (in module ttnn)": [[186, "ttnn.leaky_relu_bw"]], "lerp (in module ttnn)": [[187, "ttnn.lerp"]], "lerp_bw (in module ttnn)": [[188, "ttnn.lerp_bw"]], "lez (in module ttnn)": [[189, "ttnn.lez"]], "lgamma (in module ttnn)": [[190, "ttnn.lgamma"]], "lgamma_bw (in module ttnn)": [[191, "ttnn.lgamma_bw"]], "linear (in module ttnn)": [[192, "ttnn.linear"]], "load_tensor (in module ttnn)": [[193, "ttnn.load_tensor"]], "log (in module ttnn)": [[194, "ttnn.log"]], "log10 (in module ttnn)": [[195, "ttnn.log10"]], "log10_bw (in module ttnn)": [[196, "ttnn.log10_bw"]], "log1p (in module ttnn)": [[197, "ttnn.log1p"]], "log1p_bw (in module ttnn)": [[198, "ttnn.log1p_bw"]], "log2 (in module ttnn)": [[199, "ttnn.log2"]], "log2_bw (in module ttnn)": [[200, "ttnn.log2_bw"]], "log_bw (in module ttnn)": [[201, "ttnn.log_bw"]], "log_sigmoid (in module ttnn)": [[202, "ttnn.log_sigmoid"]], "log_sigmoid_bw (in module ttnn)": [[203, "ttnn.log_sigmoid_bw"]], "logaddexp (in module ttnn)": [[204, "ttnn.logaddexp"]], "logaddexp2 (in module ttnn)": [[205, "ttnn.logaddexp2"]], "logaddexp2_bw (in module ttnn)": [[206, "ttnn.logaddexp2_bw"]], "logaddexp_bw (in module ttnn)": [[207, "ttnn.logaddexp_bw"]], "logical_and (in module ttnn)": [[208, "ttnn.logical_and"]], "logical_and_ (in module ttnn)": [[209, "ttnn.logical_and_"]], "logical_not (in module ttnn)": [[210, "ttnn.logical_not"]], "logical_not_ (in module ttnn)": [[211, "ttnn.logical_not_"]], "logical_or (in module ttnn)": [[212, "ttnn.logical_or"]], "logical_or_ (in module ttnn)": [[213, "ttnn.logical_or_"]], "logical_xor (in module ttnn)": [[214, "ttnn.logical_xor"]], "logical_xor_ (in module ttnn)": [[215, "ttnn.logical_xor_"]], "logit (in module ttnn)": [[216, "ttnn.logit"]], "logit_bw (in module ttnn)": [[217, "ttnn.logit_bw"]], "logiteps_bw (in module ttnn)": [[218, "ttnn.logiteps_bw"]], "lt (in module ttnn)": [[219, "ttnn.lt"]], "lt_ (in module ttnn)": [[220, "ttnn.lt_"]], "ltz (in module ttnn)": [[221, "ttnn.ltz"]], "mac (in module ttnn)": [[222, "ttnn.mac"]], "manage_device() (in module ttnn)": [[223, "ttnn.manage_device"]], "matmul (in module ttnn)": [[224, "ttnn.matmul"]], "matmul_batched_weights (in module ttnn)": [[225, "ttnn.matmul_batched_weights"]], "max (in module ttnn)": [[226, "ttnn.max"]], "max_bw (in module ttnn)": [[227, "ttnn.max_bw"]], "max_pool2d (in module ttnn)": [[228, "ttnn.max_pool2d"]], "maximum (in module ttnn)": [[229, "ttnn.maximum"]], "mean (in module ttnn)": [[230, "ttnn.mean"]], "min (in module ttnn)": [[231, "ttnn.min"]], "min_bw (in module ttnn)": [[232, "ttnn.min_bw"]], "minimum (in module ttnn)": [[233, "ttnn.minimum"]], "mish (in module ttnn)": [[234, "ttnn.mish"]], "preprocess_model() (in module ttnn.model_preprocessing)": [[235, "ttnn.model_preprocessing.preprocess_model"]], "preprocess_model_parameters() (in module ttnn.model_preprocessing)": [[236, "ttnn.model_preprocessing.preprocess_model_parameters"]], "moreh_sum (in module ttnn)": [[237, "ttnn.moreh_sum"]], "mse_loss (in module ttnn)": [[238, "ttnn.mse_loss"]], "mul_bw (in module ttnn)": [[239, "ttnn.mul_bw"]], "multigammaln (in module ttnn)": [[240, "ttnn.multigammaln"]], "multigammaln_bw (in module ttnn)": [[241, "ttnn.multigammaln_bw"]], "multiply (in module ttnn)": [[242, "ttnn.multiply"]], "ne (in module ttnn)": [[243, "ttnn.ne"]], "ne_ (in module ttnn)": [[244, "ttnn.ne_"]], "neg (in module ttnn)": [[245, "ttnn.neg"]], "neg_bw (in module ttnn)": [[246, "ttnn.neg_bw"]], "nextafter (in module ttnn)": [[247, "ttnn.nextafter"]], "nez (in module ttnn)": [[248, "ttnn.nez"]], "nonzero (in module ttnn)": [[249, "ttnn.nonzero"]], "normalize_global (in module ttnn)": [[250, "ttnn.normalize_global"]], "normalize_hw (in module ttnn)": [[251, "ttnn.normalize_hw"]], "ones (in module ttnn)": [[252, "ttnn.ones"]], "ones_like (in module ttnn)": [[253, "ttnn.ones_like"]], "open_device() (in module ttnn)": [[254, "ttnn.open_device"]], "outer (in module ttnn)": [[255, "ttnn.outer"]], "pad (in module ttnn)": [[256, "ttnn.pad"]], "pad_to_tile_shape() (in module ttnn)": [[257, "ttnn.pad_to_tile_shape"]], "permute (in module ttnn)": [[258, "ttnn.permute"]], "polar (in module ttnn)": [[259, "ttnn.polar"]], "polar_bw (in module ttnn)": [[260, "ttnn.polar_bw"]], "polygamma (in module ttnn)": [[261, "ttnn.polygamma"]], "polygamma_bw (in module ttnn)": [[262, "ttnn.polygamma_bw"]], "polyval (in module ttnn)": [[263, "ttnn.polyval"]], "pow (in module ttnn)": [[264, "ttnn.pow"]], "pow_bw (in module ttnn)": [[265, "ttnn.pow_bw"]], "prelu (in module ttnn)": [[266, "ttnn.prelu"]], "prepare_conv_bias() (in module ttnn)": [[267, "ttnn.prepare_conv_bias"]], "prepare_conv_transpose2d_bias() (in module ttnn)": [[268, "ttnn.prepare_conv_transpose2d_bias"]], "prepare_conv_transpose2d_weights() (in module ttnn)": [[269, "ttnn.prepare_conv_transpose2d_weights"]], "prepare_conv_weights() (in module ttnn)": [[270, "ttnn.prepare_conv_weights"]], "prod (in module ttnn)": [[271, "ttnn.prod"]], "prod_bw (in module ttnn)": [[272, "ttnn.prod_bw"]], "rad2deg (in module ttnn)": [[273, "ttnn.rad2deg"]], "rad2deg_bw (in module ttnn)": [[274, "ttnn.rad2deg_bw"]], "rand (in module ttnn)": [[275, "ttnn.rand"]], "rdiv (in module ttnn)": [[276, "ttnn.rdiv"]], "rdiv_bw (in module ttnn)": [[277, "ttnn.rdiv_bw"]], "real (in module ttnn)": [[278, "ttnn.real"]], "real_bw (in module ttnn)": [[279, "ttnn.real_bw"]], "reallocate (in module ttnn)": [[280, "ttnn.reallocate"]], "reciprocal (in module ttnn)": [[281, "ttnn.reciprocal"]], "reciprocal_bw (in module ttnn)": [[282, "ttnn.reciprocal_bw"]], "reduce_scatter (in module ttnn)": [[283, "ttnn.reduce_scatter"]], "register_post_operation_hook() (in module ttnn)": [[284, "ttnn.register_post_operation_hook"]], "register_pre_operation_hook() (in module ttnn)": [[285, "ttnn.register_pre_operation_hook"]], "reglu (in module ttnn)": [[286, "ttnn.reglu"]], "relu (in module ttnn)": [[287, "ttnn.relu"]], "relu6 (in module ttnn)": [[288, "ttnn.relu6"]], "relu6_bw (in module ttnn)": [[289, "ttnn.relu6_bw"]], "relu_bw (in module ttnn)": [[290, "ttnn.relu_bw"]], "relu_max (in module ttnn)": [[291, "ttnn.relu_max"]], "relu_min (in module ttnn)": [[292, "ttnn.relu_min"]], "remainder (in module ttnn)": [[293, "ttnn.remainder"]], "remainder_bw (in module ttnn)": [[294, "ttnn.remainder_bw"]], "repeat (in module ttnn)": [[295, "ttnn.repeat"]], "repeat_bw (in module ttnn)": [[296, "ttnn.repeat_bw"]], "repeat_interleave (in module ttnn)": [[297, "ttnn.repeat_interleave"]], "reshape (in module ttnn)": [[298, "ttnn.reshape"]], "rms_norm (in module ttnn)": [[299, "ttnn.rms_norm"]], "round (in module ttnn)": [[300, "ttnn.round"]], "round_bw (in module ttnn)": [[301, "ttnn.round_bw"]], "rpow (in module ttnn)": [[302, "ttnn.rpow"]], "rpow_bw (in module ttnn)": [[303, "ttnn.rpow_bw"]], "rsqrt (in module ttnn)": [[304, "ttnn.rsqrt"]], "rsqrt_bw (in module ttnn)": [[305, "ttnn.rsqrt_bw"]], "rsub (in module ttnn)": [[306, "ttnn.rsub"]], "rsub_bw (in module ttnn)": [[307, "ttnn.rsub_bw"]], "scale_causal_mask_hw_dims_softmax_in_place (in module ttnn)": [[308, "ttnn.scale_causal_mask_hw_dims_softmax_in_place"]], "scale_mask_softmax (in module ttnn)": [[309, "ttnn.scale_mask_softmax"]], "scale_mask_softmax_in_place (in module ttnn)": [[310, "ttnn.scale_mask_softmax_in_place"]], "scatter (in module ttnn)": [[311, "ttnn.scatter"]], "selu (in module ttnn)": [[312, "ttnn.selu"]], "selu_bw (in module ttnn)": [[313, "ttnn.selu_bw"]], "set_printoptions() (in module ttnn)": [[314, "ttnn.set_printoptions"]], "sigmoid (in module ttnn)": [[315, "ttnn.sigmoid"]], "sigmoid_accurate (in module ttnn)": [[316, "ttnn.sigmoid_accurate"]], "sigmoid_bw (in module ttnn)": [[317, "ttnn.sigmoid_bw"]], "sign (in module ttnn)": [[318, "ttnn.sign"]], "sign_bw (in module ttnn)": [[319, "ttnn.sign_bw"]], "signbit (in module ttnn)": [[320, "ttnn.signbit"]], "silu (in module ttnn)": [[321, "ttnn.silu"]], "silu_bw (in module ttnn)": [[322, "ttnn.silu_bw"]], "sin (in module ttnn)": [[323, "ttnn.sin"]], "sin_bw (in module ttnn)": [[324, "ttnn.sin_bw"]], "sinh (in module ttnn)": [[325, "ttnn.sinh"]], "sinh_bw (in module ttnn)": [[326, "ttnn.sinh_bw"]], "slice (in module ttnn)": [[327, "ttnn.slice"]], "softmax (in module ttnn)": [[328, "ttnn.softmax"]], "softmax_in_place (in module ttnn)": [[329, "ttnn.softmax_in_place"]], "softplus (in module ttnn)": [[330, "ttnn.softplus"]], "softplus_bw (in module ttnn)": [[331, "ttnn.softplus_bw"]], "softshrink (in module ttnn)": [[332, "ttnn.softshrink"]], "softshrink_bw (in module ttnn)": [[333, "ttnn.softshrink_bw"]], "softsign (in module ttnn)": [[334, "ttnn.softsign"]], "softsign_bw (in module ttnn)": [[335, "ttnn.softsign_bw"]], "sort (in module ttnn)": [[336, "ttnn.sort"]], "sparse_matmul (in module ttnn)": [[337, "ttnn.sparse_matmul"]], "sqrt (in module ttnn)": [[338, "ttnn.sqrt"]], "sqrt_bw (in module ttnn)": [[339, "ttnn.sqrt_bw"]], "square (in module ttnn)": [[340, "ttnn.square"]], "square_bw (in module ttnn)": [[341, "ttnn.square_bw"]], "squared_difference (in module ttnn)": [[342, "ttnn.squared_difference"]], "squared_difference_bw (in module ttnn)": [[343, "ttnn.squared_difference_bw"]], "std (in module ttnn)": [[344, "ttnn.std"]], "sub_bw (in module ttnn)": [[345, "ttnn.sub_bw"]], "subalpha (in module ttnn)": [[346, "ttnn.subalpha"]], "subalpha_bw (in module ttnn)": [[347, "ttnn.subalpha_bw"]], "subtract (in module ttnn)": [[348, "ttnn.subtract"]], "sum (in module ttnn)": [[349, "ttnn.sum"]], "swiglu (in module ttnn)": [[350, "ttnn.swiglu"]], "swish (in module ttnn)": [[351, "ttnn.swish"]], "synchronize_device() (in module ttnn)": [[352, "ttnn.synchronize_device"]], "tan (in module ttnn)": [[353, "ttnn.tan"]], "tan_bw (in module ttnn)": [[354, "ttnn.tan_bw"]], "tanh (in module ttnn)": [[355, "ttnn.tanh"]], "tanh_bw (in module ttnn)": [[356, "ttnn.tanh_bw"]], "tanhshrink (in module ttnn)": [[357, "ttnn.tanhshrink"]], "tanhshrink_bw (in module ttnn)": [[358, "ttnn.tanhshrink_bw"]], "threshold (in module ttnn)": [[359, "ttnn.threshold"]], "threshold_bw (in module ttnn)": [[360, "ttnn.threshold_bw"]], "tilize (in module ttnn)": [[361, "ttnn.tilize"]], "tilize_with_val_padding (in module ttnn)": [[362, "ttnn.tilize_with_val_padding"]], "to_device (in module ttnn)": [[363, "ttnn.to_device"]], "to_layout (in module ttnn)": [[364, "ttnn.to_layout"]], "to_memory_config (in module ttnn)": [[365, "ttnn.to_memory_config"]], "to_torch (in module ttnn)": [[366, "ttnn.to_torch"]], "topk (in module ttnn)": [[367, "ttnn.topk"]], "attention_softmax (in module ttnn.transformer)": [[368, "ttnn.transformer.attention_softmax"]], "attention_softmax_ (in module ttnn.transformer)": [[369, "ttnn.transformer.attention_softmax_"]], "concatenate_heads (in module ttnn.transformer)": [[370, "ttnn.transformer.concatenate_heads"]], "scaled_dot_product_attention (in module ttnn.transformer)": [[371, "ttnn.transformer.scaled_dot_product_attention"]], "scaled_dot_product_attention_decode (in module ttnn.transformer)": [[372, "ttnn.transformer.scaled_dot_product_attention_decode"]], "split_query_key_value_and_split_heads (in module ttnn.transformer)": [[373, "ttnn.transformer.split_query_key_value_and_split_heads"]], "tril (in module ttnn)": [[374, "ttnn.tril"]], "triu (in module ttnn)": [[375, "ttnn.triu"]], "trunc (in module ttnn)": [[376, "ttnn.trunc"]], "trunc_bw (in module ttnn)": [[377, "ttnn.trunc_bw"]], "unary_chain (in module ttnn)": [[378, "ttnn.unary_chain"]], "untilize (in module ttnn)": [[379, "ttnn.untilize"]], "untilize_with_unpadding (in module ttnn)": [[380, "ttnn.untilize_with_unpadding"]], "upsample (in module ttnn)": [[381, "ttnn.upsample"]], "var (in module ttnn)": [[382, "ttnn.var"]], "where (in module ttnn)": [[383, "ttnn.where"]], "where_bw (in module ttnn)": [[384, "ttnn.where_bw"]], "xlogy (in module ttnn)": [[385, "ttnn.xlogy"]], "xlogy_bw (in module ttnn)": [[386, "ttnn.xlogy_bw"]], "zeros (in module ttnn)": [[387, "ttnn.zeros"]], "zeros_like (in module ttnn)": [[388, "ttnn.zeros_like"]], "shape (class in ttnn)": [[395, "ttnn.Shape"]], "rank (ttnn.shape property)": [[395, "ttnn.Shape.rank"]], "to_rank() (ttnn.shape method)": [[395, "ttnn.Shape.to_rank"]]}})