#!/usr/bin/env python3
"""
Parse TT-Metal profiler reports and extract key metrics in a readable format.

This script parses the raw ops_perf_results CSV files generated by the tracy profiler
and produces cleaned-up prefill.csv and decode.csv files with relevant columns.

The script automatically:
  - Splits data into prefill and decode phases (based on first SdpaDecode op)
  - Extracts operation names from verbose OP CODE strings
  - Formats tensor shapes in a readable format
  - Calculates core utilization percentages
  - Optionally zeros out outlier ops that skew timing data

=============================================================================
USAGE
=============================================================================

    python scripts/parse_profiler_report.py <csv_file> [options]

=============================================================================
ARGUMENTS
=============================================================================

    csv_file            Path to the raw ops_perf_results_*.csv file from tracy

=============================================================================
OPTIONS
=============================================================================

    --sort-by COLUMN    Sort output by column (default: CALL_IDX)
                        Common columns: CALL_IDX, KERNEL_DUR_us, OP_NAME, CORES
    --filter-op NAME    Filter by operation name (substring match, case-insensitive)
                        Example: --filter-op Matmul, --filter-op AllGather
    --device-id ID      Filter by device ID (integer)
                        Example: --device-id 0
    --limit N           Limit output to first N rows per phase
    --phase PHASE       Show only 'prefill' or 'decode' phase
                        Default: show both phases separately
    --summary           Print summary statistics to terminal
                        Shows total time and percentage breakdown per operation
    --no-outliers       Zero out kernel duration for outlier ops that skew timing:
                        - DramPrefetcherOperation
                        - EmbeddingsDeviceOperation
    --output-dir DIR    Directory to write prefill.csv and decode.csv
                        Default: current directory

=============================================================================
OUTPUT FILES
=============================================================================

    prefill.csv         Parsed prefill phase operations
    decode.csv          Parsed decode phase operations
    summary.txt         Summary statistics (when run via run_profiler_sweep.sh)

    Output columns:
      - OP_NAME: Extracted operation name (e.g., MatmulDeviceOperation)
      - DEVICE_ID: Device ID
      - CALL_IDX: Global call count (unique identifier for each op invocation)
      - KERNEL_DUR_us: Kernel duration in microseconds
      - CORES: Number of cores used
      - AVAIL_CORES: Available worker cores
      - CORE_UTIL%: Core utilization percentage
      - MATH_FIDELITY: Math fidelity setting (HiFi2, HiFi4, LoFi)
      - IN0_SHAPE, IN1_SHAPE: Input tensor shapes
      - IN0_LAYOUT, IN0_DTYPE, IN0_MEM: Input tensor properties
      - OUT0_SHAPE, OUT0_MEM: Output tensor properties
      - FPU_UTIL%, DRAM_BW_UTIL%: Utilization percentages

=============================================================================
EXAMPLES
=============================================================================

    # Basic parsing with summary (most common usage)
    python scripts/parse_profiler_report.py report.csv --device-id 0 --summary --no-outliers

    # Parse and save to specific directory
    python scripts/parse_profiler_report.py report.csv --output-dir results/128/ --summary --no-outliers

    # Analyze only decode phase, sorted by kernel duration
    python scripts/parse_profiler_report.py report.csv --phase decode --sort-by KERNEL_DUR_us --summary

    # Filter to only attention operations
    python scripts/parse_profiler_report.py report.csv --filter-op Sdpa --summary

    # Filter to only matmul operations on device 0
    python scripts/parse_profiler_report.py report.csv --filter-op Matmul --device-id 0 --summary

    # Quick look at top 20 ops by duration
    python scripts/parse_profiler_report.py report.csv --sort-by KERNEL_DUR_us --limit 20

=============================================================================
RELATED SCRIPTS
=============================================================================

    run_profiler_sweep.sh         - Automates profiling across prompt lengths
    compare_profiler_results.py   - Compare results across prompt lengths
    compare_two_runs.py           - Compare two different runs (Excel output)
"""

import argparse
import csv
import re
import sys
from collections import defaultdict
from pathlib import Path

# Operations to zero out as outliers (they skew timing data)
OUTLIER_OPS = {"DramPrefetcherOperation", "EmbeddingsDeviceOperation"}


def extract_op_name(full_name: str) -> str:
    """Extract the operation name from the full OP CODE string.

    Examples:
        MeshDeviceOperationAdapter<ttnn::prim::AllGatherAsyncDeviceOperation> -> AllGatherAsyncDeviceOperation
        MeshDeviceOperationAdapter<ttnn::operations::matmul::MatmulDeviceOperation> -> MatmulDeviceOperation
    """
    # Handle MeshDeviceOperationAdapter<...::XxxOperation> - get the last component before >
    match = re.search(r"<.*::(\w+)>", full_name)
    if match:
        return match.group(1)
    # Handle other formats ending in Operation or Op
    match = re.search(r"(\w+Operation|\w+Op)$", full_name)
    if match:
        return match.group(1)
    return full_name


def format_shape(w, z, y, x) -> str:
    """Format tensor shape as WxZxYxX or simplified versions.

    Input format: "128[128]" or "4096[4096]"
    Output format: "128[128]x4096[4096]" or simplified if dimensions are 1
    """
    parts = []
    dims = [w, z, y, x]

    for val in dims:
        if val and val.strip():
            val = val.strip()
            # Skip dimensions that are just "1[1]"
            if val != "1[1]":
                parts.append(val)

    if not parts:
        return ""

    # Join with 'x'
    return "x".join(parts)


def clean_memory(mem: str) -> str:
    """Clean memory config string."""
    if not mem:
        return ""
    # Remove DEV_N_ prefix
    mem = re.sub(r"DEV_\d+_", "", mem)
    return mem


def parse_row(row: dict) -> dict:
    """Parse a CSV row and extract the required fields."""
    # Get kernel duration in microseconds
    kernel_dur_ns = row.get("DEVICE KERNEL DURATION [ns]", "0")
    try:
        kernel_dur_us = float(kernel_dur_ns) / 1000.0
    except (ValueError, TypeError):
        kernel_dur_us = 0.0

    # Get core counts
    try:
        cores = int(row.get("CORE COUNT", 0))
    except (ValueError, TypeError):
        cores = 0

    try:
        avail_cores = int(row.get("AVAILABLE WORKER CORE COUNT", 0))
    except (ValueError, TypeError):
        avail_cores = 0

    # Calculate core utilization
    core_util = (cores / avail_cores * 100) if avail_cores > 0 else 0.0

    # Format shapes
    in0_shape = format_shape(
        row.get("INPUT_0_W_PAD[LOGICAL]", ""),
        row.get("INPUT_0_Z_PAD[LOGICAL]", ""),
        row.get("INPUT_0_Y_PAD[LOGICAL]", ""),
        row.get("INPUT_0_X_PAD[LOGICAL]", ""),
    )

    in1_shape = format_shape(
        row.get("INPUT_1_W_PAD[LOGICAL]", ""),
        row.get("INPUT_1_Z_PAD[LOGICAL]", ""),
        row.get("INPUT_1_Y_PAD[LOGICAL]", ""),
        row.get("INPUT_1_X_PAD[LOGICAL]", ""),
    )

    out0_shape = format_shape(
        row.get("OUTPUT_0_W_PAD[LOGICAL]", ""),
        row.get("OUTPUT_0_Z_PAD[LOGICAL]", ""),
        row.get("OUTPUT_0_Y_PAD[LOGICAL]", ""),
        row.get("OUTPUT_0_X_PAD[LOGICAL]", ""),
    )

    # Get FPU and DRAM BW utilization
    try:
        fpu_util = float(row.get("PM FPU UTIL (%)", 0) or 0)
    except (ValueError, TypeError):
        fpu_util = 0.0

    try:
        dram_bw_util = float(row.get("DRAM BW UTIL (%)", 0) or 0)
    except (ValueError, TypeError):
        dram_bw_util = 0.0

    return {
        "OP_NAME": extract_op_name(row.get("OP CODE", "")),
        "DEVICE_ID": row.get("DEVICE ID", ""),
        "CALL_IDX": row.get("GLOBAL CALL COUNT", ""),
        "KERNEL_DUR_us": kernel_dur_us,
        "CORES": cores,
        "AVAIL_CORES": avail_cores,
        "CORE_UTIL%": core_util,
        "MATH_FIDELITY": row.get("MATH FIDELITY", ""),
        "IN0_SHAPE": in0_shape,
        "IN0_LAYOUT": row.get("INPUT_0_LAYOUT", ""),
        "IN0_DTYPE": row.get("INPUT_0_DATATYPE", ""),
        "IN0_MEM": clean_memory(row.get("INPUT_0_MEMORY", "")),
        "IN1_SHAPE": in1_shape,
        "IN1_MEM": clean_memory(row.get("INPUT_1_MEMORY", "")),
        "OUT0_SHAPE": out0_shape,
        "OUT0_MEM": clean_memory(row.get("OUTPUT_0_MEMORY", "")),
        "FPU_UTIL%": fpu_util,
        "DRAM_BW_UTIL%": dram_bw_util,
    }


def split_prefill_decode(data: list) -> tuple:
    """Split data into prefill and decode phases based on first SdpaDecode occurrence.

    Returns:
        (prefill_data, decode_data) tuple
    """
    # Find the first SdpaDecode index
    first_sdpa_decode_idx = None
    for i, row in enumerate(data):
        if "SdpaDecode" in row.get("OP_NAME", ""):
            first_sdpa_decode_idx = i
            break

    if first_sdpa_decode_idx is None:
        # No SdpaDecode found - all data is prefill
        return data, []

    prefill_data = data[:first_sdpa_decode_idx]
    decode_data = data[first_sdpa_decode_idx:]

    return prefill_data, decode_data


def zero_outliers(data: list) -> list:
    """Zero out kernel duration for outlier operations."""
    for row in data:
        if row.get("OP_NAME", "") in OUTLIER_OPS:
            row["KERNEL_DUR_us"] = 0.0
    return data


def compute_summary(data: list, phase_name: str = None) -> dict:
    """Compute summary statistics for operations.

    Returns:
        Dictionary with op_name -> {total_time_us, count, percentage, min_cores, max_cores}
    """
    op_stats = defaultdict(lambda: {"total_time_us": 0.0, "count": 0, "min_cores": float("inf"), "max_cores": 0})
    total_time = 0.0

    for row in data:
        op_name = row.get("OP_NAME", "Unknown")
        kernel_dur = row.get("KERNEL_DUR_us", 0.0)
        cores = row.get("CORES", 0)
        op_stats[op_name]["total_time_us"] += kernel_dur
        op_stats[op_name]["count"] += 1
        if cores > 0:
            op_stats[op_name]["min_cores"] = min(op_stats[op_name]["min_cores"], cores)
            op_stats[op_name]["max_cores"] = max(op_stats[op_name]["max_cores"], cores)
        total_time += kernel_dur

    # Calculate percentages and fix min_cores for ops with no core data
    for op_name in op_stats:
        if total_time > 0:
            op_stats[op_name]["percentage"] = (op_stats[op_name]["total_time_us"] / total_time) * 100
        else:
            op_stats[op_name]["percentage"] = 0.0
        # If min_cores is still inf, set to 0
        if op_stats[op_name]["min_cores"] == float("inf"):
            op_stats[op_name]["min_cores"] = 0

    return {"phase": phase_name, "total_time_us": total_time, "op_stats": dict(op_stats)}


def print_summary(summary: dict):
    """Print summary statistics to terminal."""
    phase = summary.get("phase", "All")
    total_time = summary.get("total_time_us", 0.0)
    op_stats = summary.get("op_stats", {})

    if not op_stats:
        print(f"\n=== {phase} Summary ===")
        print("No data to summarize")
        return

    # Sort by total time descending
    sorted_ops = sorted(op_stats.items(), key=lambda x: x[1]["total_time_us"], reverse=True)

    print(f"\n{'='*80}")
    print(f"=== {phase} Summary ===")
    print(f"{'='*80}")
    print(f"Total time: {total_time:.2f} us ({total_time/1000:.2f} ms)")
    print(f"{'='*80}")

    # Calculate column widths
    max_op_len = max(len(op) for op in op_stats.keys())
    max_op_len = max(max_op_len, len("OP_NAME"))

    header = f"{'OP_NAME':<{max_op_len}}  {'TOTAL_TIME_us':>14}  {'COUNT':>6}  {'PERCENT':>8}  {'MIN_CORES':>10}  {'MAX_CORES':>10}"
    print(header)
    print("-" * len(header))

    for op_name, stats in sorted_ops:
        print(
            f"{op_name:<{max_op_len}}  {stats['total_time_us']:>14.2f}  {stats['count']:>6}  {stats['percentage']:>7.2f}%  {stats['min_cores']:>10}  {stats['max_cores']:>10}"
        )


def write_csv(data: list, output_path: Path, phase_label: str = None):
    """Write data to a CSV file."""
    if not data:
        print(f"No data to write for {phase_label or 'output'}")
        return

    columns = [
        "OP_NAME",
        "DEVICE_ID",
        "CALL_IDX",
        "KERNEL_DUR_us",
        "CORES",
        "AVAIL_CORES",
        "CORE_UTIL%",
        "MATH_FIDELITY",
        "IN0_SHAPE",
        "IN0_LAYOUT",
        "IN0_DTYPE",
        "IN0_MEM",
        "IN1_SHAPE",
        "IN1_MEM",
        "OUT0_SHAPE",
        "OUT0_MEM",
        "FPU_UTIL%",
        "DRAM_BW_UTIL%",
    ]

    with open(output_path, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(columns)
        for row in data:
            values = []
            for col in columns:
                val = row.get(col, "")
                if isinstance(val, float):
                    if col in ["KERNEL_DUR_us"]:
                        values.append(f"{val:.2f}")
                    elif col in ["CORE_UTIL%", "FPU_UTIL%", "DRAM_BW_UTIL%"]:
                        values.append(f"{val:.1f}")
                    else:
                        values.append(str(val))
                else:
                    values.append(str(val))
            writer.writerow(values)

    print(f"Wrote {len(data)} rows to {output_path}")


def main():
    parser = argparse.ArgumentParser(description="Parse TT-Metal profiler reports")
    parser.add_argument("csv_file", help="Path to the CSV file")
    parser.add_argument("--sort-by", default="CALL_IDX", help="Sort by column (default: CALL_IDX)")
    parser.add_argument("--filter-op", help="Filter by operation name (substring match)")
    parser.add_argument("--device-id", type=int, help="Filter by device ID")
    parser.add_argument("--limit", type=int, help="Limit output to N rows")
    parser.add_argument(
        "--phase",
        choices=["prefill", "decode"],
        help="Show only 'prefill' or 'decode' phase (default: show both separately)",
    )
    parser.add_argument("--summary", action="store_true", help="Show summary statistics (total time and %% per op)")
    parser.add_argument(
        "--no-outliers",
        action="store_true",
        help="Zero out kernel duration for outlier ops (DramPrefetcher, EmbeddingsDeviceOperation)",
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default=".",
        help="Directory to write prefill.csv and decode.csv (default: current directory)",
    )

    args = parser.parse_args()

    csv_path = Path(args.csv_file)
    if not csv_path.exists():
        print(f"Error: File not found: {csv_path}")
        sys.exit(1)

    # Parse the CSV
    data = []
    with open(csv_path, "r") as f:
        reader = csv.DictReader(f)
        for row in reader:
            # Skip host-only entries (no device ID or no core count)
            # These are summary rows without actual device execution data
            device_id = row.get("DEVICE ID", "").strip()
            core_count = row.get("CORE COUNT", "").strip()
            if not device_id or not core_count:
                continue

            parsed = parse_row(row)

            # Apply filters
            if args.filter_op and args.filter_op.lower() not in parsed["OP_NAME"].lower():
                continue
            if args.device_id is not None and str(args.device_id) != str(parsed["DEVICE_ID"]):
                continue

            data.append(parsed)

    # Sort by CALL_IDX first to ensure proper order for phase splitting
    try:
        data.sort(key=lambda x: float(x.get("CALL_IDX", 0)) if x.get("CALL_IDX") else 0)
    except (ValueError, TypeError):
        pass

    # Zero out outliers if requested
    if args.no_outliers:
        data = zero_outliers(data)

    # Split into prefill/decode phases
    prefill_data, decode_data = split_prefill_decode(data)

    # Determine which data to show based on --phase
    # Default behavior (no --phase): show both prefill and decode separately
    if args.phase == "prefill":
        display_data = [("Prefill", prefill_data)]
    elif args.phase == "decode":
        display_data = [("Decode", decode_data)]
    else:
        # 'both' or None (default) - show both phases separately
        display_data = [("Prefill", prefill_data), ("Decode", decode_data)]

    # Prepare output directory
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    for phase_label, phase_data in display_data:
        if not phase_data:
            if phase_label:
                print(f"\n=== {phase_label} Phase ===")
                print("No data for this phase")
            continue

        # Sort by requested column
        sort_col = args.sort_by
        if sort_col in phase_data[0] if phase_data else []:
            try:
                # Try numeric sort first
                phase_data.sort(key=lambda x: float(x.get(sort_col, 0)) if x.get(sort_col) else 0)
            except (ValueError, TypeError):
                # Fall back to string sort
                phase_data.sort(key=lambda x: str(x.get(sort_col, "")))

        # Limit
        limited_data = phase_data
        if args.limit:
            limited_data = phase_data[: args.limit]

        # Write to CSV file
        output_filename = f"{phase_label.lower()}.csv" if phase_label else "output.csv"
        output_path = output_dir / output_filename
        write_csv(limited_data, output_path, phase_label)

        # Print summary to terminal
        if args.summary:
            summary = compute_summary(phase_data, phase_label or "All")
            print_summary(summary)


if __name__ == "__main__":
    main()
