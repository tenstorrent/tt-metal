Clang Static Analyzer justification report
Input: debug3.log
Findings: 74 (analyzer=clangsa)

========================================================================
Finding #1
========================================================================
File:    /work/tt_metal/llrt/rtoptions.hpp
Line:    136 (column 7)
Checker: optin.performance.Padding
Severity: LOW
Report hash: 9b767189680ff608455d9131c04a6dab

Summary:
  Excessive padding in 'class tt::llrt::RunTimeOptions' (67 padding bytes, where 3 is optimal). Optimal fields order: root_dir, cache_dir_, logs_dir_, kernel_dir, system_kernel_dir, core_grid_override_todeprecate, custom_fabric_mesh_graph_desc_path, profiler_noc_events_report_path, visible_devices, arch_name, mock_cluster_desc_path, dispatch_timeout_command_to_execute, watcher_waypoint_str, watcher_noc_sanitize_str, watcher_assert_str, watcher_pause_str, watcher_ring_buffer_str, watcher_stack_usage_str, watcher_dispatch_str, watcher_eth_link_status_str, watcher_sanitize_read_only_l1_str, watcher_sanitize_write_only_l1_str, simulator_path, watcher_disabled_features, inspector_settings, fabric_telemetry_settings, feature_targets, profiler_perf_counter_mode, watcher_debug_delay, num_hw_cqs, dispatch_core_type, arc_debug_buffer_size, timeout_duration_for_operations, dispatch_progress_update_ms, profiler_program_support_count, reliability_mode, fabric_router_sync_timeout_ms, watcher_settings, is_cache_dir_env_var_set, is_kernel_dir_env_var_set, is_core_grid_override_todeprecate_env_var_set, is_custom_fabric_mesh_graph_desc_path_set, build_map_enabled, record_noc_transfer_data, lightweight_kernel_asserts, enable_llk_asserts, disable_sfploadmacro, fabric_profiling_settings, test_mode_enabled, profiler_enabled, profile_dispatch_cores, profiler_sync_enabled, profiler_mid_run_dump, profiler_trace_profiler, profiler_trace_tracking, profiler_cpp_post_process, profiler_sum, profiler_buffer_usage_enabled, profiler_noc_events_enabled, profiler_disable_dump_to_files, profiler_disable_push_to_tracy, experimental_noc_debug_dump_enabled, null_kernels, kernels_early_return, clear_l1, clear_dram, skip_loading_fw, jit_analytics_enabled, riscv_debug_info_enabled, validate_kernel_binaries, using_slow_dispatch, enable_dispatch_data_collection, enable_hw_cache_invalidation, skip_deleting_built_cache, erisc_iram_enabled, fast_dispatch, skip_eth_cores_with_retrain, disable_relaxed_memory_ordering, enable_gathering, disable_dma_ops, force_context_reinit, tracy_mid_run_push, disable_fabric_2_erisc_mode, enable_2_erisc_mode, log_kernels_compilation_commands, enable_fabric_bw_telemetry, enable_fabric_telemetry, runtime_target_device_, use_mesh_graph_descriptor_2_0, force_jit_compile, numa_based_affinity, disable_xip_dump, dump_build_commands, erisc_iram_enabled_env_var, consider reordering the fields or adding explicit padding members

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt_metal/llrt/rtoptions.hpp:136:7
  → Excessive padding in 'class tt::llrt::RunTimeOptions' (67 padding bytes, where 3 is optimal). Optimal fields order: root_dir, cache_dir_, logs_dir_, kernel_dir, system_kernel_dir, core_grid_override_todeprecate, custom_fabric_mesh_graph_desc_path, profiler_noc_events_report_path, visible_devices, arch_name, mock_cluster_desc_path, dispatch_timeout_command_to_execute, watcher_waypoint_str, watcher_noc_sanitize_str, watcher_assert_str, watcher_pause_str, watcher_ring_buffer_str, watcher_stack_usage_str, watcher_dispatch_str, watcher_eth_link_status_str, watcher_sanitize_read_only_l1_str, watcher_sanitize_write_only_l1_str, simulator_path, watcher_disabled_features, inspector_settings, fabric_telemetry_settings, feature_targets, profiler_perf_counter_mode, watcher_debug_delay, num_hw_cqs, dispatch_core_type, arc_debug_buffer_size, timeout_duration_for_operations, dispatch_progress_update_ms, profiler_program_support_count, reliability_mode, fabric_router_sync_timeout_ms, watcher_settings, is_cache_dir_env_var_set, is_kernel_dir_env_var_set, is_core_grid_override_todeprecate_env_var_set, is_custom_fabric_mesh_graph_desc_path_set, build_map_enabled, record_noc_transfer_data, lightweight_kernel_asserts, enable_llk_asserts, disable_sfploadmacro, fabric_profiling_settings, test_mode_enabled, profiler_enabled, profile_dispatch_cores, profiler_sync_enabled, profiler_mid_run_dump, profiler_trace_profiler, profiler_trace_tracking, profiler_cpp_post_process, profiler_sum, profiler_buffer_usage_enabled, profiler_noc_events_enabled, profiler_disable_dump_to_files, profiler_disable_push_to_tracy, experimental_noc_debug_dump_enabled, null_kernels, kernels_early_return, clear_l1, clear_dram, skip_loading_fw, jit_analytics_enabled, riscv_debug_info_enabled, validate_kernel_binaries, using_slow_dispatch, enable_dispatch_data_collection, enable_hw_cache_invalidation, skip_deleting_built_cache, erisc_iram_enabled, fast_dispatch, skip_eth_cores_with_retrain, disable_relaxed_memory_ordering, enable_gathering, disable_dma_ops, force_context_reinit, tracy_mid_run_push, disable_fabric_2_erisc_mode, enable_2_erisc_mode, log_kernels_compilation_commands, enable_fabric_bw_telemetry, enable_fabric_telemetry, runtime_target_device_, use_mesh_graph_descriptor_2_0, force_jit_compile, numa_based_affinity, disable_xip_dump, dump_build_commands, erisc_iram_enabled_env_var, consider reordering the fields or adding explicit padding members
   134 | };
   135 |
   136 | class RunTimeOptions {
                   ^
   137 |     std::string root_dir;
   138 |

========================================================================
Finding #2
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/prefetcher/prefetcher/device/dram_prefetcher_program_factory.cpp
Line:    277 (column 54)
Checker: core.UndefinedBinaryOperatorResult
Severity: HIGH
Report hash: ed73cfbc55bf855dc47dd35b68f09b9e

Summary:
  The result of the '/' expression is undefined

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/prefetcher/prefetcher/device/dram_prefetcher_program_factory.cpp:39:5
  → Assuming the condition is true
    37 |     Tensor& /*output_tensor*/) {
    38 |     const auto& input_tensors = tensor_args.input_tensors;
    39 |     TT_FATAL(!input_tensors.empty(), "Must have at least one input tensor");
                 ^
    40 |     TT_FATAL(operation_attributes.global_cb.has_value(), "Global circular buffer must be provided");
    41 |     const auto& global_cb = *(operation_attributes.global_cb);

  Step 2: /work/ttnn/cpp/ttnn/operations/prefetcher/prefetcher/device/dram_prefetcher_program_factory.cpp:40:5
  → Assuming the condition is false
    38 |     const auto& input_tensors = tensor_args.input_tensors;
    39 |     TT_FATAL(!input_tensors.empty(), "Must have at least one input tensor");
    40 |     TT_FATAL(operation_attributes.global_cb.has_value(), "Global circular buffer must be provided");
                 ^
    41 |     const auto& global_cb = *(operation_attributes.global_cb);
    42 |     const uint32_t num_layers = operation_attributes.num_layers;

  Step 3: /work/ttnn/cpp/ttnn/operations/prefetcher/prefetcher/device/dram_prefetcher_program_factory.cpp:87:26
  → Assuming 't' is >= 'num_tensors'
    85 |     std::vector<std::vector<uint32_t>> tensor_shapes(num_tensors, std::vector<uint32_t>(2));
    86 |     std::vector<uint32_t> tensor_tile_sizes(num_tensors);
    87 |     for (uint32_t t = 0; t < num_tensors; t++) {
                                      ^
    88 |         uint32_t height_in_tiles = tensor_buffers[t]->shard_spec().shape()[0] / tensor_tiles[t].get_tile_shape()[0];
    89 |         uint32_t width_in_tiles = tensor_buffers[t]->shard_spec().shape()[1] / tensor_tiles[t].get_tile_shape()[1];

  Step 4: /work/ttnn/cpp/ttnn/operations/prefetcher/prefetcher/device/dram_prefetcher_program_factory.cpp:106:5
  → Assuming the condition is true
   104 |     uint32_t max_tensor_size = max_block_size_per_reader_core / num_receivers_per_reader * num_blocks;
   105 |
   106 |     TT_FATAL(
                 ^
   107 |         max_tensor_size <= global_cb.size(),
   108 |         "largest tensor {} must fit in global cb {}",

  Step 5: /work/ttnn/cpp/ttnn/operations/prefetcher/prefetcher/device/dram_prefetcher_program_factory.cpp:116:26
  → Assuming 'i' is >= 'num_readers'
   114 |     auto reader_core_range_vec = corerange_to_cores(all_reader_core_range, std::nullopt, true);
   115 |     std::vector<CoreRange> active_reader_core_range_vec;
   116 |     for (uint32_t i = 0; i < num_readers; ++i) {
                                      ^
   117 |         auto core = reader_core_range_vec[i];
   118 |         active_reader_core_range_vec.push_back(CoreRange{core, core});

  Step 6: /work/ttnn/cpp/ttnn/operations/prefetcher/prefetcher/device/dram_prefetcher_program_factory.cpp:127:5
  → Assuming the condition is true
   125 |     uint32_t reader_cb_size = max_block_size_per_reader_core * total_num_blocks_in_buffer;
   126 |
   127 |     TT_FATAL(reader_cb_size <= global_cb.size(), "reader_cb_size must not be larger than global cb");
                 ^
   128 |
   129 |     uint32_t reader_cb_index = tt::CBIndex::c_0;

  Step 7: /work/ttnn/cpp/ttnn/operations/prefetcher/prefetcher/device/dram_prefetcher_program_factory.cpp:228:26
  → Loop body executed 0 times
   226 |     uint32_t max_page_size = 8192;
   227 |
   228 |     for (uint32_t t = 0; t < num_tensors; t++) {
                                      ^
   229 |         auto [page_size, num_pages] = get_max_page_size_and_num_pages(
   230 |             max_page_size, tensor_block_num_tiles[t], tt::tile_size(tensor_data_formats[t]));

  Step 8: /work/ttnn/cpp/ttnn/operations/prefetcher/prefetcher/device/dram_prefetcher_program_factory.cpp:245:35
  → Assuming the condition is true
   243 |
   244 |     // Runtime args for the reader cores
   245 |     for (uint32_t core_index = 0; core_index < reader_core_range.num_cores(); core_index++) {
                                               ^
   246 |         const auto& core = reader_cores[core_index];
   247 |

  Step 9: /work/ttnn/cpp/ttnn/operations/prefetcher/prefetcher/device/dram_prefetcher_program_factory.cpp:254:28
  → Loop body executed 0 times
   252 |
   253 |         // Compare with previous cores' vc
   254 |         for (size_t j = 0; j < core_index; ++j) {
                                        ^
   255 |             const CoreCoord& prev_core = reader_cores[j];
   256 |             if (prev_core.y == core.y and

  Step 10: /work/ttnn/cpp/ttnn/operations/prefetcher/prefetcher/device/dram_prefetcher_program_factory.cpp:276:32
  → Entering loop body
   274 |         writer_rt_args.insert(writer_rt_args.end(), tensor_block_num_tiles.begin(), tensor_block_num_tiles.end());
   275 |         writer_rt_args.insert(writer_rt_args.end(), tensor_tile_sizes.begin(), tensor_tile_sizes.end());
   276 |         for (auto tensor_shape : tensor_shapes) {  // block_height_in_itles
                                            ^
   277 |             writer_rt_args.push_back(tensor_shape[0] / num_blocks);
   278 |         }

  Step 11: /work/ttnn/cpp/ttnn/operations/prefetcher/prefetcher/device/dram_prefetcher_program_factory.cpp:277:54
  → The result of the '/' expression is undefined
   275 |         writer_rt_args.insert(writer_rt_args.end(), tensor_tile_sizes.begin(), tensor_tile_sizes.end());
   276 |         for (auto tensor_shape : tensor_shapes) {  // block_height_in_itles
   277 |             writer_rt_args.push_back(tensor_shape[0] / num_blocks);
                                                                  ^
   278 |         }
   279 |

========================================================================
Finding #3
========================================================================
File:    /work/tests/tt_metal/tt_metal/perf_microbenchmark/routing/test_tt_fabric_mux_bandwidth.cpp
Line:    326 (column 9)
Checker: cplusplus.StringChecker
Severity: HIGH
Report hash: 55d950c40cc88fb88d9cf6af4bbb4eeb

Summary:
  The parameter must not be null

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tests/tt_metal/tt_metal/perf_microbenchmark/routing/test_tt_fabric_mux_bandwidth.cpp:326:9
  → The parameter must not be null
   324 | int main(int argc, char** argv) {
   325 |     const std::string default_log_file_path =
   326 |         std::string(std::getenv("TT_METAL_HOME")) + "/generated/fabric_mux_bandwidth_temp.txt";
                     ^
   327 |     const size_t default_num_full_size_channels = 8;
   328 |     const size_t default_num_header_only_channels = 0;

========================================================================
Finding #4
========================================================================
File:    /work/tt_metal/distributed/mesh_device.cpp
Line:    242 (column 57)
Checker: optin.cplusplus.VirtualCall
Severity: MEDIUM
Report hash: fa9ecd264417a88f7747fd479113446a

Summary:
  Call to virtual method 'MeshDeviceImpl::id' during construction bypasses virtual dispatch

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt_metal/distributed/mesh_device.cpp:370:12
  → Calling 'MeshDeviceImpl::create_unit_meshes'
   368 |     tt::stl::Span<const std::uint32_t> l1_bank_remap,
   369 |     size_t worker_l1_size) {
   370 |     return MeshDeviceImpl::create_unit_meshes(
                        ^
   371 |         device_ids,
   372 |         l1_small_size,

  Step 2: /work/tt_metal/distributed/mesh_device.cpp:380:1
  → Entered call from 'MeshDevice::create_unit_meshes'
   378 | }
   379 |
   380 | std::map<int, std::shared_ptr<MeshDevice>> MeshDeviceImpl::create_unit_meshes(
             ^
   381 |     const std::vector<int>& device_ids,
   382 |     size_t l1_small_size,

  Step 3: /work/tt_metal/distributed/mesh_device.cpp:388:5
  → Assuming the condition is true
   386 |     tt::stl::Span<const std::uint32_t> /*l1_bank_remap*/,
   387 |     size_t worker_l1_size) {
   388 |     TT_FATAL(
                 ^
   389 |         !device_ids.empty(), "Cannot create unit meshes with empty device_ids. At least one device ID is required.");
   390 |

  Step 4: /work/tt_metal/distributed/mesh_device.cpp:395:32
  → Loop body skipped when range is empty
   393 |     std::vector<tt::tt_fabric::FabricNodeId> fabric_node_ids;
   394 |     fabric_node_ids.reserve(device_ids.size());
   395 |     for (const auto& device_id : device_ids) {
                                            ^
   396 |         auto fabric_node_id =
   397 |             MetalContext::instance().get_control_plane().get_fabric_node_id_from_physical_chip_id(device_id);

  Step 5: /work/tt_metal/distributed/mesh_device.cpp:409:10
  → Assuming the condition is false
   407 |     // Now create ScopedDevices after validation passes
   408 |     auto mapped_devices_full_system_device_ids =
   409 |         (*MetalContext::instance().global_distributed_context().size() > 1)
                      ^
   410 |             ? SystemMesh::instance().get_mapped_devices(std::nullopt).device_ids
   411 |             : wrap_to_maybe_remote(device_ids);

  Step 6: /work/tt_metal/distributed/mesh_device.cpp:425:27
  → Calling 'make_unique<tt::tt_metal::distributed::MeshDeviceImpl, std::shared_ptr<tt::tt_metal::distributed::MeshDeviceImpl::ScopedDevices>, std::unique_ptr<tt::tt_metal::distributed::MeshDeviceView>, std::shared_ptr<tt::tt_metal::distributed::MeshDevice>>'
   423 |
   424 |     auto mesh_device = std::shared_ptr<MeshDevice>(new MeshDevice());
   425 |     mesh_device->pimpl_ = std::make_unique<MeshDeviceImpl>(
                                       ^
   426 |         std::move(scoped_devices),
   427 |         std::make_unique<MeshDeviceView>(MeshShape(1, device_ids.size()), root_devices, fabric_node_ids),

  Step 7: /work/tt_metal/distributed/mesh_device.cpp:228:1
  → Entered call from 'make_unique<tt::tt_metal::distributed::MeshDeviceImpl, std::shared_ptr<tt::tt_metal::distributed::MeshDeviceImpl::ScopedDevices>, std::unique_ptr<tt::tt_metal::distributed::MeshDeviceView>, std::shared_ptr<tt::tt_metal::distributed::MeshDevice>>'
   226 | void MeshDeviceImpl::mark_allocations_safe() { this->allocator_impl()->mark_allocations_safe(); }
   227 |
   228 | MeshDeviceImpl::MeshDeviceImpl(
             ^
   229 |     std::shared_ptr<ScopedDevices> mesh_handle,
   230 |     std::unique_ptr<MeshDeviceView> mesh_device_view,

  Step 8: /work/tt_metal/distributed/mesh_device.cpp:242:57
  → Call to virtual method 'MeshDeviceImpl::id' during construction bypasses virtual dispatch
   240 |     const auto& mpi_context = MetalContext::instance().global_distributed_context();
   241 |     distributed_context_ =
   242 |         mpi_context.split(distributed::multihost::Color(id()), distributed::multihost::Key(*mpi_context.rank()));
                                                                     ^
   243 | }
   244 |

========================================================================
Finding #5
========================================================================
File:    /work/tests/tt_metal/tt_metal/perf_microbenchmark/dispatch/test_prefetcher.cpp
Line:    97 (column 9)
Checker: core.CallAndMessage
Severity: HIGH
Report hash: 88689c99739f42f071c3aeeaa802567f

Summary:
  3rd function call argument is an uninitialized value

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tests/tt_metal/tt_metal/perf_microbenchmark/dispatch/test_prefetcher.cpp:1788:9
  → Calling 'update_host_data'
  1786 |         std::vector<uint32_t> payload = payload_generator_->generate_payload(length);
  1787 |
  1788 |         DeviceDataUpdater::update_host_data(device_data_, payload, length);
                     ^
  1789 |
  1790 |         // Create the HostMemDeviceCommand with pre-calculated size

  Step 2: /work/tests/tt_metal/tt_metal/perf_microbenchmark/dispatch/test_prefetcher.cpp:85:1
  → Entered call from 'SmokeTestHelper::add_host_write'
    83 | // The shadow model must have command header + generated payload for validation since dispatch
    84 | // copies both into completion buffer
    85 | void update_host_data(Common::DeviceData& device_data, const std::vector<uint32_t>& data, uint32_t data_size_bytes) {
             ^
    86 |     uint32_t data_size_words = data_size_bytes / sizeof(uint32_t);
    87 |     CQDispatchCmd expected_cmd{};

  Step 3: /work/tests/tt_metal/tt_metal/perf_microbenchmark/dispatch/test_prefetcher.cpp:96:26
  → Entering loop body
    94 |
    95 |     uint32_t* cmd_as_words = reinterpret_cast<uint32_t*>(&expected_cmd);
    96 |     for (uint32_t i = 0; i < sizeof(CQDispatchCmd) / sizeof(uint32_t); i++) {
                                      ^
    97 |         device_data.push_one(device_data.get_host_core(), 0, cmd_as_words[i]);
    98 |     }

  Step 4: /work/tests/tt_metal/tt_metal/perf_microbenchmark/dispatch/test_prefetcher.cpp:97:9
  → 3rd function call argument is an uninitialized value
    95 |     uint32_t* cmd_as_words = reinterpret_cast<uint32_t*>(&expected_cmd);
    96 |     for (uint32_t i = 0; i < sizeof(CQDispatchCmd) / sizeof(uint32_t); i++) {
    97 |         device_data.push_one(device_data.get_host_core(), 0, cmd_as_words[i]);
                     ^
    98 |     }
    99 |

========================================================================
Finding #6
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/data_movement/sharded/sharded_to_interleaved/device/sharded_to_interleaved_program_factory.cpp
Line:    240 (column 22)
Checker: deadcode.DeadStores
Severity: LOW
Report hash: 10e3cecdc398f4318cc307af79ab1776

Summary:
  Value stored to 'padded_shard_width' during its initialization is never read

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/data_movement/sharded/sharded_to_interleaved/device/sharded_to_interleaved_program_factory.cpp:240:22
  → Value stored to 'padded_shard_width' during its initialization is never read
   238 |             }
   239 |             uint32_t l1_alignment = hal::get_l1_alignment();
   240 |             uint32_t padded_shard_width = align(output_unit_size, dst_buffer->alignment());
                                  ^
   241 |             if (is_blackhole or is_l1_aligned) {
   242 |                 if (!dst_is_dram or is_l1_aligned) {

========================================================================
Finding #7
========================================================================
File:    /work/tests/ttnn/unit_tests/gtests/ccl/test_fabric_edm_common.hpp
Line:    210 (column 29)
Checker: optin.cplusplus.VirtualCall
Severity: MEDIUM
Report hash: 15d287c6d0093c50c5c00624d58507ea

Summary:
  Call to virtual method 'MeshFabric1DFixture::SetupDevices' during construction bypasses virtual dispatch

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tests/ttnn/unit_tests/gtests/ccl/test_fabric_edm_common.hpp:378:9
  → Assuming 'num_devices' is >= 4
   376 |     std::vector<MemoryConfig> mem_configs) {
   377 |     auto num_devices = tt::tt_metal::GetNumAvailableDevices();
   378 |     if (num_devices < 4) {
                     ^
   379 |         log_info(tt::LogTest, "This test can only be run on T3000 devices");
   380 |         return true;

  Step 2: /work/tests/ttnn/unit_tests/gtests/ccl/test_fabric_edm_common.hpp:387:25
  → Calling default constructor for 'MeshFabric1DFixture'
   385 |     Program program = tt::tt_metal::CreateProgram();
   386 |
   387 |     MeshFabric1DFixture test_fixture;
                                     ^
   388 |     auto full_mesh_device = test_fixture.mesh_device_;
   389 |

  Step 3: /work/tests/ttnn/unit_tests/gtests/ccl/test_fabric_edm_common.hpp:210:5
  → Entered call from 'RunPipelinedWorkersTest<tt::tt_metal::Layout>'
   208 |     }
   209 |
   210 |     MeshFabric1DFixture() { this->SetupDevices(); }
                 ^
   211 |
   212 |     MeshFabric1DFixture(tt::tt_fabric::FabricConfig fabric_config) : BaseFabricFixture(fabric_config) {

========================================================================
Finding #8
========================================================================
File:    /work/tests/ttnn/unit_tests/gtests/ccl/test_fabric_edm_common.hpp
Line:    213 (column 9)
Checker: optin.cplusplus.VirtualCall
Severity: MEDIUM
Report hash: ae9974dd531a9756662e9fd9873e08ba

Summary:
  Call to virtual method 'MeshFabric1DFixture::SetupDevices' during construction bypasses virtual dispatch

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tests/ttnn/unit_tests/gtests/ccl/test_fabric_edm_common.hpp:246:43
  → Calling constructor for 'MeshFabric1DFixture'
   244 | class MeshFabric1DRingDeviceInitFixture : public MeshFabric1DFixture {
   245 | public:
   246 |     MeshFabric1DRingDeviceInitFixture() : MeshFabric1DFixture(tt::tt_fabric::FabricConfig::FABRIC_1D_RING) {}
                                                       ^
   247 | };
   248 |

  Step 2: /work/tests/ttnn/unit_tests/gtests/ccl/test_fabric_edm_common.hpp:212:5
  → Entered call from default constructor for 'MeshFabric1DRingDeviceInitFixture'
   210 |     MeshFabric1DFixture() { this->SetupDevices(); }
   211 |
   212 |     MeshFabric1DFixture(tt::tt_fabric::FabricConfig fabric_config) : BaseFabricFixture(fabric_config) {
                 ^
   213 |         this->SetupDevices();
   214 |     }

  Step 3: /work/tests/ttnn/unit_tests/gtests/ccl/test_fabric_edm_common.hpp:213:9
  → Call to virtual method 'MeshFabric1DFixture::SetupDevices' during construction bypasses virtual dispatch
   211 |
   212 |     MeshFabric1DFixture(tt::tt_fabric::FabricConfig fabric_config) : BaseFabricFixture(fabric_config) {
   213 |         this->SetupDevices();
                     ^
   214 |     }
   215 |

========================================================================
Finding #9
========================================================================
File:    /work/tests/ttnn/unit_tests/gtests/ccl/test_fabric_edm_common.hpp
Line:    218 (column 13)
Checker: optin.cplusplus.VirtualCall
Severity: MEDIUM
Report hash: 78be51b3e96e0096aa394042411a5d32

Summary:
  Call to virtual method 'MeshFabric1DFixture::TearDown' during destruction bypasses virtual dispatch

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tests/ttnn/unit_tests/gtests/ccl/test_fabric_edm_common.hpp:244:7
  → Calling '~MeshFabric1DFixture'
   242 | };
   243 |
   244 | class MeshFabric1DRingDeviceInitFixture : public MeshFabric1DFixture {
                   ^
   245 | public:
   246 |     MeshFabric1DRingDeviceInitFixture() : MeshFabric1DFixture(tt::tt_fabric::FabricConfig::FABRIC_1D_RING) {}

  Step 2: /work/tests/ttnn/unit_tests/gtests/ccl/test_fabric_edm_common.hpp:216:5
  → Entered call from destructor for 'MeshFabric1DRingDeviceInitFixture'
   214 |     }
   215 |
   216 |     ~MeshFabric1DFixture() override {
                 ^
   217 |         if (device_open) {
   218 |             TearDown();

  Step 3: /work/tests/ttnn/unit_tests/gtests/ccl/test_fabric_edm_common.hpp:217:13
  → Assuming field 'device_open' is true
   215 |
   216 |     ~MeshFabric1DFixture() override {
   217 |         if (device_open) {
                         ^
   218 |             TearDown();
   219 |         }

  Step 4: /work/tests/ttnn/unit_tests/gtests/ccl/test_fabric_edm_common.hpp:218:13
  → Call to virtual method 'MeshFabric1DFixture::TearDown' during destruction bypasses virtual dispatch
   216 |     ~MeshFabric1DFixture() override {
   217 |         if (device_open) {
   218 |             TearDown();
                         ^
   219 |         }
   220 |     }

========================================================================
Finding #10
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/pool/generic/device/pool_multi_core_program_factory.cpp
Line:    204 (column 5)
Checker: core.DivideZero
Severity: HIGH
Report hash: 93b41a71caf84c04939b82488a035690

Summary:
  Division by zero

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/pool/generic/device/pool_multi_core_program_factory.cpp:286:35
  → Assuming the condition is false
   284 |     const MemoryConfig& /*out_mem_config*/,
   285 |     const std::optional<DeviceComputeKernelConfig>& compute_kernel_config,
   286 |     std::optional<int32_t> divisor_override,
                                               ^
   287 |     uint32_t memory_used,
   288 |     const Layout& output_layout,

  Step 2: /work/ttnn/cpp/ttnn/operations/pool/generic/device/pool_multi_core_program_factory.cpp:287:35
  → Assuming the condition is false
   285 |     const std::optional<DeviceComputeKernelConfig>& compute_kernel_config,
   286 |     std::optional<int32_t> divisor_override,
   287 |     uint32_t memory_used,
                                      ^
   288 |     const Layout& output_layout,
   289 |     bool config_tensor_in_dram) {

  Step 3: /work/ttnn/cpp/ttnn/operations/pool/generic/device/pool_multi_core_program_factory.cpp:319:42
  → Assuming the condition is false
   317 |     uint32_t pad_h = pad_t + pad_b;
   318 |     uint32_t pad_w = pad_l + pad_r;
   319 |     const uint32_t in_h_padded = in_h + pad_h + ceil_pad_h;
                                                      ^
   320 |     const uint32_t in_w_padded = in_w + pad_w + ceil_pad_w;
   321 |     const bool one_scalar_per_core = is_pool_op_one_scalar_per_core(

  Step 4: /work/ttnn/cpp/ttnn/operations/pool/generic/device/pool_multi_core_program_factory.cpp:325:5
  → Assuming the condition is true
   323 |
   324 |     const auto& input_shape = input.padded_shape();
   325 |     const uint32_t shard_width = input.shard_spec()->shape[1];
                 ^
   326 |     const uint32_t in_c_per_shard_ceil = in_c % shard_width != 0 && num_shards_c > 1
   327 |                                              ? (in_c - (in_c % shard_width)) / (num_shards_c - 1)

  Step 5: /work/ttnn/cpp/ttnn/operations/pool/generic/device/pool_multi_core_program_factory.cpp:331:9
  → Assuming field 'is_wide_reduction' is false
   329 |     const uint32_t in_nbytes_c = in_c_per_shard_ceil * params.nbytes;  // row of input (channels)
   330 |     const uint32_t shard_width_bytes = input_shape[3] / num_shards_c * params.nbytes;
   331 |
             ^
   332 |     TT_FATAL(
   333 |         input_shape[3] % num_shards_c == 0,

  Step 6: /work/ttnn/cpp/ttnn/operations/pool/generic/device/pool_multi_core_program_factory.cpp:348:9
  → Assuming field 'is_avg_pool' is false
   346 |     uint32_t next_cb_index = tt::CBIndex::c_0;
   347 |     const uint32_t in_scalar_cb_id_0 = next_cb_index++;
   348 |     const uint32_t in_scalar_cb_pagesize = tile_size(params.data_format);
                     ^
   349 |     const uint32_t in_scalar_cb_npages = params.multi_buffering_factor;
   350 |     tt::tt_metal::create_cb(

  Step 7: /work/ttnn/cpp/ttnn/operations/pool/generic/device/pool_multi_core_program_factory.cpp:379:5
  → Assuming the condition is true
   377 |
   378 |     // reader indices
   379 |     const uint32_t in_reader_indices_cb_id = next_cb_index++;
                 ^
   380 |     const uint32_t in_reader_indices_cb_pagesize =
   381 |         tt::round_up(reader_indices_size, 4);  // pagesize needs to be multiple of 4

  Step 8: /work/ttnn/cpp/ttnn/operations/pool/generic/device/pool_multi_core_program_factory.cpp:389:9
  → Assuming 'config_tensor_in_dram' is false
   387 |         reader_indices_storage.get_buffer()->page_size() <= max_reader_indices_size,
   388 |         "Reader indices buffer page size {} exceeds max expected size {}",
   389 |         reader_indices_storage.get_buffer()->page_size(),
                     ^
   390 |         max_reader_indices_size);
   391 |

  Step 9: /work/ttnn/cpp/ttnn/operations/pool/generic/device/pool_multi_core_program_factory.cpp:402:9
  → Assuming 'return_indices' is false
   400 |
   401 |     log_debug(
   402 |         tt::LogOp,
                     ^
   403 |         "In Reader Indices CB {} :: PS = {}, NP = {}",
   404 |         in_reader_indices_cb_id,

  Step 10: /work/ttnn/cpp/ttnn/operations/pool/generic/device/pool_multi_core_program_factory.cpp:423:9
  → Assuming field 'split_reader' is false
   421 |     const uint32_t in_cb_page_padded = tt::round_up(
   422 |         in_cb_sz,
   423 |         tt::constants::TILE_HW);  // NOTE: ceil to tile size since triscs work with tilesize instead of pagesize
                     ^
   424 |     const uint32_t in_cb_pagesize = params.nbytes * in_cb_page_padded;
   425 |     const uint32_t in_cb_npages = params.multi_buffering_factor;

  Step 11: /work/ttnn/cpp/ttnn/operations/pool/generic/device/pool_multi_core_program_factory.cpp:532:34
  → Assuming 'output_layout' is not equal to TILE
   530 |             right_inc -= index_correction;
   531 |             down_left_wrap_inc -= index_correction;
   532 |             up_left_wrap_inc -= index_correction;
                                              ^
   533 |
   534 |             intra_kernel_right_inc = dilation_w * sticks_per_chunk;

  Step 12: /work/ttnn/cpp/ttnn/operations/pool/generic/device/pool_multi_core_program_factory.cpp:595:29
  → Loop body skipped when range is empty
   593 |             program,
   594 |             all_cores,
   595 |             out_idx_cb_pagesize,
                                         ^
   596 |             out_idx_cb_npages,
   597 |             params.index_format,

  Step 13: /work/ttnn/cpp/ttnn/operations/pool/generic/device/pool_multi_core_program_factory.cpp:609:9
  → Assuming 'one_scalar_per_core' is false
   607 |     }
   608 |
   609 |     /**
                    ^
   610 |      * Reader Kernel: input rows -> input cb
   611 |      */

  Step 14: /work/ttnn/cpp/ttnn/operations/pool/generic/device/pool_multi_core_program_factory.cpp:630:25
  → Calling 'create_scalar_config_tensor'
   628 |             .ceil_h = ceil_pad_h,
   629 |             .ceil_w = ceil_pad_w,
   630 |             .count_include_pad = count_include_pad,
                                     ^
   631 |             .pad_t = pad_t,
   632 |             .pad_b = pad_b,

  Step 15: /work/ttnn/cpp/ttnn/operations/pool/generic/device/pool_multi_core_program_factory.cpp:132:1
  → Entered call from 'pool2d_multi_core_sharded_with_halo_v2_impl_new'
   130 | // is filled with max_out_nhw_per_core number of ScalarInfos for each core and then sharded across the cores.
   131 | // Since we don't usually have that many different scalars, we fill the rest of the config tensor with 0s.
   132 | static Tensor create_scalar_config_tensor(
             ^
   133 |     const AvgPoolConfig& config,
   134 |     TensorMemoryLayout in_memory_layout,

  Step 16: /work/ttnn/cpp/ttnn/operations/pool/generic/device/pool_multi_core_program_factory.cpp:157:30
  → Loop body executed 0 times
   155 |         uint32_t output_stick_h = 0;
   156 |         uint32_t output_stick_w = 0;
   157 |         for (uint32_t i = 0; i < num_iterations; ++i) {
                                          ^
   158 |             scalars_per_core.emplace_back(get_bf16_avg_pool_config_scalars(config, output_stick_h, output_stick_w));
   159 |             max_scalars_cnt = std::max(max_scalars_cnt, scalars_per_core.back().size());

  Step 17: /work/ttnn/cpp/ttnn/operations/pool/generic/device/pool_multi_core_program_factory.cpp:179:5
  → 'entries_per_core' initialized to 0
   177 |
   178 |     constexpr uint32_t entry_size = 3;
   179 |     const uint32_t entries_per_core = entry_size * max_scalars_cnt;
                 ^
   180 |
   181 |     TT_FATAL(

  Step 18: /work/ttnn/cpp/ttnn/operations/pool/generic/device/pool_multi_core_program_factory.cpp:204:5
  → Division by zero
   202 |             // but have only one array of scalars
   203 |             uint32_t repeats = config_tensor_in_dram ? 1 : num_shards_c;
   204 |             push_back_scalar_info_or_zero(config_vector, scalars_per_core[0], max_scalars_cnt, repeats);
                 ^
   205 |             break;
   206 |         }

========================================================================
Finding #11
========================================================================
File:    /work/tt_metal/programming_examples/matmul/matmul_common/bmm_op.hpp
Line:    182 (column 16)
Checker: core.DivideZero
Severity: HIGH
Report hash: 10bf22adb39ba7c14101d48608073c49

Summary:
  Division by zero

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt_metal/programming_examples/matmul/matmul_common/bmm_op.hpp:124:36
  → Entering loop body
   122 |     uint32_t Mpc_min = 1;
   123 |
   124 |     for (auto it = Nt_fac.begin(); it != Nt_fac.end(); ++it) {
                                                ^
   125 |         auto ele = *it;
   126 |         if (ele > num_cores_x) {

  Step 2: /work/tt_metal/programming_examples/matmul/matmul_common/bmm_op.hpp:126:13
  → Assuming 'ele' is > 'num_cores_x'
   124 |     for (auto it = Nt_fac.begin(); it != Nt_fac.end(); ++it) {
   125 |         auto ele = *it;
   126 |         if (ele > num_cores_x) {
                         ^
   127 |             Npc_min *= ele;
   128 |             Nt_fac.erase(it);

  Step 3: /work/tt_metal/programming_examples/matmul/matmul_common/bmm_op.hpp:132:36
  → Loop body executed 0 times
   130 |         }
   131 |     }
   132 |     for (auto it = Mt_fac.begin(); it != Mt_fac.end(); ++it) {
                                                ^
   133 |         auto ele = *it;
   134 |         if (ele > num_cores_y) {

  Step 4: /work/tt_metal/programming_examples/matmul/matmul_common/bmm_op.hpp:141:9
  → Assuming the condition is false
   139 |     }
   140 |
   141 |     if (Npc_min > get_maximum_block_dim(Mpc_min, in0_block_w)) {
                     ^
   142 |         return {0, 0, 0, 0};
   143 |     }

  Step 5: /work/tt_metal/programming_examples/matmul/matmul_common/bmm_op.hpp:171:14
  → Assuming 'Npc_min' is > 1
   169 |     }
   170 |
   171 |     else if (Npc_min > 1) {
                          ^
   172 |         auto Mpc_choices = get_possible_products(Mt_fac);
   173 |         auto Mpc_max = get_maximum_block_dim(Npc_min, in0_block_w);

  Step 6: /work/tt_metal/programming_examples/matmul/matmul_common/bmm_op.hpp:174:24
  → Entering loop body
   172 |         auto Mpc_choices = get_possible_products(Mt_fac);
   173 |         auto Mpc_max = get_maximum_block_dim(Npc_min, in0_block_w);
   174 |         for (auto& ele : Mpc_choices) {
                                    ^
   175 |             if (ele * Mpc_min <= Mpc_max) {
   176 |                 Mpc = ele * Mpc_min;

  Step 7: /work/tt_metal/programming_examples/matmul/matmul_common/bmm_op.hpp:175:17
  → Assuming the condition is true
   173 |         auto Mpc_max = get_maximum_block_dim(Npc_min, in0_block_w);
   174 |         for (auto& ele : Mpc_choices) {
   175 |             if (ele * Mpc_min <= Mpc_max) {
                             ^
   176 |                 Mpc = ele * Mpc_min;
   177 |             } else {

  Step 8: /work/tt_metal/programming_examples/matmul/matmul_common/bmm_op.hpp:176:17
  → The value 0 is assigned to 'Mpc'
   174 |         for (auto& ele : Mpc_choices) {
   175 |             if (ele * Mpc_min <= Mpc_max) {
   176 |                 Mpc = ele * Mpc_min;
                             ^
   177 |             } else {
   178 |                 break;

  Step 9: /work/tt_metal/programming_examples/matmul/matmul_common/bmm_op.hpp:182:16
  → Division by zero
   180 |         }
   181 |
   182 |         if (Mt / Mpc > num_cores_y or Nt / Npc > num_cores_x) {
                            ^
   183 |             return {0, 0, 0, 0};
   184 |         }

========================================================================
Finding #12
========================================================================
File:    /work/tt_stl/tests/test_optional_reference.cpp
Line:    47 (column 5)
Checker: deadcode.DeadStores
Severity: LOW
Report hash: 5f490a208250b019ae149a8b06abf521

Summary:
  Value stored to 'value' is never read

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt_stl/tests/test_optional_reference.cpp:47:5
  → Value stored to 'value' is never read
    45 |
    46 |     // Modify original value
    47 |     value = 100;
                 ^
    48 |     EXPECT_EQ(*ref, 100);  // Reference should see the change
    49 | }

========================================================================
Finding #13
========================================================================
File:    /work/tt_stl/tests/test_optional_reference.cpp
Line:    94 (column 5)
Checker: cplusplus.Move
Severity: HIGH
Report hash: 8062042b1462ef6a27a4fa623c470b49

Summary:
  Method called on moved-from object 'ref1'

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt_stl/tests/test_optional_reference.cpp:88:29
  → Object 'ref1' is moved
    86 |     int value = 42;
    87 |     optional_reference<int> ref1(value);
    88 |     optional_reference<int> ref2(std::move(ref1)); // NOLINT(performance-move-const-arg)
                                         ^
    89 |
    90 |     EXPECT_TRUE(ref2.has_value());

  Step 2: /work/tt_stl/tests/test_optional_reference.cpp:91:5
  → Assuming the condition is true
    89 |
    90 |     EXPECT_TRUE(ref2.has_value());
    91 |     EXPECT_EQ(*ref2, 42);
                 ^
    92 |     // Note: ref1 should still be valid after move (it's just a pointer copy)
    93 |     // NOLINTNEXTLINE(bugprone-use-after-move)

  Step 3: /work/tt_stl/tests/test_optional_reference.cpp:94:5
  → Method called on moved-from object 'ref1'
    92 |     // Note: ref1 should still be valid after move (it's just a pointer copy)
    93 |     // NOLINTNEXTLINE(bugprone-use-after-move)
    94 |     EXPECT_TRUE(ref1.has_value());
                 ^
    95 | }
    96 |

========================================================================
Finding #14
========================================================================
File:    /work/tt_metal/distributed/multihost/mpi_distributed_context.cpp
Line:    128 (column 5)
Checker: optin.mpi.MPI-Checker
Severity: MEDIUM
Report hash: 994cb2a526c709d2390145c716f47c0d

Summary:
  Request  has no matching nonblocking call.

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt_metal/distributed/multihost/mpi_distributed_context.cpp:128:5
  → Request  has no matching nonblocking call.
   126 | Status MPIRequest::wait() {
   127 |     MPI_Status status{};
   128 |     MPI_CHECK(MPI_Wait(&req_, &status));
                 ^
   129 |     done_ = true;
   130 |

========================================================================
Finding #15
========================================================================
File:    /work/tt_metal/distributed/multihost/mpi_distributed_context.cpp
Line:    247 (column 12)
Checker: optin.mpi.MPI-Checker
Severity: MEDIUM
Report hash: 4d430dfacd6b4a621cfe9de457eee03d

Summary:
  Request 'req' has no matching wait.

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt_metal/distributed/multihost/mpi_distributed_context.cpp:245:5
  → Request is previously used by nonblocking call here.
   243 |     check_size_fits_int(buf.size());
   244 |     MPI_Request req{};
   245 |     MPI_CHECK(MPI_Isend(
                 ^
   246 |         const_cast<std::byte*>(buf.data()), static_cast<int>(buf.size()), MPI_CHAR, *dest, *tag, comm_, &req));
   247 |     return std::make_shared<MPIRequest>(req);

  Step 2: /work/tt_metal/distributed/multihost/mpi_distributed_context.cpp:247:12
  → Request 'req' has no matching wait.
   245 |     MPI_CHECK(MPI_Isend(
   246 |         const_cast<std::byte*>(buf.data()), static_cast<int>(buf.size()), MPI_CHAR, *dest, *tag, comm_, &req));
   247 |     return std::make_shared<MPIRequest>(req);
                        ^
   248 | }
   249 |

========================================================================
Finding #16
========================================================================
File:    /work/tt_metal/distributed/multihost/mpi_distributed_context.cpp
Line:    254 (column 12)
Checker: optin.mpi.MPI-Checker
Severity: MEDIUM
Report hash: 1620d00f8d2df30632ab6688cfddf071

Summary:
  Request 'req' has no matching wait.

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt_metal/distributed/multihost/mpi_distributed_context.cpp:253:5
  → Request is previously used by nonblocking call here.
   251 |     check_size_fits_int(buf.size());
   252 |     MPI_Request req{};
   253 |     MPI_CHECK(MPI_Irecv(buf.data(), static_cast<int>(buf.size()), MPI_CHAR, *src, *tag, comm_, &req));
                 ^
   254 |     return std::make_shared<MPIRequest>(req);
   255 | }

  Step 2: /work/tt_metal/distributed/multihost/mpi_distributed_context.cpp:254:12
  → Request 'req' has no matching wait.
   252 |     MPI_Request req{};
   253 |     MPI_CHECK(MPI_Irecv(buf.data(), static_cast<int>(buf.size()), MPI_CHAR, *src, *tag, comm_, &req));
   254 |     return std::make_shared<MPIRequest>(req);
                        ^
   255 | }
   256 |

========================================================================
Finding #17
========================================================================
File:    /work/tt_stl/tests/test_strong_type.cpp
Line:    78 (column 5)
Checker: cplusplus.Move
Severity: HIGH
Report hash: c068880d24d02f730e0fa8cbf9f73d83

Summary:
  Method called on moved-from object 'from'

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt_stl/tests/test_strong_type.cpp:73:5
  → Assuming the condition is true
    71 |
    72 |     MoveOnlyType from(std::make_unique<int>(42));
    73 |     EXPECT_EQ(**from, 42);
                 ^
    74 |
    75 |     MoveOnlyType to = std::move(from);

  Step 2: /work/tt_stl/tests/test_strong_type.cpp:75:23
  → Object 'from' is moved
    73 |     EXPECT_EQ(**from, 42);
    74 |
    75 |     MoveOnlyType to = std::move(from);
                                   ^
    76 |
    77 |     // NOLINTNEXTLINE(bugprone-use-after-move)

  Step 3: /work/tt_stl/tests/test_strong_type.cpp:78:5
  → Method called on moved-from object 'from'
    76 |
    77 |     // NOLINTNEXTLINE(bugprone-use-after-move)
    78 |     EXPECT_THAT(*from, IsNull());
                 ^
    79 |     EXPECT_EQ(**to, 42);
    80 | }

========================================================================
Finding #18
========================================================================
File:    /work/tt-train/sources/examples/nano_gpt/main.cpp
Line:    400 (column 56)
Checker: core.CallAndMessage
Severity: HIGH
Report hash: a9d5db884d5688f787645b346344b18c

Summary:
  Called C++ object pointer is uninitialized

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt-train/sources/examples/nano_gpt/main.cpp:399:17
  → Calling 'visit<(lambda at /work/tt-train/sources/examples/nano_gpt/main.cpp:400:21), std::variant<ttml::models::gpt2::TransformerConfig, ttml::models::llama::LlamaConfig> &>'
   397 |                         std::get<std::string>(data_source), sequence_length);
   398 |
   399 |                 std::visit(
                             ^
   400 |                     [&](auto &&arg) { arg.vocab_size = tokenizer->get_vocab_size(); }, model_config.transformer_config);
   401 |

  Step 2: /work/tt-train/sources/examples/nano_gpt/main.cpp:400:35
  → Entered call from '__invoke_impl<void, (lambda at /work/tt-train/sources/examples/nano_gpt/main.cpp:400:21), ttml::models::llama::LlamaConfig &>'
   398 |
   399 |                 std::visit(
   400 |                     [&](auto &&arg) { arg.vocab_size = tokenizer->get_vocab_size(); }, model_config.transformer_config);
                                               ^
   401 |
   402 |                 return dataset;

========================================================================
Finding #19
========================================================================
File:    /work/tt-train/sources/ttml/ops/scaled_dot_product_attention.cpp
Line:    116 (column 14)
Checker: deadcode.DeadStores
Severity: LOW
Report hash: 594cd19e8f1e9bf1eff81803782d7cde

Summary:
  Value stored to 'group_num' during its initialization is never read

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt-train/sources/ttml/ops/scaled_dot_product_attention.cpp:116:14
  → Value stored to 'group_num' during its initialization is never read
   114 |     }
   115 |
   116 |     uint32_t group_num = query_heads;  // (G) number of KV groups, H for MHA mode
                          ^
   117 |     if (query_heads != key_heads || query_heads != value_heads) {
   118 |         // grouped query mode

========================================================================
Finding #20
========================================================================
File:    /work/tests/ttnn/unit_tests/gtests/ccl/test_fabric_edm_common.hpp
Line:    210 (column 29)
Checker: optin.cplusplus.VirtualCall
Severity: MEDIUM
Report hash: 15d287c6d0093c50c5c00624d58507ea

Summary:
  Call to virtual method 'MeshFabric1DFixture::SetupDevices' during construction bypasses virtual dispatch

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tests/ttnn/unit_tests/gtests/ccl/test_fabric_edm_common.hpp:210:29
  → Call to virtual method 'MeshFabric1DFixture::SetupDevices' during construction bypasses virtual dispatch
   208 |     }
   209 |
   210 |     MeshFabric1DFixture() { this->SetupDevices(); }
                                         ^
   211 |
   212 |     MeshFabric1DFixture(tt::tt_fabric::FabricConfig fabric_config) : BaseFabricFixture(fabric_config) {

========================================================================
Finding #21
========================================================================
File:    /work/tt_metal/experimental/udm/mesh_utils.cpp
Line:    106 (column 19)
Checker: core.DivideZero
Severity: HIGH
Report hash: 915e921f9f91dcb55ee3217a5d41e0fe

Summary:
  Division by zero

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt_metal/experimental/udm/mesh_utils.cpp:203:12
  → Calling 'map_tensor_to_gcores_nd'
   201 |     const MeshTensorBuilder& tensor_builder, const MeshBuilder& mesh_builder, int partition_dim) {
   202 |     // Single-dimension partitioning - just call the ND version
   203 |     return map_tensor_to_gcores_nd(tensor_builder, mesh_builder, {partition_dim});
                        ^
   204 | }
   205 |

  Step 2: /work/tt_metal/experimental/udm/mesh_utils.cpp:206:1
  → Entered call from 'map_tensor_to_gcores'
   204 | }
   205 |
   206 | GlobalCoresInfo map_tensor_to_gcores_nd(
             ^
   207 |     const MeshTensorBuilder& tensor_builder, const MeshBuilder& mesh_builder, const std::vector<int>& partition_dims) {
   208 |     TT_FATAL(!partition_dims.empty(), "partition_dims cannot be empty");

  Step 3: /work/tt_metal/experimental/udm/mesh_utils.cpp:208:5
  → Assuming the condition is true
   206 | GlobalCoresInfo map_tensor_to_gcores_nd(
   207 |     const MeshTensorBuilder& tensor_builder, const MeshBuilder& mesh_builder, const std::vector<int>& partition_dims) {
   208 |     TT_FATAL(!partition_dims.empty(), "partition_dims cannot be empty");
                 ^
   209 |
   210 |     // Get mesh and grid dimensions from mesh_builder

  Step 4: /work/tt_metal/experimental/udm/mesh_utils.cpp:217:5
  → Assuming 'total_gcores' is > 0
   215 |     uint32_t total_gcores = mesh_volume * grid_volume;
   216 |
   217 |     TT_FATAL(total_gcores > 0, "No gcores in mesh");
                 ^
   218 |
   219 |     // Get the mesh tensor shape in pages (for proper work distribution based on memory pages)

  Step 5: /work/tt_metal/experimental/udm/mesh_utils.cpp:226:35
  → Assuming 'i' is >= 0
   224 |     std::vector<uint32_t> mesh_tensor_strides(tensor_rank);
   225 |     uint32_t stride = 1;
   226 |     for (int i = tensor_rank - 1; i >= 0; --i) {
                                               ^
   227 |         mesh_tensor_strides[i] = stride;
   228 |         stride *= mesh_tensor_shape_in_pages[i];

  Step 6: /work/tt_metal/experimental/udm/mesh_utils.cpp:234:18
  → Loop body skipped when range is empty
   232 |     std::vector<int> normalized_dims;
   233 |     std::vector<uint32_t> dim_sizes;
   234 |     for (int dim : partition_dims) {
                              ^
   235 |         if (dim < 0) {
   236 |             dim = tensor_rank + dim;

  Step 7: /work/tt_metal/experimental/udm/mesh_utils.cpp:249:43
  → Calling 'factor_cores_into_dims'
   247 |     // Factor the total gcores into partition dimensions
   248 |     // We need to distribute work across mesh×grid dimensions
   249 |     std::vector<uint32_t> cores_per_dim = factor_cores_into_dims(total_gcores, partition_dims.size());
                                                       ^
   250 |
   251 |     GlobalCoresInfo info;

  Step 8: /work/tt_metal/experimental/udm/mesh_utils.cpp:80:1
  → Entered call from 'map_tensor_to_gcores_nd'
    78 |  * For example: factor_cores(12, 2) might return {3, 4} or {4, 3}
    79 |  */
    80 | std::vector<uint32_t> factor_cores_into_dims(uint32_t num_cores, size_t num_dims) {
             ^
    81 |     TT_FATAL(num_cores > 0 && num_dims > 0, "num_cores and num_dims must be positive");
    82 |

  Step 9: /work/tt_metal/experimental/udm/mesh_utils.cpp:81:5
  → Assuming 'num_dims' is > 0
    79 |  */
    80 | std::vector<uint32_t> factor_cores_into_dims(uint32_t num_cores, size_t num_dims) {
    81 |     TT_FATAL(num_cores > 0 && num_dims > 0, "num_cores and num_dims must be positive");
                 ^
    82 |
    83 |     std::vector<uint32_t> factors(num_dims, 1);

  Step 10: /work/tt_metal/experimental/udm/mesh_utils.cpp:85:9
  → Assuming 'num_dims' is not equal to 1
    83 |     std::vector<uint32_t> factors(num_dims, 1);
    84 |
    85 |     if (num_dims == 1) {
                     ^
    86 |         factors[0] = num_cores;
    87 |         return factors;

  Step 11: /work/tt_metal/experimental/udm/mesh_utils.cpp:95:24
  → Entering loop body
    93 |     double target = std::pow(static_cast<double>(num_cores), 1.0 / static_cast<double>(num_dims));
    94 |
    95 |     for (size_t d = 0; d < num_dims - 1; ++d) {
                                    ^
    96 |         // Find the largest factor <= target that divides remaining
    97 |         uint32_t factor = static_cast<uint32_t>(target);

  Step 12: /work/tt_metal/experimental/udm/mesh_utils.cpp:97:9
  → 'factor' initialized here
    95 |     for (size_t d = 0; d < num_dims - 1; ++d) {
    96 |         // Find the largest factor <= target that divides remaining
    97 |         uint32_t factor = static_cast<uint32_t>(target);
                     ^
    98 |         while (factor > 1 && remaining % factor != 0) {
    99 |             factor--;

  Step 13: /work/tt_metal/experimental/udm/mesh_utils.cpp:98:16
  → Assuming 'factor' is <= 1
    96 |         // Find the largest factor <= target that divides remaining
    97 |         uint32_t factor = static_cast<uint32_t>(target);
    98 |         while (factor > 1 && remaining % factor != 0) {
                            ^
    99 |             factor--;
   100 |         }

  Step 14: /work/tt_metal/experimental/udm/mesh_utils.cpp:101:13
  → Assuming 'factor' is not equal to 1
    99 |             factor--;
   100 |         }
   101 |         if (factor == 1 && remaining > 1) {
                         ^
   102 |             factor = remaining;  // Use all remaining if no good factor found
   103 |         }

  Step 15: /work/tt_metal/experimental/udm/mesh_utils.cpp:106:19
  → Division by zero
   104 |
   105 |         factors[d] = factor;
   106 |         remaining /= factor;
                               ^
   107 |
   108 |         // Update target for remaining dimensions

========================================================================
Finding #22
========================================================================
File:    /work/tt_stl/tests/test_cleanup.cpp
Line:    59 (column 14)
Checker: deadcode.DeadStores
Severity: LOW
Report hash: 59e7decc799c3698b7ffd8f59fc6e9ae

Summary:
  Value stored to 'cleanup' during its initialization is never read

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt_stl/tests/test_cleanup.cpp:59:14
  → Value stored to 'cleanup' during its initialization is never read
    57 |     bool cleanup_called = false;
    58 |     try {
    59 |         auto cleanup = make_cleanup([&cleanup_called]() { cleanup_called = true; });
                          ^
    60 |         throw std::runtime_error("test exception");
    61 |     } catch (const std::runtime_error& e) {

========================================================================
Finding #23
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/pool/rotate/device/rotate_bilinear_program_factory.cpp
Line:    206 (column 13)
Checker: core.NullDereference
Severity: HIGH
Report hash: d9e847109480da3ca5ea75990ecd7aeb

Summary:
  Dereference of undefined pointer value

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/pool/rotate/device/rotate_bilinear_program_factory.cpp:64:9
  → Assuming the condition is false
    62 |
    63 |     float center_x, center_y;
    64 |     if (operation_attributes.center.has_value()) {
                     ^
    65 |         center_x = std::get<0>(operation_attributes.center.value()) - 0.5f;
    66 |         center_y = std::get<1>(operation_attributes.center.value()) - 0.5f;

  Step 2: /work/ttnn/cpp/ttnn/operations/pool/rotate/device/rotate_bilinear_program_factory.cpp:72:30
  → Assuming the condition is true
    70 |     }
    71 |
    72 |     const bool is_bfloat16 = input_tensor.dtype() == DataType::BFLOAT16;
                                          ^
    73 |     uint32_t fill_value_bits;
    74 |     if (is_bfloat16) {

  Step 3: /work/ttnn/cpp/ttnn/operations/pool/rotate/device/rotate_bilinear_program_factory.cpp:97:9
  → Assuming 'is_input_sharded' is false
    95 |     uint32_t num_cores_x = 0;
    96 |
    97 |     if (is_input_sharded) {
                     ^
    98 |         const auto input_shard_spec = input_tensor.shard_spec().value();
    99 |         all_cores = input_shard_spec.grid;

  Step 4: /work/ttnn/cpp/ttnn/operations/pool/rotate/device/rotate_bilinear_program_factory.cpp:113:16
  → Assuming 'is_output_sharded' is true
   111 |         num_sticks_per_core_group_1 = input_nsticks_per_core;
   112 |         core_group_1 = all_cores;
   113 |     } else if (is_output_sharded) {
                            ^
   114 |         const auto output_shard_spec = output_tensor.shard_spec().value();
   115 |         output_nsticks_per_core = output_shard_spec.shape[0];

  Step 5: /work/ttnn/cpp/ttnn/operations/pool/rotate/device/rotate_bilinear_program_factory.cpp:119:35
  → Assuming field 'orientation' is not equal to ROW_MAJOR
   117 |         all_cores = output_shard_spec.grid;
   118 |         logical_cores = corerange_to_cores(
   119 |             all_cores, num_cores, output_shard_spec.orientation == tt::tt_metal::ShardOrientation::ROW_MAJOR);
                                               ^
   120 |         num_sticks_per_core_group_1 = output_nsticks_per_core;
   121 |         core_group_1 = all_cores;

  Step 6: /work/ttnn/cpp/ttnn/operations/pool/rotate/device/rotate_bilinear_program_factory.cpp:172:32
  → Assuming 'fill_value_bits' is not equal to 0
   170 |         any_sharded ? output_tensor.buffer() : nullptr);
   171 |
   172 |     const bool fill_is_zero = (fill_value_bits == 0);
                                            ^
   173 |
   174 |     std::vector<uint32_t> reader_compile_time_args = {

  Step 7: /work/ttnn/cpp/ttnn/operations/pool/rotate/device/rotate_bilinear_program_factory.cpp:246:29
  → Calling 'operator()'
   244 |     tt::tt_metal::KernelHandle compute_kernel_id = 0;
   245 |     if (any_sharded || core_group_1.num_cores() > 0) {
   246 |         compute_kernel_id = create_compute_kernel(
                                         ^
   247 |             any_sharded ? all_cores : core_group_1,
   248 |             any_sharded ? output_nsticks_per_core : num_sticks_per_core_group_1);

  Step 8: /work/ttnn/cpp/ttnn/operations/pool/rotate/device/rotate_bilinear_program_factory.cpp:197:101
  → Entered call from 'BilinearProgramFactory::create'
   195 |     const uint32_t in_nblocks_c = (uint32_t)std::ceil((float)in_ntiles_c / MAX_TILES_PER_REDUCTION);
   196 |
   197 |     auto create_compute_kernel = [&](tt::tt_metal::CoreRangeSet cores, uint32_t total_interpolations) {
                                                                                                                 ^
   198 |         std::vector<uint32_t> compute_compile_time_args = {
   199 |             in_ntiles_c,

  Step 9: /work/ttnn/cpp/ttnn/operations/pool/rotate/device/rotate_bilinear_program_factory.cpp:206:13
  → Dereference of undefined pointer value
   204 |             in_nblocks_c,
   205 |             MAX_ROWS_FOR_REDUCTION,
   206 |             input_cb_index,
                         ^
   207 |             DUMMY_CB_ID,
   208 |             scalar_cb_index,

========================================================================
Finding #24
========================================================================
File:    /work/tests/tt_metal/tt_metal/api/test_tilize_untilize.cpp
Line:    553 (column 66)
Checker: core.CallAndMessage
Severity: HIGH
Report hash: 8797cac8e7c12f70ea120e43705dcaeb

Summary:
  1st function call argument is an uninitialized value

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tests/tt_metal/tt_metal/api/test_tilize_untilize.cpp:534:9
  → Assuming 'from_layout' is not equal to 'to_layout'
   532 |     auto transpose = std::get<4>(params);
   533 |
   534 |     if (from_layout == to_layout) {
                     ^
   535 |         return;
   536 |     }

  Step 2: /work/tests/tt_metal/tt_metal/api/test_tilize_untilize.cpp:539:9
  → Assuming the condition is false
   537 |
   538 |     // Not supported by reference
   539 |     if (tile_shape[0] < 16 && transpose) {
                     ^
   540 |         return;
   541 |     }

  Step 3: /work/tests/tt_metal/tt_metal/api/test_tilize_untilize.cpp:562:5
  → Calling 'operator()'
   560 |     };
   561 |
   562 |     run_for_type(bfloat16{});
                 ^
   563 |     run_for_type(float{});
   564 |     run_for_type(int32_t{});

  Step 4: /work/tests/tt_metal/tt_metal/api/test_tilize_untilize.cpp:547:38
  → Entered call from 'NonSquareTilesTestFixture_ConvertLayout_Test::TestBody'
   545 |     size_t n_elements = n_rows * n_cols;
   546 |
   547 |     auto run_for_type = [&](auto type) {
                                                  ^
   548 |         using Type = decltype(type);
   549 |         const auto& data = get_test_data<Type>();

  Step 5: /work/tests/tt_metal/tt_metal/api/test_tilize_untilize.cpp:553:66
  → 1st function call argument is an uninitialized value
   551 |
   552 |         auto output =
   553 |             convert_layout(input, shape, from_layout, to_layout, tile_shape, face_shape, transpose, transpose);
                                                                              ^
   554 |
   555 |         auto output_ref = reference::convert_layout(

========================================================================
Finding #25
========================================================================
File:    /work/tests/ttnn/unit_tests/gtests/multiprocess/test_send_recv_pipeline.cpp
Line:    240 (column 13)
Checker: core.CallAndMessage
Severity: HIGH
Report hash: d05bc1d0214b358056d5e7546331c0e3

Summary:
  2nd function call argument is an uninitialized value

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tests/ttnn/unit_tests/gtests/multiprocess/test_send_recv_pipeline.cpp:157:9
  → Assuming 'arch' is equal to BLACKHOLE
   155 | TEST_F(MeshDevice4StagePipelineSendRecvFixture, TestSendRecvPipeline) {
   156 |     auto arch = tt::tt_metal::MetalContext::instance().get_cluster().arch();
   157 |     if (arch != ARCH::BLACKHOLE) {
                     ^
   158 |         GTEST_SKIP() << "This test can only run on Blackhole systems";
   159 |     }

  Step 2: /work/tests/ttnn/unit_tests/gtests/multiprocess/test_send_recv_pipeline.cpp:207:37
  → Assuming the condition is true
   205 |     };
   206 |
   207 |     const bool is_pipeline_start = (*distributed_context->rank() == *pipeline_start_rank);
                                                 ^
   208 |     const bool is_pipeline_end = (*distributed_context->rank() == *pipeline_end_rank);
   209 |     const bool is_intermediate = !is_pipeline_start && !is_pipeline_end;

  Step 3: /work/tests/ttnn/unit_tests/gtests/multiprocess/test_send_recv_pipeline.cpp:208:35
  → Assuming the condition is false
   206 |
   207 |     const bool is_pipeline_start = (*distributed_context->rank() == *pipeline_start_rank);
   208 |     const bool is_pipeline_end = (*distributed_context->rank() == *pipeline_end_rank);
                                               ^
   209 |     const bool is_intermediate = !is_pipeline_start && !is_pipeline_end;
   210 |

  Step 4: /work/tests/ttnn/unit_tests/gtests/multiprocess/test_send_recv_pipeline.cpp:244:30
  → Entering loop body
   242 |             ttnn::experimental::send_async(intermediate_tensor, send_socket);
   243 |         };
   244 |         for (uint32_t i = 0; i < NUM_ITERATIONS; i++) {
                                          ^
   245 |             run_sender_step(i);
   246 |         }

  Step 5: /work/tests/ttnn/unit_tests/gtests/multiprocess/test_send_recv_pipeline.cpp:245:13
  → Calling 'operator()'
   243 |         };
   244 |         for (uint32_t i = 0; i < NUM_ITERATIONS; i++) {
   245 |             run_sender_step(i);
                         ^
   246 |         }
   247 |     } else {

  Step 6: /work/tests/ttnn/unit_tests/gtests/multiprocess/test_send_recv_pipeline.cpp:230:46
  → Entered call from 'MeshDevice4StagePipelineSendRecvFixture_TestSendRecvPipeline_Test::TestBody'
   228 |         auto send_socket = distributed::MeshSocket(mesh_device_, send_socket_config);
   229 |
   230 |         auto run_sender_step = [&](uint32_t i) {
                                                          ^
   231 |             auto input_tensor =
   232 |                 ttnn::distributed::distribute_tensor(

  Step 7: /work/tests/ttnn/unit_tests/gtests/multiprocess/test_send_recv_pipeline.cpp:240:13
  → 2nd function call argument is an uninitialized value
   238 |                     .to_device(mesh_device_.get(), tensor_spec.memory_config());
   239 |
   240 |             ttnn::experimental::send_async(input_tensor, intermed_send);
                         ^
   241 |             ttnn::experimental::recv_async(intermediate_tensor, intermed_recv);
   242 |             ttnn::experimental::send_async(intermediate_tensor, send_socket);

========================================================================
Finding #26
========================================================================
File:    /work/tests/ttnn/unit_tests/gtests/multiprocess/test_send_recv_pipeline.cpp
Line:    276 (column 17)
Checker: core.CallAndMessage
Severity: HIGH
Report hash: 0a58404a8ef485ecc2a5e4c62089445e

Summary:
  2nd function call argument is an uninitialized value

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tests/ttnn/unit_tests/gtests/multiprocess/test_send_recv_pipeline.cpp:157:9
  → Assuming 'arch' is equal to BLACKHOLE
   155 | TEST_F(MeshDevice4StagePipelineSendRecvFixture, TestSendRecvPipeline) {
   156 |     auto arch = tt::tt_metal::MetalContext::instance().get_cluster().arch();
   157 |     if (arch != ARCH::BLACKHOLE) {
                     ^
   158 |         GTEST_SKIP() << "This test can only run on Blackhole systems";
   159 |     }

  Step 2: /work/tests/ttnn/unit_tests/gtests/multiprocess/test_send_recv_pipeline.cpp:207:37
  → Assuming the condition is false
   205 |     };
   206 |
   207 |     const bool is_pipeline_start = (*distributed_context->rank() == *pipeline_start_rank);
                                                 ^
   208 |     const bool is_pipeline_end = (*distributed_context->rank() == *pipeline_end_rank);
   209 |     const bool is_intermediate = !is_pipeline_start && !is_pipeline_end;

  Step 3: /work/tests/ttnn/unit_tests/gtests/multiprocess/test_send_recv_pipeline.cpp:208:35
  → Assuming the condition is false
   206 |
   207 |     const bool is_pipeline_start = (*distributed_context->rank() == *pipeline_start_rank);
   208 |     const bool is_pipeline_end = (*distributed_context->rank() == *pipeline_end_rank);
                                               ^
   209 |     const bool is_intermediate = !is_pipeline_start && !is_pipeline_end;
   210 |

  Step 4: /work/tests/ttnn/unit_tests/gtests/multiprocess/test_send_recv_pipeline.cpp:281:34
  → Entering loop body
   279 |             };
   280 |
   281 |             for (uint32_t i = 0; i < NUM_ITERATIONS; i++) {
                                              ^
   282 |                 run_intermed_step();
   283 |             }

  Step 5: /work/tests/ttnn/unit_tests/gtests/multiprocess/test_send_recv_pipeline.cpp:282:17
  → Calling 'operator()'
   280 |
   281 |             for (uint32_t i = 0; i < NUM_ITERATIONS; i++) {
   282 |                 run_intermed_step();
                             ^
   283 |             }
   284 |         } else {

  Step 6: /work/tests/ttnn/unit_tests/gtests/multiprocess/test_send_recv_pipeline.cpp:274:42
  → Entered call from 'MeshDevice4StagePipelineSendRecvFixture_TestSendRecvPipeline_Test::TestBody'
   272 |             auto [intermed_send, intermed_recv] = create_intermed_socket_pair(my_recv, my_sender);
   273 |
   274 |             auto run_intermed_step = [&]() {
                                                      ^
   275 |                 ttnn::experimental::recv_async(intermediate_tensor, recv_socket);
   276 |                 ttnn::experimental::send_async(intermediate_tensor, intermed_send);

  Step 7: /work/tests/ttnn/unit_tests/gtests/multiprocess/test_send_recv_pipeline.cpp:276:17
  → 2nd function call argument is an uninitialized value
   274 |             auto run_intermed_step = [&]() {
   275 |                 ttnn::experimental::recv_async(intermediate_tensor, recv_socket);
   276 |                 ttnn::experimental::send_async(intermediate_tensor, intermed_send);
                             ^
   277 |                 ttnn::experimental::recv_async(intermediate_tensor, intermed_recv);
   278 |                 ttnn::experimental::send_async(intermediate_tensor, send_socket);

========================================================================
Finding #27
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/compute_throttle_utils.cpp
Line:    63 (column 41)
Checker: cplusplus.StringChecker
Severity: HIGH
Report hash: eb71c6e9dbd1940ccae7c5fd4c7b65cc

Summary:
  The parameter must not be null

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/compute_throttle_utils.cpp:47:38
  → Assuming 'arch' is equal to WORMHOLE_B0
    45 |     // TODO: determine min core threshold for throttle to be needed on BH
    46 |     constexpr uint32_t BH_MM_MAX_CORES_NO_THROTTLE = 0;
    47 |     const bool mm_throttle_needed = (arch == tt::ARCH::WORMHOLE_B0 && num_cores > WH_B0_MM_MAX_CORES_NO_THROTTLE) ||
                                                  ^
    48 |                                     (arch == tt::ARCH::BLACKHOLE && num_cores > BH_MM_MAX_CORES_NO_THROTTLE);
    49 |

  Step 2: /work/ttnn/cpp/ttnn/operations/compute_throttle_utils.cpp:63:41
  → The parameter must not be null
    61 |     // If environment variable is set, this overrides the throttle level parameter
    62 |     if (mm_throttle_env_enabled) {
    63 |         uint_throttle_level = std::stoi(std::getenv("TT_MM_THROTTLE_PERF"));
                                                     ^
    64 |     }
    65 |

========================================================================
Finding #28
========================================================================
File:    /work/tests/ttnn/unit_tests/gtests/test_graph_basic.cpp
Line:    185 (column 18)
Checker: deadcode.DeadStores
Severity: LOW
Report hash: 2ff39ce746aa85b7228e742033e85075

Summary:
  Value stored to 'capture' during its initialization is never read

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tests/ttnn/unit_tests/gtests/test_graph_basic.cpp:185:18
  → Value stored to 'capture' during its initialization is never read
   183 |         auto capture = ttnn::graph::ScopedGraphCapture(IGraphProcessor::RunMode::NO_DISPATCH);
   184 |         try {
   185 |             auto capture = ttnn::graph::ScopedGraphCapture(IGraphProcessor::RunMode::NO_DISPATCH);
                              ^
   186 |             operation(tt::tt_metal::DataType::BFLOAT16);
   187 |             throw std::runtime_error("Expected");

========================================================================
Finding #29
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/pool/grid_sample/device/grid_sample_bilinear_program_factory.cpp
Line:    224 (column 13)
Checker: core.NullDereference
Severity: HIGH
Report hash: 3446545fb3e02a5d9950a0560a80bb53

Summary:
  Dereference of undefined pointer value

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/pool/grid_sample/device/grid_sample_bilinear_program_factory.cpp:55:9
  → Assuming 'is_sharded' is true
    53 |     const uint32_t total_grid_nsticks = grid_tensor.physical_volume() / grid_shape[-1];
    54 |
    55 |     if (is_sharded) {
                     ^
    56 |         const auto grid_shard_spec = grid_tensor.shard_spec().value();
    57 |         grid_nsticks_per_core = grid_shard_spec.shape[0];

  Step 2: /work/ttnn/cpp/ttnn/operations/pool/grid_sample/device/grid_sample_bilinear_program_factory.cpp:63:35
  → Assuming field 'orientation' is not equal to ROW_MAJOR
    61 |         all_cores = grid_shard_spec.grid;
    62 |         logical_cores = corerange_to_cores(
    63 |             all_cores, num_cores, grid_shard_spec.orientation == tt::tt_metal::ShardOrientation::ROW_MAJOR);
                                               ^
    64 |
    65 |     } else {

  Step 3: /work/ttnn/cpp/ttnn/operations/pool/grid_sample/device/grid_sample_bilinear_program_factory.cpp:100:23
  → Assuming 'enable_split_reader' is false
    98 |     uint32_t input_cb_index_1 = DUMMY_CB_ID;
    99 |     tt::tt_metal::CBHandle input_cb_handle_1 = 0;
   100 |     if (is_sharded && enable_split_reader) {
                                   ^
   101 |         std::tie(input_cb_index_1, input_cb_handle_1) = tt::tt_metal::create_cb(
   102 |             cb_idx++, program, all_cores, input_cb_page_size, BUFFERING_FACTOR, input_cb_data_format);

  Step 4: /work/ttnn/cpp/ttnn/operations/pool/grid_sample/device/grid_sample_bilinear_program_factory.cpp:147:9
  → Assuming 'use_precomputed_grid' is false
   145 |         static_cast<uint32_t>(grid_tensor.dtype()),  // ct_arg[9]: grid_dtype (shared)
   146 |         grid_hw,                                     // ct_arg[10]: grid_hw (shared)
   147 |         use_precomputed_grid ? 1U : 0U               // ct_arg[11]: use_precomputed_grid (shared)
                     ^
   148 |     };
   149 |

  Step 5: /work/ttnn/cpp/ttnn/operations/pool/grid_sample/device/grid_sample_bilinear_program_factory.cpp:263:9
  → Calling 'operator()'
   261 |
   262 |     if (is_sharded || core_group_1.num_cores() > 0) {
   263 |         create_compute_kernel(
                     ^
   264 |             is_sharded ? all_cores : core_group_1,
   265 |             grid_batching_factor * (is_sharded ? grid_nsticks_per_core : num_sticks_per_core_group_1));

  Step 6: /work/ttnn/cpp/ttnn/operations/pool/grid_sample/device/grid_sample_bilinear_program_factory.cpp:214:101
  → Entered call from 'GridSampleBilinearProgramFactory::create'
   212 |     const uint32_t in_nblocks_c = (uint32_t)std::ceil((float)in_ntiles_c / MAX_TILES_PER_REDUCTION);
   213 |
   214 |     auto create_compute_kernel = [&](tt::tt_metal::CoreRangeSet cores, uint32_t total_interpolations) {
                                                                                                                 ^
   215 |         // Compute kernel compile-time arguments
   216 |         std::vector<uint32_t> compute_compile_time_args = {

  Step 7: /work/ttnn/cpp/ttnn/operations/pool/grid_sample/device/grid_sample_bilinear_program_factory.cpp:224:13
  → Dereference of undefined pointer value
   222 |             in_nblocks_c,                      // ct_arg[5]: in_nblocks_c
   223 |             MAX_ROWS_FOR_REDUCTION,            // ct_arg[6]: MAX_ROWS_FOR_REDUCTION
   224 |             input_cb_index_0,                  // ct_arg[7]: input_cb_index_0
                         ^
   225 |             input_cb_index_1,                  // ct_arg[8]: input_cb_index_1
   226 |             scalar_cb_index_0,                 // ct_arg[9]: scalar_cb_index_0

========================================================================
Finding #30
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/experimental/transformer/create_qkv_heads/device/create_qkv_heads_program_factory.cpp
Line:    46 (column 5)
Checker: core.DivideZero
Severity: HIGH
Report hash: e49e8afe2fd9fa2a01ae9cbfbdb2e0bf

Summary:
  Division by zero

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/experimental/transformer/create_qkv_heads/device/create_qkv_heads_program_factory.cpp:34:5
  → Assuming the condition is true
    32 |
    33 |     // Validation
    34 |     TT_FATAL(head_dim % TILE_WIDTH == 0, "head dim {} needs to be a multiple of tile width {}", head_dim, TILE_WIDTH);
                 ^
    35 |     TT_FATAL(heads_per_group.size() == 3, "heads_per_group size ({}) must equal 3", heads_per_group.size());
    36 |

  Step 2: /work/ttnn/cpp/ttnn/operations/experimental/transformer/create_qkv_heads/device/create_qkv_heads_program_factory.cpp:35:5
  → Assuming the condition is true
    33 |     // Validation
    34 |     TT_FATAL(head_dim % TILE_WIDTH == 0, "head dim {} needs to be a multiple of tile width {}", head_dim, TILE_WIDTH);
    35 |     TT_FATAL(heads_per_group.size() == 3, "heads_per_group size ({}) must equal 3", heads_per_group.size());
                 ^
    36 |
    37 |     const uint32_t total_heads_per_group =

  Step 3: /work/ttnn/cpp/ttnn/operations/experimental/transformer/create_qkv_heads/device/create_qkv_heads_program_factory.cpp:39:5
  → 'elements_per_group' initialized to 0
    37 |     const uint32_t total_heads_per_group =
    38 |         std::accumulate(heads_per_group.begin(), heads_per_group.end(), 0u);  // num q heads + 2 * num_kv_heads
    39 |     const uint32_t elements_per_group =
                 ^
    40 |         head_dim * total_heads_per_group;  // head_dim * (num q heads + 2 * num kv heads)
    41 |     const uint32_t tiles_per_group =

  Step 4: /work/ttnn/cpp/ttnn/operations/experimental/transformer/create_qkv_heads/device/create_qkv_heads_program_factory.cpp:46:5
  → Division by zero
    44 |     const auto& input_shape = input_tensor.padded_shape();
    45 |
    46 |     TT_FATAL(
                 ^
    47 |         input_shape[3] % elements_per_group == 0,
    48 |         "flattened inner tensor dimension {} does not divide evenly into head dim {}, heads per group, and groups {}",

========================================================================
Finding #31
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/normalization/groupnorm/device/groupnorm_no_mcast_program_factory.cpp
Line:    922 (column 11)
Checker: deadcode.DeadStores
Severity: LOW
Report hash: 0168148e42f216f916e17721a897017a

Summary:
  Value stored to 'winv_group_2' during its initialization is never read

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/normalization/groupnorm/device/groupnorm_no_mcast_program_factory.cpp:922:11
  → Value stored to 'winv_group_2' during its initialization is never read
   920 |     uint32_t packed_winv_value_group_1 =
   921 |         pack_two_bfloat16_into_uint32({bfloat_winv_value_group_1, bfloat_winv_value_group_1});
   922 |     float winv_group_2 = winv_group_1;
                       ^
   923 |     bfloat16 bfloat_winv_value_group_2 = bfloat_winv_value_group_1;
   924 |     uint32_t packed_winv_value_group_2 = packed_winv_value_group_1;

========================================================================
Finding #32
========================================================================
File:    /work/tt-train/sources/ttml/metal/ops/silu_bw/device/silu_bw_program_factory.hpp
Line:    13 (column 12)
Checker: core.uninitialized.Assign
Severity: HIGH
Report hash: 05edbc838b8e43718aab96b3acbf7063

Summary:
  Value assigned to field 'silu_bw_kernel_group_2_id' in implicit constructor is garbage or undefined

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt-train/sources/ttml/metal/ops/silu_bw/device/silu_bw_program_factory.cpp:108:5
  → Assuming the condition is true
   106 |     auto padded_tensor_shape = input.padded_shape();
   107 |     auto padded_tensor_volume = input.physical_volume();
   108 |     TT_FATAL(
                 ^
   109 |         padded_tensor_volume % tt::constants::TILE_HW == 0, "Padded input tensor volume must be divisible by TILE_HW");
   110 |     TT_FATAL(padded_tensor_shape.rank() == 4U, "Input tensor must be 4D");

  Step 2: /work/tt-train/sources/ttml/metal/ops/silu_bw/device/silu_bw_program_factory.cpp:110:5
  → Assuming the condition is true
   108 |     TT_FATAL(
   109 |         padded_tensor_volume % tt::constants::TILE_HW == 0, "Padded input tensor volume must be divisible by TILE_HW");
   110 |     TT_FATAL(padded_tensor_shape.rank() == 4U, "Input tensor must be 4D");
                 ^
   111 |     uint32_t Wt = padded_tensor_shape[-1] / tt::constants::TILE_WIDTH;
   112 |     uint32_t Ht = padded_tensor_shape[-2] / tt::constants::TILE_HEIGHT;

  Step 3: /work/tt-train/sources/ttml/metal/ops/silu_bw/device/silu_bw_program_factory.cpp:153:5
  → Assuming the condition is true
   151 |     // -------------------------------------------------------------------------
   152 |     auto* input_buffer = input.buffer();
   153 |     TT_FATAL(
                 ^
   154 |         input_buffer->buffer_type() == tt::tt_metal::BufferType::DRAM,
   155 |         "Input buffer must be in DRAM. Input buffer of type {}",

  Step 4: /work/tt-train/sources/ttml/metal/ops/silu_bw/device/silu_bw_program_factory.cpp:159:5
  → Assuming the condition is true
   157 |
   158 |     auto* dLdout_buffer = dLdout.buffer();
   159 |     TT_FATAL(
                 ^
   160 |         dLdout_buffer->buffer_type() == tt::tt_metal::BufferType::DRAM,
   161 |         "dL_dout buffer must be in DRAM. dL_dout buffer of type {}",

  Step 5: /work/tt-train/sources/ttml/metal/ops/silu_bw/device/silu_bw_program_factory.cpp:165:5
  → Assuming the condition is true
   163 |
   164 |     auto* dL_da_buffer = output.buffer();
   165 |     TT_FATAL(
                 ^
   166 |         dL_da_buffer->buffer_type() == tt::tt_metal::BufferType::DRAM,
   167 |         "dL_da buffer must be in DRAM. dL_da buffer of type {}",

  Step 6: /work/tt-train/sources/ttml/metal/ops/silu_bw/device/silu_bw_program_factory.cpp:195:9
  → Assuming the condition is false
   193 |
   194 |     // Group 2 (if present) compile-time arguments
   195 |     if (!core_group_2.ranges().empty()) {
                     ^
   196 |         std::vector<uint32_t> compute_group_2_args = {
   197 |             num_rows_per_core_group_2,  // per_core_block_cnt

  Step 7: /work/tt-train/sources/ttml/metal/ops/silu_bw/device/silu_bw_program_factory.cpp:224:12
  → Calling constructor for 'CachedProgram<ttml::metal::ops::silu_bw::device::SiLUBackwardProgramFactory::shared_variables_t>'
   222 |     // 6) Return the fully configured program & relevant shared variables
   223 |     // -------------------------------------------------------------------------
   224 |     return cached_program_t{
                        ^
   225 |         std::move(program),
   226 |         {/* silu_bw_reader_kernel_id  = */ kernels.reader,

  Step 8: /work/tt_metal/api/tt-metalium/program_cache.hpp:33:5
  → Entered call from 'SiLUBackwardProgramFactory::create'
    31 |     shared_variables_t& shared_variables;
    32 |
    33 |     CachedProgram(tt::tt_metal::Program&& program, shared_variables_t&& shared_variables) :
                 ^
    34 |         owned_program{std::move(program)},
    35 |         owned_shared_variables{std::move(shared_variables)},

  Step 9: /work/tt_metal/api/tt-metalium/program_cache.hpp:35:9
  → Calling constructor for 'optional<ttml::metal::ops::silu_bw::device::SiLUBackwardProgramFactory::shared_variables_t>'
    33 |     CachedProgram(tt::tt_metal::Program&& program, shared_variables_t&& shared_variables) :
    34 |         owned_program{std::move(program)},
    35 |         owned_shared_variables{std::move(shared_variables)},
                     ^
    36 |         program{*owned_program},
    37 |         shared_variables{*owned_shared_variables} {}

  Step 10: /work/tt-train/sources/ttml/metal/ops/silu_bw/device/silu_bw_program_factory.hpp:13:12
  → Value assigned to field 'silu_bw_kernel_group_2_id' in implicit constructor is garbage or undefined
    11 |
    12 | struct SiLUBackwardProgramFactory {
    13 |     struct shared_variables_t {
                        ^
    14 |         tt::tt_metal::KernelHandle silu_bw_reader_kernel_id;
    15 |         tt::tt_metal::KernelHandle silu_bw_writer_kernel_id;

========================================================================
Finding #33
========================================================================
File:    /work/tt-train/sources/ttml/metal/ops/sdpa_bw/device/sdpa_bw_kv_program_factory.hpp
Line:    13 (column 12)
Checker: core.uninitialized.Assign
Severity: HIGH
Report hash: 299243a069c3e2c56d7a485c7850d18f

Summary:
  Value assigned to field 'sdpa_bw_kernel_group_2' in implicit constructor is garbage or undefined

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt-train/sources/ttml/metal/ops/sdpa_bw/device/sdpa_bw_kv_program_factory.cpp:219:9
  → Assuming field 'mask_type' is equal to None
   217 |     // Create mask buffer if using attention mask from DRAM or generating causal mask on-the-fly
   218 |     // Not needed for AttentionMaskType::None
   219 |     if (args.mask_type != AttentionMaskType::None) {
                     ^
   220 |         [[maybe_unused]] auto cb_attn_mask = create_circular_buffer(
   221 |             program, all_cores, kAttnMaskCbIndex, data_format, bfloat16_single_tile_size_bytes, 2 * kSingleTileBuffer);

  Step 2: /work/tt-train/sources/ttml/metal/ops/sdpa_bw/device/sdpa_bw_kv_program_factory.cpp:305:25
  → Assuming the condition is false
   303 |     auto* key_buffer = key.buffer();
   304 |     auto* value_buffer = value.buffer();
   305 |     auto* mask_buffer = tensor_args.attn_mask.has_value() ? tensor_args.attn_mask.value().buffer() : nullptr;
                                     ^
   306 |     auto* intermediates_buffer = intermediates.buffer();
   307 |

  Step 3: /work/tt-train/sources/ttml/metal/ops/sdpa_bw/device/sdpa_bw_kv_program_factory.cpp:400:9
  → Assuming the condition is false
   398 |
   399 |     // Group 2 (if present)
   400 |     if (!core_group_2.ranges().empty()) {
                     ^
   401 |         std::vector<uint32_t> compute_group_2_args = {
   402 |             num_rows_per_core_group_2,  // 0: per_core_block_cnt

  Step 4: /work/tt-train/sources/ttml/metal/ops/sdpa_bw/device/sdpa_bw_kv_program_factory.cpp:450:12
  → Calling constructor for 'CachedProgram<ttml::metal::ops::sdpa_bw::device::SDPABackwardKVProgramFactory::shared_variables_t>'
   448 |     // 6) Return the fully configured program & relevant shared variables
   449 |     // -------------------------------------------------------------------------
   450 |     return cached_program_t{
                        ^
   451 |         std::move(program),
   452 |         {.sdpa_bw_reader_kernel = kernels.reader,

  Step 5: /work/tt_metal/api/tt-metalium/program_cache.hpp:33:5
  → Entered call from 'SDPABackwardKVProgramFactory::create'
    31 |     shared_variables_t& shared_variables;
    32 |
    33 |     CachedProgram(tt::tt_metal::Program&& program, shared_variables_t&& shared_variables) :
                 ^
    34 |         owned_program{std::move(program)},
    35 |         owned_shared_variables{std::move(shared_variables)},

  Step 6: /work/tt_metal/api/tt-metalium/program_cache.hpp:35:9
  → Calling constructor for 'optional<ttml::metal::ops::sdpa_bw::device::SDPABackwardKVProgramFactory::shared_variables_t>'
    33 |     CachedProgram(tt::tt_metal::Program&& program, shared_variables_t&& shared_variables) :
    34 |         owned_program{std::move(program)},
    35 |         owned_shared_variables{std::move(shared_variables)},
                     ^
    36 |         program{*owned_program},
    37 |         shared_variables{*owned_shared_variables} {}

  Step 7: /work/tt-train/sources/ttml/metal/ops/sdpa_bw/device/sdpa_bw_kv_program_factory.hpp:13:12
  → Value assigned to field 'sdpa_bw_kernel_group_2' in implicit constructor is garbage or undefined
    11 |
    12 | struct SDPABackwardKVProgramFactory {
    13 |     struct shared_variables_t {
                        ^
    14 |         tt::tt_metal::KernelHandle sdpa_bw_reader_kernel{};
    15 |         tt::tt_metal::KernelHandle sdpa_bw_writer_kernel{};

========================================================================
Finding #34
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/data_movement/fold/device/fold_device_op.cpp
Line:    70 (column 10)
Checker: deadcode.DeadStores
Severity: LOW
Report hash: 5c28b1ae5572a62f84b1315a04199aed

Summary:
  Value stored to 'output_dtype' during its initialization is never read

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/data_movement/fold/device/fold_device_op.cpp:70:10
  → Value stored to 'output_dtype' during its initialization is never read
    68 |     auto input_tensor = tensors.input_tensor;
    69 |     const ttnn::Shape& input_shape = input_tensor.logical_shape();
    70 |     auto output_dtype = input_tensor.dtype();
                      ^
    71 |     switch (input_tensor.dtype()) {
    72 |         case tt::tt_metal::DataType::FLOAT32: output_dtype = tt::tt_metal::DataType::FLOAT32; break;

========================================================================
Finding #35
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/reduction/topk/device/topk_utils.cpp
Line:    74 (column 65)
Checker: core.DivideZero
Severity: HIGH
Report hash: d41e50ebdefa677ef924c9290bc46e2d

Summary:
  Division by zero

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/reduction/topk/device/topk_utils.cpp:178:9
  → Calling 'find_topk_core_config'
   176 |     // Attempt to find a valid configuration
   177 |     const auto config =
   178 |         find_topk_core_config(width, min_dim, max_dim, k, core_range, l1_size, value_tile_size, index_tile_size);
                     ^
   179 |     return config.has_value();
   180 | }

  Step 2: /work/ttnn/cpp/ttnn/operations/reduction/topk/device/topk_utils.cpp:55:1
  → Entered call from 'verify_multi_core_cost'
    53 |  * @return Optional TopKCoreConfig with optimal settings, or nullopt if impossible
    54 |  */
    55 | std::optional<TopKCoreConfig> find_topk_core_config(
             ^
    56 |     uint32_t width,
    57 |     uint32_t min_dim,

  Step 3: /work/ttnn/cpp/ttnn/operations/reduction/topk/device/topk_utils.cpp:74:67
  → Calling 'largest_power_of_two'
    72 |     // This ensures we start with a split size that can utilize most available cores
    73 |     const uint32_t start_split_size =
    74 |         static_cast<uint32_t>(width / tt::constants::TILE_WIDTH / largest_power_of_two(max_cores)) *
                                                                               ^
    75 |         tt::constants::TILE_WIDTH;
    76 |     // Search for optimal split size by trying powers of 2 from conservative start to max_dim

  Step 4: /work/ttnn/cpp/ttnn/operations/reduction/topk/device/topk_utils.cpp:23:1
  → Entered call from 'find_topk_core_config'
    21 |  * - largest_power_of_two(100) = 64 (2^6)
    22 |  */
    23 | uint32_t largest_power_of_two(uint32_t x) { return x == 0 ? 0 : (1U << (31 - __builtin_clz(x))); }
             ^
    24 |
    25 | /**

========================================================================
Finding #36
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/pool/upsample/device/upsample_program_factory_multicore_sharded.cpp
Line:    154 (column 52)
Checker: core.DivideZero
Severity: HIGH
Report hash: e373be52a3a1fa6abe62d6b64659b79e

Summary:
  Division by zero

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/pool/upsample/device/upsample_program_factory_multicore_sharded.cpp:58:50
  → Assuming field 'orientation' is not equal to ROW_MAJOR
    56 |
    57 |     auto logical_cores = corerange_to_cores(
    58 |         shard_spec.grid, shard_spec.num_cores(), shard_spec.orientation == ShardOrientation::ROW_MAJOR);
                                                              ^
    59 |     auto ranges = shard_spec.grid.ranges();
    60 |

  Step 2: /work/ttnn/cpp/ttnn/operations/pool/upsample/device/upsample_program_factory_multicore_sharded.cpp:61:9
  → Assuming 'is_height_sharded' is true
    59 |     auto ranges = shard_spec.grid.ranges();
    60 |
    61 |     if (!is_height_sharded) {
                     ^
    62 |         auto all_cores = shard_spec.grid;
    63 |         if (shard_spec.orientation == ShardOrientation::ROW_MAJOR) {

  Step 3: /work/ttnn/cpp/ttnn/operations/pool/upsample/device/upsample_program_factory_multicore_sharded.cpp:83:26
  → Assuming 'b' is >= 'batch_size'
    81 |     uint32_t elem_num = 0;
    82 |     // Create map of core and respective offsets in input
    83 |     for (uint32_t b = 0; b < batch_size; ++b) {
                                      ^
    84 |         for (uint32_t h = 0; h < in_h; ++h) {
    85 |             for (uint32_t j = 0; j < scale_factor_h; ++j) {

  Step 4: /work/ttnn/cpp/ttnn/operations/pool/upsample/device/upsample_program_factory_multicore_sharded.cpp:130:29
  → Assuming 'elem_num' is <= 'elems_per_core_reader'
   128 |
   129 |     elem_num = logical_core_to_stick_map.size() - last_ind;
   130 |     elems_per_core_reader = elem_num > elems_per_core_reader ? elem_num : elems_per_core_reader;
                                         ^
   131 |
   132 |     /* Each entry in config_vector contains 4 elements:

  Step 5: /work/ttnn/cpp/ttnn/operations/pool/upsample/device/upsample_program_factory_multicore_sharded.cpp:143:5
  → 'elems_per_core' initialized to 0
   141 |     const uint32_t config_buffer_entry_size = 4;
   142 |     elems_per_core_reader *= config_buffer_entry_size;
   143 |     const uint32_t elems_per_core =
                 ^
   144 |         2 * elems_per_core_reader;  // because two readers per tensix core which get equal number of stick intervals
   145 |

  Step 6: /work/ttnn/cpp/ttnn/operations/pool/upsample/device/upsample_program_factory_multicore_sharded.cpp:166:36
  → Entering loop body
   164 |
   165 |     uint32_t per_core_start_idx = 0;
   166 |     for (size_t i = ch_start_core; i <= ch_end_core; i++) {
                                                ^
   167 |         for (size_t ind = 0, j = 0; ind < dst_core_end_idx_map.size(); ind++) {
   168 |             const size_t chan_slice_begin = config_vector.size();

  Step 7: /work/ttnn/cpp/ttnn/operations/pool/upsample/device/upsample_program_factory_multicore_sharded.cpp:167:37
  → Assuming the condition is false
   165 |     uint32_t per_core_start_idx = 0;
   166 |     for (size_t i = ch_start_core; i <= ch_end_core; i++) {
   167 |         for (size_t ind = 0, j = 0; ind < dst_core_end_idx_map.size(); ind++) {
                                                 ^
   168 |             const size_t chan_slice_begin = config_vector.size();
   169 |             if (ind % 2 == 0) {

  Step 8: /work/ttnn/cpp/ttnn/operations/pool/upsample/device/upsample_program_factory_multicore_sharded.cpp:189:42
  → Passing the value 0 via 2nd parameter 'elems_per_core_reader'
   187 |             }
   188 |         }
   189 |         pad_uneven_shards(config_vector, elems_per_core, per_core_start_idx);
                                                      ^
   190 |     }
   191 |

  Step 9: /work/ttnn/cpp/ttnn/operations/pool/upsample/device/upsample_program_factory_multicore_sharded.cpp:151:115
  → Entered call from 'create_config_tensor'
   149 |     // In case last input shard is not full, fill the rest of the config vector with placeholder values
   150 |     const auto pad_uneven_shards = [config_buffer_entry_size](
   151 |                                        auto& config_vector, uint32_t elems_per_core_reader, size_t slice_begin = 0) {
                                                                                                                               ^
   152 |         const uint32_t slice_length = config_vector.size() - slice_begin;
   153 |         const uint32_t remainder =

  Step 10: /work/ttnn/cpp/ttnn/operations/pool/upsample/device/upsample_program_factory_multicore_sharded.cpp:154:52
  → Division by zero
   152 |         const uint32_t slice_length = config_vector.size() - slice_begin;
   153 |         const uint32_t remainder =
   154 |             (elems_per_core_reader - (slice_length % elems_per_core_reader)) % elems_per_core_reader;
                                                                ^
   155 |         if (remainder != 0) {
   156 |             for (int i = 0; i < remainder / config_buffer_entry_size; i++) {

========================================================================
Finding #37
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/data_movement/clone/device/clone_program_factory.cpp
Line:    162 (column 42)
Checker: core.NullDereference
Severity: HIGH
Report hash: 31260136f71e0ce3bae4d3048b0973a8

Summary:
  Dereference of undefined pointer value

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/data_movement/clone/device/clone_program_factory.cpp:28:26
  → Assuming 'input_data_format' is not equal to 'output_data_format'
    26 |     auto input_data_format = datatype_to_dataformat_converter(input.dtype());
    27 |     auto output_data_format = datatype_to_dataformat_converter(output.dtype());
    28 |     bool convert_dtype = input_data_format != output_data_format;
                                      ^
    29 |     bool tilized = output.layout() == Layout::TILE;
    30 |     auto compute_unit_size = [&](const auto& tensor, const auto& data_format) {

  Step 2: /work/ttnn/cpp/ttnn/operations/data_movement/clone/device/clone_program_factory.cpp:29:20
  → Assuming the condition is true
    27 |     auto output_data_format = datatype_to_dataformat_converter(output.dtype());
    28 |     bool convert_dtype = input_data_format != output_data_format;
    29 |     bool tilized = output.layout() == Layout::TILE;
                                ^
    30 |     auto compute_unit_size = [&](const auto& tensor, const auto& data_format) {
    31 |         return tilized ? tt::tile_size(data_format) : tensor.logical_shape()[-1] * tensor.element_size();

  Step 3: /work/ttnn/cpp/ttnn/operations/data_movement/clone/device/clone_program_factory.cpp:39:23
  → Assuming 'output_memory_layout' is not equal to INTERLEAVED
    37 |
    38 |     auto output_memory_layout = output.memory_config().memory_layout();
    39 |     bool is_sharded = output_memory_layout != TensorMemoryLayout::INTERLEAVED;
                                   ^
    40 |
    41 |     uint32_t num_cores;

  Step 4: /work/ttnn/cpp/ttnn/operations/data_movement/clone/device/clone_program_factory.cpp:170:9
  → Calling 'operator()'
   168 |             }
   169 |         };
   170 |         create_compute_kernel(core_group_1, num_units_per_core_group_1);
                     ^
   171 |         create_compute_kernel(core_group_2, num_units_per_core_group_2);
   172 |     }

  Step 5: /work/ttnn/cpp/ttnn/operations/data_movement/clone/device/clone_program_factory.cpp:150:93
  → Entered call from 'ProgramFactory::create'
   148 |         auto [math_fidelity, math_approx_mode, fp32_dest_acc_en, packer_l1_acc, dst_full_sync_en] =
   149 |             get_compute_kernel_config_args(input.device()->arch(), operation_attributes.compute_kernel_config);
   150 |         auto create_compute_kernel = [&](const auto& core_group, uint32_t num_units_per_core) {
                                                                                                         ^
   151 |             if (!core_group.ranges().empty()) {
   152 |                 std::vector<uint32_t> compute_kernel_args = {

  Step 6: /work/ttnn/cpp/ttnn/operations/data_movement/clone/device/clone_program_factory.cpp:151:17
  → Assuming the condition is true
   149 |             get_compute_kernel_config_args(input.device()->arch(), operation_attributes.compute_kernel_config);
   150 |         auto create_compute_kernel = [&](const auto& core_group, uint32_t num_units_per_core) {
   151 |             if (!core_group.ranges().empty()) {
                             ^
   152 |                 std::vector<uint32_t> compute_kernel_args = {
   153 |                     (uint32_t)src_cb_id,

  Step 7: /work/ttnn/cpp/ttnn/operations/data_movement/clone/device/clone_program_factory.cpp:162:42
  → Dereference of undefined pointer value
   160 |                     core_group,
   161 |                     ComputeConfig{
   162 |                         .math_fidelity = math_fidelity,
                                                      ^
   163 |                         .fp32_dest_acc_en = fp32_dest_acc_en,
   164 |                         .dst_full_sync_en = dst_full_sync_en,

========================================================================
Finding #38
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/data_movement/pad/device/pad_rm_reader_writer_multi_core_program_factory.cpp
Line:    31 (column 13)
Checker: deadcode.DeadStores
Severity: LOW
Report hash: 689485ce305c048af8f64a733094681d

Summary:
  Value stored to 'ncores_h' is never read

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/data_movement/pad/device/pad_rm_reader_writer_multi_core_program_factory.cpp:31:13
  → Value stored to 'ncores_h' is never read
    29 |     switch (nbatch) {
    30 |         case 1:
    31 |             ncores_h = 1;
                         ^
    32 |             nbatch_per_core_h = 1;
    33 |             ntiles_per_core_h = 1;

========================================================================
Finding #39
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/data_movement/pad/device/pad_rm_reader_writer_multi_core_program_factory.cpp
Line:    33 (column 13)
Checker: deadcode.DeadStores
Severity: LOW
Report hash: 54ac379b770f487e53841bda69095c7e

Summary:
  Value stored to 'ntiles_per_core_h' is never read

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/data_movement/pad/device/pad_rm_reader_writer_multi_core_program_factory.cpp:33:13
  → Value stored to 'ntiles_per_core_h' is never read
    31 |             ncores_h = 1;
    32 |             nbatch_per_core_h = 1;
    33 |             ntiles_per_core_h = 1;
                         ^
    34 |             switch (ntiles_h) {
    35 |                 case 2:

========================================================================
Finding #40
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/data_movement/pad/device/pad_rm_reader_writer_multi_core_program_factory.cpp
Line:    57 (column 13)
Checker: deadcode.DeadStores
Severity: LOW
Report hash: 689485ce305c048af8f64a733094681d

Summary:
  Value stored to 'ncores_h' is never read

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/data_movement/pad/device/pad_rm_reader_writer_multi_core_program_factory.cpp:57:13
  → Value stored to 'ncores_h' is never read
    55 |
    56 |         case 2:
    57 |             ncores_h = 1;
                         ^
    58 |             ncores_per_batch_h = 1;
    59 |             nbatch_per_core_h = 1;

========================================================================
Finding #41
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/data_movement/pad/device/pad_rm_reader_writer_multi_core_program_factory.cpp
Line:    58 (column 13)
Checker: deadcode.DeadStores
Severity: LOW
Report hash: bc81b2ccbc0a75d1443c891c9476ad17

Summary:
  Value stored to 'ncores_per_batch_h' is never read

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/data_movement/pad/device/pad_rm_reader_writer_multi_core_program_factory.cpp:58:13
  → Value stored to 'ncores_per_batch_h' is never read
    56 |         case 2:
    57 |             ncores_h = 1;
    58 |             ncores_per_batch_h = 1;
                         ^
    59 |             nbatch_per_core_h = 1;
    60 |             ntiles_per_core_h = 1;

========================================================================
Finding #42
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/data_movement/pad/device/pad_rm_reader_writer_multi_core_program_factory.cpp
Line:    60 (column 13)
Checker: deadcode.DeadStores
Severity: LOW
Report hash: 54ac379b770f487e53841bda69095c7e

Summary:
  Value stored to 'ntiles_per_core_h' is never read

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/data_movement/pad/device/pad_rm_reader_writer_multi_core_program_factory.cpp:60:13
  → Value stored to 'ntiles_per_core_h' is never read
    58 |             ncores_per_batch_h = 1;
    59 |             nbatch_per_core_h = 1;
    60 |             ntiles_per_core_h = 1;
                         ^
    61 |             switch (ntiles_h) {
    62 |                 case 2:

========================================================================
Finding #43
========================================================================
File:    /work/tt_stl/tests/test_small_vector.cpp
Line:    209 (column 9)
Checker: cplusplus.Move
Severity: HIGH
Report hash: 80fec6b75dccd77642b0adf0685917f2

Summary:
  Method called on moved-from object 'large'

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt_stl/tests/test_small_vector.cpp:186:9
  → Assuming the condition is true
   184 |         small.emplace_back(20);
   185 |         auto moved = std::move(small);
   186 |         EXPECT_EQ(moved.size(), 2u);
                     ^
   187 |         EXPECT_EQ(moved[0].value, 10);
   188 |         EXPECT_EQ(moved[1].value, 20);

  Step 2: /work/tt_stl/tests/test_small_vector.cpp:187:9
  → Assuming the condition is true
   185 |         auto moved = std::move(small);
   186 |         EXPECT_EQ(moved.size(), 2u);
   187 |         EXPECT_EQ(moved[0].value, 10);
                     ^
   188 |         EXPECT_EQ(moved[1].value, 20);
   189 |         // NOLINTNEXTLINE(bugprone-use-after-move)

  Step 3: /work/tt_stl/tests/test_small_vector.cpp:188:9
  → Assuming the condition is true
   186 |         EXPECT_EQ(moved.size(), 2u);
   187 |         EXPECT_EQ(moved[0].value, 10);
   188 |         EXPECT_EQ(moved[1].value, 20);
                     ^
   189 |         // NOLINTNEXTLINE(bugprone-use-after-move)
   190 |         EXPECT_TRUE(small.empty());

  Step 4: /work/tt_stl/tests/test_small_vector.cpp:194:25
  → Entering loop body
   192 |     {
   193 |         Vec large;
   194 |         for (int i = 0; i < kInlineCapacity + 2; ++i) {
                                     ^
   195 |             large.emplace_back(i);
   196 |         }

  Step 5: /work/tt_stl/tests/test_small_vector.cpp:200:21
  → Object 'large' is moved
   198 |         auto* const oldData = large.data();
   199 |         const std::size_t oldCapacity = large.capacity();
   200 |         Vec moved = std::move(large);
                                 ^
   201 |         EXPECT_EQ(moved.size(), kInlineCapacity + 2);
   202 |         EXPECT_GE(moved.capacity(), oldCapacity);

  Step 6: /work/tt_stl/tests/test_small_vector.cpp:201:9
  → Assuming the condition is true
   199 |         const std::size_t oldCapacity = large.capacity();
   200 |         Vec moved = std::move(large);
   201 |         EXPECT_EQ(moved.size(), kInlineCapacity + 2);
                     ^
   202 |         EXPECT_GE(moved.capacity(), oldCapacity);
   203 |         // When moved, it is typical for the heap storage to be transferred.

  Step 7: /work/tt_stl/tests/test_small_vector.cpp:202:9
  → Assuming the condition is true
   200 |         Vec moved = std::move(large);
   201 |         EXPECT_EQ(moved.size(), kInlineCapacity + 2);
   202 |         EXPECT_GE(moved.capacity(), oldCapacity);
                     ^
   203 |         // When moved, it is typical for the heap storage to be transferred.
   204 |         EXPECT_EQ(moved.data(), oldData);

  Step 8: /work/tt_stl/tests/test_small_vector.cpp:204:9
  → Assuming the condition is true
   202 |         EXPECT_GE(moved.capacity(), oldCapacity);
   203 |         // When moved, it is typical for the heap storage to be transferred.
   204 |         EXPECT_EQ(moved.data(), oldData);
                     ^
   205 |         // NOLINTNEXTLINE(bugprone-use-after-move)
   206 |         EXPECT_TRUE(large.empty());

  Step 9: /work/tt_stl/tests/test_small_vector.cpp:209:9
  → Method called on moved-from object 'large'
   207 |         // The moved-from vector may have shrunk its capacity but should be
   208 |         // functional; pushing into it should work.
   209 |         large.emplace_back(42);
                     ^
   210 |         EXPECT_EQ(large.size(), 1u);
   211 |         EXPECT_EQ(large[0].value, 42);

========================================================================
Finding #44
========================================================================
File:    /work/tests/tt_metal/tt_metal/perf_microbenchmark/dispatch/benchmark_rw_buffer.cpp
Line:    155 (column 15)
Checker: deadcode.DeadStores
Severity: LOW
Report hash: 1beb32a96f577cc17357da0dee257250

Summary:
  Value stored to '_' during its initialization is never read

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tests/tt_metal/tt_metal/perf_microbenchmark/dispatch/benchmark_rw_buffer.cpp:155:15
  → Value stored to '_' during its initialization is never read
   153 |     experimental::ShardDataTransferSetPinnedMemory(write_transfer, pinned_mem);
   154 |
   155 |     for (auto _ : state) {
                           ^
   156 |         bool blocking = true;
   157 |         mesh_device->mesh_command_queue().enqueue_write_shards(device_buffer, {write_transfer}, blocking);

========================================================================
Finding #45
========================================================================
File:    /work/tests/tt_metal/tt_metal/perf_microbenchmark/dispatch/benchmark_rw_buffer.cpp
Line:    231 (column 15)
Checker: deadcode.DeadStores
Severity: LOW
Report hash: 2f45282669aab795ebe9aebb57e48c2f

Summary:
  Value stored to '_' during its initialization is never read

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tests/tt_metal/tt_metal/perf_microbenchmark/dispatch/benchmark_rw_buffer.cpp:231:15
  → Value stored to '_' during its initialization is never read
   229 |     experimental::ShardDataTransferSetPinnedMemory(read_transfer, pinned_mem);
   230 |
   231 |     for (auto _ : state) {
                           ^
   232 |         mesh_device->mesh_command_queue().enqueue_read_shards({read_transfer}, device_buffer, /*blocking=*/true);
   233 |     }

========================================================================
Finding #46
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/ccl/mesh_partition/device/mesh_partition_program_factory.cpp
Line:    129 (column 35)
Checker: core.CallAndMessage
Severity: HIGH
Report hash: 7ff091465baa45b54115294f9f521fa8

Summary:
  1st function call argument is an uninitialized value

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/ccl/mesh_partition/device/mesh_partition_program_factory.cpp:127:41
  → Calling 'visit<(lambda at /work/ttnn/cpp/ttnn/operations/ccl/mesh_partition/device/mesh_partition_program_factory.cpp:128:9), std::variant<ttnn::prim::SliceRmProgramFactory, ttnn::prim::SliceRmShardedProgramFactory, ttnn::prim::SliceRmStrideProgramFactory, ttnn::prim::SliceTileProgramFactory, ttnn::prim::SliceTileTensorArgsProgramFactory> &>'
   125 |     SliceOp::validate_on_program_cache_miss(slice_attrs, slice_tensor_args);
   126 |     auto program_factory = SliceOp::select_program_factory(slice_attrs, slice_tensor_args);
   127 |     auto program_and_shared_variables = std::visit(
                                                     ^
   128 |         [&](auto&& factory) -> std::pair<Program, SliceSharedVariables> {
   129 |             auto cached_program = factory.create(slice_attrs, slice_tensor_args, tensor_return_value);

  Step 2: /work/ttnn/cpp/ttnn/operations/ccl/mesh_partition/device/mesh_partition_program_factory.cpp:128:71
  → Entered call from '__invoke_impl<std::pair<tt::tt_metal::Program, std::variant<ttnn::prim::SliceRmProgramFactory::shared_variables_t, ttnn::prim::SliceRmShardedProgramFactory::shared_variables_t, ttnn::prim::SliceRmStrideProgramFactory::shared_variables_t, ttnn::prim::SliceTileProgramFactory::shared_variables_t, ttnn::prim::SliceTileTensorArgsProgramFactory::shared_variables_t>>, (lambda at /work/ttnn/cpp/ttnn/operations/ccl/mesh_partition/device/mesh_partition_program_factory.cpp:128:9), ttnn::prim::SliceTileTensorArgsProgramFactory &>'
   126 |     auto program_factory = SliceOp::select_program_factory(slice_attrs, slice_tensor_args);
   127 |     auto program_and_shared_variables = std::visit(
   128 |         [&](auto&& factory) -> std::pair<Program, SliceSharedVariables> {
                                                                                   ^
   129 |             auto cached_program = factory.create(slice_attrs, slice_tensor_args, tensor_return_value);
   130 |             return {std::move(cached_program.program), std::move(cached_program.shared_variables)};

  Step 3: /work/ttnn/cpp/ttnn/operations/ccl/mesh_partition/device/mesh_partition_program_factory.cpp:129:35
  → 1st function call argument is an uninitialized value
   127 |     auto program_and_shared_variables = std::visit(
   128 |         [&](auto&& factory) -> std::pair<Program, SliceSharedVariables> {
   129 |             auto cached_program = factory.create(slice_attrs, slice_tensor_args, tensor_return_value);
                                               ^
   130 |             return {std::move(cached_program.program), std::move(cached_program.shared_variables)};
   131 |         },

========================================================================
Finding #47
========================================================================
File:    /work/tt-train/tests/benchmark/matmuls_benchmark.cpp
Line:    269 (column 15)
Checker: deadcode.DeadStores
Severity: LOW
Report hash: a04a8c35d10cdcbf7fcc32801ee3f765

Summary:
  Value stored to '_' during its initialization is never read

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt-train/tests/benchmark/matmuls_benchmark.cpp:269:15
  → Value stored to '_' during its initialization is never read
   267 |     std::vector<BenchmarkResult> results;
   268 |
   269 |     for (auto _ : state) {
                           ^
   270 |         results.clear();
   271 |

========================================================================
Finding #48
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/data_movement/sort/device/sort_program_factory.cpp
Line:    663 (column 46)
Checker: core.DivideZero
Severity: HIGH
Report hash: d6337a420d456b5af47ba6c062c32184

Summary:
  Division by zero

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/data_movement/sort/device/sort_program_factory.cpp:325:52
  → Calling 'SortProgramFactoryCrossCoreDataExchange::rounddown_pow2'
   323 |     const auto compute_with_storage_grid_size = device->compute_with_storage_grid_size();
   324 |     const uint32_t total_number_of_cores_physical = compute_with_storage_grid_size.y * compute_with_storage_grid_size.x;
   325 |     const uint32_t total_number_of_cores_virtual = rounddown_pow2(total_number_of_cores_physical);
                                                                ^
   326 |     uint32_t number_of_tiles_per_core = get_number_of_tiles_per_core(
   327 |         total_number_of_cores_virtual,

  Step 2: /work/ttnn/cpp/ttnn/operations/data_movement/sort/device/sort_program_factory.cpp:679:1
  → Entered call from 'SortProgramFactoryCrossCoreDataExchange::create'
   677 | }
   678 |
   679 | uint32_t SortProgramFactoryCrossCoreDataExchange::rounddown_pow2(uint32_t n) {
             ^
   680 |     if (n == 0) {
   681 |         return 0;

  Step 3: /work/ttnn/cpp/ttnn/operations/data_movement/sort/device/sort_program_factory.cpp:680:9
  → Assuming 'n' is equal to 0
   678 |
   679 | uint32_t SortProgramFactoryCrossCoreDataExchange::rounddown_pow2(uint32_t n) {
   680 |     if (n == 0) {
                     ^
   681 |         return 0;
   682 |     }

  Step 4: /work/ttnn/cpp/ttnn/operations/data_movement/sort/device/sort_program_factory.cpp:681:9
  → Returning zero
   679 | uint32_t SortProgramFactoryCrossCoreDataExchange::rounddown_pow2(uint32_t n) {
   680 |     if (n == 0) {
   681 |         return 0;
                     ^
   682 |     }
   683 |     return 1 << (31 - std::countl_zero(n));

  Step 5: /work/ttnn/cpp/ttnn/operations/data_movement/sort/device/sort_program_factory.cpp:327:9
  → Passing the value 0 via 1st parameter 'total_number_of_cores'
   325 |     const uint32_t total_number_of_cores_virtual = rounddown_pow2(total_number_of_cores_physical);
   326 |     uint32_t number_of_tiles_per_core = get_number_of_tiles_per_core(
   327 |         total_number_of_cores_virtual,
                     ^
   328 |         Wt,
   329 |         tensor_args.input_tensor.dtype(),

  Step 6: /work/ttnn/cpp/ttnn/operations/data_movement/sort/device/sort_program_factory.cpp:326:41
  → Calling 'SortProgramFactoryCrossCoreDataExchange::get_number_of_tiles_per_core'
   324 |     const uint32_t total_number_of_cores_physical = compute_with_storage_grid_size.y * compute_with_storage_grid_size.x;
   325 |     const uint32_t total_number_of_cores_virtual = rounddown_pow2(total_number_of_cores_physical);
   326 |     uint32_t number_of_tiles_per_core = get_number_of_tiles_per_core(
                                                     ^
   327 |         total_number_of_cores_virtual,
   328 |         Wt,

  Step 7: /work/ttnn/cpp/ttnn/operations/data_movement/sort/device/sort_program_factory.cpp:650:1
  → Entered call from 'SortProgramFactoryCrossCoreDataExchange::create'
   648 | }
   649 |
   650 | uint32_t SortProgramFactoryCrossCoreDataExchange::get_number_of_tiles_per_core(
             ^
   651 |     uint32_t total_number_of_cores,
   652 |     uint32_t Wt,

  Step 8: /work/ttnn/cpp/ttnn/operations/data_movement/sort/device/sort_program_factory.cpp:663:46
  → Division by zero
   661 |             constexpr uint32_t MIN_TILES_PER_CORE = 2;
   662 |             constexpr uint32_t MAX_TILES_PER_CORE = 128;
   663 |             const auto max_val = std::max(Wt / total_number_of_cores, MIN_TILES_PER_CORE);
                                                          ^
   664 |             return std::min(MAX_TILES_PER_CORE, max_val);
   665 |         }

========================================================================
Finding #49
========================================================================
File:    /work/tt-train/sources/examples/nano_gpt/3tier/optimizer_worker.cpp
Line:    100 (column 34)
Checker: core.NullDereference
Severity: HIGH
Report hash: dc81db9e400cb9cbf69aebbad2e43861

Summary:
  Dereference of undefined pointer value

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt-train/sources/examples/nano_gpt/3tier/optimizer_worker.cpp:75:9
  → Assuming field 'socket_type' is not equal to FABRIC
    73 |     three_tier_arch::DeviceConfig device_config = three_tier_arch::parse_device_config(yaml_config);
    74 |
    75 |     if (config.socket_type == ttnn::distributed::SocketType::FABRIC) {
                     ^
    76 |         auto num_devices = device_config.mesh_shape[0] * device_config.mesh_shape[1];
    77 |         ttml::ttnn_fixed::distributed::enable_fabric(num_devices);

  Step 2: /work/tt-train/sources/examples/nano_gpt/3tier/optimizer_worker.cpp:95:36
  → Assuming field 'enable_tp' is false
    93 |
    94 |     auto num_devices = static_cast<uint32_t>(device->num_devices());
    95 |     auto should_be_divisible_by = (device_config.enable_tp ? num_devices : 1U) * 32U;
                                                ^
    96 |     vocab_size = round_up_to_tile(vocab_size, should_be_divisible_by);
    97 |     std::visit(

  Step 3: /work/tt-train/sources/examples/nano_gpt/3tier/optimizer_worker.cpp:97:5
  → Calling 'visit<(lambda at /work/tt-train/sources/examples/nano_gpt/3tier/optimizer_worker.cpp:98:9), std::variant<ttml::models::gpt2::TransformerConfig, ttml::models::llama::LlamaConfig> &>'
    95 |     auto should_be_divisible_by = (device_config.enable_tp ? num_devices : 1U) * 32U;
    96 |     vocab_size = round_up_to_tile(vocab_size, should_be_divisible_by);
    97 |     std::visit(
                 ^
    98 |         [&](auto &&arg) {
    99 |             if constexpr (requires { arg.vocab_size; }) {

  Step 4: /work/tt-train/sources/examples/nano_gpt/3tier/optimizer_worker.cpp:98:23
  → Entered call from '__invoke_impl<void, (lambda at /work/tt-train/sources/examples/nano_gpt/3tier/optimizer_worker.cpp:98:9), ttml::models::llama::LlamaConfig &>'
    96 |     vocab_size = round_up_to_tile(vocab_size, should_be_divisible_by);
    97 |     std::visit(
    98 |         [&](auto &&arg) {
                                   ^
    99 |             if constexpr (requires { arg.vocab_size; }) {
   100 |                 arg.vocab_size = vocab_size;

  Step 5: /work/tt-train/sources/examples/nano_gpt/3tier/optimizer_worker.cpp:99:27
  → Assuming the condition is true
    97 |     std::visit(
    98 |         [&](auto &&arg) {
    99 |             if constexpr (requires { arg.vocab_size; }) {
                                       ^
   100 |                 arg.vocab_size = vocab_size;
   101 |             } else {

  Step 6: /work/tt-train/sources/examples/nano_gpt/3tier/optimizer_worker.cpp:100:34
  → Dereference of undefined pointer value
    98 |         [&](auto &&arg) {
    99 |             if constexpr (requires { arg.vocab_size; }) {
   100 |                 arg.vocab_size = vocab_size;
                                              ^
   101 |             } else {
   102 |                 throw std::runtime_error(

========================================================================
Finding #50
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/sliding_window/op_slicing/op_slicing.cpp
Line:    72 (column 13)
Checker: deadcode.DeadStores
Severity: LOW
Report hash: 0667fe75e599e770d223d1bd6e5151b5

Summary:
  Value stored to 'input_slice_width_start' is never read

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/sliding_window/op_slicing/op_slicing.cpp:72:13
  → Value stored to 'input_slice_width_start' is never read
    70 |             std::tie(input_slice_height_end, input_slice_width_end) = input_slice_end;
    71 |
    72 |             input_slice_width_start = 0;
                         ^
    73 |             input_slice_width_end = input_width;
    74 |

========================================================================
Finding #51
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/sliding_window/op_slicing/op_slicing.cpp
Line:    73 (column 13)
Checker: deadcode.DeadStores
Severity: LOW
Report hash: b2dad01920a5a2b889eeb17cafe78115

Summary:
  Value stored to 'input_slice_width_end' is never read

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/sliding_window/op_slicing/op_slicing.cpp:73:13
  → Value stored to 'input_slice_width_end' is never read
    71 |
    72 |             input_slice_width_start = 0;
    73 |             input_slice_width_end = input_width;
                         ^
    74 |
    75 |             input_slice_height_start = std::max<int>(0, input_slice_height_start);

========================================================================
Finding #52
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/sliding_window/op_slicing/op_slicing.cpp
Line:    94 (column 13)
Checker: deadcode.DeadStores
Severity: LOW
Report hash: 9e42575a731053f75bc72e4d9bed751d

Summary:
  Value stored to 'input_slice_height_start' is never read

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/sliding_window/op_slicing/op_slicing.cpp:94:13
  → Value stored to 'input_slice_height_start' is never read
    92 |             std::tie(input_slice_height_end, input_slice_width_end) = input_slice_end;
    93 |
    94 |             input_slice_height_start = 0;
                         ^
    95 |             input_slice_height_end = input_height;
    96 |             input_slice_width_start = std::max<int>(0, input_slice_width_start);

========================================================================
Finding #53
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/sliding_window/op_slicing/op_slicing.cpp
Line:    95 (column 13)
Checker: deadcode.DeadStores
Severity: LOW
Report hash: 19e289c63b5833fb29f4783b76bb48cc

Summary:
  Value stored to 'input_slice_height_end' is never read

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/sliding_window/op_slicing/op_slicing.cpp:95:13
  → Value stored to 'input_slice_height_end' is never read
    93 |
    94 |             input_slice_height_start = 0;
    95 |             input_slice_height_end = input_height;
                         ^
    96 |             input_slice_width_start = std::max<int>(0, input_slice_width_start);
    97 |             input_slice_width_end = std::min<int>(input_width, input_slice_width_end);

========================================================================
Finding #54
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/point_to_point/device/host/point_to_point_device_op.cpp
Line:    81 (column 98)
Checker: core.NullDereference
Severity: HIGH
Report hash: 6b3f948765d0296e331ab3f06e7baa0c

Summary:
  Dereference of undefined pointer value

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/point_to_point/device/host/point_to_point_device_op.cpp:77:5
  → Assuming line_hops is not equal to 0
    75 |     const auto [line_hops, line_is_forward, dim] = fabric_1d_routing_vector(sender_coord, receiver_coord);
    76 |
    77 |     TT_FATAL(line_hops != 0, "Should not be send/receiving to the same device");
                 ^
    78 |
    79 |     auto get_neighbor_id = [&sender_coord, &mesh_device, &mesh_shape, dim](

  Step 2: /work/ttnn/cpp/ttnn/operations/point_to_point/device/host/point_to_point_device_op.cpp:87:9
  → Assuming 'topology' is not equal to Ring
    85 |     };
    86 |
    87 |     if (topology == ::ttnn::ccl::Topology::Ring) {
                     ^
    88 |         int ring_hops = line_hops + ((line_hops < 0 ? -1 : 1) * mesh_shape[dim]);
    89 |

  Step 3: /work/ttnn/cpp/ttnn/operations/point_to_point/device/host/point_to_point_device_op.cpp:97:33
  → Calling 'operator()'
    95 |         }
    96 |     }
    97 |     const auto next_fabric_id = get_neighbor_id(line_is_forward, MeshCoordinate::BoundaryMode::NONE);
                                             ^
    98 |     return {line_hops, !line_is_forward, next_fabric_id};
    99 | }

  Step 4: /work/ttnn/cpp/ttnn/operations/point_to_point/device/host/point_to_point_device_op.cpp:80:91
  → Entered call from 'fabric_1d_routing'
    78 |
    79 |     auto get_neighbor_id = [&sender_coord, &mesh_device, &mesh_shape, dim](
    80 |                                bool is_forward, MeshCoordinate::BoundaryMode boundary_mode) {
                                                                                                       ^
    81 |         const auto neighbor_coord = sender_coord.get_neighbor(mesh_shape, (is_forward ? 1 : -1), dim, boundary_mode);
    82 |

  Step 5: /work/ttnn/cpp/ttnn/operations/point_to_point/device/host/point_to_point_device_op.cpp:81:98
  → Dereference of undefined pointer value
    79 |     auto get_neighbor_id = [&sender_coord, &mesh_device, &mesh_shape, dim](
    80 |                                bool is_forward, MeshCoordinate::BoundaryMode boundary_mode) {
    81 |         const auto neighbor_coord = sender_coord.get_neighbor(mesh_shape, (is_forward ? 1 : -1), dim, boundary_mode);
                                                                                                              ^
    82 |
    83 |         TT_FATAL(neighbor_coord.has_value(), "Can't find neighbor for {}", sender_coord);

========================================================================
Finding #55
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/moreh/moreh_layer_norm_backward/device/moreh_layer_norm_backward_gamma_beta_grad_program_factory.cpp
Line:    303 (column 35)
Checker: core.CallAndMessage
Severity: HIGH
Report hash: ee947fda4c2c87aa9c94128f4711131d

Summary:
  Called C++ object pointer is null

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/moreh/moreh_layer_norm_backward/device/moreh_layer_norm_backward_gamma_beta_grad_program_factory.cpp:281:31
  → Assuming the condition is true
   279 |     auto* rstd_buffer = tensor_args.rstd.buffer();
   280 |
   281 |     auto* gamma_grad_buffer = tensor_return_value.at(0).has_value() ? tensor_return_value.at(0)->buffer() : nullptr;
                                           ^
   282 |     auto* beta_grad_buffer = tensor_return_value.at(1).has_value() ? tensor_return_value.at(1)->buffer() : nullptr;
   283 |

  Step 2: /work/ttnn/cpp/ttnn/operations/moreh/moreh_layer_norm_backward/device/moreh_layer_norm_backward_gamma_beta_grad_program_factory.cpp:282:30
  → Assuming the condition is false
   280 |
   281 |     auto* gamma_grad_buffer = tensor_return_value.at(0).has_value() ? tensor_return_value.at(0)->buffer() : nullptr;
   282 |     auto* beta_grad_buffer = tensor_return_value.at(1).has_value() ? tensor_return_value.at(1)->buffer() : nullptr;
                                          ^
   283 |
   284 |     auto num_cores = cached_program.shared_variables.num_cores;

  Step 3: /work/ttnn/cpp/ttnn/operations/moreh/moreh_layer_norm_backward/device/moreh_layer_norm_backward_gamma_beta_grad_program_factory.cpp:287:26
  → Assuming 'i' is < 'num_cores'
   285 |     auto num_cores_y = cached_program.shared_variables.num_cores_y;
   286 |
   287 |     for (uint32_t i = 0; i < num_cores; ++i) {
                                      ^
   288 |         CoreCoord core = {i / num_cores_y, i % num_cores_y};
   289 |         {

  Step 4: /work/ttnn/cpp/ttnn/operations/moreh/moreh_layer_norm_backward/device/moreh_layer_norm_backward_gamma_beta_grad_program_factory.cpp:299:17
  → Assuming the condition is true
   297 |         {
   298 |             auto& runtime_args = GetRuntimeArgs(program, writer_kernel_id, core);
   299 |             if (gamma_grad_buffer != nullptr) {
                             ^
   300 |                 runtime_args[0] = gamma_grad_buffer->address();
   301 |             }

  Step 5: /work/ttnn/cpp/ttnn/operations/moreh/moreh_layer_norm_backward/device/moreh_layer_norm_backward_gamma_beta_grad_program_factory.cpp:303:35
  → Called C++ object pointer is null
   301 |             }
   302 |             if (gamma_grad_buffer != nullptr) {
   303 |                 runtime_args[1] = beta_grad_buffer->address();
                                               ^
   304 |             }
   305 |         }

========================================================================
Finding #56
========================================================================
File:    /work/tests/tt_metal/tt_metal/dispatch/random_program_fixture.hpp
Line:    297 (column 40)
Checker: core.DivideZero
Severity: HIGH
Report hash: 3745c7dc5af8f0cf7275105dc044193b

Summary:
  Division by zero

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tests/tt_metal/tt_metal/dispatch/dispatch_trace/test_EnqueueTrace.cpp:939:26
  → Entering loop body
   937 |     std::unordered_map<uint64_t, distributed::MeshTraceId> program_ids_to_trace_ids;
   938 |
   939 |     for (uint32_t i = 0; i < NUM_WORKLOADS; i++) {
                                      ^
   940 |         if (i % 10 == 0) {
   941 |             log_info(tt::LogTest, "Creating Program {}", i);

  Step 2: /work/tests/tt_metal/tt_metal/dispatch/dispatch_trace/test_EnqueueTrace.cpp:949:9
  → Calling 'UnitMeshRandomProgramFixture::create_kernel'
   947 |         kernel_properties.max_kernel_size_bytes = MAX_KERNEL_SIZE_BYTES / 2;
   948 |         kernel_properties.max_num_rt_args = MAX_NUM_RUNTIME_ARGS / 4;
   949 |         this->create_kernel(program, CoreType::ETH, false, kernel_properties);
                     ^
   950 |         this->workloads[i].add_program(device_range_, std::move(program));
   951 |         tt::tt_metal::distributed::MeshWorkload& workload = this->workloads[i];

  Step 3: /work/tests/tt_metal/tt_metal/dispatch/random_program_fixture.hpp:87:5
  → Entered call from 'UnitMeshRandomProgramTraceFixture_ActiveEthTestProgramsTraceAndNoTrace_Test::TestBody'
    85 |         srand(seed);
    86 |     }
    87 |     void create_kernel(
                 ^
    88 |         Program& program,
    89 |         const CoreType kernel_core_type,

  Step 4: /work/tests/tt_metal/tt_metal/dispatch/random_program_fixture.hpp:115:38
  → Calling 'UnitMeshRandomProgramFixture::create_kernel'
   113 |             const uint32_t num_unique_rt_args = unique_rt_args.size() - sem_ids.size() - cb_page_sizes.size();
   114 |
   115 |             KernelHandle kernel_id = this->create_kernel(
                                                  ^
   116 |                 program,
   117 |                 cores,

  Step 5: /work/tests/tt_metal/tt_metal/dispatch/random_program_fixture.hpp:225:5
  → Entered call from 'UnitMeshRandomProgramFixture::create_kernel'
   223 |
   224 | private:
   225 |     KernelHandle create_kernel(
                 ^
   226 |         Program& program,
   227 |         const CoreRangeSet& cores,

  Step 6: /work/tests/tt_metal/tt_metal/dispatch/random_program_fixture.hpp:265:31
  → Calling 'UnitMeshRandomProgramFixture::get_processor'
   263 |         if (create_eth_config) {
   264 |             compile_args.push_back(static_cast<uint32_t>(HalProgrammableCoreType::ACTIVE_ETH));
   265 |             const auto proc = this->get_processor(true);
                                           ^
   266 |             config = EthernetConfig{
   267 |                 .noc = static_cast<NOC>(proc), .processor = proc, .compile_args = compile_args, .defines = defines};

  Step 7: /work/tests/tt_metal/tt_metal/dispatch/random_program_fixture.hpp:300:5
  → Entered call from 'UnitMeshRandomProgramFixture::create_kernel'
   298 |     }
   299 |
   300 |     DataMovementProcessor get_processor(bool is_eth) {
                 ^
   301 |         int max_index = 1;
   302 |         int num = 0;

  Step 8: /work/tests/tt_metal/tt_metal/dispatch/random_program_fixture.hpp:309:15
  → Calling 'UnitMeshRandomProgramFixture::generate_random_num'
   307 |                         1;
   308 |         }
   309 |         num = this->generate_random_num(0, max_index);
                           ^
   310 |         DataMovementProcessor processor;
   311 |         if (num == 0) {

  Step 9: /work/tests/tt_metal/tt_metal/dispatch/random_program_fixture.hpp:284:5
  → Entered call from 'UnitMeshRandomProgramFixture::get_processor'
   282 |
   283 |     // Generates a random number within the given bounds (inclusive) that is divisible by divisible_by
   284 |     uint32_t generate_random_num(const uint32_t min, const uint32_t max, const uint32_t divisible_by = 1) {
                 ^
   285 |         TT_FATAL(max >= min, "max: {}, min: {} - max must be >= min", max, min);
   286 |

  Step 10: /work/tests/tt_metal/tt_metal/dispatch/random_program_fixture.hpp:297:40
  → Division by zero
   295 |             divisible_by);
   296 |
   297 |         return adjusted_min + ((rand() % ((adjusted_max - adjusted_min) / divisible_by + 1)) * divisible_by);
                                                    ^
   298 |     }
   299 |

========================================================================
Finding #57
========================================================================
File:    /work/tt_metal/fabric/fabric_tensix_builder.cpp
Line:    510 (column 31)
Checker: core.DivideZero
Severity: HIGH
Report hash: 2b3b3e8f30f8b246d018c3d1009dd5dd

Summary:
  Division by zero

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt_metal/fabric/fabric_tensix_builder.cpp:182:10
  → Calling 'FabricTensixDatamoverConfig::initialize_channel_mappings'
   180 | FabricTensixDatamoverConfig::FabricTensixDatamoverConfig() {
   181 |     // Initialize channel mappings and configurations, skipping the rest initilization if there are no ethernet found
   182 |     if (!initialize_channel_mappings()) {
                      ^
   183 |         return;
   184 |     }

  Step 2: /work/tt_metal/fabric/fabric_tensix_builder.cpp:287:1
  → Entered call from default constructor for 'FabricTensixDatamoverConfig'
   285 | }
   286 |
   287 | bool FabricTensixDatamoverConfig::initialize_channel_mappings() {
             ^
   288 |     // Get logical fabric mux cores from the first available device (same for all devices), except for TG
   289 |     const bool is_TG =

  Step 3: /work/tt_metal/fabric/fabric_tensix_builder.cpp:290:10
  → Assuming the condition is false
   288 |     // Get logical fabric mux cores from the first available device (same for all devices), except for TG
   289 |     const bool is_TG =
   290 |         (tt_metal::MetalContext::instance().get_cluster().get_cluster_type() == tt_metal::ClusterType::TG);
                      ^
   291 |     TT_FATAL(!is_TG, "Fabric with tensix extension is not supported for TG");
   292 |

  Step 4: /work/tt_metal/fabric/fabric_tensix_builder.cpp:294:5
  → Assuming the condition is true
   292 |
   293 |     const auto& all_active_devices = tt_metal::MetalContext::instance().device_manager()->get_all_active_devices();
   294 |     TT_FATAL(!all_active_devices.empty(), "No active devices found in DeviceManager");
                 ^
   295 |
   296 |     // Calculate and cache min/max ethernet channels once for later use

  Step 5: /work/tt_metal/fabric/fabric_tensix_builder.cpp:308:5
  → Assuming the condition is true
   306 |     logical_dispatch_mux_cores_ = tt::get_logical_dispatch_cores(device_id, num_hw_cqs, dispatch_core_config);
   307 |
   308 |     TT_FATAL(!logical_fabric_mux_cores_.empty(), "No logical fabric mux cores found for device {}", device_id);
                 ^
   309 |
   310 |     // Initialize translated mux cores (coordinates should be same across devices)

  Step 6: /work/tt_metal/fabric/fabric_tensix_builder.cpp:312:5
  → Assuming the condition is true
   310 |     // Initialize translated mux cores (coordinates should be same across devices)
   311 |     auto* device = tt_metal::MetalContext::instance().device_manager()->get_active_device(device_id);
   312 |     TT_FATAL(device != nullptr, "Device {} not found in DeviceManager", device_id);
                 ^
   313 |     for (const auto& logical_core : logical_fabric_mux_cores_) {
   314 |         CoreCoord translated_core = device->worker_core_from_logical_core(logical_core);

  Step 7: /work/tt_metal/fabric/fabric_tensix_builder.cpp:313:35
  → Loop body skipped when range is empty
   311 |     auto* device = tt_metal::MetalContext::instance().device_manager()->get_active_device(device_id);
   312 |     TT_FATAL(device != nullptr, "Device {} not found in DeviceManager", device_id);
   313 |     for (const auto& logical_core : logical_fabric_mux_cores_) {
                                               ^
   314 |         CoreCoord translated_core = device->worker_core_from_logical_core(logical_core);
   315 |         translated_fabric_or_dispatch_mux_cores_.insert(translated_core);

  Step 8: /work/tt_metal/fabric/fabric_tensix_builder.cpp:318:35
  → Loop body skipped when range is empty
   316 |         translated_fabric_mux_cores_.insert(translated_core);
   317 |     }
   318 |     for (const auto& logical_core : logical_dispatch_mux_cores_) {
                                               ^
   319 |         CoreCoord translated_core = device->worker_core_from_logical_core(logical_core);
   320 |         translated_fabric_or_dispatch_mux_cores_.insert(translated_core);

  Step 9: /work/tt_metal/fabric/fabric_tensix_builder.cpp:325:9
  → Assuming field 'max_eth_channels_' is not equal to 0
   323 |
   324 |     // Check if we found any active ethernet channels
   325 |     if (max_eth_channels_ == 0) {
                     ^
   326 |         log_warning(tt::LogMetal, "No active ethernet channels found in the system");
   327 |         return false;

  Step 10: /work/tt_metal/fabric/fabric_tensix_builder.cpp:330:5
  → Assuming the condition is true
   328 |     }
   329 |
   330 |     TT_FATAL(!logical_fabric_mux_cores_.empty(), "logical_fabric_mux_cores_ is empty before division");
                 ^
   331 |
   332 |     // Calculate number of configs per core (should always be 1: one eth channel per core)

  Step 11: /work/tt_metal/fabric/fabric_tensix_builder.cpp:335:5
  → Assuming field 'num_configs_per_core_' is equal to 1
   333 |     num_configs_per_core_ =
   334 |         (max_eth_channels_ + logical_fabric_mux_cores_.size() - 1) / logical_fabric_mux_cores_.size();
   335 |     TT_FATAL(
                 ^
   336 |         num_configs_per_core_ == 1,
   337 |         "Expected 1 config per core (one eth channel per core), but got {} configs per core",

  Step 12: /work/tt_metal/fabric/fabric_tensix_builder.cpp:352:59
  → The value 0 is assigned to field 'num_used_riscs_per_tensix_'
   350 |             num_used_riscs_per_tensix_ = 2;
   351 |             break;
   352 |         case tt::tt_fabric::FabricTensixConfig::DISABLED: num_used_riscs_per_tensix_ = 0; break;
                                                                       ^
   353 |         default: TT_THROW("Unsupported FabricTensixConfig mode: {}", static_cast<int>(fabric_tensix_config));
   354 |     }

  Step 13: /work/tt_metal/fabric/fabric_tensix_builder.cpp:185:5
  → Calling 'FabricTensixDatamoverConfig::calculate_buffer_allocations'
   183 |         return;
   184 |     }
   185 |     calculate_buffer_allocations();
                 ^
   186 |     create_configs();  // Mode-aware config creation
   187 | }

  Step 14: /work/tt_metal/fabric/fabric_tensix_builder.cpp:491:1
  → Entered call from default constructor for 'FabricTensixDatamoverConfig'
   489 | }
   490 |
   491 | void FabricTensixDatamoverConfig::calculate_buffer_allocations() {
             ^
   492 |     const auto& hal = tt_metal::MetalContext::instance().hal();
   493 |     const auto& fabric_context = tt_metal::MetalContext::instance().get_control_plane().get_fabric_context();

  Step 15: /work/tt_metal/fabric/fabric_tensix_builder.cpp:510:31
  → Division by zero
   508 |
   509 |     // Reserve space for both core types with proper alignment
   510 |     space_per_risc_ = l1_size / num_used_riscs_per_tensix_;             // Split between MUX and RELAY
                                           ^
   511 |     space_per_risc_ = (space_per_risc_ / l1_alignment) * l1_alignment;  // Align down to L1 alignment
   512 |

========================================================================
Finding #58
========================================================================
File:    /work/tt_metal/distributed/mesh_device_view.cpp
Line:    262 (column 23)
Checker: core.NullDereference
Severity: HIGH
Report hash: 254a74b26f73582bb4d60a7985fbbd12

Summary:
  Dereference of undefined pointer value

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt_metal/distributed/mesh_device_view.cpp:529:12
  → Calling 'MeshDeviceViewImpl::get_line_coordinates'
   527 | std::vector<MeshCoordinate> MeshDeviceView::get_line_coordinates(
   528 |     size_t length, const Shape2D& mesh_shape, const Shape2D& mesh_offset) {
   529 |     return MeshDeviceViewImpl::get_line_coordinates(length, mesh_shape, mesh_offset);
                        ^
   530 | }
   531 |

  Step 2: /work/tt_metal/distributed/mesh_device_view.cpp:203:1
  → Entered call from 'MeshDeviceView::get_line_coordinates'
   201 | bool MeshDeviceViewImpl::is_mesh_2d() const { return shape_2d_.has_value(); }
   202 |
   203 | std::vector<MeshCoordinate> MeshDeviceViewImpl::get_line_coordinates(
             ^
   204 |     size_t length, const Shape2D& mesh_shape, const Shape2D& mesh_offset) {
   205 |     const auto [num_rows, num_cols] = mesh_shape;

  Step 3: /work/tt_metal/distributed/mesh_device_view.cpp:209:5
  → Assuming start_row is < num_rows
   207 |
   208 |     // Validate starting position
   209 |     TT_FATAL(
                 ^
   210 |         start_row < num_rows && start_col < num_cols,
   211 |         "Mesh offset ({}, {}) is out of bounds for mesh shape ({}, {})",

  Step 4: /work/tt_metal/distributed/mesh_device_view.cpp:224:9
  → Assuming the condition is false
   222 |     // that cause fabric initialization issues on T3K
   223 |     // https://github.com/tenstorrent/tt-metal/issues/33737
   224 |     if (mesh_shape == Shape2D(2, 4) || mesh_shape == Shape2D(4, 2)) {
                     ^
   225 |         auto ring_coords = get_ring_coordinates(mesh_shape, mesh_shape);
   226 |         MeshCoordinate start_coord(start_row, start_col);

  Step 5: /work/tt_metal/distributed/mesh_device_view.cpp:324:23
  → Calling 'function::operator()'
   322 |
   323 |     // First try to find a ring path (preferred)
   324 |     bool found_path = dfs(line_coords, visited, /*require_ring=*/true);
                                   ^
   325 |
   326 |     // If ring not possible, fall back to any valid path

  Step 6: /work/tt_metal/distributed/mesh_device_view.cpp:281:115
  → Entered call from '__invoke_impl<bool, (lambda at /work/tt_metal/distributed/mesh_device_view.cpp:281:9) &, std::vector<tt::tt_metal::distributed::MeshCoordinate> &, std::unordered_set<tt::tt_metal::distributed::MeshCoordinate> &, bool>'
   279 |     // First tries to find a ring path, then falls back to any valid path if ring is impossible
   280 |     std::function<bool(std::vector<MeshCoordinate>&, std::unordered_set<MeshCoordinate>&, bool require_ring)> dfs =
   281 |         [&](std::vector<MeshCoordinate>& path, std::unordered_set<MeshCoordinate>& visited, bool require_ring) -> bool {
                                                                                                                               ^
   282 |         if (path.size() >= length) {
   283 |             return true;

  Step 7: /work/tt_metal/distributed/mesh_device_view.cpp:282:13
  → Assuming the condition is false
   280 |     std::function<bool(std::vector<MeshCoordinate>&, std::unordered_set<MeshCoordinate>&, bool require_ring)> dfs =
   281 |         [&](std::vector<MeshCoordinate>& path, std::unordered_set<MeshCoordinate>& visited, bool require_ring) -> bool {
   282 |         if (path.size() >= length) {
                         ^
   283 |             return true;
   284 |         }

  Step 8: /work/tt_metal/distributed/mesh_device_view.cpp:289:26
  → Calling 'operator()'
   287 |
   288 |         // Get unvisited neighbors
   289 |         auto neighbors = get_neighbors(current);
                                      ^
   290 |         for (const auto& neighbor : neighbors) {
   291 |             if (visited.contains(neighbor)) {

  Step 9: /work/tt_metal/distributed/mesh_device_view.cpp:257:88
  → Entered call from 'operator()'
   255 |
   256 |     // Lambda to get valid neighbors (not checking visited - that's done in DFS)
   257 |     auto get_neighbors = [&](const MeshCoordinate& coord) -> std::vector<MeshCoordinate> {
                                                                                                    ^
   258 |         std::vector<MeshCoordinate> neighbors;
   259 |         const size_t row = coord[0];

  Step 10: /work/tt_metal/distributed/mesh_device_view.cpp:262:23
  → Dereference of undefined pointer value
   260 |         const size_t col = coord[1];
   261 |
   262 |         if (col + 1 < num_cols) {
                                   ^
   263 |             neighbors.emplace_back(row, col + 1);
   264 |         }

========================================================================
Finding #59
========================================================================
File:    /work/tt_metal/llrt/hal.cpp
Line:    98 (column 13)
Checker: optin.core.EnumCastOutOfRange
Severity: MEDIUM
Report hash: e599f57d3bb0853ed8b3b35439ccfb8c

Summary:
  The value '2' provided to the cast expression is not in the valid range of values for 'HalProcessorClassType'

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt_metal/llrt/hal.hpp:701:12
  → Calling 'HalCoreInfoType::get_processor_class_name'
   699 |     HalProgrammableCoreType programmable_core_type, uint32_t processor_index, bool is_abbreviated) const {
   700 |     auto idx = get_programmable_core_type_index(programmable_core_type);
   701 |     return this->core_info_[idx].get_processor_class_name(processor_index, is_abbreviated);
                        ^
   702 | }
   703 |

  Step 2: /work/tt_metal/llrt/hal.cpp:101:1
  → Entered call from 'Hal::get_processor_class_name'
    99 | }
   100 |
   101 | const std::string& HalCoreInfoType::get_processor_class_name(uint32_t processor_index, bool is_abbreviated) const {
             ^
   102 |     auto [processor_class, processor_type_idx] = get_processor_class_and_type_from_index(processor_index);
   103 |     uint32_t processor_class_idx = ttsl::as_underlying_type<HalProcessorClassType>(processor_class);

  Step 3: /work/tt_metal/llrt/hal.cpp:102:50
  → Calling 'HalCoreInfoType::get_processor_class_and_type_from_index'
   100 |
   101 | const std::string& HalCoreInfoType::get_processor_class_name(uint32_t processor_index, bool is_abbreviated) const {
   102 |     auto [processor_class, processor_type_idx] = get_processor_class_and_type_from_index(processor_index);
                                                              ^
   103 |     uint32_t processor_class_idx = ttsl::as_underlying_type<HalProcessorClassType>(processor_class);
   104 |     TT_ASSERT(ttsl::as_underlying_type<HalProcessorClassType>(processor_class) < this->processor_classes_names_.size());

  Step 4: /work/tt_metal/llrt/hal.cpp:87:1
  → Entered call from 'HalCoreInfoType::get_processor_class_name'
    85 | }
    86 |
    87 | std::pair<HalProcessorClassType, uint32_t> HalCoreInfoType::get_processor_class_and_type_from_index(
             ^
    88 |     uint32_t processor_index) const {
    89 |     uint32_t processor_class_idx = 0;

  Step 5: /work/tt_metal/llrt/hal.cpp:90:12
  → Assuming the condition is true
    88 |     uint32_t processor_index) const {
    89 |     uint32_t processor_class_idx = 0;
    90 |     for (; processor_class_idx < this->processor_classes_.size(); processor_class_idx++) {
                        ^
    91 |         auto processor_count = get_processor_types_count(processor_class_idx);
    92 |         if (processor_index < processor_count) {

  Step 6: /work/tt_metal/llrt/hal.cpp:92:13
  → Assuming 'processor_index' is >= 'processor_count'
    90 |     for (; processor_class_idx < this->processor_classes_.size(); processor_class_idx++) {
    91 |         auto processor_count = get_processor_types_count(processor_class_idx);
    92 |         if (processor_index < processor_count) {
                         ^
    93 |             break;
    94 |         }

  Step 7: /work/tt_metal/llrt/hal.cpp:97:5
  → Assuming the condition is true
    95 |         processor_index -= processor_count;
    96 |     }
    97 |     TT_ASSERT(processor_class_idx < this->processor_classes_.size());
                 ^
    98 |     return {static_cast<HalProcessorClassType>(processor_class_idx), processor_index};
    99 | }

  Step 8: /work/tt_metal/llrt/hal.cpp:98:13
  → The value '2' provided to the cast expression is not in the valid range of values for 'HalProcessorClassType'
    96 |     }
    97 |     TT_ASSERT(processor_class_idx < this->processor_classes_.size());
    98 |     return {static_cast<HalProcessorClassType>(processor_class_idx), processor_index};
                         ^
    99 | }
   100 |

========================================================================
Finding #60
========================================================================
File:    /work/tt_metal/impl/device/device.cpp
Line:    89 (column 5)
Checker: optin.cplusplus.VirtualCall
Severity: MEDIUM
Report hash: 95e1d26d428dec23db380c0fdda629df

Summary:
  Call to virtual method 'Device::initialize' during construction bypasses virtual dispatch

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt_metal/impl/device/device.cpp:89:5
  → Call to virtual method 'Device::initialize' during construction bypasses virtual dispatch
    87 |     id_(device_id) {
    88 |     ZoneScoped;
    89 |     this->initialize(num_hw_cqs, l1_small_size, trace_region_size, worker_l1_size, l1_bank_remap, minimal);
                 ^
    90 | }
    91 |

========================================================================
Finding #61
========================================================================
File:    /work/tt_metal/impl/device/device.cpp
Line:    147 (column 9)
Checker: optin.cplusplus.VirtualCall
Severity: MEDIUM
Report hash: 2ecee555084c1333797e5b4b771f385b

Summary:
  Call to virtual method 'Device::id' during construction bypasses virtual dispatch

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt_metal/impl/device/device.cpp:89:5
  → Calling 'Device::initialize'
    87 |     id_(device_id) {
    88 |     ZoneScoped;
    89 |     this->initialize(num_hw_cqs, l1_small_size, trace_region_size, worker_l1_size, l1_bank_remap, minimal);
                 ^
    90 | }
    91 |

  Step 2: /work/tt_metal/impl/device/device.cpp:396:1
  → Entered call from constructor for 'Device'
   394 | }
   395 |
   396 | bool Device::initialize(
             ^
   397 |     const uint8_t num_hw_cqs,
   398 |     size_t l1_small_size,

  Step 3: /work/tt_metal/impl/device/device.cpp:412:5
  → Assuming 'num_hw_cqs' is > 0
   410 |         this->program_cache_.is_enabled() ? "" : "NOT ");
   411 |     log_debug(tt::LogMetal, "Running with {} cqs ", num_hw_cqs);
   412 |     TT_FATAL(
                 ^
   413 |         num_hw_cqs > 0 and num_hw_cqs <= dispatch_core_manager::MAX_NUM_HW_CQS,
   414 |         "num_hw_cqs can be between 1 and {}",

  Step 4: /work/tt_metal/impl/device/device.cpp:419:9
  → Assuming 'worker_l1_size' is not equal to 0
   417 |     num_hw_cqs_ = num_hw_cqs;
   418 |     const auto& hal = MetalContext::instance().hal();
   419 |     if (worker_l1_size == 0) {
                     ^
   420 |         worker_l1_size = hal.get_dev_size(HalProgrammableCoreType::TENSIX, HalL1MemAddrType::DEFAULT_UNRESERVED);
   421 |     }

  Step 5: /work/tt_metal/impl/device/device.cpp:427:5
  → Assuming 'worker_l1_size' is <= 'max_worker_l1_size'
   425 |                                 hal.get_dev_addr(HalProgrammableCoreType::TENSIX, HalL1MemAddrType::KERNEL_CONFIG);
   426 |
   427 |     TT_FATAL(
                 ^
   428 |         worker_l1_size <= max_worker_l1_size,
   429 |         "Worker L1 size {} is larger than max size {}",

  Step 6: /work/tt_metal/impl/device/device.cpp:440:9
  → Calling 'Device::initialize_allocator'
   438 |         max_alignment);
   439 |     default_allocator_ =
   440 |         initialize_allocator(l1_small_size, trace_region_size, worker_l1_unreserved_start, l1_bank_remap);
                     ^
   441 |
   442 |     // For minimal setup, don't initialize FW, watcher, dprint. They won't work if we're attaching to a hung chip.

  Step 7: /work/tt_metal/impl/device/device.cpp:139:1
  → Entered call from 'Device::initialize'
   137 | }
   138 |
   139 | std::unique_ptr<AllocatorImpl> Device::initialize_allocator(
             ^
   140 |     size_t l1_small_size,
   141 |     size_t trace_region_size,

  Step 8: /work/tt_metal/impl/device/device.cpp:147:9
  → Call to virtual method 'Device::id' during construction bypasses virtual dispatch
   145 |     const metal_SocDescriptor& soc_desc = tt::tt_metal::MetalContext::instance().get_cluster().get_soc_desc(this->id_);
   146 |     auto config = L1BankingAllocator::generate_config(
   147 |         this->id(),
                     ^
   148 |         this->num_hw_cqs(),
   149 |         l1_small_size,

========================================================================
Finding #62
========================================================================
File:    /work/tt_metal/impl/device/device.cpp
Line:    148 (column 9)
Checker: optin.cplusplus.VirtualCall
Severity: MEDIUM
Report hash: 797016e00ba3c7ad59ac81c133b5de27

Summary:
  Call to virtual method 'Device::num_hw_cqs' during construction bypasses virtual dispatch

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt_metal/impl/device/device.cpp:89:5
  → Calling 'Device::initialize'
    87 |     id_(device_id) {
    88 |     ZoneScoped;
    89 |     this->initialize(num_hw_cqs, l1_small_size, trace_region_size, worker_l1_size, l1_bank_remap, minimal);
                 ^
    90 | }
    91 |

  Step 2: /work/tt_metal/impl/device/device.cpp:396:1
  → Entered call from constructor for 'Device'
   394 | }
   395 |
   396 | bool Device::initialize(
             ^
   397 |     const uint8_t num_hw_cqs,
   398 |     size_t l1_small_size,

  Step 3: /work/tt_metal/impl/device/device.cpp:412:5
  → Assuming 'num_hw_cqs' is > 0
   410 |         this->program_cache_.is_enabled() ? "" : "NOT ");
   411 |     log_debug(tt::LogMetal, "Running with {} cqs ", num_hw_cqs);
   412 |     TT_FATAL(
                 ^
   413 |         num_hw_cqs > 0 and num_hw_cqs <= dispatch_core_manager::MAX_NUM_HW_CQS,
   414 |         "num_hw_cqs can be between 1 and {}",

  Step 4: /work/tt_metal/impl/device/device.cpp:419:9
  → Assuming 'worker_l1_size' is not equal to 0
   417 |     num_hw_cqs_ = num_hw_cqs;
   418 |     const auto& hal = MetalContext::instance().hal();
   419 |     if (worker_l1_size == 0) {
                     ^
   420 |         worker_l1_size = hal.get_dev_size(HalProgrammableCoreType::TENSIX, HalL1MemAddrType::DEFAULT_UNRESERVED);
   421 |     }

  Step 5: /work/tt_metal/impl/device/device.cpp:427:5
  → Assuming 'worker_l1_size' is <= 'max_worker_l1_size'
   425 |                                 hal.get_dev_addr(HalProgrammableCoreType::TENSIX, HalL1MemAddrType::KERNEL_CONFIG);
   426 |
   427 |     TT_FATAL(
                 ^
   428 |         worker_l1_size <= max_worker_l1_size,
   429 |         "Worker L1 size {} is larger than max size {}",

  Step 6: /work/tt_metal/impl/device/device.cpp:440:9
  → Calling 'Device::initialize_allocator'
   438 |         max_alignment);
   439 |     default_allocator_ =
   440 |         initialize_allocator(l1_small_size, trace_region_size, worker_l1_unreserved_start, l1_bank_remap);
                     ^
   441 |
   442 |     // For minimal setup, don't initialize FW, watcher, dprint. They won't work if we're attaching to a hung chip.

  Step 7: /work/tt_metal/impl/device/device.cpp:139:1
  → Entered call from 'Device::initialize'
   137 | }
   138 |
   139 | std::unique_ptr<AllocatorImpl> Device::initialize_allocator(
             ^
   140 |     size_t l1_small_size,
   141 |     size_t trace_region_size,

  Step 8: /work/tt_metal/impl/device/device.cpp:148:9
  → Call to virtual method 'Device::num_hw_cqs' during construction bypasses virtual dispatch
   146 |     auto config = L1BankingAllocator::generate_config(
   147 |         this->id(),
   148 |         this->num_hw_cqs(),
                     ^
   149 |         l1_small_size,
   150 |         trace_region_size,

========================================================================
Finding #63
========================================================================
File:    /work/tt_metal/impl/device/device.cpp
Line:    458 (column 5)
Checker: optin.cplusplus.VirtualCall
Severity: MEDIUM
Report hash: a93745434ac45387d64f016edb3e7cc4

Summary:
  Call to virtual method 'Device::disable_and_clear_program_cache' during destruction bypasses virtual dispatch

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt_metal/impl/device/device.cpp:474:9
  → Assuming field 'initialized_' is true
   472 | Device::~Device() {
   473 |     log_debug(tt::LogMetal, "Device {} destructor", this->id_);
   474 |     if (this->initialized_) {
                     ^
   475 |         this->close();
   476 |     }

  Step 2: /work/tt_metal/impl/device/device.cpp:475:9
  → Calling 'Device::close'
   473 |     log_debug(tt::LogMetal, "Device {} destructor", this->id_);
   474 |     if (this->initialized_) {
   475 |         this->close();
                     ^
   476 |     }
   477 | }

  Step 3: /work/tt_metal/impl/device/device.cpp:452:1
  → Entered call from '~Device'
   450 | }
   451 |
   452 | bool Device::close() {
             ^
   453 |     log_trace(tt::LogMetal, "Closing device {}", this->id_);
   454 |     if (not this->initialized_) {

  Step 4: /work/tt_metal/impl/device/device.cpp:458:5
  → Call to virtual method 'Device::disable_and_clear_program_cache' during destruction bypasses virtual dispatch
   456 |     }
   457 |
   458 |     this->disable_and_clear_program_cache();
                 ^
   459 |     this->set_program_cache_misses_allowed(true);
   460 |

========================================================================
Finding #64
========================================================================
File:    /work/tt_metal/impl/device/device.cpp
Line:    475 (column 9)
Checker: optin.cplusplus.VirtualCall
Severity: MEDIUM
Report hash: dd18b1443e7492da06d90cff8e99dbae

Summary:
  Call to virtual method 'Device::close' during destruction bypasses virtual dispatch

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt_metal/impl/device/device.cpp:474:9
  → Assuming field 'initialized_' is true
   472 | Device::~Device() {
   473 |     log_debug(tt::LogMetal, "Device {} destructor", this->id_);
   474 |     if (this->initialized_) {
                     ^
   475 |         this->close();
   476 |     }

  Step 2: /work/tt_metal/impl/device/device.cpp:475:9
  → Call to virtual method 'Device::close' during destruction bypasses virtual dispatch
   473 |     log_debug(tt::LogMetal, "Device {} destructor", this->id_);
   474 |     if (this->initialized_) {
   475 |         this->close();
                     ^
   476 |     }
   477 | }

========================================================================
Finding #65
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/conv/conv_transpose2d/conv_transpose2d.cpp
Line:    527 (column 13)
Checker: deadcode.DeadStores
Severity: LOW
Report hash: b1a67206e6e69a1974428efd0f16a89e

Summary:
  Value stored to 'output_slice_width' is never read

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/conv/conv_transpose2d/conv_transpose2d.cpp:527:13
  → Value stored to 'output_slice_width' is never read
   525 |                 additional_padded_width);
   526 |             this_output_pad[1] += additional_padded_width;
   527 |             output_slice_width += additional_padded_width;
                         ^
   528 |         }
   529 |         auto this_op_padding = std::array<uint32_t, 4>(

========================================================================
Finding #66
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/conv/conv2d/device/conv2d_device_operation.cpp
Line:    128 (column 17)
Checker: deadcode.DeadStores
Severity: LOW
Report hash: eb10cdef26b52c3f54fd59cf0ccc2051

Summary:
  Value stored to 'out_width_ntiles' is never read

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/conv/conv2d/device/conv2d_device_operation.cpp:128:17
  → Value stored to 'out_width_ntiles' is never read
   126 |             // TODO: We should clean this up and relax constraints on out_subblock h and w
   127 |             if (args.memory_config.shard_spec().value().orientation == ShardOrientation::COL_MAJOR) {
   128 |                 out_width_ntiles = tt::div_up(out_width_ntiles, args.parallelization_config.grid_size.y);
                             ^
   129 |             } else {
   130 |                 out_width_ntiles = tt::div_up(out_width_ntiles, args.parallelization_config.grid_size.x);

========================================================================
Finding #67
========================================================================
File:    /work/ttnn/cpp/ttnn/operations/conv/conv2d/device/conv2d_device_operation.cpp
Line:    130 (column 17)
Checker: deadcode.DeadStores
Severity: LOW
Report hash: 63b4a204f74035744204dbe276644314

Summary:
  Value stored to 'out_width_ntiles' is never read

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/ttnn/cpp/ttnn/operations/conv/conv2d/device/conv2d_device_operation.cpp:130:17
  → Value stored to 'out_width_ntiles' is never read
   128 |                 out_width_ntiles = tt::div_up(out_width_ntiles, args.parallelization_config.grid_size.y);
   129 |             } else {
   130 |                 out_width_ntiles = tt::div_up(out_width_ntiles, args.parallelization_config.grid_size.x);
                             ^
   131 |             }
   132 |         }

========================================================================
Finding #68
========================================================================
File:    /work/tt_metal/distributed/fd_mesh_command_queue.cpp
Line:    370 (column 5)
Checker: core.CallAndMessage
Severity: HIGH
Report hash: 752d059921c6ac7035cff171de974996

Summary:
  Called C++ object pointer is uninitialized

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt_metal/distributed/fd_mesh_command_queue.cpp:260:5
  → Assuming the condition is true
   258 |     uint64_t command_hash = *mesh_device_->get_active_sub_device_manager_id();
   259 |     std::unordered_set<SubDeviceId> sub_device_ids = mesh_workload.impl().determine_sub_device_ids(mesh_device_);
   260 |     TT_FATAL(sub_device_ids.size() == 1, "Programs must be executed on a single sub-device");
                 ^
   261 |     SubDeviceId sub_device_id = *(sub_device_ids.begin());
   262 |     auto mesh_device_id = mesh_device_->id();

  Step 2: /work/tt_metal/distributed/fd_mesh_command_queue.cpp:266:9
  → Assuming the condition is false
   264 |     auto dispatch_core_config = MetalContext::instance().get_dispatch_core_manager().get_dispatch_core_config();
   265 |     CoreType dispatch_core_type = get_core_type_from_config(dispatch_core_config);
   266 |     if (!sysmem_manager.get_bypass_mode()) {
                     ^
   267 |         auto& sub_device_cq_owner = cq_shared_state_->sub_device_cq_owner;
   268 |         auto& sub_device = sub_device_cq_owner[*sub_device_id];

  Step 3: /work/tt_metal/distributed/fd_mesh_command_queue.cpp:272:5
  → Assuming the condition is true
   270 |     }
   271 |
   272 |     TT_FATAL(
                 ^
   273 |         mesh_workload.impl().get_program_binary_status(mesh_device_id) != ProgramBinaryStatus::NotSent,
   274 |         "Expected program binaries to be written to the MeshDevice.");

  Step 4: /work/tt_metal/distributed/fd_mesh_command_queue.cpp:283:9
  → Assuming 'mcast_go_signals' is false
   281 |     uint32_t num_virtual_eth_cores = 0;
   282 |
   283 |     if (mcast_go_signals) {
                     ^
   284 |         num_workers += mesh_device_->num_worker_cores(HalProgrammableCoreType::TENSIX, sub_device_id);
   285 |     }

  Step 5: /work/tt_metal/distributed/fd_mesh_command_queue.cpp:286:9
  → Assuming 'unicast_go_signals' is false
   284 |         num_workers += mesh_device_->num_worker_cores(HalProgrammableCoreType::TENSIX, sub_device_id);
   285 |     }
   286 |     if (unicast_go_signals) {
                     ^
   287 |         // Issue #19729: Running MeshWorkloads on Active Eth cores is supported through multiple workarounds
   288 |         // in the dispatch infra. This support should eventually be deprecated.

  Step 6: /work/tt_metal/distributed/fd_mesh_command_queue.cpp:299:47
  → Assuming the condition is false
   297 |     program_dispatch::ProgramDispatchMetadata dispatch_metadata;
   298 |     // Expected number of workers from the previous run
   299 |     uint32_t expected_num_workers_completed = sysmem_manager.get_bypass_mode()
                                                           ^
   300 |                                                   ? trace_ctx_->descriptors[sub_device_id].num_completion_worker_cores
   301 |                                                   : expected_num_workers_completed_[*sub_device_id];

  Step 7: /work/tt_metal/distributed/fd_mesh_command_queue.cpp:306:9
  → Assuming field 'wrapped' is false
   304 |
   305 |     // Need to stall and reset counters if host wraps
   306 |     if (updated_worker_counts.wrapped) [[unlikely]] {
                     ^
   307 |         get_config_buffer_mgr(*sub_device_id).mark_completely_full(0);
   308 |         if (sysmem_manager.get_bypass_mode()) {

  Step 8: /work/tt_metal/distributed/fd_mesh_command_queue.cpp:318:9
  → Assuming the condition is false
   316 |     }
   317 |
   318 |     if (sysmem_manager.get_bypass_mode()) {
                     ^
   319 |         if (mcast_go_signals) {
   320 |             // The workload contains programs that required a go signal mcast. Capture this here

  Step 9: /work/tt_metal/distributed/fd_mesh_command_queue.cpp:349:9
  → Assuming 'use_prefetcher_cache' is false
   347 |     auto max_program_kernels_sizeB = mesh_workload.impl().max_program_kernels_sizeB_;
   348 |     bool use_prefetcher_cache = mesh_workload.impl().use_prefetcher_cache_;
   349 |     if (use_prefetcher_cache) {
                     ^
   350 |         bool is_cached;
   351 |         uint32_t cache_offset;

  Step 10: /work/tt_metal/distributed/fd_mesh_command_queue.cpp:370:5
  → Assuming the condition is true
   368 |     // current device state. Write the finalized program command sequence to each
   369 |     // physical device tied to the program.
   370 |     TracyTTMetalEnqueueMeshWorkloadTrace(mesh_device_, mesh_workload, this->trace_id());
                 ^
   371 |     for (auto& [device_range, program] : mesh_workload.get_programs()) {
   372 |         auto& program_cmd_seq = mesh_workload.impl().get_dispatch_cmds_for_program(program, command_hash);

  Step 11: /work/tt_metal/distributed/fd_mesh_command_queue.cpp:68:1
  → Entered call from 'FDMeshCommandQueue::enqueue_mesh_workload'
    66 | // NOLINTBEGIN(cppcoreguidelines-missing-std-forward)
    67 | template <typename Container, typename Func>
    68 | void for_each_local(MeshDevice* mesh_device, const Container& container, Func&& func) {
             ^
    69 |     std::for_each(std::cbegin(container), std::cend(container), [&](const auto& coord) {
    70 |         if (mesh_device->impl().is_local(coord)) {

  Step 12: /work/tt_metal/distributed/fd_mesh_command_queue.cpp:69:5
  → Calling 'for_each<tt::tt_metal::distributed::MeshCoordinateRange::Iterator, (lambda at /work/tt_metal/distributed/fd_mesh_command_queue.cpp:69:65)>'
    67 | template <typename Container, typename Func>
    68 | void for_each_local(MeshDevice* mesh_device, const Container& container, Func&& func) {
    69 |     std::for_each(std::cbegin(container), std::cend(container), [&](const auto& coord) {
                 ^
    70 |         if (mesh_device->impl().is_local(coord)) {
    71 |             std::invoke(func, coord);

  Step 13: /work/tt_metal/distributed/fd_mesh_command_queue.cpp:70:13
  → Assuming the condition is true
    68 | void for_each_local(MeshDevice* mesh_device, const Container& container, Func&& func) {
    69 |     std::for_each(std::cbegin(container), std::cend(container), [&](const auto& coord) {
    70 |         if (mesh_device->impl().is_local(coord)) {
                         ^
    71 |             std::invoke(func, coord);
    72 |         }

  Step 14: /work/tt_metal/distributed/fd_mesh_command_queue.cpp:71:13
  → Calling 'invoke<(lambda at /work/tt_metal/distributed/fd_mesh_command_queue.cpp:370:5) &, const tt::tt_metal::distributed::MeshCoordinate &>'
    69 |     std::for_each(std::cbegin(container), std::cend(container), [&](const auto& coord) {
    70 |         if (mesh_device->impl().is_local(coord)) {
    71 |             std::invoke(func, coord);
                         ^
    72 |         }
    73 |     });

========================================================================
Finding #69
========================================================================
File:    /work/tt-train/sources/ttml/metal/ops/sdpa_bw/device/sdpa_bw_q_program_factory.hpp
Line:    13 (column 12)
Checker: core.uninitialized.Assign
Severity: HIGH
Report hash: fca7737d2fb082bef7d9149d74425abf

Summary:
  Value assigned to field 'sdpa_bw_q_kernel_group_2' in implicit constructor is garbage or undefined

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt-train/sources/ttml/metal/ops/sdpa_bw/device/sdpa_bw_q_program_factory.cpp:222:9
  → Assuming field 'mask_type' is equal to None
   220 |     // Create mask buffer if using attention mask from DRAM or generating causal mask on-the-fly
   221 |     // Not needed for AttentionMaskType::None
   222 |     if (args.mask_type != AttentionMaskType::None) {
                     ^
   223 |         [[maybe_unused]] auto cb_attn_mask =  // CBIndex::c_5
   224 |             create_circular_buffer(

  Step 2: /work/tt-train/sources/ttml/metal/ops/sdpa_bw/device/sdpa_bw_q_program_factory.cpp:304:30
  → Assuming the condition is false
   302 |     auto* key_buffer = key.buffer();
   303 |     auto* value_buffer = value.buffer();
   304 |     auto* attn_mask_buffer = tensor_args.attn_mask.has_value() ? tensor_args.attn_mask.value().buffer() : nullptr;
                                          ^
   305 |     auto* intermediates_buffer = intermediates.buffer();
   306 |

  Step 3: /work/tt-train/sources/ttml/metal/ops/sdpa_bw/device/sdpa_bw_q_program_factory.cpp:389:9
  → Assuming the condition is false
   387 |
   388 |     // Group 2 compile-time arguments
   389 |     if (!core_group_2.ranges().empty()) {
                     ^
   390 |         std::vector<uint32_t> compute_group_2_args = {
   391 |             num_rows_per_core_group_2,  // 0: per_core_block_cnt

  Step 4: /work/tt-train/sources/ttml/metal/ops/sdpa_bw/device/sdpa_bw_q_program_factory.cpp:436:12
  → Calling constructor for 'CachedProgram<ttml::metal::ops::sdpa_bw::device::SDPABackwardQProgramFactory::shared_variables_t>'
   434 |     // 6) Return the fully configured program & relevant shared variables
   435 |     // -------------------------------------------------------------------------
   436 |     return cached_program_t{
                        ^
   437 |         std::move(program),
   438 |         {.sdpa_bw_q_reader_kernel = kernels.reader,

  Step 5: /work/tt_metal/api/tt-metalium/program_cache.hpp:33:5
  → Entered call from 'SDPABackwardQProgramFactory::create'
    31 |     shared_variables_t& shared_variables;
    32 |
    33 |     CachedProgram(tt::tt_metal::Program&& program, shared_variables_t&& shared_variables) :
                 ^
    34 |         owned_program{std::move(program)},
    35 |         owned_shared_variables{std::move(shared_variables)},

  Step 6: /work/tt_metal/api/tt-metalium/program_cache.hpp:35:9
  → Calling constructor for 'optional<ttml::metal::ops::sdpa_bw::device::SDPABackwardQProgramFactory::shared_variables_t>'
    33 |     CachedProgram(tt::tt_metal::Program&& program, shared_variables_t&& shared_variables) :
    34 |         owned_program{std::move(program)},
    35 |         owned_shared_variables{std::move(shared_variables)},
                     ^
    36 |         program{*owned_program},
    37 |         shared_variables{*owned_shared_variables} {}

  Step 7: /work/tt-train/sources/ttml/metal/ops/sdpa_bw/device/sdpa_bw_q_program_factory.hpp:13:12
  → Value assigned to field 'sdpa_bw_q_kernel_group_2' in implicit constructor is garbage or undefined
    11 |
    12 | struct SDPABackwardQProgramFactory {
    13 |     struct shared_variables_t {
                        ^
    14 |         tt::tt_metal::KernelHandle sdpa_bw_q_reader_kernel{};
    15 |         tt::tt_metal::KernelHandle sdpa_bw_q_writer_kernel{};

========================================================================
Finding #70
========================================================================
File:    /work/tt-train/sources/examples/nano_gpt/3tier/aggregator_worker.cpp
Line:    107 (column 34)
Checker: core.NullDereference
Severity: HIGH
Report hash: dc81db9e400cb9cbf69aebbad2e43861

Summary:
  Dereference of undefined pointer value

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt-train/sources/examples/nano_gpt/3tier/aggregator_worker.cpp:90:9
  → Assuming field 'socket_type' is not equal to FABRIC
    88 |     three_tier_arch::DeviceConfig device_config = three_tier_arch::parse_device_config(yaml_config);
    89 |
    90 |     if (config.socket_type == ttnn::distributed::SocketType::FABRIC) {
                     ^
    91 |         auto num_devices = device_config.mesh_shape[0] * device_config.mesh_shape[1];
    92 |         ttml::ttnn_fixed::distributed::enable_fabric(num_devices);

  Step 2: /work/tt-train/sources/examples/nano_gpt/3tier/aggregator_worker.cpp:102:36
  → Assuming field 'enable_tp' is false
   100 |
   101 |     auto num_devices = static_cast<uint32_t>(device->num_devices());
   102 |     auto should_be_divisible_by = (device_config.enable_tp ? num_devices : 1U) * 32U;
                                                ^
   103 |     vocab_size = three_tier_arch::round_up_to_tile(vocab_size, should_be_divisible_by);
   104 |     std::visit(

  Step 3: /work/tt-train/sources/examples/nano_gpt/3tier/aggregator_worker.cpp:104:5
  → Calling 'visit<(lambda at /work/tt-train/sources/examples/nano_gpt/3tier/aggregator_worker.cpp:105:9), std::variant<ttml::models::gpt2::TransformerConfig, ttml::models::llama::LlamaConfig> &>'
   102 |     auto should_be_divisible_by = (device_config.enable_tp ? num_devices : 1U) * 32U;
   103 |     vocab_size = three_tier_arch::round_up_to_tile(vocab_size, should_be_divisible_by);
   104 |     std::visit(
                 ^
   105 |         [&](auto &&arg) {
   106 |             if constexpr (requires { arg.vocab_size; }) {

  Step 4: /work/tt-train/sources/examples/nano_gpt/3tier/aggregator_worker.cpp:105:23
  → Entered call from '__invoke_impl<void, (lambda at /work/tt-train/sources/examples/nano_gpt/3tier/aggregator_worker.cpp:105:9), ttml::models::llama::LlamaConfig &>'
   103 |     vocab_size = three_tier_arch::round_up_to_tile(vocab_size, should_be_divisible_by);
   104 |     std::visit(
   105 |         [&](auto &&arg) {
                                   ^
   106 |             if constexpr (requires { arg.vocab_size; }) {
   107 |                 arg.vocab_size = vocab_size;

  Step 5: /work/tt-train/sources/examples/nano_gpt/3tier/aggregator_worker.cpp:106:27
  → Assuming the condition is true
   104 |     std::visit(
   105 |         [&](auto &&arg) {
   106 |             if constexpr (requires { arg.vocab_size; }) {
                                       ^
   107 |                 arg.vocab_size = vocab_size;
   108 |             } else {

  Step 6: /work/tt-train/sources/examples/nano_gpt/3tier/aggregator_worker.cpp:107:34
  → Dereference of undefined pointer value
   105 |         [&](auto &&arg) {
   106 |             if constexpr (requires { arg.vocab_size; }) {
   107 |                 arg.vocab_size = vocab_size;
                                              ^
   108 |             } else {
   109 |                 throw std::runtime_error(

========================================================================
Finding #71
========================================================================
File:    /work/tt-train/sources/ttml/metal/ops/layernorm_fw/device/layernorm_fw_program_factory.hpp
Line:    13 (column 12)
Checker: core.uninitialized.Assign
Severity: HIGH
Report hash: baeb5e99fa194a9a79b49337618f1111

Summary:
  Value assigned to field 'layernorm_fw_kernel_group_2_id' in implicit constructor is garbage or undefined

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt-train/sources/ttml/metal/ops/layernorm_fw/device/layernorm_fw_program_factory.cpp:193:5
  → Assuming the condition is true
   191 |     // Check input shape is [B, N, S, C]
   192 |     const auto& input_shape = input.logical_shape();
   193 |     TT_FATAL(input_shape.rank() == 4, "Input tensor must be 4D [B, N, S, C], got shape {}", input_shape);
                 ^
   194 |
   195 |     // Check gamma shape is [1, 1, 1, C]

  Step 2: /work/tt-train/sources/ttml/metal/ops/layernorm_fw/device/layernorm_fw_program_factory.cpp:197:5
  → Assuming the condition is true
   195 |     // Check gamma shape is [1, 1, 1, C]
   196 |     const auto& gamma_shape = gamma.logical_shape();
   197 |     TT_FATAL(gamma_shape.rank() == 4, "Gamma tensor must be 4D [1, 1, 1, C], got shape {}", gamma_shape);
                 ^
   198 |
   199 |     // Check beta shape is [1, 1, 1, C]

  Step 3: /work/tt-train/sources/ttml/metal/ops/layernorm_fw/device/layernorm_fw_program_factory.cpp:201:5
  → Assuming the condition is true
   199 |     // Check beta shape is [1, 1, 1, C]
   200 |     const auto& beta_shape = beta.logical_shape();
   201 |     TT_FATAL(beta_shape.rank() == 4, "Beta tensor must be 4D [1, 1, 1, C], got shape {}", beta_shape);
                 ^
   202 |
   203 |     tt::tt_metal::Program program = tt::tt_metal::CreateProgram();

  Step 4: /work/tt-train/sources/ttml/metal/ops/layernorm_fw/device/layernorm_fw_program_factory.cpp:215:5
  → Assuming the condition is true
   213 |     uint32_t padded_tensor_volume = padded_tensor_shape.volume();
   214 |
   215 |     TT_FATAL(
                 ^
   216 |         padded_tensor_volume % tt::constants::TILE_HW == 0, "Padded input tensor volume must be divisible by TILE_HW");
   217 |     TT_FATAL(padded_tensor_shape.rank() == 4U, "Input tensor must be 4D");

  Step 5: /work/tt-train/sources/ttml/metal/ops/layernorm_fw/device/layernorm_fw_program_factory.cpp:217:5
  → Assuming the condition is true
   215 |     TT_FATAL(
   216 |         padded_tensor_volume % tt::constants::TILE_HW == 0, "Padded input tensor volume must be divisible by TILE_HW");
   217 |     TT_FATAL(padded_tensor_shape.rank() == 4U, "Input tensor must be 4D");
                 ^
   218 |     uint32_t Wt = padded_tensor_shape[-1] / tt::constants::TILE_WIDTH;
   219 |     uint32_t total_rows_to_process =

  Step 6: /work/tt-train/sources/ttml/metal/ops/layernorm_fw/device/layernorm_fw_program_factory.cpp:330:9
  → Assuming 'mask_w' is equal to 0
   328 |
   329 |     std::map<std::string, std::string> defines;
   330 |     if (mask_w != 0) {
                     ^
   331 |         defines[kMaskWDefineKey] = "1";
   332 |     }

  Step 7: /work/tt-train/sources/ttml/metal/ops/layernorm_fw/device/layernorm_fw_program_factory.cpp:377:9
  → Assuming the condition is false
   375 |
   376 |     // Group 2 (if present) compile-time arguments
   377 |     if (!core_group_2.ranges().empty()) {
                     ^
   378 |         std::vector<uint32_t> compute_group_2_args = {
   379 |             num_rows_per_core_group_2,  // per_core_block_cnt

  Step 8: /work/tt-train/sources/ttml/metal/ops/layernorm_fw/device/layernorm_fw_program_factory.cpp:412:12
  → Calling constructor for 'CachedProgram<ttml::metal::ops::layernorm_fw::device::LayerNormForwardProgramFactory::shared_variables_t>'
   410 |     // 6) Return the fully configured program & relevant shared variables
   411 |     // -------------------------------------------------------------------------
   412 |     return cached_program_t{
                        ^
   413 |         std::move(program),
   414 |         {/* layernorm_fw_reader_kernel_id  = */ kernels.reader,

  Step 9: /work/tt_metal/api/tt-metalium/program_cache.hpp:33:5
  → Entered call from 'LayerNormForwardProgramFactory::create'
    31 |     shared_variables_t& shared_variables;
    32 |
    33 |     CachedProgram(tt::tt_metal::Program&& program, shared_variables_t&& shared_variables) :
                 ^
    34 |         owned_program{std::move(program)},
    35 |         owned_shared_variables{std::move(shared_variables)},

  Step 10: /work/tt_metal/api/tt-metalium/program_cache.hpp:35:9
  → Calling constructor for 'optional<ttml::metal::ops::layernorm_fw::device::LayerNormForwardProgramFactory::shared_variables_t>'
    33 |     CachedProgram(tt::tt_metal::Program&& program, shared_variables_t&& shared_variables) :
    34 |         owned_program{std::move(program)},
    35 |         owned_shared_variables{std::move(shared_variables)},
                     ^
    36 |         program{*owned_program},
    37 |         shared_variables{*owned_shared_variables} {}

  Step 11: /work/tt-train/sources/ttml/metal/ops/layernorm_fw/device/layernorm_fw_program_factory.hpp:13:12
  → Value assigned to field 'layernorm_fw_kernel_group_2_id' in implicit constructor is garbage or undefined
    11 |
    12 | struct LayerNormForwardProgramFactory {
    13 |     struct shared_variables_t {
                        ^
    14 |         tt::tt_metal::KernelHandle layernorm_fw_reader_kernel_id;
    15 |         tt::tt_metal::KernelHandle layernorm_fw_writer_kernel_id;

========================================================================
Finding #72
========================================================================
File:    /work/tests/tt_metal/tt_metal/perf_microbenchmark/10_dram_read_remote_cb_sync/test_dram_read_remote_cb.cpp
Line:    185 (column 14)
Checker: deadcode.DeadStores
Severity: LOW
Report hash: 1b8669b6a0656c01ebe28544e31fe63f

Summary:
  Value stored to 'next_layer_single_tile_size' during its initialization is never read

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tests/tt_metal/tt_metal/perf_microbenchmark/10_dram_read_remote_cb_sync/test_dram_read_remote_cb.cpp:185:14
  → Value stored to 'next_layer_single_tile_size' during its initialization is never read
   183 |     uint32_t next_layer_receiver_block_num_tile = next_layer_block_num_tiles / num_receivers;
   184 |
   185 |     uint32_t next_layer_single_tile_size = single_tile_size;
                          ^
   186 |     if (tile_format == tt::DataFormat::Float16_b) {
   187 |         next_layer_single_tile_size = 1088;

========================================================================
Finding #73
========================================================================
File:    /work/tests/tt_metal/multihost/fabric_tests/mesh_socket_test_context.cpp
Line:    27 (column 30)
Checker: cplusplus.StringChecker
Severity: HIGH
Report hash: 06481c7ce52a85748701d579b2d3f118

Summary:
  The parameter must not be null

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tests/tt_metal/multihost/fabric_tests/mesh_socket_test_context.cpp:27:30
  → The parameter must not be null
    25 | void MeshSocketTestContext::initialize() {
    26 |     log_info(tt::LogTest, "Initializing MeshSocketTestContext...");
    27 |     const auto mesh_id_str = std::string(std::getenv("TT_MESH_ID"));
                                          ^
    28 |     local_mesh_id_ = MeshId{std::stoi(mesh_id_str)};
    29 |     if (config_.physical_mesh_config.has_value()) {

========================================================================
Finding #74
========================================================================
File:    /work/tt_metal/impl/allocator/algorithms/free_list_opt.cpp
Line:    68 (column 5)
Checker: optin.cplusplus.VirtualCall
Severity: MEDIUM
Report hash: 84df36333dc61e59799f6eef910b6878

Summary:
  Call to virtual method 'FreeListOpt::init' during construction bypasses virtual dispatch

Why this was reported (bug path with code):
------------------------------------------------------------------------

  Step 1: /work/tt_metal/impl/allocator/algorithms/free_list_opt.cpp:60:28
  → Loop body skipped when range is empty
    58 |     meta_block_is_allocated_.reserve(initial_block_count);
    59 |     free_blocks_segregated_by_size_.resize(size_segregated_count);
    60 |     for (auto& free_blocks : free_blocks_segregated_by_size_) {
                                        ^
    61 |         free_blocks.reserve(initial_block_count);
    62 |     }

  Step 2: /work/tt_metal/impl/allocator/algorithms/free_list_opt.cpp:64:23
  → Loop body skipped when range is empty
    62 |     }
    63 |     allocated_block_table_.resize(n_alloc_table_buckets);
    64 |     for (auto& bucket : allocated_block_table_) {
                                   ^
    65 |         bucket.reserve(n_alloc_table_init_bucket_size);
    66 |     }

  Step 3: /work/tt_metal/impl/allocator/algorithms/free_list_opt.cpp:68:5
  → Call to virtual method 'FreeListOpt::init' during construction bypasses virtual dispatch
    66 |     }
    67 |
    68 |     init();
                 ^
    69 | }
    70 |

========================================================================
Total: 74 finding(s)
========================================================================
