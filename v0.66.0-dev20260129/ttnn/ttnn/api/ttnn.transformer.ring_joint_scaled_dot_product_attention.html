<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ttnn.transformer.ring_joint_scaled_dot_product_attention &mdash; TT-NN  documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/tt_theme.css" type="text/css" />
    <link rel="shortcut icon" href="../../_static/favicon.png"/>
    <link rel="canonical" href="/tt-metal/latest/ttnn/ttnn/api/ttnn.transformer.ring_joint_scaled_dot_product_attention.html" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js?v=b3ba4146"></script>
        <script src="../../_static/doctools.js?v=888ff710"></script>
        <script src="../../_static/sphinx_highlight.js?v=4825356b"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="../../_static/posthog.js?v=aa5946f9"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="ttnn.transformer.scaled_dot_product_attention" href="ttnn.transformer.scaled_dot_product_attention.html" />
    <link rel="prev" title="ttnn.transformer.ring_distributed_scaled_dot_product_attention" href="ttnn.transformer.ring_distributed_scaled_dot_product_attention.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >



<a href="https://docs.tenstorrent.com/">
    <img src="../../_static/tt_logo.svg" class="logo" alt="Logo"/>
</a>

<a href="../../index.html">
    TT-NN
</a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">TTNN</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../about.html">What is TT-NN?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installing.html">Install</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/tutorials/ttnn_intro.html">TT-NN Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensor.html">Tensor</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../api.html">APIs</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../api.html#device">Device</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#memory-config">Memory Config</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../api.html#operations">Operations</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../api.html#core">Core</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#tensor-creation">Tensor Creation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#matrix-multiplication">Matrix Multiplication</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#pointwise-unary">Pointwise Unary</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#pointwise-binary">Pointwise Binary</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#pointwise-ternary">Pointwise Ternary</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#quantization">Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#losses">Losses</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#reduction">Reduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#data-movement">Data Movement</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#normalization">Normalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#normalization-program-configs">Normalization Program Configs</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="../api.html#transformer">Transformer</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="ttnn.transformer.attention_softmax.html">ttnn.transformer.attention_softmax</a></li>
<li class="toctree-l4"><a class="reference internal" href="ttnn.transformer.attention_softmax_.html">ttnn.transformer.attention_softmax_</a></li>
<li class="toctree-l4"><a class="reference internal" href="ttnn.transformer.chunked_flash_mla_prefill.html">ttnn.transformer.chunked_flash_mla_prefill</a></li>
<li class="toctree-l4"><a class="reference internal" href="ttnn.transformer.chunked_scaled_dot_product_attention.html">ttnn.transformer.chunked_scaled_dot_product_attention</a></li>
<li class="toctree-l4"><a class="reference internal" href="ttnn.transformer.concatenate_heads.html">ttnn.transformer.concatenate_heads</a></li>
<li class="toctree-l4"><a class="reference internal" href="ttnn.transformer.flash_mla_prefill.html">ttnn.transformer.flash_mla_prefill</a></li>
<li class="toctree-l4"><a class="reference internal" href="ttnn.transformer.flash_multi_latent_attention_decode.html">ttnn.transformer.flash_multi_latent_attention_decode</a></li>
<li class="toctree-l4"><a class="reference internal" href="ttnn.transformer.joint_scaled_dot_product_attention.html">ttnn.transformer.joint_scaled_dot_product_attention</a></li>
<li class="toctree-l4"><a class="reference internal" href="ttnn.transformer.paged_flash_multi_latent_attention_decode.html">ttnn.transformer.paged_flash_multi_latent_attention_decode</a></li>
<li class="toctree-l4"><a class="reference internal" href="ttnn.transformer.paged_scaled_dot_product_attention_decode.html">ttnn.transformer.paged_scaled_dot_product_attention_decode</a></li>
<li class="toctree-l4"><a class="reference internal" href="ttnn.transformer.ring_distributed_scaled_dot_product_attention.html">ttnn.transformer.ring_distributed_scaled_dot_product_attention</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">ttnn.transformer.ring_joint_scaled_dot_product_attention</a></li>
<li class="toctree-l4"><a class="reference internal" href="ttnn.transformer.scaled_dot_product_attention.html">ttnn.transformer.scaled_dot_product_attention</a></li>
<li class="toctree-l4"><a class="reference internal" href="ttnn.transformer.scaled_dot_product_attention_decode.html">ttnn.transformer.scaled_dot_product_attention_decode</a></li>
<li class="toctree-l4"><a class="reference internal" href="ttnn.transformer.split_query_key_value_and_split_heads.html">ttnn.transformer.split_query_key_value_and_split_heads</a></li>
<li class="toctree-l4"><a class="reference internal" href="ttnn.transformer.windowed_scaled_dot_product_attention.html">ttnn.transformer.windowed_scaled_dot_product_attention</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#ccl">CCL</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#embedding">Embedding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#convolution">Convolution</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#pooling">Pooling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#prefetcher">Prefetcher</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#vision">Vision</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#generic">Generic</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#kv-cache">KV Cache</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api.html#backward-operations">Backward operations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#model-conversion">Model Conversion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#reports">Reports</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#operation-hooks">Operation Hooks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onboarding.html">Onboarding New Functionality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converting_torch_model_to_ttnn.html">Converting PyTorch Model to TT-NN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../adding_new_ttnn_operation.html">Adding New TT-NN Operation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../profiling_ttnn_operations.html">Profiling TT-NN Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demos.html">Building and Uplifting Demos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tools/index.html">Tools</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../resources/support.html">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../resources/contributing.html">Contributing as a developer</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">TT-NN</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../api.html">APIs</a></li>
      <li class="breadcrumb-item active">ttnn.transformer.ring_joint_scaled_dot_product_attention</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/ttnn/api/ttnn.transformer.ring_joint_scaled_dot_product_attention.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="ttnn-transformer-ring-joint-scaled-dot-product-attention">
<h1>ttnn.transformer.ring_joint_scaled_dot_product_attention<a class="headerlink" href="#ttnn-transformer-ring-joint-scaled-dot-product-attention" title="Permalink to this heading"></a>
</h1>
<span class="target" id="id1"></span><dl class="py function">
<dt class="sig sig-object py" id="ttnn.transformer.ring_joint_scaled_dot_product_attention">
<span class="sig-prename descclassname"><span class="pre">ttnn.transformer.</span></span><span class="sig-name descname"><span class="pre">ring_joint_scaled_dot_product_attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_tensor_q</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ttnn.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_tensor_k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ttnn.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_tensor_v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ttnn.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">joint_tensor_q</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ttnn.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">joint_tensor_k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ttnn.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">joint_tensor_v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ttnn.Tensor</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">persistent_output_buffer_k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ttnn.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">persistent_output_buffer_v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ttnn.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">joint_strategy</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logical_n</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">program_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ttnn.SDPAProgramConfig</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compute_kernel_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ttnn.DeviceComputeKernelConfig</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multi_device_global_semaphore</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">ttnn.GlobalSemaphore</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_links</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cluster_axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mesh_device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ttnn.MeshDevice</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">topology</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ttnn.ccl.Topology</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">subdevice_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tt.tt_metal.SubDeviceId</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccl_core_grid_offset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ttnn.CoreCoord</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ttnn.transformer.ring_joint_scaled_dot_product_attention" title="Permalink to this definition"></a>
</dt>
<dd>
<p>RingJointAttention operation that efficiently performs non-causal attention over two
sets of query, key, and value tensors, where the first set is sharded across devices in the sequence dimension.
Internally, these are concatenated in the sequence dimension (joint_strategy = “rear”),
then attention is computed once. The output is split (“sliced”) into two parts: one for the original Q/K/V chunk,
and one for the joint Q/K/V chunk.</p>
<p>This op handles optional padding via an attention mask to omit padded tokens from
both the “original” and “joint” sequences.</p>
<p>Since N must be divisible by the number of devices, the logical N must be passed in.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span>
</dt>
<dd class="field-odd">
<ul class="simple">
<li><p><strong>input_tensor_q</strong> (<em>ttnn.Tensor</em>) – Original queries  [b x nh x N/num_devices x dh].</p></li>
<li><p><strong>input_tensor_k</strong> (<em>ttnn.Tensor</em>) – Original keys     [b x nh x N/num_devices x dh].</p></li>
<li><p><strong>input_tensor_v</strong> (<em>ttnn.Tensor</em>) – Original values   [b x nh x N/num_devices x dh].</p></li>
<li><p><strong>joint_tensor_q</strong> (<em>ttnn.Tensor</em>) – Joint queries     [b x nh x L x dh].</p></li>
<li><p><strong>joint_tensor_k</strong> (<em>ttnn.Tensor</em>) – Joint keys        [b x nh x L x dh].</p></li>
<li><p><strong>joint_tensor_v</strong> (<em>ttnn.Tensor</em>) – Joint values      [b x nh x L x dh].</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span>
</dt>
<dd class="field-even">
<ul class="simple">
<li><p><strong>persistent_output_buffer_k</strong> (<em>ttnn.Tensor</em>) – Persistent buffer for gathered K tensor.</p></li>
<li><p><strong>persistent_output_buffer_v</strong> (<em>ttnn.Tensor</em>) – Persistent buffer for gathered V tensor.</p></li>
<li><p><strong>joint_strategy</strong> (<em>str</em>) – Strategy for joint attention. Must be “rear”.</p></li>
<li><p><strong>logical_n</strong> (<em>int</em>) – The logical sequence length N before sharding across devices.</p></li>
<li><p><strong>program_config</strong> (<em>ttnn.SDPAProgramConfig</em>) – Program configuration for the operation.</p></li>
<li><p><strong>scale</strong> (<em>float</em><em>, </em><em>optional</em>) – Scale factor for QK^T. Defaults to None.</p></li>
<li><p><strong>compute_kernel_config</strong> (<em>ttnn.DeviceComputeKernelConfig</em><em>, </em><em>optional</em>) – Defaults to None.</p></li>
<li><p><strong>dim</strong> (<em>int</em>) – Dimension along which to perform the ring all-gather operation.</p></li>
<li><p><strong>multi_device_global_semaphore</strong> (<em>List</em><em>[</em><em>ttnn.GlobalSemaphore</em><em>]</em>) – Global semaphores for multi-device synchronization.</p></li>
<li><p><strong>num_links</strong> (<em>int</em>) – Number of communication links to use for ring all-gather.</p></li>
<li><p><strong>cluster_axis</strong> (<em>int</em>) – Axis of the mesh device along which to perform the all-gather.</p></li>
<li><p><strong>mesh_device</strong> (<em>ttnn.MeshDevice</em>) – Multi-device mesh for distributed computation.</p></li>
<li><p><strong>topology</strong> (<em>ttnn.ccl.Topology</em>) – Communication topology (Ring or Linear).</p></li>
<li><p><strong>subdevice_id</strong> (<em>Optional</em><em>[</em><em>tt.tt_metal.SubDeviceId</em><em>]</em>) – Sub-device identifier. Defaults to None.</p></li>
<li><p><strong>ccl_core_grid_offset</strong> (<em>ttnn.CoreCoord</em>) – Core grid offset for CCL operations.</p></li>
</ul>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span>
</dt>
<dd class="field-odd">
<p><em>(ttnn.Tensor, ttnn.Tensor, ttnn.Tensor)</em> –   - The attention output for the original Q/K/V shape [b x nh x N/num_devices x dh].
- The attention output for the joint Q/K/V shape    [b x nh x L x dh].
- The final log-sum-exp of the operation.           [b x nh x (N/num_devices + L) x 1]</p>
</dd>
</dl>
</dd>
</dl>

</section>



           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="ttnn.transformer.ring_distributed_scaled_dot_product_attention.html" class="btn btn-neutral float-left" title="ttnn.transformer.ring_distributed_scaled_dot_product_attention" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ttnn.transformer.scaled_dot_product_attention.html" class="btn btn-neutral float-right" title="ttnn.transformer.scaled_dot_product_attention" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Tenstorrent.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
        Version: <span id="current-version">latest</span>
        <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
        <dl id="version-list">
            <dt>Versions</dt>
        </dl>
        <br>
        </dl>
    </div>
</div>

<script>
const VERSIONS_URL = 'https://raw.githubusercontent.com/tenstorrent/tt-metal/refs/heads/main/docs/published_versions.json';

async function loadVersions() {
    try {
        const response = await fetch(VERSIONS_URL);
        const data = await response.json();
        const versionList = document.getElementById('version-list');
        const projectCode = location.pathname.split('/')[3];

        data.versions.forEach(version => {
            const dd = document.createElement('dd');
            const link = document.createElement('a');
            link.href = `https://docs.tenstorrent.com/tt-metal/${version}/${projectCode}/index.html`;
            link.textContent = version;
            dd.appendChild(link);
            versionList.appendChild(dd);
        });
    } catch (error) {
        console.error('Error loading versions:', error);
    }
}

loadVersions();

function getCurrentVersion() {
    return window.location.pathname.split('/')[2];
}
document.getElementById('current-version').textContent = getCurrentVersion();

const versionEl = document.createElement("span");
versionEl.innerText = getCurrentVersion();
versionEl.className = "project-versions";
const wySideSearchEl = document.getElementsByClassName("wy-side-nav-search").item(0);
if (wySideSearchEl) {
    const projectNameEl = wySideSearchEl.children.item(1);
    if (projectNameEl) projectNameEl.appendChild(versionEl);
}

</script><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>