{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 18:13:41.903 | DEBUG    | ttnn:<module>:136 - Initial ttnn.CONFIG:\n",
      "{'cache_path': PosixPath('/home/ubuntu/.cache/ttnn'),\n",
      " 'comparison_mode_pcc': 0.9999,\n",
      " 'enable_comparison_mode': False,\n",
      " 'enable_detailed_buffer_report': False,\n",
      " 'enable_detailed_tensor_report': False,\n",
      " 'enable_fast_runtime_mode': True,\n",
      " 'enable_graph_report': False,\n",
      " 'enable_logging': False,\n",
      " 'enable_model_cache': False,\n",
      " 'model_cache_path': PosixPath('/home/ubuntu/.cache/ttnn/models'),\n",
      " 'report_name': None,\n",
      " 'root_report_path': PosixPath('generated/ttnn/reports'),\n",
      " 'throw_exception_on_fallback': False,\n",
      " 'tmp_dir': PosixPath('/tmp/ttnn')}\n",
      "2024-07-11 18:13:41.989 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.logical_xor be migrated to C++?\n",
      "2024-07-11 18:13:41.990 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.xlogy be migrated to C++?\n",
      "2024-07-11 18:13:41.990 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.maximum be migrated to C++?\n",
      "2024-07-11 18:13:41.991 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.minimum be migrated to C++?\n",
      "2024-07-11 18:13:41.992 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.atan2 be migrated to C++?\n",
      "2024-07-11 18:13:41.992 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.hypot be migrated to C++?\n",
      "2024-07-11 18:13:41.993 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.nextafter be migrated to C++?\n",
      "2024-07-11 18:13:41.993 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.polyval be migrated to C++?\n",
      "2024-07-11 18:13:41.994 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.isclose be migrated to C++?\n",
      "2024-07-11 18:13:41.995 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.all_gather be migrated to C++?\n",
      "2024-07-11 18:13:41.996 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.pearson_correlation_coefficient be migrated to C++?\n",
      "2024-07-11 18:13:42.001 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.conv2d be migrated to C++?\n",
      "2024-07-11 18:13:42.002 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.reshape be migrated to C++?\n",
      "2024-07-11 18:13:42.003 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.unsqueeze_to_4D be migrated to C++?\n",
      "2024-07-11 18:13:42.004 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.squeeze be migrated to C++?\n",
      "2024-07-11 18:13:42.004 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.from_torch be migrated to C++?\n",
      "2024-07-11 18:13:42.005 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.to_torch be migrated to C++?\n",
      "2024-07-11 18:13:42.006 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.to_device be migrated to C++?\n",
      "2024-07-11 18:13:42.006 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.from_device be migrated to C++?\n",
      "2024-07-11 18:13:42.007 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.allocate_tensor_on_device be migrated to C++?\n",
      "2024-07-11 18:13:42.008 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.copy_host_to_device_tensor be migrated to C++?\n",
      "2024-07-11 18:13:42.008 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.deallocate be migrated to C++?\n",
      "2024-07-11 18:13:42.009 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.clone be migrated to C++?\n",
      "2024-07-11 18:13:42.010 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.reallocate be migrated to C++?\n",
      "2024-07-11 18:13:42.010 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.load_tensor be migrated to C++?\n",
      "2024-07-11 18:13:42.011 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.dump_tensor be migrated to C++?\n",
      "2024-07-11 18:13:42.012 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.as_tensor be migrated to C++?\n",
      "2024-07-11 18:13:42.013 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.arange be migrated to C++?\n",
      "2024-07-11 18:13:42.015 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.mse_loss be migrated to C++?\n",
      "2024-07-11 18:13:42.016 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.l1_loss be migrated to C++?\n",
      "2024-07-11 18:13:42.017 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.matmul be migrated to C++?\n",
      "2024-07-11 18:13:42.018 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.linear be migrated to C++?\n",
      "2024-07-11 18:13:42.020 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.mac be migrated to C++?\n",
      "2024-07-11 18:13:42.021 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.addcmul be migrated to C++?\n",
      "2024-07-11 18:13:42.022 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.addcdiv be migrated to C++?\n",
      "2024-07-11 18:13:42.022 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.lerp be migrated to C++?\n",
      "2024-07-11 18:13:42.027 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.logit be migrated to C++?\n",
      "2024-07-11 18:13:42.027 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.polygamma be migrated to C++?\n",
      "2024-07-11 18:13:42.028 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.hardshrink be migrated to C++?\n",
      "2024-07-11 18:13:42.029 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.celu be migrated to C++?\n",
      "2024-07-11 18:13:42.029 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.softshrink be migrated to C++?\n",
      "2024-07-11 18:13:42.030 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.clip be migrated to C++?\n",
      "2024-07-11 18:13:42.030 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.threshold be migrated to C++?\n",
      "2024-07-11 18:13:42.031 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.glu be migrated to C++?\n",
      "2024-07-11 18:13:42.032 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.reglu be migrated to C++?\n",
      "2024-07-11 18:13:42.032 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.swiglu be migrated to C++?\n",
      "2024-07-11 18:13:42.033 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.geglu be migrated to C++?\n",
      "2024-07-11 18:13:42.035 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.matmul be migrated to C++?\n",
      "2024-07-11 18:13:42.036 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.linear be migrated to C++?\n",
      "2024-07-11 18:13:42.037 | WARNING  | ttnn.decorators:operation_decorator:758 - Should ttnn.conv2d be migrated to C++?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                 Device\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Opening user mode device driver\n",
      "\n",
      "\u001b[32m2024-07-11 18:13:42.053\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected 1 PCI device : {0}\n",
      "\u001b[32m2024-07-11 18:13:42.066\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - init_detect_tt_device_numanodes(): Could not determine NumaNodeSet for TT device (physical_device_id: 0 pci_bus_id: 0000:07:00.0)\n",
      "\u001b[32m2024-07-11 18:13:42.066\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Could not find NumaNodeSet for TT Device (physical_device_id: 0 pci_bus_id: 0000:07:00.0)\n",
      "\u001b[32m2024-07-11 18:13:42.067\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - bind_area_memory_nodeset(): Unable to determine TT Device to NumaNode mapping for physical_device_id: 0. Skipping membind.\n",
      "\u001b[0;33m---- ttSiliconDevice::init_hugepage: bind_area_to_memory_nodeset() failed (physical_device_id: 0 ch: 0). Hugepage allocation is not on NumaNode matching TT Device. Side-Effect is decreased Device->Host perf (Issue #893).\n",
      "\u001b[0m\u001b[32m2024-07-11 18:13:42.094\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Software version 6.0.0, Ethernet FW version 6.9.0 (Device 0)\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Initializing device 0. Program cache is NOT enabled\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | AI CLK for device 0 is:   800 MHz\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ttnn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable program cache\n",
    "\n",
    "Enabling the program cache will speed up the execution of operations that run repeatedly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Enabling program cache on device 0\n"
     ]
    }
   ],
   "source": [
    "device.enable_program_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1024\n",
    "k = 1024\n",
    "n = 1024"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize tensors a and b with random values using torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_a = torch.randn((m, k), dtype=torch.bfloat16)\n",
    "torch_b = torch.randn((k, n), dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ttnn.from_torch(torch_a, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "b = ttnn.from_torch(torch_b, layout=ttnn.TILE_LAYOUT, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiply tensor a and b\n",
    "The operation will run longer the first time because the kernels need to get compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = a @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-running the operation shows significant speed up by utilizing program caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = a @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the layout of matrix multiplication output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layout.TILE\n"
     ]
    }
   ],
   "source": [
    "print(output.layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, matrix multiplication produces outputs in a tile layout. That is because it's much more efficient to use this layout for computing matrix multiplications on Tenstorrent accelerators compared to a row-major layout.\n",
    "\n",
    "And this is aslo why the logs show 2 tilize operations, as the inputs get automatically convered to the tile layout if they are in a row-major layout.\n",
    "\n",
    "Learn more about tile layout here: TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the result of the matrix multiplication\n",
    "\n",
    "To inspect the results we will first convert to row-major layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing ttnn tensor\n",
      "shape: ttnn.Shape([1024, 1024])\n",
      "chunk of a tensor:\n",
      "ttnn.Tensor([[33.75000,  9.25000,  ..., -39.50000, -6.62500]], shape=Shape([1, 32]), dtype=DataType::BFLOAT16, layout=Layout::ROW_MAJOR)\n"
     ]
    }
   ],
   "source": [
    "output = ttnn.to_layout(output, ttnn.ROW_MAJOR_LAYOUT)\n",
    "\n",
    "print(\"Printing ttnn tensor\")\n",
    "print(f\"shape: {output.shape}\")\n",
    "print(f\"chunk of a tensor:\\n{output[:1, :32]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiply tensor a and b by using more performant config\n",
    "By default, matrix multiplication might not be as effecient as it could be. To speed it up further, the user can specify how many cores they want matrix multiplication to use. This can speed up the operation significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ttnn.from_torch(torch_a)\n",
    "b = ttnn.from_torch(torch_b)\n",
    "\n",
    "a = ttnn.to_device(a, device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "b = ttnn.to_device(b, device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "\n",
    "a = ttnn.to_layout(a, ttnn.TILE_LAYOUT)\n",
    "b = ttnn.to_layout(b, ttnn.TILE_LAYOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run once to compile the kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ttnn.matmul(a, b, memory_config=ttnn.L1_MEMORY_CONFIG, core_grid=ttnn.CoreGrid(y=8, x=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enjoy a massive speed up on the subsequent runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ttnn.matmul(a, b, memory_config=ttnn.L1_MEMORY_CONFIG, core_grid=ttnn.CoreGrid(y=8, x=8))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Closing device 0\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Disabling and clearing program cache on device 0\n"
     ]
    }
   ],
   "source": [
    "ttnn.close_device(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
