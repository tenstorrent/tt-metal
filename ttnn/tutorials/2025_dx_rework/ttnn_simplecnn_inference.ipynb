{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0da10bde",
   "metadata": {},
   "source": [
    "# Running a Simple CNN Inference on CIFAR-10 using TT-NN\n",
    "\n",
    "This tutorial demonstrates how to use **TTNN** to perform inference with a simple Convolutional Neural Network (CNN) on the CIFAR-10 dataset.\n",
    "\n",
    "We will:\n",
    "- Load the CIFAR-10 dataset\n",
    "- Define a simple CNN using TT-NN operations\n",
    "- Run inference on sample images\n",
    "- Observe the outputs and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233d6aa4",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519491bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import ttnn\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cab60f",
   "metadata": {},
   "source": [
    "## Open Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb980bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ttnn.open_device(device_id=0, l1_small_size=8192)\n",
    "logger.info(\"\\n--- Simple CNN Inference Using TT-NN on CIFAR-10 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e6b24",
   "metadata": {},
   "source": [
    "## Load CIFAR-10 Dataset\n",
    "\n",
    "We will normalize the images and load the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8d1d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input transforms: Convert to tensor and normalize\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Load CIFAR-10 test data\n",
    "testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ad440d",
   "metadata": {},
   "source": [
    "## Load or Initialize Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6634a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"simple_cnn_cifar10_weights.pt\"):\n",
    "    weights = torch.load(\"simple_cnn_cifar10_weights.pt\")\n",
    "    logger.info(\"Loaded pretrained weights\")\n",
    "else:\n",
    "    logger.warning(\"Weights not found, using random weights\")\n",
    "    torch.manual_seed(0)\n",
    "    weights = {\n",
    "        \"conv1.weight\": torch.randn((16, 3, 3, 3)),\n",
    "        \"conv1.bias\": torch.randn((16,)),\n",
    "        \"conv2.weight\": torch.randn((32, 16, 3, 3)),\n",
    "        \"conv2.bias\": torch.randn((32,)),\n",
    "        \"fc1.weight\": torch.randn((128, 2048)),\n",
    "        \"fc1.bias\": torch.randn((128,)),\n",
    "        \"fc2.weight\": torch.randn((10, 128)),\n",
    "        \"fc2.bias\": torch.randn((10,)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c50dbd",
   "metadata": {},
   "source": [
    "## Define Convolution + Pooling Stage\n",
    "\n",
    "This function applies convolution, activation, and max pooling using TT-NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd4403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_pool_stage(\n",
    "    input_tensor: ttnn.Tensor,\n",
    "    input_BHWC: ttnn.Shape,\n",
    "    conv_outchannels: int,\n",
    "    weights: dict,\n",
    "    weight_str: str,\n",
    "    bias_str: str,\n",
    "    activation_str: str,\n",
    "    device: ttnn.Device,\n",
    "    log_first_sample: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform convolution + activation + max pooling using TT-NN.\n",
    "    Args:\n",
    "        input_tensor: Input TT tensor in BHWC format.\n",
    "        input_BHWC: Tuple representing (Batch, Height, Width, Channels) of the input tensor.\n",
    "        conv_outchannels: Number of output channels for the convolution layer.\n",
    "        weights: Dictionary containing model weights and biases.\n",
    "        weight_str: Key name for convolution weights in the weights dict.\n",
    "        bias_str: Key name for convolution biases in the weights dict.\n",
    "        activation_str: Activation function name (e.g., 'relu') to apply after conv.\n",
    "        device: Target TT device to execute the operations on.\n",
    "        log_first_sample: Whether to log detailed info (used for debugging first sample).\n",
    "    Returns:\n",
    "        Output tensor after conv + max pooling (TT format).\n",
    "    \"\"\"\n",
    "    # Extract weight and bias tensors from weights dictionary\n",
    "    W = weights[weight_str]\n",
    "    B = weights[bias_str]\n",
    "    B = B.view(1, 1, 1, -1)  # Reshape bias for broadcast compatibility\n",
    "\n",
    "    # Convert weights and bias to TT tensor format (bfloat16, row-major)\n",
    "    W_ttnn = ttnn.from_torch(W, layout=ttnn.ROW_MAJOR_LAYOUT, dtype=ttnn.bfloat16)\n",
    "    B_ttnn = ttnn.from_torch(B, layout=ttnn.ROW_MAJOR_LAYOUT, dtype=ttnn.bfloat16)\n",
    "\n",
    "    # Define convolution parameters\n",
    "    conv_kernel_size = (3, 3)\n",
    "    conv_stride = (1, 1)\n",
    "    conv_padding = (1, 1)\n",
    "\n",
    "    # Set up TT-NN convolution configuration including activation function\n",
    "    conv_config = ttnn.Conv2dConfig(weights_dtype=ttnn.bfloat16, activation=activation_str)\n",
    "\n",
    "    # Optional detailed logging for the first sample (shape, config, etc.)\n",
    "    if log_first_sample:\n",
    "        logger.info(\"=====================================================================\")\n",
    "        logger.info(\"Input parameters to conv2d:\")\n",
    "        logger.info(f\"  input_tensor shape: {input_tensor.shape}\")\n",
    "        logger.info(f\"  weight_tensor shape: {W_ttnn.shape}\")\n",
    "        logger.info(f\"  bias_tensor shape: {B_ttnn.shape}\")\n",
    "        logger.info(f\"  in_channels: {input_BHWC[3]}\")\n",
    "        logger.info(f\"  out_channels: {conv_outchannels}\")\n",
    "        logger.info(f\"  device: {device}\")\n",
    "        logger.info(f\"  kernel_size: {conv_kernel_size}\")\n",
    "        logger.info(f\"  stride: {conv_stride}\")\n",
    "        logger.info(f\"  padding: {conv_padding}\")\n",
    "        logger.info(f\"  batch_size: {input_BHWC[0]}\")\n",
    "        logger.info(f\"  input_height: {input_BHWC[1]}\")\n",
    "        logger.info(f\"  input_width: {input_BHWC[2]}\")\n",
    "        logger.info(f\"  conv_config: {conv_config}\")\n",
    "        logger.info(f\"  groups: {0}\")\n",
    "\n",
    "    # Perform convolution\n",
    "    conv1_out = ttnn.conv2d(\n",
    "        input_tensor=input_tensor,\n",
    "        weight_tensor=W_ttnn,\n",
    "        bias_tensor=B_ttnn,\n",
    "        in_channels=input_BHWC[3],\n",
    "        out_channels=conv_outchannels,\n",
    "        device=device,\n",
    "        kernel_size=conv_kernel_size,\n",
    "        stride=conv_stride,\n",
    "        padding=conv_padding,\n",
    "        batch_size=input_BHWC[0],\n",
    "        input_height=input_BHWC[1],\n",
    "        input_width=input_BHWC[2],\n",
    "        conv_config=conv_config,\n",
    "        groups=0,\n",
    "    )\n",
    "\n",
    "    # Define max pooling parameters\n",
    "    max_pool2d_kernel_size = [2, 2]\n",
    "    max_pool2d_stride = [2, 2]\n",
    "    max_pool2d_padding = [0, 0]\n",
    "    max_pool2d_dilation = [1, 1]\n",
    "\n",
    "    # Optional logging for max pooling input and parameters\n",
    "    if log_first_sample:\n",
    "        logger.info(\"Input parameters to max_pool2d:\")\n",
    "        logger.info(f\"  input shape: {conv1_out.shape}\")\n",
    "        logger.info(f\"  batch_size: {input_BHWC[0]}\")\n",
    "        logger.info(f\"  input_h: {input_BHWC[1]}\")\n",
    "        logger.info(f\"  input_w: {input_BHWC[2]}\")\n",
    "        logger.info(f\"  channels: {conv_outchannels}\")\n",
    "        logger.info(f\"  kernel_size: {max_pool2d_kernel_size}\")\n",
    "        logger.info(f\"  stride: {max_pool2d_stride}\")\n",
    "        logger.info(f\"  padding: {max_pool2d_padding}\")\n",
    "        logger.info(f\"  dilation: {max_pool2d_dilation}\")\n",
    "        logger.info(f\"  ceil_mode: {False}\")\n",
    "\n",
    "    # Perform max pooling\n",
    "    max_pool2d_out = ttnn.max_pool2d(\n",
    "        conv1_out,\n",
    "        batch_size=input_BHWC[0],\n",
    "        input_h=input_BHWC[1],\n",
    "        input_w=input_BHWC[2],\n",
    "        channels=conv_outchannels,\n",
    "        kernel_size=max_pool2d_kernel_size,\n",
    "        stride=max_pool2d_stride,\n",
    "        padding=max_pool2d_padding,\n",
    "        dilation=max_pool2d_dilation,\n",
    "        ceil_mode=False,\n",
    "    )\n",
    "\n",
    "    # Log output shape after pooling\n",
    "    if log_first_sample:\n",
    "        logger.info(f\"max_pool2d output shape: {max_pool2d_out.shape}\")\n",
    "        logger.info(\"=====================================================================\")\n",
    "\n",
    "    return max_pool2d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9604d5d",
   "metadata": {},
   "source": [
    "## Run Inference on Test Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef5ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Run inference on a few test samples\n",
    "for i, (image, label) in enumerate(testloader):\n",
    "    if i >= 5:\n",
    "        break\n",
    "\n",
    "    # Convert image to TT tensor\n",
    "    ttnn_image = ttnn.from_torch(image, layout=ttnn.ROW_MAJOR_LAYOUT, dtype=ttnn.bfloat16, device=device)\n",
    "    ttnn_image_permuated = ttnn.permute(ttnn_image, (0, 2, 3, 1))  # BCHW -> BHWC\n",
    "\n",
    "    # Only log details for first sample\n",
    "    log_this = i == 0\n",
    "\n",
    "    # Apply first conv + pool stage\n",
    "    conv1_pool = conv_pool_stage(\n",
    "        ttnn_image_permuated,\n",
    "        ttnn_image_permuated.shape,\n",
    "        16,\n",
    "        weights,\n",
    "        \"conv1.weight\",\n",
    "        \"conv1.bias\",\n",
    "        \"relu\",\n",
    "        device,\n",
    "        log_first_sample=log_this,\n",
    "    )\n",
    "\n",
    "    # Apply second conv + pool stage\n",
    "    conv2_pool = conv_pool_stage(\n",
    "        conv1_pool,\n",
    "        (1, 16, 16, 16),\n",
    "        32,\n",
    "        weights,\n",
    "        \"conv2.weight\",\n",
    "        \"conv2.bias\",\n",
    "        \"relu\",\n",
    "        device,\n",
    "        log_first_sample=log_this,\n",
    "    )\n",
    "\n",
    "    # Flatten for FC layers\n",
    "    B, H, W, C = conv2_pool.shape\n",
    "    out_flat = ttnn.to_torch(conv2_pool)  # Convert back to torch\n",
    "    out_flat = out_flat.permute(0, 3, 1, 2).contiguous().view(B, -1)  # BHWC -> BCHW -> Flatten\n",
    "\n",
    "    # Prepare fully connected layers\n",
    "    W3 = weights[\"fc1.weight\"]\n",
    "    B3 = weights[\"fc1.bias\"]\n",
    "    W4 = weights[\"fc2.weight\"]\n",
    "    B4 = weights[\"fc2.bias\"]\n",
    "\n",
    "    # Convert to TT format for FC1\n",
    "    W3_tt = ttnn.from_torch(W3.T, dtype=ttnn.bfloat16, layout=ttnn.ROW_MAJOR_LAYOUT, device=device)\n",
    "    W3_tt = ttnn.to_layout(W3_tt, ttnn.TILE_LAYOUT)\n",
    "    B3_tt = ttnn.from_torch(B3.view(1, -1), dtype=ttnn.bfloat16, layout=ttnn.ROW_MAJOR_LAYOUT, device=device)\n",
    "    B3_tt = ttnn.to_layout(B3_tt, ttnn.TILE_LAYOUT)\n",
    "\n",
    "    # Convert input to TT format\n",
    "    x_tt = ttnn.from_torch(out_flat, dtype=ttnn.bfloat16, layout=ttnn.ROW_MAJOR_LAYOUT, device=device)\n",
    "    x_tt = ttnn.to_layout(x_tt, ttnn.TILE_LAYOUT)\n",
    "\n",
    "    # Apply FC1 + ReLU\n",
    "    out = ttnn.linear(x_tt, W3_tt, bias=B3_tt)\n",
    "    out = ttnn.relu(out)\n",
    "\n",
    "    # Convert to TT format for FC2\n",
    "    W4_tt = ttnn.from_torch(W4.T, dtype=ttnn.bfloat16, layout=ttnn.ROW_MAJOR_LAYOUT, device=device)\n",
    "    W4_tt = ttnn.to_layout(W4_tt, ttnn.TILE_LAYOUT)\n",
    "    B4_tt = ttnn.from_torch(B4.view(1, -1), dtype=ttnn.bfloat16, layout=ttnn.ROW_MAJOR_LAYOUT, device=device)\n",
    "    B4_tt = ttnn.to_layout(B4_tt, ttnn.TILE_LAYOUT)\n",
    "\n",
    "    # Apply FC2 (output logits)\n",
    "    out = ttnn.linear(out, W4_tt, bias=B4_tt)\n",
    "\n",
    "    # Convert prediction back to torch\n",
    "    prediction = ttnn.to_torch(out)\n",
    "    predicted_label = torch.argmax(prediction, dim=1).item()\n",
    "    correct += predicted_label == label.item()\n",
    "    total += 1\n",
    "\n",
    "    logger.info(f\"Sample {i+1}: Predicted={predicted_label}, Actual={label.item()}\")\n",
    "\n",
    "logger.info(f\"\\nTT-NN SimpleCNN Inference Accuracy: {correct}/{total} = {100.0 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728d82f7",
   "metadata": {},
   "source": [
    "## Close Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8d4e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttnn.close_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f125bcbf",
   "metadata": {},
   "source": [
    "We have built and run a simple CNN using Tenstorrent's TT-NN library on the CIFAR-10 dataset, observed predictions, and computed accuracy on a few samples.\n",
    "\n",
    "For full-scale inference or training, pre-trained weights should be used, and additional optimization strategies may be applied."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
