{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c0da19-4ccb-41cc-8e57-f8e5426d87e3",
   "metadata": {},
   "source": [
    "# Building CLIP model using TT-NN\n",
    "\n",
    "CLIP <GIVE REFERENCE>, is a foundational model of modern AI. \n",
    "\n",
    "<PUT WHAT DOES CLIP DO HERE> \n",
    "\n",
    "![CLIP Diagram](https://media.githubusercontent.com/media/tenstorrent/tutorial-assets/nmaurice/clip-tutorial/media/clip_tutorial/CLIP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc6c99f-80a5-4002-a3e5-6b4029f21705",
   "metadata": {},
   "source": [
    "We start by importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d82f03-2f2d-4460-92a0-c6021ff4244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttnn\n",
    "import torch\n",
    "from loguru import logger\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import CLIPTokenizer, CLIPModel\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, InterpolationMode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbf6215-2106-433a-a5cc-ca653e171a8b",
   "metadata": {},
   "source": [
    "We define the following helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0e772f-4de5-4612-b615-0bf1a8d295b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def open_ttnn():\n",
    "    global device\n",
    "    device = ttnn.open_device(device_id=0, l1_small_size=8192)\n",
    "\n",
    "def close_ttnn():\n",
    "    global device\n",
    "    if device is not None:\n",
    "        ttnn.close_device(device)\n",
    "\n",
    "def get_device():\n",
    "    global device\n",
    "    return device\n",
    "\n",
    "def convert_from_ttnn(x):\n",
    "    global device\n",
    "    if isinstance(x, ttnn._ttnn.tensor.Tensor):\n",
    "        return ttnn.to_torch(x)\n",
    "    return x\n",
    "\n",
    "def to_ttnn(torch_tensor, dtype=None, layout=ttnn.TILE_LAYOUT):\n",
    "    global device\n",
    "    ttnn_tensor = ttnn.from_torch(torch_tensor, device=device, layout=layout, dtype=dtype)\n",
    "    return ttnn_tensor\n",
    "\n",
    "def to_torch_shape(ttnn_shape):\n",
    "    return tuple(ttnn_shape)\n",
    "\n",
    "# Change dtype of ttnn tensor and optionally reshape\n",
    "def convert_ttnn_dtype(ttnn_tensor, dtype, new_shape=None):\n",
    "    # HACK: Can't convert dtype on device\n",
    "    device = get_device()\n",
    "    host_tensor = ttnn.from_device(ttnn_tensor)\n",
    "    host_tensor = ttnn.to_dtype(host_tensor, dtype=dtype)\n",
    "    if new_shape is not None:\n",
    "        host_tensor = ttnn.reshape(host_tensor, new_shape)\n",
    "\n",
    "    return ttnn.to_device(host_tensor, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c632f6ee-1653-4d83-943b-9efbfbfdb444",
   "metadata": {},
   "source": [
    "As TT-NN does not support weight loading by itself, we rely on torch.load().\n",
    "We introduce the following helper function to convert weights from torch tensors to TT-NN tensors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bb9e44-9d89-4606-abe6-76d7e37c597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_model_to_ttnn(state_dict):\n",
    "    ttnn_state_dict = {}\n",
    "    logger.info(f\"Converting model to ttnn\")\n",
    "\n",
    "    # Convert state dict to ttnn tensor\n",
    "    for key, value in state_dict.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            state_dict[key] = to_ttnn(value)\n",
    "        elif isinstance(value, torch.Size):\n",
    "            state_dict[key] = ttnn.Size(value)\n",
    "\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d1d9a7-9f4f-446f-9b3a-d5f002b2abdb",
   "metadata": {},
   "source": [
    "CLIP uses 2 types of transformers: a text transformer, and vision transformer.\n",
    "To re-use code, we define a generic transformer class that will be used for both images and text. \n",
    "\n",
    "The transformer models used by CLIP have several layers, or resblocks, which contain the following sub-operations. \n",
    "- LayerNorm\n",
    "- MultiHeadAttention (masked for text inference, unmasked for image inference)\n",
    "- LayerNorm\n",
    "- MLP (Linear + GELU + Linear)\n",
    "\n",
    "The output of each block is re-used as the input of the next one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7f12bd-eeea-4e39-a43b-7498ad7a9fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transformer:\n",
    "    def __init__(self, state_dict, heads, attention_mask=None, prefix=\"\"):\n",
    "        self.layers = []\n",
    "        self.heads = heads\n",
    "        self.attention_mask = attention_mask\n",
    "        self.prefix = prefix\n",
    "\n",
    "        layer_pattern = re.compile(f\"{prefix}\\.layers\\.(\\d+)\\.\")\n",
    "\n",
    "        # Count number of layers\n",
    "        layers_ids = set()\n",
    "        for k in state_dict.keys():\n",
    "            re_match = re.search(layer_pattern, k)\n",
    "            if re_match:\n",
    "                layers_ids.add(re_match.group(1))\n",
    "\n",
    "        num_layers = len(layers_ids)\n",
    "\n",
    "        for i in range(0, num_layers):\n",
    "            resblock_prefix = f\"{prefix}.layers.{i}\"\n",
    "\n",
    "            self.layers.append(\n",
    "                {\n",
    "                    \"ln_1_weight\": convert_ttnn_dtype(\n",
    "                        state_dict[f\"{resblock_prefix}.layer_norm1.weight\"], ttnn.bfloat16\n",
    "                    ),\n",
    "                    \"ln_1_bias\": convert_ttnn_dtype(state_dict[f\"{resblock_prefix}.layer_norm1.bias\"], ttnn.bfloat16),\n",
    "                    \"q_proj_weight\": convert_ttnn_dtype(\n",
    "                        state_dict[f\"{resblock_prefix}.self_attn.q_proj.weight\"], ttnn.bfloat16\n",
    "                    ),\n",
    "                    \"q_proj_bias\": convert_ttnn_dtype(\n",
    "                        state_dict[f\"{resblock_prefix}.self_attn.q_proj.bias\"], ttnn.bfloat16\n",
    "                    ),\n",
    "                    \"k_proj_weight\": convert_ttnn_dtype(\n",
    "                        state_dict[f\"{resblock_prefix}.self_attn.k_proj.weight\"], ttnn.bfloat16\n",
    "                    ),\n",
    "                    \"k_proj_bias\": convert_ttnn_dtype(\n",
    "                        state_dict[f\"{resblock_prefix}.self_attn.k_proj.bias\"], ttnn.bfloat16\n",
    "                    ),\n",
    "                    \"v_proj_weight\": convert_ttnn_dtype(\n",
    "                        state_dict[f\"{resblock_prefix}.self_attn.v_proj.weight\"], ttnn.bfloat16\n",
    "                    ),\n",
    "                    \"v_proj_bias\": convert_ttnn_dtype(\n",
    "                        state_dict[f\"{resblock_prefix}.self_attn.v_proj.bias\"], ttnn.bfloat16\n",
    "                    ),\n",
    "                    \"out_proj_weight\": convert_ttnn_dtype(\n",
    "                        state_dict[f\"{resblock_prefix}.self_attn.out_proj.weight\"], ttnn.bfloat16\n",
    "                    ),\n",
    "                    \"out_proj_bias\": convert_ttnn_dtype(\n",
    "                        state_dict[f\"{resblock_prefix}.self_attn.out_proj.bias\"], ttnn.bfloat16\n",
    "                    ),\n",
    "                    \"ln_2_weight\": convert_ttnn_dtype(\n",
    "                        state_dict[f\"{resblock_prefix}.layer_norm2.weight\"], ttnn.bfloat16\n",
    "                    ),\n",
    "                    \"ln_2_bias\": convert_ttnn_dtype(state_dict[f\"{resblock_prefix}.layer_norm2.bias\"], ttnn.bfloat16),\n",
    "                    \"mlp_c_fc_weight\": convert_ttnn_dtype(\n",
    "                        state_dict[f\"{resblock_prefix}.mlp.fc1.weight\"], ttnn.bfloat16\n",
    "                    ),\n",
    "                    \"mlp_c_fc_bias\": convert_ttnn_dtype(state_dict[f\"{resblock_prefix}.mlp.fc1.bias\"], ttnn.bfloat16),\n",
    "                    \"mlp_c_proj_weight\": convert_ttnn_dtype(\n",
    "                        state_dict[f\"{resblock_prefix}.mlp.fc2.weight\"], ttnn.bfloat16\n",
    "                    ),\n",
    "                    \"mlp_c_proj_bias\": convert_ttnn_dtype(state_dict[f\"{resblock_prefix}.mlp.fc2.bias\"], ttnn.bfloat16),\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        def mlp(x, layer):\n",
    "            x = ttnn.linear(x, layer[\"mlp_c_fc_weight\"], bias=layer[\"mlp_c_fc_bias\"], transpose_b=True)\n",
    "            x = ttnn.gelu(x)\n",
    "            x = ttnn.linear(x, layer[\"mlp_c_proj_weight\"], bias=layer[\"mlp_c_proj_bias\"], transpose_b=True)\n",
    "            return x\n",
    "\n",
    "        def multi_head_attention(\n",
    "            hidden_states,\n",
    "            fused_qkv_weight,\n",
    "            fused_qkv_bias,\n",
    "            self_output_weight,\n",
    "            self_output_bias,\n",
    "            attention_mask=None,\n",
    "            prefix=\"\",\n",
    "        ):\n",
    "            seq_length, batch_size, hidden_size = hidden_states.shape\n",
    "\n",
    "            self._embed_dim = hidden_size\n",
    "            self._head_dim = hidden_size // self.heads\n",
    "            self._scale = self._head_dim**-0.5\n",
    "            self._attention_dropout = 0.0  # Unused\n",
    "\n",
    "            compute_kernel_config = ttnn.WormholeComputeKernelConfig(\n",
    "                math_fidelity=ttnn.MathFidelity.HiFi4,\n",
    "                math_approx_mode=False,\n",
    "                fp32_dest_acc_en=True,\n",
    "                packer_l1_acc=True,\n",
    "            )\n",
    "\n",
    "            # TODO: No KV-caching for now\n",
    "            (q_weights, k_weights, v_weights) = fused_qkv_weight\n",
    "            (q_bias, k_bias, v_bias) = fused_qkv_bias\n",
    "\n",
    "            # Compute Q, K, V projections\n",
    "            q = ttnn.linear(hidden_states, q_weights, bias=q_bias, transpose_b=True)\n",
    "            k = ttnn.linear(hidden_states, k_weights, bias=k_bias, transpose_b=True)\n",
    "            v = ttnn.linear(hidden_states, v_weights, bias=v_bias, transpose_b=True)\n",
    "\n",
    "            # Reshape to [batch_size, seq_length, num_heads, head_dim]\n",
    "            q = ttnn.reshape(q, (seq_length, batch_size * self.heads, self._head_dim))\n",
    "            k = ttnn.reshape(k, (seq_length, batch_size * self.heads, self._head_dim))\n",
    "            v = ttnn.reshape(v, (seq_length, batch_size * self.heads, self._head_dim))\n",
    "\n",
    "            # Transpose to [batch_size, num_heads, seq_length, head_dim] for attention computation\n",
    "            q = ttnn.transpose(q, 0, 1)\n",
    "            k = ttnn.transpose(k, 0, 1)\n",
    "            v = ttnn.transpose(v, 0, 1)\n",
    "\n",
    "            # Compute attention scores with proper scaling\n",
    "            scores = ttnn.matmul(q, ttnn.transpose(k, -2, -1))\n",
    "            scores = scores * self._scale\n",
    "\n",
    "            # Apply attention mask if provided (matching PyTorch MHA behavior)\n",
    "            if attention_mask is not None:\n",
    "                # Convert attention mask to the right shape and add to scores\n",
    "                # PyTorch MHA expects mask to be broadcastable to [batch_size, num_heads, seq_len, seq_len]\n",
    "                scores = scores + attention_mask\n",
    "\n",
    "            attn_weights = ttnn.softmax(\n",
    "                scores, dim=-1, numeric_stable=True, compute_kernel_config=compute_kernel_config\n",
    "            )\n",
    "\n",
    "            # Apply dropout if needed (currently disabled)\n",
    "            # attn_weights = ttnn.experimental.dropout(attn_weights, self._attention_dropout)\n",
    "\n",
    "            # Apply attention weights to values\n",
    "            attn_output = ttnn.matmul(attn_weights, v)\n",
    "\n",
    "            # Reshape to [batch_size, seq_length, embed_dim]\n",
    "            attn_output = ttnn.transpose(attn_output, 0, 1)\n",
    "            attn_output = ttnn.reshape(attn_output, (seq_length, batch_size, self._embed_dim))\n",
    "\n",
    "            # Apply output projection\n",
    "            dense_out = ttnn.linear(\n",
    "                attn_output,\n",
    "                self_output_weight,\n",
    "                bias=self_output_bias,\n",
    "                compute_kernel_config=compute_kernel_config,\n",
    "                transpose_b=True,\n",
    "            )\n",
    "\n",
    "            return dense_out\n",
    "\n",
    "        def residual_attention_block(x, layer, i=0):\n",
    "            # LayerNorm\n",
    "            residual = x\n",
    "            x = ttnn.layer_norm(x, weight=layer[\"ln_1_weight\"], bias=layer[\"ln_1_bias\"])\n",
    "\n",
    "            # Multihead attention / Self-Attention\n",
    "            # This must be equal to nn.MultiheadAttention(d_model, n_head)(x, x, x, need_weights=False, attn_mask=self.attn_mask)\n",
    "            x_attn = multi_head_attention(\n",
    "                x,\n",
    "                fused_qkv_weight=(layer[\"q_proj_weight\"], layer[\"k_proj_weight\"], layer[\"v_proj_weight\"]),\n",
    "                fused_qkv_bias=(layer[\"q_proj_bias\"], layer[\"k_proj_bias\"], layer[\"v_proj_bias\"]),\n",
    "                self_output_weight=layer[\"out_proj_weight\"],\n",
    "                self_output_bias=layer[\"out_proj_bias\"],\n",
    "                attention_mask=self.attention_mask,\n",
    "                prefix=f\"{self.prefix}.layers.{i}.attn\",\n",
    "            )  # Vision transformer doesn't use attention mask\n",
    "\n",
    "            x = residual + x_attn\n",
    "\n",
    "            # LayerNorm\n",
    "            x_post_ln_2 = ttnn.layer_norm(x, weight=layer[\"ln_2_weight\"], bias=layer[\"ln_2_bias\"])\n",
    "\n",
    "            # Multi-Layer Perceptron\n",
    "            x = x + mlp(x_post_ln_2, layer)\n",
    "\n",
    "            return x\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            layer = self.layers[i]\n",
    "            x = residual_attention_block(x, layer, i)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228fe5e8-5c22-4866-89ce-8651b464df2f",
   "metadata": {},
   "source": [
    "We define the VisionTransformer.\n",
    "\n",
    "Its `forward()` method pre-process image embeddings and calls the generic transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe31cab8-b934-4da0-a02c-a2793ba3c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer:\n",
    "    def __init__(self, state_dict):\n",
    "        torch.manual_seed(0)\n",
    "        self.output_dim = 0\n",
    "\n",
    "        conv2_state_dict_name = \"vision_model.embeddings.patch_embedding.weight\"\n",
    "        self.vision_width = state_dict[conv2_state_dict_name].shape[0]\n",
    "        self.patch_size = state_dict[conv2_state_dict_name].shape[-1]\n",
    "        self.vision_heads = self.vision_width // 64\n",
    "\n",
    "        self.class_embedding = convert_ttnn_dtype(\n",
    "            state_dict[\"vision_model.embeddings.class_embedding\"], dtype=ttnn.bfloat16\n",
    "        )\n",
    "        self.positional_embedding = convert_ttnn_dtype(\n",
    "            state_dict[\"vision_model.embeddings.position_embedding.weight\"], dtype=ttnn.bfloat16\n",
    "        )\n",
    "\n",
    "        self.proj = convert_ttnn_dtype(state_dict[\"visual_projection.weight\"], dtype=ttnn.bfloat16)\n",
    "\n",
    "        # Weights for convolution layer\n",
    "        # For sharding; use all cores; strategy = block sharding\n",
    "        core_grid = ttnn.CoreGrid(x=8, y=8)\n",
    "        # Error: Physical shard shape (8216, 4) must be tile {32, 32} sized\n",
    "        # memory_config = ttnn.create_sharded_memory_config(conv1_weights_shape, core_grid, ttnn.ShardStrategy.HEIGHT)\n",
    "        memory_config = ttnn.DRAM_MEMORY_CONFIG\n",
    "        self.conv1_weights = ttnn.to_layout(\n",
    "            state_dict[conv2_state_dict_name],\n",
    "            layout=ttnn.ROW_MAJOR_LAYOUT,\n",
    "            memory_config=memory_config,\n",
    "            dtype=ttnn.bfloat16,\n",
    "        )\n",
    "        self.conv1_weights = convert_ttnn_dtype(self.conv1_weights, dtype=ttnn.bfloat16)\n",
    "\n",
    "        assert self.conv1_weights.dtype == ttnn.bfloat16\n",
    "\n",
    "        self.ln_pre_weights = state_dict[\"vision_model.pre_layrnorm.weight\"]  # TODO: What's this ?\n",
    "        self.ln_pre_bias = state_dict[\"vision_model.pre_layrnorm.bias\"]  # TODO: What's this ?\n",
    "\n",
    "        self.ln_post_weights = state_dict[\"vision_model.post_layernorm.weight\"]  # TODO: What's this ?\n",
    "        self.ln_post_bias = state_dict[\"vision_model.post_layernorm.bias\"]  # TODO: What's this ?\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            state_dict, self.vision_heads, attention_mask=None, prefix=\"vision_model.encoder\"\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        (batch_size, in_channels, height, width) = x.shape\n",
    "\n",
    "        # Note: ttnn.conv2d uses 'Array of Struct' shape for input tensor:\n",
    "        # (N, H, W, C_in)\n",
    "        # whereas torch.nn.Conv2d uses 'Struct of Array' shape for input tensor:\n",
    "        # (N, C_in, H, W)\n",
    "        #\n",
    "        # # Moreover, ttnn.conv2d produces a flattened output tensor:\n",
    "        # (N, C_in, H, W) -> (1, 1, N * H * W, C_out)\n",
    "        # whereas torch.nn.Conv2d produces a 4D tensor:\n",
    "        # (N, C_out, H_out, W_out)\n",
    "\n",
    "        # Also:\n",
    "        # ttnn.conv2d only take a tuple for kernel_size and stride\n",
    "\n",
    "        # Change tensor layout to (N, H, W, C_in)\n",
    "        x = ttnn.permute(x, [0, 2, 3, 1])  # (N, C_in, H, W) -> (N, H, W, C_in)\n",
    "\n",
    "        # Note: ttnn.conv2d requires row-major layout for weight tensor\n",
    "        x = ttnn.to_layout(x, layout=ttnn.ROW_MAJOR_LAYOUT)\n",
    "\n",
    "        old_memory_config = x.memory_config()\n",
    "        out_channels = 768\n",
    "        in_channels = 3\n",
    "\n",
    "        bias_tensor = ttnn.zeros(\n",
    "            (1, 1, 1, out_channels), dtype=ttnn.bfloat16, device=device, layout=ttnn.ROW_MAJOR_LAYOUT\n",
    "        )\n",
    "\n",
    "        x = ttnn.conv2d(\n",
    "            input_tensor=x, \n",
    "            weight_tensor=self.conv1_weights,\n",
    "            bias_tensor=bias_tensor,\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            batch_size=batch_size,\n",
    "            input_height=height,\n",
    "            input_width=width,\n",
    "            kernel_size=(self.patch_size, self.patch_size),\n",
    "            stride=(self.patch_size, self.patch_size),\n",
    "            padding=(0, 0),\n",
    "            dilation=(1, 1),\n",
    "            groups=0,  # No grouped convolution (?)\n",
    "            device=get_device(),\n",
    "            return_weights_and_bias=False,\n",
    "            return_output_dim=False,\n",
    "        )\n",
    "\n",
    "        # ERROR: Number of shards along height 7 must not exceed number of cores 2\n",
    "        output_height = height // self.patch_size\n",
    "        output_width = width // self.patch_size\n",
    "\n",
    "        # Check Convolution result\n",
    "        x = ttnn.to_layout(x, layout=ttnn.TILE_LAYOUT)\n",
    "        host_tensor = ttnn.to_torch(x, dtype=torch.float32)\n",
    "\n",
    "        host_tensor = torch.reshape(host_tensor, (batch_size, output_height, output_width, out_channels))\n",
    "\n",
    "        x = ttnn.reshape(x, (x.shape[0], x.shape[1] * x.shape[2], x.shape[3]))\n",
    "\n",
    "        class_embedding = convert_ttnn_dtype(self.class_embedding, x.dtype, (x.shape[0], 1, x.shape[-1]))\n",
    "\n",
    "        # TODO: See why we use zero tensor here and addition here ?\n",
    "        zero_tensor = ttnn.zeros(\n",
    "            shape=(x.shape[0], 1, x.shape[-1]), dtype=x.dtype, device=device, layout=ttnn.TILE_LAYOUT\n",
    "        )\n",
    "\n",
    "        class_embedding = ttnn.reshape(class_embedding, zero_tensor.shape)\n",
    "        class_embedding = class_embedding + zero_tensor\n",
    "\n",
    "        # TODO: Do this in L1 Sharded Memory\n",
    "        # For now, move data to DRAM\n",
    "        x = ttnn.to_memory_config(x, memory_config=ttnn.DRAM_MEMORY_CONFIG)\n",
    "\n",
    "        class_embedding = ttnn.reshape(\n",
    "            class_embedding, (class_embedding.shape[0], class_embedding.shape[1], class_embedding.shape[2])\n",
    "        )\n",
    "        class_embedding = ttnn.to_memory_config(class_embedding, memory_config=x.memory_config())\n",
    "\n",
    "        # ERROR: RuntimeError: bad optional access (???)\n",
    "        # TODO: Avoid move to host and keep on device\n",
    "        x = ttnn.concat([class_embedding, x], dim=1, memory_config=None)  # shape = [*, grid ** 2 + 1, width]\n",
    "\n",
    "        positional_embedding = convert_ttnn_dtype(self.positional_embedding, x.dtype, (1, x.shape[1], x.shape[2]))\n",
    "        x = x + positional_embedding\n",
    "\n",
    "        # LayerNorm\n",
    "        x = ttnn.layer_norm(x, weight=self.ln_pre_weights, bias=self.ln_pre_bias)\n",
    "\n",
    "        # Permute\n",
    "        x = ttnn.permute(x, (1, 0, 2))  # NLD -> LND\n",
    "\n",
    "        # Transformer\n",
    "        x = self.transformer.forward(x)\n",
    "\n",
    "        # Permute\n",
    "        x = ttnn.permute(x, (1, 0, 2))  # LND -> NLD\n",
    "\n",
    "        # LayerNorm\n",
    "        x = ttnn.layer_norm(x[:, 0, :], weight=self.ln_post_weights, bias=self.ln_post_bias)\n",
    "\n",
    "        if self.proj is not None:\n",
    "            self.proj = ttnn.transpose(self.proj, 0, 1)\n",
    "            x = ttnn.matmul(x, self.proj)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ea07a-dc5e-42cd-b214-cdcbfdcacf89",
   "metadata": {},
   "source": [
    "We then define a class for the CLIP model. \n",
    "\n",
    "The CLIP class instantiates:\n",
    "- a Text Transformer (Transformer)\n",
    "- a VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ce4f9-c677-4f38-9db3-f066c4da9752",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP:\n",
    "    def __init__(self, state_dict):\n",
    "        self.token_embedding = convert_ttnn_dtype(\n",
    "            state_dict[\"text_model.embeddings.token_embedding.weight\"], dtype=ttnn.bfloat16\n",
    "        )\n",
    "        self.positional_embedding = convert_ttnn_dtype(\n",
    "            state_dict[\"text_model.embeddings.position_embedding.weight\"], dtype=ttnn.bfloat16\n",
    "        )\n",
    "\n",
    "        self.text_projection = convert_ttnn_dtype(state_dict[\"text_projection.weight\"], dtype=ttnn.bfloat16)\n",
    "        self.context_length = self.positional_embedding.shape[0]\n",
    "        self.vocab_size = self.token_embedding.shape[0]\n",
    "        self.transformer_width = state_dict[\"text_model.final_layer_norm.weight\"].shape[0]\n",
    "        transformer_heads = self.transformer_width // 64\n",
    "\n",
    "        self.ln_final_weights = state_dict[\"text_model.final_layer_norm.weight\"]\n",
    "        self.ln_final_bias = state_dict[\"text_model.final_layer_norm.bias\"]\n",
    "\n",
    "        self.logit_scale = state_dict[\"logit_scale\"].item()\n",
    "\n",
    "        self.visual = VisionTransformer(state_dict)\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            state_dict, transformer_heads, attention_mask=self.build_attention_mask(), prefix=\"text_model.encoder\"\n",
    "        )\n",
    "        \n",
    "    def build_attention_mask(self):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = ttnn.full(shape=[self.context_length, self.context_length], fill_value=float(\"-inf\"), dtype=ttnn.bfloat16, device=get_device(), layout=ttnn.TILE_LAYOUT)\n",
    "        mask = ttnn.triu(mask, diagonal=1)\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.visual.forward(image)\n",
    "\n",
    "    def encode_text(self, tokens):\n",
    "        tokens = convert_ttnn_dtype(tokens, dtype=ttnn.uint32)\n",
    "\n",
    "        x = ttnn.embedding(tokens, weight=self.token_embedding, dtype=ttnn.bfloat16)\n",
    "\n",
    "        assert x.dtype == ttnn.bfloat16\n",
    "\n",
    "        # Add positional embedding\n",
    "        x = x + self.positional_embedding\n",
    "\n",
    "        # Permute\n",
    "        x = ttnn.permute(x, (1, 0, 2))  # NLD -> LND\n",
    "        x = self.transformer.forward(x)\n",
    "\n",
    "        # Permute back\n",
    "        x = ttnn.permute(x, (1, 0, 2))  # LND -> NLD\n",
    "\n",
    "        # LayerNorm\n",
    "        x = ttnn.layer_norm(x, weight=self.ln_final_weights, bias=self.ln_final_bias)\n",
    "\n",
    "        # TODO: Change to TTNN\n",
    "        # text_projection = ttnn.transpose(self.text_projection, -2, -1)\n",
    "\n",
    "        torch_tokens = ttnn.to_torch(tokens)\n",
    "        text_projection = ttnn.to_torch(self.text_projection)\n",
    "        torch_x = ttnn.to_torch(x)\n",
    "\n",
    "        torch_x = torch_x[torch.arange(torch_x.shape[0]), torch_tokens.argmax(dim=-1)] @ text_projection.t()\n",
    "\n",
    "        return ttnn.from_torch(torch_x, device=get_device(), layout=ttnn.TILE_LAYOUT)\n",
    "\n",
    "    def forward(self, image, tokens):\n",
    "        text_features = self.encode_text(tokens)\n",
    "        image_features = self.encode_image(image)\n",
    "\n",
    "        # Normalize features\n",
    "        norm_image_features = ttnn.operations.moreh.norm(image_features, p=2.0, dim=1, keepdim=True)\n",
    "        norm_text_features = ttnn.operations.moreh.norm(text_features, p=2.0, dim=1, keepdim=True)\n",
    "\n",
    "        image_features = ttnn.divide(image_features, norm_image_features)\n",
    "        text_features = ttnn.divide(text_features, norm_text_features)\n",
    "\n",
    "        # Cosine similarity as logits\n",
    "        logit_scale = math.exp(self.logit_scale)\n",
    "\n",
    "        text_features_t = ttnn.transpose(text_features, 0, 1)\n",
    "        logits_per_image = logit_scale * image_features @ text_features_t\n",
    "        logits_per_text = ttnn.transpose(logits_per_image, 0, 1)\n",
    "\n",
    "        return logits_per_image, logits_per_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940036cb-a7e8-43ce-9716-36a84d362d2e",
   "metadata": {},
   "source": [
    "While input images can have any dimension and color spaces, our CLIP models only handles 224x224 RGB images. \n",
    "\n",
    "We therefore pre-process the image to the dimensions and RGB colorspace of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e08e26c-5fe8-4162-9cf7-37e2c5ce24f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, model_resolution):\n",
    "    def _convert_image_to_rgb(image):\n",
    "        return image.convert(\"RGB\")\n",
    "\n",
    "    # Pre-process image on host with torch\n",
    "    transform_fn = Compose(\n",
    "        [\n",
    "            Resize(model_resolution, interpolation=InterpolationMode.BICUBIC),\n",
    "            CenterCrop(model_resolution),\n",
    "            _convert_image_to_rgb,\n",
    "            ToTensor(),\n",
    "            Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ]\n",
    "    )\n",
    "    return transform_fn(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be67cd47-2f80-490b-98a1-aa6a7ba84832",
   "metadata": {},
   "source": [
    "We use an utility function to download image an URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8108429-2409-4a3c-9a77-c0adbb4537ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url):\n",
    "    \"\"\"\n",
    "    Download an image from a URL and return it as a PIL Image object.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the image to download\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: The downloaded image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        \n",
    "        # Convert the response content to a PIL Image\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        return image\n",
    "    except requests.RequestException as e:\n",
    "        raise Exception(f\"Failed to download image from {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to process downloaded image: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa56ff5-8698-4705-8aee-32f81f61156b",
   "metadata": {},
   "source": [
    "Having defined each step of our CLIP model, we can now use it on a input image and text. \n",
    "\n",
    "The weights are automatically downloaded using `CLIPModel.from_pretrained()`. \n",
    "\n",
    "We also tokenize text inputs using the tokenizer model that matches our CLIP model. In this case, we download the `openai/clip-vit-base-patch32` tokenizer model using `CLIPTokenizer.from_pretrained()`.\n",
    "As TT-NN does not handle tokenization, we use `CLIPTokenizer` from the `transformers` library to handle it for us. \n",
    "We then convert the output of `tokenizer()` to TT-NN tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64691ea0-cbf8-452c-b9d3-4b57157a480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    open_ttnn()\n",
    "\n",
    "    logging_file = open(\"logging.csv\", \"w\")\n",
    "\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    state_dict = convert_model_to_ttnn(model.state_dict())\n",
    "\n",
    "    clip = CLIP(state_dict)\n",
    "\n",
    "    # Download image from URL\n",
    "    image_url = \"https://media.githubusercontent.com/media/tenstorrent/tutorial-assets/nmaurice/clip-tutorial/media/clip_tutorial/CLIP.png\"\n",
    "    image = download_image(image_url)\n",
    "\n",
    "    # Preprocess image\n",
    "    image = preprocess_image(image, 224).unsqueeze(0).to(\"cpu\")\n",
    "\n",
    "    preferred_dtype = ttnn.bfloat16\n",
    "    tt_image = to_ttnn(image, preferred_dtype)\n",
    "\n",
    "    prompts = [\"a diagram\", \"a dog\", \"a cat\"]\n",
    "\n",
    "    # Tokenize text\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    tokenized_inputs = tokenizer(prompts, padding=\"max_length\", max_length=clip.context_length, return_tensors=\"pt\")\n",
    "    tokens_pretrained_host = tokenized_inputs[\"input_ids\"]\n",
    "    tokens_pretrained = ttnn.from_torch(tokens_pretrained_host, device=get_device(), layout=ttnn.TILE_LAYOUT)\n",
    "\n",
    "    logits_per_image, logits_per_text = clip.forward(tt_image, tokens_pretrained)\n",
    "    probs = ttnn.softmax(logits_per_image, dim=-1)\n",
    "    print(f\"Label probs: {probs}\")\n",
    "\n",
    "    logging_file.close()\n",
    "\n",
    "    close_ttnn()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
