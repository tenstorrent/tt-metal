{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c0da19-4ccb-41cc-8e57-f8e5426d87e3",
   "metadata": {},
   "source": [
    "# Building CLIP Model for Zero Shot Imag Classification using TT-NN\n",
    "\n",
    "CLIP (Contrastive Language-Image Pre-Training) is a foundational multimodal AI model developed by OpenAI that learns visual concepts from natural language supervision. Unlike traditional computer vision models that are trained on fixed categories, CLIP can understand and classify images based on arbitrary text descriptions.\n",
    "\n",
    "## What CLIP Does\n",
    "\n",
    "CLIP bridges the gap between vision and language by learning to associate images with their textual descriptions. The model consists of two main components:\n",
    "\n",
    "1. **Vision Encoder**: A Vision Transformer (ViT) that processes images and converts them into feature embeddings\n",
    "2. **Text Encoder**: A Transformer that processes text descriptions and converts them into feature embeddings\n",
    "\n",
    "![CLIP Diagram](https://media.githubusercontent.com/media/tenstorrent/tutorial-assets/nmaurice/clip-tutorial/media/clip_tutorial/CLIP.png)\n",
    "\n",
    "During inference, CLIP can:\n",
    "- **Zero-shot image classification**: Classify images into categories it has never explicitly seen during training by comparing image embeddings with text embeddings of category descriptions\n",
    "- **Image-text similarity**: Measure how well an image matches a given text description\n",
    "- **Content-based image retrieval**: Find images that best match a text query\n",
    "\n",
    "\n",
    "\n",
    "In this tutorial, we implement CLIP for image classification. Our application will classifiy an image using natural language prompts such as \"a diagram\", \"a dog\", or \"a cat\". \n",
    "We use pre-trained weights of OpenAI's clip-vit-base-patch32 model and focus on inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc6c99f-80a5-4002-a3e5-6b4029f21705",
   "metadata": {},
   "source": [
    "## Imports and Dependencies\n",
    "\n",
    "We start by importing the necessary libraries for our CLIP implementation:\n",
    "\n",
    "- **ttnn**\n",
    "- **torch**: model loading and tensor pre-processing\n",
    "- **transformers**: Hugging Face library for downloading pre-trained models and tokenzing prompts\n",
    "- **PIL**: Python Imaging Library for image pre-processing\n",
    "- **torchvision**: Computer vision utilities for image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d82f03-2f2d-4460-92a0-c6021ff4244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttnn\n",
    "import torch\n",
    "from loguru import logger\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import CLIPTokenizer, CLIPModel\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import time\n",
    "\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, InterpolationMode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbf6215-2106-433a-a5cc-ca653e171a8b",
   "metadata": {},
   "source": [
    "## TT-NN Device Management and Utility Functions\n",
    "\n",
    "We define helper functions to manage TT-NN devices and handle tensor conversions between PyTorch and TT-NN formats. These utilities simplify device initialization, tensor format conversions, and memory management throughout our CLIP implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0e772f-4de5-4612-b615-0bf1a8d295b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def open_ttnn():\n",
    "    \"\"\"Initialize TT-NN device with specified L1 cache size.\"\"\"\n",
    "    global device\n",
    "    device = ttnn.open_device(device_id=0, l1_small_size=8192)\n",
    "\n",
    "def close_ttnn():\n",
    "    \"\"\"Clean up and close the TT-NN device.\"\"\"\n",
    "    global device\n",
    "    if device is not None:\n",
    "        ttnn.close_device(device)\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Get the current TT-NN device handle.\"\"\"\n",
    "    global device\n",
    "    return device\n",
    "\n",
    "def convert_from_ttnn(x):\n",
    "    \"\"\"Convert TT-NN tensor to PyTorch tensor if needed.\"\"\"\n",
    "    global device\n",
    "    if isinstance(x, ttnn._ttnn.tensor.Tensor):\n",
    "        return ttnn.to_torch(x)\n",
    "    return x\n",
    "\n",
    "def to_ttnn(torch_tensor, dtype=None, layout=ttnn.TILE_LAYOUT):\n",
    "    \"\"\"Convert PyTorch tensor to TT-NN tensor with specified dtype and layout.\"\"\"\n",
    "    global device\n",
    "    ttnn_tensor = ttnn.from_torch(torch_tensor, device=device, layout=layout, dtype=dtype)\n",
    "    return ttnn_tensor\n",
    "\n",
    "def to_torch_shape(ttnn_shape):\n",
    "    \"\"\"Convert TT-NN shape to PyTorch-compatible tuple.\"\"\"\n",
    "    return tuple(ttnn_shape)\n",
    "\n",
    "def convert_ttnn_dtype(ttnn_tensor, dtype, new_shape=None):\n",
    "    \"\"\"\n",
    "    Change dtype of TT-NN tensor and optionally reshape.\n",
    "    Note: Currently requires moving tensor to host for dtype conversion.\n",
    "    \"\"\"\n",
    "    device = get_device()\n",
    "    # Move tensor to host for dtype conversion (current TT-NN limitation)\n",
    "    host_tensor = ttnn.from_device(ttnn_tensor)\n",
    "    host_tensor = ttnn.to_dtype(host_tensor, dtype=dtype)\n",
    "    if new_shape is not None:\n",
    "        host_tensor = ttnn.reshape(host_tensor, new_shape)\n",
    "\n",
    "    return ttnn.to_device(host_tensor, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c632f6ee-1653-4d83-943b-9efbfbfdb444",
   "metadata": {},
   "source": [
    "## Model Weight Conversion\n",
    "\n",
    "Since TT-NN does not natively support weight loading from pre-trained models, we rely on PyTorch's model loading capabilities and then convert the weights to TT-NN format. The following helper function converts an entire model's state dictionary from PyTorch tensors to TT-NN tensors, enabling us to use pre-trained CLIP weights on TT hardware.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bb9e44-9d89-4606-abe6-76d7e37c597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_model_to_ttnn(state_dict):\n",
    "    \"\"\"\n",
    "    Convert a PyTorch model's state dictionary to TT-NN format.\n",
    "    \n",
    "    Args:\n",
    "        state_dict: PyTorch model state dictionary containing weights and biases\n",
    "        \n",
    "    Returns:\n",
    "        dict: State dictionary with tensors converted to TT-NN format\n",
    "    \"\"\"\n",
    "    ttnn_state_dict = {}\n",
    "    logger.info(f\"Converting model to TT-NN format\")\n",
    "\n",
    "    # Convert each tensor in the state dictionary to TT-NN format\n",
    "    for key, value in state_dict.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            # Convert PyTorch tensors to TT-NN tensors\n",
    "            state_dict[key] = to_ttnn(value)\n",
    "        elif isinstance(value, torch.Size):\n",
    "            # Convert PyTorch Size objects to TT-NN Size objects\n",
    "            state_dict[key] = ttnn.Size(value)\n",
    "\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d1d9a7-9f4f-446f-9b3a-d5f002b2abdb",
   "metadata": {},
   "source": [
    "## Generic Transformer Implementation\n",
    "\n",
    "CLIP uses two types of transformers: a text transformer and a vision transformer. To maximize code reuse, we define a generic Transformer class that can be used for both modalities with appropriate configuration.\n",
    "\n",
    "### Transformer Architecture\n",
    "\n",
    "The transformer models used by CLIP consist of multiple layers (residual blocks), each containing the following sub-operations in sequence:\n",
    "\n",
    "1. **Layer Normalization**: Normalizes inputs for stable inference (and training)\n",
    "2. **Multi-Head Self-Attention**: \n",
    "   - For text: Uses causal masking to prevent attending to future tokens\n",
    "   - For vision: Uses full attention across all image patches\n",
    "3. **Layer Normalization**: Second normalization layer\n",
    "4. **MLP (Multi-Layer Perceptron)**: Two linear layers with GELU activation (Linear → GELU → Linear)\n",
    "\n",
    "Each block uses residual connections, where the output of each sub-operation is added to its input, enabling deeper networks and better gradient flow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7f12bd-eeea-4e39-a43b-7498ad7a9fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transformer:\n",
    "    def __init__(self, state_dict, heads, attention_mask=None, prefix=\"\"):\n",
    "        \"\"\"\n",
    "        Initialize a generic Transformer that can be used for both text and vision encoding.\n",
    "        \n",
    "        Args:\n",
    "            state_dict: Model weights dictionary\n",
    "            heads: Number of attention heads\n",
    "            attention_mask: Attention mask for causal attention (used for text, None for vision)\n",
    "            prefix: Prefix for layer names in state_dict (e.g., \"text_model.encoder\" or \"vision_model.encoder\")\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.heads = heads\n",
    "        self.attention_mask = attention_mask\n",
    "        self.prefix = prefix\n",
    "\n",
    "        # Use regex to find all layer indices in the state dictionary\n",
    "        layer_pattern = re.compile(f\"{prefix}\\.layers\\.(\\d+)\\.\")\n",
    "\n",
    "        # Count number of transformer layers by finding unique layer indices\n",
    "        layers_ids = set()\n",
    "        for k in state_dict.keys():\n",
    "            re_match = re.search(layer_pattern, k)\n",
    "            if re_match:\n",
    "                layers_ids.add(re_match.group(1))\n",
    "\n",
    "        num_layers = len(layers_ids)\n",
    "\n",
    "        # Initialize each transformer layer with converted weights\n",
    "        for i in range(0, num_layers):\n",
    "            resblock_prefix = f\"{prefix}.layers.{i}\"\n",
    "\n",
    "            # Extract and convert all weights for this layer to bfloat16 precision\n",
    "            self.layers.append(\n",
    "                {\n",
    "                    # First layer normalization weights\n",
    "                    \"ln_1_weight\": convert_ttnn_dtype(\n",
    "                        state_dict[f\"{resblock_prefix}.layer_norm1.weight\"], ttnn.bfloat16\n",
    "                    ),\n",
    "                    \"ln_1_bias\": convert_ttnn_dtype(state_dict[f\"{resblock_prefix}.layer_norm1.bias\"], ttnn.bfloat16),\n",
    "                    \n",
    "                    # Multi-head attention projection weights (Q, K, V)\n",
    "                    \"q_proj_weight\": convert_ttnn_dtype(\n",
    "                        state_dict[f\"{resblock_prefix}.self_attn.q_proj.weight\"], ttnn.bfloat16\n",
    "                    ),\n",
    "                    \"q_proj_bias\": convert_ttnn_dtype(\n",
    "                        state_dict[f\"{resblock_prefix}.self_attn.q_proj.bias\"], ttnn.bfloat16\n",
    "                    ),\n",
    "                    \"k_proj_weight\": convert_ttnn_dtype(\n",
    "                        state_dict[f\"{resblock_prefix}.self_attn.k_proj.weight\"], ttnn.bfloat16\n",
    "                    ),\n",
    "                    \"k_proj_bias\": convert_ttnn_dtype(\n",
    "                        state_dict[f\"{resblock_prefix}.self_attn.k_proj.bias\"], ttnn.bfloat16\n",
    "                    ),\n",
    "                    \"v_proj_weight\": convert_ttnn_dtype(\n",
    "                        state_dict[f\"{resblock_prefix}.self_attn.v_proj.weight\"], ttnn.bfloat16\n",
    "                    ),\n",
    "                    \"v_proj_bias\": convert_ttnn_dtype(\n",
    "                        state_dict[f\"{resblock_prefix}.self_attn.v_proj.bias\"], ttnn.bfloat16\n",
    "                    ),\n",
    "                    \n",
    "                    # Attention output projection weights\n",
    "                    \"out_proj_weight\": convert_ttnn_dtype(\n",
    "                        state_dict[f\"{resblock_prefix}.self_attn.out_proj.weight\"], ttnn.bfloat16\n",
    "                    ),\n",
    "                    \"out_proj_bias\": convert_ttnn_dtype(\n",
    "                        state_dict[f\"{resblock_prefix}.self_attn.out_proj.bias\"], ttnn.bfloat16\n",
    "                    ),\n",
    "                    \n",
    "                    # Second layer normalization weights\n",
    "                    \"ln_2_weight\": convert_ttnn_dtype(\n",
    "                        state_dict[f\"{resblock_prefix}.layer_norm2.weight\"], ttnn.bfloat16\n",
    "                    ),\n",
    "                    \"ln_2_bias\": convert_ttnn_dtype(state_dict[f\"{resblock_prefix}.layer_norm2.bias\"], ttnn.bfloat16),\n",
    "                    \n",
    "                    # MLP weights (feed-forward network)\n",
    "                    \"mlp_c_fc_weight\": convert_ttnn_dtype(\n",
    "                        state_dict[f\"{resblock_prefix}.mlp.fc1.weight\"], ttnn.bfloat16\n",
    "                    ),\n",
    "                    \"mlp_c_fc_bias\": convert_ttnn_dtype(state_dict[f\"{resblock_prefix}.mlp.fc1.bias\"], ttnn.bfloat16),\n",
    "                    \"mlp_c_proj_weight\": convert_ttnn_dtype(\n",
    "                        state_dict[f\"{resblock_prefix}.mlp.fc2.weight\"], ttnn.bfloat16\n",
    "                    ),\n",
    "                    \"mlp_c_proj_bias\": convert_ttnn_dtype(state_dict[f\"{resblock_prefix}.mlp.fc2.bias\"], ttnn.bfloat16),\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        def mlp(x, layer):\n",
    "            x = ttnn.linear(x, layer[\"mlp_c_fc_weight\"], bias=layer[\"mlp_c_fc_bias\"], transpose_b=True)\n",
    "            x = ttnn.gelu(x)\n",
    "            x = ttnn.linear(x, layer[\"mlp_c_proj_weight\"], bias=layer[\"mlp_c_proj_bias\"], transpose_b=True)\n",
    "            return x\n",
    "\n",
    "        def multi_head_attention(\n",
    "            hidden_states,\n",
    "            fused_qkv_weight,\n",
    "            fused_qkv_bias,\n",
    "            self_output_weight,\n",
    "            self_output_bias,\n",
    "            attention_mask=None,\n",
    "            prefix=\"\",\n",
    "        ):\n",
    "            seq_length, batch_size, hidden_size = hidden_states.shape\n",
    "\n",
    "            self._embed_dim = hidden_size\n",
    "            self._head_dim = hidden_size // self.heads\n",
    "            self._scale = self._head_dim**-0.5\n",
    "            self._attention_dropout = 0.0  # Unused\n",
    "\n",
    "            compute_kernel_config = ttnn.WormholeComputeKernelConfig(\n",
    "                math_fidelity=ttnn.MathFidelity.HiFi4,\n",
    "                math_approx_mode=False,\n",
    "                fp32_dest_acc_en=True,\n",
    "                packer_l1_acc=True,\n",
    "            )\n",
    "\n",
    "            # Note: KV-caching not implemented (not needed for single forward pass)\n",
    "            (q_weights, k_weights, v_weights) = fused_qkv_weight\n",
    "            (q_bias, k_bias, v_bias) = fused_qkv_bias\n",
    "\n",
    "            # Compute Q, K, V projections\n",
    "            q = ttnn.linear(hidden_states, q_weights, bias=q_bias, transpose_b=True)\n",
    "            k = ttnn.linear(hidden_states, k_weights, bias=k_bias, transpose_b=True)\n",
    "            v = ttnn.linear(hidden_states, v_weights, bias=v_bias, transpose_b=True)\n",
    "\n",
    "            # Reshape to [batch_size, seq_length, num_heads, head_dim]\n",
    "            q = ttnn.reshape(q, (seq_length, batch_size * self.heads, self._head_dim))\n",
    "            k = ttnn.reshape(k, (seq_length, batch_size * self.heads, self._head_dim))\n",
    "            v = ttnn.reshape(v, (seq_length, batch_size * self.heads, self._head_dim))\n",
    "\n",
    "            # Transpose to [batch_size, num_heads, seq_length, head_dim] for attention computation\n",
    "            q = ttnn.transpose(q, 0, 1)\n",
    "            k = ttnn.transpose(k, 0, 1)\n",
    "            v = ttnn.transpose(v, 0, 1)\n",
    "\n",
    "            # Compute attention scores with proper scaling\n",
    "            scores = ttnn.matmul(q, ttnn.transpose(k, -2, -1))\n",
    "            scores = scores * self._scale\n",
    "\n",
    "            # Apply attention mask if provided (matching PyTorch MHA behavior)\n",
    "            if attention_mask is not None:\n",
    "                # Convert attention mask to the right shape and add to scores\n",
    "                # PyTorch MHA expects mask to be broadcastable to [batch_size, num_heads, seq_len, seq_len]\n",
    "                scores = scores + attention_mask\n",
    "\n",
    "            attn_weights = ttnn.softmax(\n",
    "                scores, dim=-1, numeric_stable=True, compute_kernel_config=compute_kernel_config\n",
    "            )\n",
    "\n",
    "            # Apply attention weights to values\n",
    "            attn_output = ttnn.matmul(attn_weights, v)\n",
    "\n",
    "            # Reshape to [batch_size, seq_length, embed_dim]\n",
    "            attn_output = ttnn.transpose(attn_output, 0, 1)\n",
    "            attn_output = ttnn.reshape(attn_output, (seq_length, batch_size, self._embed_dim))\n",
    "\n",
    "            # Apply output projection\n",
    "            dense_out = ttnn.linear(\n",
    "                attn_output,\n",
    "                self_output_weight,\n",
    "                bias=self_output_bias,\n",
    "                compute_kernel_config=compute_kernel_config,\n",
    "                transpose_b=True,\n",
    "            )\n",
    "\n",
    "            return dense_out\n",
    "\n",
    "        def residual_attention_block(x, layer, i=0):\n",
    "            # LayerNorm\n",
    "            residual = x\n",
    "            x = ttnn.layer_norm(x, weight=layer[\"ln_1_weight\"], bias=layer[\"ln_1_bias\"])\n",
    "\n",
    "            # Multihead attention / Self-Attention\n",
    "            # This must be equal to nn.MultiheadAttention(d_model, n_head)(x, x, x, need_weights=False, attn_mask=self.attn_mask)\n",
    "            x_attn = multi_head_attention(\n",
    "                x,\n",
    "                fused_qkv_weight=(layer[\"q_proj_weight\"], layer[\"k_proj_weight\"], layer[\"v_proj_weight\"]),\n",
    "                fused_qkv_bias=(layer[\"q_proj_bias\"], layer[\"k_proj_bias\"], layer[\"v_proj_bias\"]),\n",
    "                self_output_weight=layer[\"out_proj_weight\"],\n",
    "                self_output_bias=layer[\"out_proj_bias\"],\n",
    "                attention_mask=self.attention_mask,\n",
    "                prefix=f\"{self.prefix}.layers.{i}.attn\",\n",
    "            )  # Vision transformer doesn't use attention mask\n",
    "\n",
    "            x = residual + x_attn\n",
    "\n",
    "            # LayerNorm\n",
    "            x_post_ln_2 = ttnn.layer_norm(x, weight=layer[\"ln_2_weight\"], bias=layer[\"ln_2_bias\"])\n",
    "\n",
    "            # Multi-Layer Perceptron\n",
    "            x = x + mlp(x_post_ln_2, layer)\n",
    "\n",
    "            return x\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            layer = self.layers[i]\n",
    "            x = residual_attention_block(x, layer, i)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228fe5e8-5c22-4866-89ce-8651b464df2f",
   "metadata": {},
   "source": [
    "## Vision Transformer Implementation\n",
    "\n",
    "The VisionTransformer class handles image processing for CLIP. It converts input images into patch embeddings, adds positional encodings, and processes them through transformer layers.\n",
    "\n",
    "### Vision Processing Pipeline\n",
    "\n",
    "1. **Patch Embedding**: Converts 2D image into sequence of patch embeddings using convolution\n",
    "2. **Class Token**: Prepends a learnable classification token to the sequence\n",
    "3. **Positional Encoding**: Adds positional information to each patch\n",
    "4. **Transformer Layers**: Processes the sequence through multiple attention layers\n",
    "5. **Classification Head**: Extracts features from the class token for final representation\n",
    "\n",
    "The `forward()` method orchestrates this entire pipeline, preprocessing image embeddings and calling the generic transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe31cab8-b934-4da0-a02c-a2793ba3c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer:\n",
    "    def __init__(self, state_dict):\n",
    "        torch.manual_seed(0)\n",
    "        self.output_dim = 0\n",
    "\n",
    "        conv2_state_dict_name = \"vision_model.embeddings.patch_embedding.weight\"\n",
    "        self.vision_width = state_dict[conv2_state_dict_name].shape[0]\n",
    "        self.patch_size = state_dict[conv2_state_dict_name].shape[-1]\n",
    "        self.vision_heads = self.vision_width // 64\n",
    "\n",
    "        self.class_embedding = convert_ttnn_dtype(\n",
    "            state_dict[\"vision_model.embeddings.class_embedding\"], dtype=ttnn.bfloat16\n",
    "        )\n",
    "        self.positional_embedding = convert_ttnn_dtype(\n",
    "            state_dict[\"vision_model.embeddings.position_embedding.weight\"], dtype=ttnn.bfloat16\n",
    "        )\n",
    "\n",
    "        self.proj = convert_ttnn_dtype(state_dict[\"visual_projection.weight\"], dtype=ttnn.bfloat16)\n",
    "\n",
    "        # Weights for convolution layer\n",
    "        # For sharding; use all cores; strategy = block sharding\n",
    "        core_grid = ttnn.CoreGrid(x=8, y=8)\n",
    "        # Error: Physical shard shape (8216, 4) must be tile {32, 32} sized\n",
    "        # memory_config = ttnn.create_sharded_memory_config(conv1_weights_shape, core_grid, ttnn.ShardStrategy.HEIGHT)\n",
    "        memory_config = ttnn.DRAM_MEMORY_CONFIG\n",
    "        self.conv1_weights = ttnn.to_layout(\n",
    "            state_dict[conv2_state_dict_name],\n",
    "            layout=ttnn.ROW_MAJOR_LAYOUT,\n",
    "            memory_config=memory_config,\n",
    "            dtype=ttnn.bfloat16,\n",
    "        )\n",
    "        self.conv1_weights = convert_ttnn_dtype(self.conv1_weights, dtype=ttnn.bfloat16)\n",
    "\n",
    "        assert self.conv1_weights.dtype == ttnn.bfloat16\n",
    "\n",
    "        # Layer normalization applied before transformer layers\n",
    "        self.ln_pre_weights = state_dict[\"vision_model.pre_layrnorm.weight\"]\n",
    "        self.ln_pre_bias = state_dict[\"vision_model.pre_layrnorm.bias\"]\n",
    "\n",
    "        # Layer normalization applied after transformer layers (to class token)\n",
    "        self.ln_post_weights = state_dict[\"vision_model.post_layernorm.weight\"]\n",
    "        self.ln_post_bias = state_dict[\"vision_model.post_layernorm.bias\"]\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            state_dict, self.vision_heads, attention_mask=None, prefix=\"vision_model.encoder\"\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        (batch_size, in_channels, height, width) = x.shape\n",
    "\n",
    "        # Note: ttnn.conv2d uses 'Array of Struct' shape for input tensor:\n",
    "        # (N, H, W, C_in)\n",
    "        # whereas torch.nn.Conv2d uses 'Struct of Array' shape for input tensor:\n",
    "        # (N, C_in, H, W)\n",
    "        #\n",
    "        # # Moreover, ttnn.conv2d produces a flattened output tensor:\n",
    "        # (N, C_in, H, W) -> (1, 1, N * H * W, C_out)\n",
    "        # whereas torch.nn.Conv2d produces a 4D tensor:\n",
    "        # (N, C_out, H_out, W_out)\n",
    "\n",
    "        # Also:\n",
    "        # ttnn.conv2d only take a tuple for kernel_size and stride\n",
    "\n",
    "        # Change tensor layout to (N, H, W, C_in)\n",
    "        x = ttnn.permute(x, [0, 2, 3, 1])  # (N, C_in, H, W) -> (N, H, W, C_in)\n",
    "\n",
    "        # Note: ttnn.conv2d requires row-major layout for weight tensor\n",
    "        x = ttnn.to_layout(x, layout=ttnn.ROW_MAJOR_LAYOUT)\n",
    "\n",
    "        out_channels = 768\n",
    "\n",
    "        x = ttnn.conv2d(\n",
    "            input_tensor=x, \n",
    "            weight_tensor=self.conv1_weights,\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            batch_size=batch_size,\n",
    "            input_height=height,\n",
    "            input_width=width,\n",
    "            kernel_size=(self.patch_size, self.patch_size),\n",
    "            stride=(self.patch_size, self.patch_size),\n",
    "            padding=(0, 0),\n",
    "            dilation=(1, 1),\n",
    "            groups=0,  # No grouped convolution (standard convolution)\n",
    "            device=get_device(),\n",
    "            return_weights_and_bias=False,\n",
    "            return_output_dim=False,\n",
    "        )\n",
    "\n",
    "        # ERROR: Number of shards along height 7 must not exceed number of cores 2\n",
    "        output_height = height // self.patch_size\n",
    "        output_width = width // self.patch_size\n",
    "\n",
    "        # Check Convolution result\n",
    "        x = ttnn.to_layout(x, layout=ttnn.TILE_LAYOUT)\n",
    "        host_tensor = ttnn.to_torch(x, dtype=torch.float32)\n",
    "\n",
    "        host_tensor = torch.reshape(host_tensor, (batch_size, output_height, output_width, out_channels))\n",
    "\n",
    "        x = ttnn.reshape(x, (x.shape[0], x.shape[1] * x.shape[2], x.shape[3]))\n",
    "\n",
    "        class_embedding = convert_ttnn_dtype(self.class_embedding, x.dtype, (x.shape[0], 1, x.shape[-1]))\n",
    "\n",
    "        # Create zero tensor to ensure proper broadcasting and memory layout\n",
    "        # This helps align the class embedding tensor with the expected shape and memory configuration\n",
    "        zero_tensor = ttnn.zeros(\n",
    "            shape=(x.shape[0], 1, x.shape[-1]), dtype=x.dtype, device=device, layout=ttnn.TILE_LAYOUT\n",
    "        )\n",
    "\n",
    "        class_embedding = ttnn.reshape(class_embedding, zero_tensor.shape)\n",
    "        class_embedding = class_embedding + zero_tensor  # Addition with zero preserves values but ensures proper layout\n",
    "\n",
    "        # Move tensor to DRAM memory for concatenation operation\n",
    "        # Future optimization: Use L1 sharded memory for better performance\n",
    "        x = ttnn.to_memory_config(x, memory_config=ttnn.DRAM_MEMORY_CONFIG)\n",
    "\n",
    "        class_embedding = ttnn.reshape(\n",
    "            class_embedding, (class_embedding.shape[0], class_embedding.shape[1], class_embedding.shape[2])\n",
    "        )\n",
    "        class_embedding = ttnn.to_memory_config(class_embedding, memory_config=x.memory_config())\n",
    "\n",
    "        # Concatenate class embedding with patch embeddings\n",
    "        # Note: Future optimization could avoid host transfers for better performance\n",
    "        x = ttnn.concat([class_embedding, x], dim=1, memory_config=None)  # shape = [*, grid ** 2 + 1, width]\n",
    "\n",
    "        positional_embedding = convert_ttnn_dtype(self.positional_embedding, x.dtype, (1, x.shape[1], x.shape[2]))\n",
    "        x = x + positional_embedding\n",
    "\n",
    "        # LayerNorm\n",
    "        x = ttnn.layer_norm(x, weight=self.ln_pre_weights, bias=self.ln_pre_bias)\n",
    "\n",
    "        # Permute\n",
    "        x = ttnn.permute(x, (1, 0, 2))  # NLD -> LND\n",
    "\n",
    "        # Transformer\n",
    "        x = self.transformer.forward(x)\n",
    "\n",
    "        # Permute\n",
    "        x = ttnn.permute(x, (1, 0, 2))  # LND -> NLD\n",
    "\n",
    "        # LayerNorm\n",
    "        x = ttnn.layer_norm(x[:, 0, :], weight=self.ln_post_weights, bias=self.ln_post_bias)\n",
    "\n",
    "        if self.proj is not None:\n",
    "            x = ttnn.matmul(x, self.proj, transpose_b=True)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ea07a-dc5e-42cd-b214-cdcbfdcacf89",
   "metadata": {},
   "source": [
    "## Complete CLIP Model Implementation\n",
    "\n",
    "We now define the main CLIP class that combines both text and vision processing capabilities. This class orchestrates the entire multimodal inference pipeline.\n",
    "\n",
    "### CLIP Architecture Components\n",
    "\n",
    "The CLIP class instantiates and manages:\n",
    "- **Text Transformer**: Processes tokenized text inputs using causal attention masking\n",
    "- **Vision Transformer**: Processes image inputs through patch-based attention\n",
    "- **Shared Embedding Space**: Projects both modalities into a common feature space for comparison\n",
    "\n",
    "### Key Methods\n",
    "- `encode_text()`: Converts text tokens to feature embeddings\n",
    "- `encode_image()`: Converts images to feature embeddings  \n",
    "- `forward()`: Performs complete inference, computing similarity scores between images and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ce4f9-c677-4f38-9db3-f066c4da9752",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP:\n",
    "    def __init__(self, state_dict):\n",
    "        self.token_embedding = ttnn.typecast(\n",
    "            state_dict[\"text_model.embeddings.token_embedding.weight\"], dtype=ttnn.bfloat16\n",
    "        )\n",
    "        self.positional_embedding = ttnn.typecast(\n",
    "            state_dict[\"text_model.embeddings.position_embedding.weight\"], dtype=ttnn.bfloat16\n",
    "        )\n",
    "\n",
    "        self.text_projection = ttnn.typecast(state_dict[\"text_projection.weight\"], dtype=ttnn.bfloat16)\n",
    "        self.context_length = self.positional_embedding.shape[0]\n",
    "        self.vocab_size = self.token_embedding.shape[0]\n",
    "        self.transformer_width = state_dict[\"text_model.final_layer_norm.weight\"].shape[0]\n",
    "        transformer_heads = self.transformer_width // 64\n",
    "\n",
    "        self.ln_final_weights = state_dict[\"text_model.final_layer_norm.weight\"]\n",
    "        self.ln_final_bias = state_dict[\"text_model.final_layer_norm.bias\"]\n",
    "\n",
    "        self.logit_scale = state_dict[\"logit_scale\"].item()\n",
    "\n",
    "        self.visual = VisionTransformer(state_dict)\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            state_dict, transformer_heads, attention_mask=self.build_attention_mask(), prefix=\"text_model.encoder\"\n",
    "        )\n",
    "        \n",
    "    def build_attention_mask(self):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = ttnn.full(shape=[self.context_length, self.context_length], fill_value=float(\"-inf\"), dtype=ttnn.bfloat16, device=get_device(), layout=ttnn.TILE_LAYOUT)\n",
    "        mask = ttnn.triu(mask, diagonal=1)\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.visual.forward(image)\n",
    "\n",
    "    def encode_text(self, tokens):\n",
    "        tokens = convert_ttnn_dtype(tokens, dtype=ttnn.uint32)\n",
    "\n",
    "        x = ttnn.embedding(tokens, weight=self.token_embedding, dtype=ttnn.bfloat16)\n",
    "\n",
    "        # Add positional embedding\n",
    "        x = x + self.positional_embedding\n",
    "\n",
    "        # Permute\n",
    "        x = ttnn.permute(x, (1, 0, 2))  # NLD -> LND\n",
    "\n",
    "        # Call Text Transformer\n",
    "        x = self.transformer.forward(x) \n",
    "\n",
    "        # Permute back\n",
    "        x = ttnn.permute(x, (1, 0, 2))  # LND -> NLD\n",
    "\n",
    "        # LayerNorm\n",
    "        x = ttnn.layer_norm(x, weight=self.ln_final_weights, bias=self.ln_final_bias)\n",
    "\n",
    "        # Extract features at the end-of-sequence token position and apply text projection\n",
    "        # Currently falling back to PyTorch for argmax operation\n",
    "        torch_tokens = ttnn.to_torch(tokens)\n",
    "        torch_x = ttnn.to_torch(x)\n",
    "\n",
    "        torch_selected_features = torch_x[torch.arange(torch_x.shape[0]), torch_tokens.argmax(dim=-1)]\n",
    "        \n",
    "        # Put tensor back on device for text projection\n",
    "        x = ttnn.from_torch(torch_selected_features, device=get_device(), layout=ttnn.TILE_LAYOUT)\n",
    "        x = ttnn.matmul(x, self.text_projection, transpose_b=True)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def forward(self, image, tokens):\n",
    "        text_features = self.encode_text(tokens)\n",
    "        image_features = self.encode_image(image)\n",
    "\n",
    "        # Normalize features\n",
    "        norm_image_features = ttnn.operations.moreh.norm(image_features, p=2.0, dim=1, keepdim=True)\n",
    "        norm_text_features = ttnn.operations.moreh.norm(text_features, p=2.0, dim=1, keepdim=True)\n",
    "\n",
    "        image_features = ttnn.divide(image_features, norm_image_features)\n",
    "        text_features = ttnn.divide(text_features, norm_text_features)\n",
    "\n",
    "        # Cosine similarity as logits\n",
    "        logit_scale = math.exp(self.logit_scale)\n",
    "\n",
    "        # Compute `logit_scale * image_features @ text_features.t()`\n",
    "        logits_per_image = ttnn.matmul(logit_scale * image_features, text_features, transpose_b=True)\n",
    "        logits_per_text = ttnn.transpose(logits_per_image, 0, 1)\n",
    "\n",
    "        return logits_per_image, logits_per_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940036cb-a7e8-43ce-9716-36a84d362d2e",
   "metadata": {},
   "source": [
    "## Image Preprocessing\n",
    "\n",
    "While input images can have any dimensions and color spaces, our CLIP model expects standardized 224×224 RGB images. We therefore preprocess images to match the model's expected input format.\n",
    "\n",
    "### Preprocessing Pipeline\n",
    "\n",
    "The preprocessing applies the following transformations in sequence:\n",
    "1. **Resize**: Scale image to 224×224 pixels using bicubic interpolation\n",
    "2. **Center Crop**: Crop the center region to ensure exact dimensions\n",
    "3. **RGB Conversion**: Convert to RGB color space if needed\n",
    "4. **Normalization**: Apply ImageNet normalization statistics used during CLIP training\n",
    "\n",
    "This preprocessing ensures consistent input format regardless of the original image properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e08e26c-5fe8-4162-9cf7-37e2c5ce24f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, model_resolution):\n",
    "    def _convert_image_to_rgb(image):\n",
    "        return image.convert(\"RGB\")\n",
    "\n",
    "    # Pre-process image on host with torch\n",
    "    transform_fn = Compose(\n",
    "        [\n",
    "            Resize(model_resolution, interpolation=InterpolationMode.BICUBIC),\n",
    "            CenterCrop(model_resolution),\n",
    "            _convert_image_to_rgb,\n",
    "            ToTensor(),\n",
    "            Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ]\n",
    "    )\n",
    "    return transform_fn(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be67cd47-2f80-490b-98a1-aa6a7ba84832",
   "metadata": {},
   "source": [
    "## Image Download Utility\n",
    "\n",
    "We use a utility function to download images from URLs for demonstration purposes. This function handles HTTP requests and converts the response into a PIL Image object that can be processed by our preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8108429-2409-4a3c-9a77-c0adbb4537ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url):\n",
    "    \"\"\"\n",
    "    Download an image from a URL and return it as a PIL Image object.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the image to download\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: The downloaded image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        \n",
    "        # Convert the response content to a PIL Image\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        return image\n",
    "    except requests.RequestException as e:\n",
    "        raise Exception(f\"Failed to download image from {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to process downloaded image: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa56ff5-8698-4705-8aee-32f81f61156b",
   "metadata": {},
   "source": [
    "## Running CLIP Inference\n",
    "\n",
    "Having defined each component of our CLIP model, we can now perform inference on an input image and text prompts. This section demonstrates the complete inference pipeline from loading pre-trained weights to computing similarity scores.\n",
    "\n",
    "### Inference Pipeline\n",
    "\n",
    "1. **Model Loading**: Download pre-trained CLIP weights using `CLIPModel.from_pretrained()`\n",
    "2. **Weight Conversion**: Convert PyTorch weights to TT-NN\n",
    "3. **Image Processing**: Download, preprocess, and convert image to TT-NN tensor\n",
    "4. **Text Processing**: Tokenize text prompts and convert to TT-NN tensors\n",
    "5. **Forward Pass**: Compute image and text embeddings, then calculate similarity scores\n",
    "6. **Results**: Apply softmax to get probability distribution over text prompts\n",
    "\n",
    "### Text Tokenization\n",
    "\n",
    "Since TT-NN does not handle tokenization natively, we use the `CLIPTokenizer` from the `transformers` library. The tokenizer converts text strings into token IDs that match the vocabulary used during CLIP training. We then convert these token tensors to TT-NN format for processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64691ea0-cbf8-452c-b9d3-4b57157a480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize TT-NN device for hardware acceleration\n",
    "    open_ttnn()\n",
    "\n",
    "    # Set up logging for debugging (optional)\n",
    "    logging_file = open(\"logging.csv\", \"w\")\n",
    "\n",
    "    # Load pre-trained CLIP model and convert weights to TT-NN format\n",
    "    print(\"Loading pre-trained CLIP model...\")\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    state_dict = convert_model_to_ttnn(model.state_dict())\n",
    "\n",
    "    # Initialize our TT-NN CLIP implementation\n",
    "    clip = CLIP(state_dict)\n",
    "\n",
    "    # Download and preprocess test image\n",
    "    print(\"Downloading and preprocessing image...\")\n",
    "    image_url = \"https://media.githubusercontent.com/media/tenstorrent/tutorial-assets/nmaurice/clip-tutorial/media/clip_tutorial/CLIP.png\"\n",
    "    image = download_image(image_url)\n",
    "\n",
    "    # Preprocess image to model requirements (224x224, normalized)\n",
    "    image = preprocess_image(image, 224).unsqueeze(0).to(\"cpu\")\n",
    "\n",
    "    # Convert image to TT-NN tensor with bfloat16 precision\n",
    "    preferred_dtype = ttnn.bfloat16\n",
    "    tt_image = to_ttnn(image, preferred_dtype)\n",
    "\n",
    "    # Define text prompts for zero-shot classification\n",
    "    prompts = [\"a diagram\", \"a dog\", \"a cat\"]\n",
    "\n",
    "    # Tokenize text prompts using CLIP's tokenizer\n",
    "    print(\"Tokenizing text prompts...\")\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    tokenized_inputs = tokenizer(prompts, padding=\"max_length\", max_length=clip.context_length, return_tensors=\"pt\")\n",
    "    tokens_pretrained_host = tokenized_inputs[\"input_ids\"]\n",
    "    tokens_pretrained = ttnn.from_torch(tokens_pretrained_host, device=get_device(), layout=ttnn.TILE_LAYOUT)\n",
    "\n",
    "    # Perform CLIP inference: compute similarity between image and text\n",
    "    print(\"Running CLIP inference...\")\n",
    "    time_start = time.time()\n",
    "    logits_per_image, logits_per_text = clip.forward(tt_image, tokens_pretrained)\n",
    "    time_end = time.time()\n",
    "    print(f\"Time taken: {time_end - time_start} seconds\")\n",
    "    \n",
    "    # Convert logits to probabilities using softmax\n",
    "    probs = ttnn.softmax(logits_per_image, dim=-1)\n",
    "    print(f\"==== Classification probabilities:\")\n",
    "    \n",
    "    # Display results\n",
    "    probs_torch = ttnn.to_torch(probs)\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(f\"'{prompt}': {probs_torch[0][i].item():.4f}\")\n",
    "\n",
    "    # Clean up resources\n",
    "    logging_file.close()\n",
    "    close_ttnn()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
