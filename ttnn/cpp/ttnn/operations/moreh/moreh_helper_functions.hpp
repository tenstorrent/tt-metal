/*
 * SPDX-FileCopyrightText: Â© 2023 Tenstorrent Inc.
 *
 * SPDX-License-Identifier: Apache-2.0
 */

#pragma once

#include <optional>
#include <variant>

#include <tt-metalium/host_api.hpp>
#include "ttnn/tensor/tensor.hpp"

namespace ttnn {
namespace operations {

using namespace tt::tt_metal;

inline bool is_dram(const Tensor& tensor) { return tensor.memory_config().buffer_type() == BufferType::DRAM; }
inline bool is_dram(const std::optional<const Tensor> tensor) {
    return tensor.has_value() ? is_dram(tensor.value()) : true;
}
inline bool is_dram(const std::optional<std::reference_wrapper<const Tensor>> tensor) {
    return tensor.has_value() ? is_dram(tensor->get()) : true;
}
inline bool is_dram(const Buffer* buffer) { return buffer->buffer_type() == BufferType::DRAM; }

inline bool is_scalar(const Tensor& tensor) {
    // TODO(dongjin): current impl requires finding a scalar in a 2d shape
    const auto& shape = tensor.get_logical_shape();
    const uint32_t rank = shape.rank();

    // TODO(dongjin): refactor dot op
    if (rank == 4) {
        return (shape[0] == 1 && shape[1] == 1 && shape[2] == 1 && shape[3] == 1);
    }
    return (rank == 2 && shape[-1] == 1 && shape[-2] == 1);
}

inline bool is_1d_tensor(const Tensor& tensor) {
    // TODO(dongjin): current impl requires finding a 1d in a 2d shape
    const auto& shape = tensor.get_logical_shape();
    const uint32_t rank = shape.rank();

    // TODO(dongjin): refactor dot op
    if (rank == 4) {
        return (shape[0] == 1 && shape[1] == 1 && shape[2] == 1);
    }
    return (rank == 2 && shape[-2] == 1);
}

inline bool is_same_shape(const Tensor& tensor_a, const Tensor& tensor_b) {
    const auto& tensor_a_shape = tensor_a.get_logical_shape();
    const auto& tensor_b_shape = tensor_b.get_logical_shape();
    return (tensor_a_shape == tensor_b_shape);
}

std::tuple<CoreRangeSet, CoreRangeSet, CoreRangeSet> add_core_offset(
    const CoreRangeSet& all_cores,
    const CoreRangeSet& core_group_1,
    const CoreRangeSet& core_group_2,
    uint32_t offset_x,
    uint32_t offset_y);

std::tuple<uint32_t, CoreRangeSet, CoreRangeSet, CoreRangeSet, uint32_t, uint32_t> split_work_to_cores_wt_core_range(
    CoreRange core_range, uint32_t units_to_divide);

[[maybe_unused]] KernelHandle CreateReadKernel(
    Program& program,
    const std::string& file_name,
    const std::variant<CoreCoord, CoreRange, CoreRangeSet>& core_spec,
    const std::vector<uint32_t>& compile_args = {},
    std::map<string, string> defines = {});

[[maybe_unused]] KernelHandle CreateWriteKernel(
    Program& program,
    const std::string& file_name,
    const std::variant<CoreCoord, CoreRange, CoreRangeSet>& core_spec,
    const std::vector<uint32_t>& compile_args = {},
    std::map<string, string> defines = {});

struct ComputeKernelArg {
    const std::variant<CoreCoord, CoreRange, CoreRangeSet>& core_spec;
    uint32_t num_tile_per_core_group;
    const std::vector<uint32_t>& compile_args = {};
};

struct ComputeKernelConfig {
    MathFidelity math_fidelity = MathFidelity::HiFi4;
    bool fp32_dest_acc_en = false;
    std::vector<UnpackToDestMode> unpack_to_dest_mode;
    bool math_approx_mode = false;
    std::map<std::string, std::string> defines;
};

[[maybe_unused]] std::vector<KernelHandle> CreateComputeKernel(
    Program& program,
    const std::string& file_name,
    const std::vector<ComputeKernelArg>& args,
    const std::map<std::string, std::string>& defines = {},
    MathFidelity math_fidelity = MathFidelity::HiFi4,
    bool fp32_dest_acc_en = false,
    bool math_approx_mode = false,
    const std::vector<UnpackToDestMode>& unpack_to_dest_mode = {});

[[maybe_unused]] KernelHandle CreateComputeKernel(
    Program& program,
    const std::string& file_name,
    ComputeKernelArg arg,
    std::map<std::string, std::string> defines = {},
    MathFidelity math_fidelity = MathFidelity::HiFi4,
    bool fp32_dest_acc_en = false,
    bool math_approx_mode = false,
    std::vector<UnpackToDestMode> unpack_to_dest_mode = {});

[[maybe_unused]] std::vector<KernelHandle> CreateComputeKernel(
    Program& program,
    const std::string& file_name,
    const std::vector<ComputeKernelArg>& args,
    const ComputeKernelConfig& config);

[[maybe_unused]] KernelHandle CreateComputeKernel(
    Program& program, const std::string& file_name, ComputeKernelArg arg, const ComputeKernelConfig& config);

struct CircularBufferArg {
    uint32_t buffer_index;
    uint32_t num_tiles;
    tt::DataFormat data_format;
    std::optional<std::variant<CoreCoord, CoreRange, CoreRangeSet>> core_range = std::nullopt;

    CircularBufferArg(uint32_t buffer_index, uint32_t num_tiles) : buffer_index(buffer_index), num_tiles(num_tiles) {
        data_format = tt::DataFormat::Invalid;
    }
    CircularBufferArg(uint32_t buffer_index, uint32_t num_tiles, tt::DataFormat data_format) :
        buffer_index(buffer_index), num_tiles(num_tiles), data_format(data_format) {}
};

[[maybe_unused]] std::vector<CBHandle> CreateCircularBuffer(
    Program& program,
    const std::variant<CoreCoord, CoreRange, CoreRangeSet>& core_range,
    tt::DataFormat data_format,
    const std::vector<CircularBufferArg>& args);

[[maybe_unused]] CBHandle CreateCircularBuffer(
    Program& program,
    const std::variant<CoreCoord, CoreRange, CoreRangeSet>& core_range,
    tt::DataFormat data_format,
    const CircularBufferArg& arg);

void check_tensor(
    const Tensor& tensor,
    const std::string& op_name,
    const std::string& tensor_name,
    const std::initializer_list<DataType>& data_types = {DataType::BFLOAT16},
    Layout layout = Layout::TILE,
    bool check_dtype = true,
    bool check_layout = true);

void check_tensor(
    std::optional<Tensor> tensor,
    const std::string& op_name,
    const std::string& tensor_name,
    const std::initializer_list<DataType>& data_types = {DataType::BFLOAT16},
    Layout layout = Layout::TILE,
    bool check_dtype = true,
    bool check_layout = true);

struct CallbackArgMap {
    std::map<uint32_t, uint32_t> input;
    std::map<uint32_t, uint32_t> optional_input;
    std::map<uint32_t, uint32_t> output;
};

using Tensors = std::vector<Tensor>;
using OptionalConstTensors = std::vector<std::optional<const Tensor>>;

// To use this function, the arguments in the reader kernel must always be sorted in the order of input followed by
// optional_input. Furthermore, input and output tensors must always start from the 0th argument.
template <typename OutputTensors = Tensors>
auto create_override_runtime_arguments_callback(
    KernelHandle reader_kernel_id, KernelHandle writer_kernel_id, uint32_t num_cores, uint32_t core_h) {
    return [reader_kernel_id = reader_kernel_id, writer_kernel_id = writer_kernel_id, num_cores, core_h](
               const void* operation,
               Program& program,
               const Tensors& input_tensors,
               const OptionalConstTensors& optional_input_tensors,
               const OutputTensors& output_tensors) -> void {
        for (uint32_t icore = 0; icore < num_cores; icore++) {
            CoreCoord core = {icore / core_h, icore % core_h};

            // readers
            {
                uint32_t rt_idx = 0;
                auto& runtime_args = GetRuntimeArgs(program, reader_kernel_id, core);
                for (uint32_t idx = 0; idx < input_tensors.size(); idx++) {
                    runtime_args[rt_idx++] = input_tensors.at(idx).buffer()->address();
                }
                for (uint32_t idx = 0; idx < optional_input_tensors.size(); idx++) {
                    auto optional_input_tensor = optional_input_tensors.at(idx);
                    runtime_args[rt_idx++] =
                        optional_input_tensor.has_value() ? optional_input_tensor.value().buffer()->address() : 0;
                }
            }

            // writer
            {
                auto& runtime_args = GetRuntimeArgs(program, writer_kernel_id, core);
                for (uint32_t idx = 0; idx < output_tensors.size(); idx++) {
                    runtime_args[idx] = output_tensors.at(idx).buffer()->address();
                }
            }
        }
    };
}

// Using this structure is not recommended because directly setting the callback argument map doesn't significantly
// reduce the amount of code.
template <typename OutputTensors = Tensors>
auto create_override_runtime_arguments_callback(
    KernelHandle reader_kernel_id,
    KernelHandle writer_kernel_id,
    uint32_t num_cores,
    uint32_t core_h,
    CallbackArgMap arg_map) {
    return [reader_kernel_id = reader_kernel_id, writer_kernel_id = writer_kernel_id, arg_map, num_cores, core_h](
               const void* operation,
               Program& program,
               const Tensors& input_tensors,
               const OptionalConstTensors& optional_input_tensors,
               const OutputTensors& output_tensors) -> void {
        for (uint32_t icore = 0; icore < num_cores; icore++) {
            CoreCoord core = {icore / core_h, icore % core_h};

            // readers
            {
                auto& runtime_args = GetRuntimeArgs(program, reader_kernel_id, core);
                for (const auto& pair : arg_map.input) {
                    runtime_args[pair.first] = input_tensors.at(pair.second).buffer()->address();
                }
                for (const auto& pair : arg_map.optional_input) {
                    auto optional_input_tensor = optional_input_tensors.at(pair.second);
                    runtime_args[pair.first] =
                        optional_input_tensor.has_value() ? optional_input_tensor.value().buffer()->address() : 0;
                }
            }

            // writer
            {
                auto& runtime_args = GetRuntimeArgs(program, writer_kernel_id, core);
                for (const auto& pair : arg_map.output) {
                    runtime_args[pair.first] = output_tensors.at(pair.second).buffer()->address();
                }
            }
        }
    };
}

bool is_hw_dim(uint32_t dim, uint32_t rank);

uint32_t compute_inner(const ttnn::Shape& shape, uint32_t dim);

uint32_t compute_outer(const ttnn::Shape& shape, uint32_t dim);

void expand_to_max_dim(ttnn::SmallVector<uint32_t>& dim, const ttnn::Shape& shape);

void validate_input_with_dim(const Tensor& input, const int64_t& dim);

void validate_output_with_keepdim(const Tensor& input, const Tensor& output, const int64_t& dim, const bool& keepdim);

void initialize_dims_with_range(ttnn::SmallVector<int64_t>& dims, uint32_t input_rank);

ttnn::SmallVector<int64_t> get_dim(
    const std::optional<std::variant<int64_t, ttnn::SmallVector<int64_t>>>& dim, uint32_t input_rank);

std::tuple<uint32_t, uint32_t, uint32_t> extract_spatial_dims(const ttnn::Shape& shape);

std::tuple<uint32_t, uint32_t, uint32_t, uint32_t> extract_and_scale_spatial_dims(
    const ttnn::Shape& shape, uint32_t dim);

}  // namespace operations
}  // namespace ttnn
