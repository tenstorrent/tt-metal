// SPDX-FileCopyrightText: Â© 2024 Tenstorrent Inc.
//
// SPDX-License-Identifier: Apache-2.0

#include "nlp_create_qkv_heads.hpp"

#include <utility>

namespace ttnn::operations::experimental::transformer {

std::tuple<ttnn::Tensor, ttnn::Tensor, ttnn::Tensor> NlpCreateHeadsOperation::invoke(
    QueueId queue_id,
    const Tensor& input_tensor_q,
    const std::optional<Tensor>& input_tensor_kv,
    const uint32_t num_q_heads,
    const std::optional<uint32_t> num_kv_heads,
    const bool transpose_k_heads,
    const std::optional<MemoryConfig>& memory_config,
    std::optional<std::vector<std::optional<Tensor>>> optional_output_tensors) {
    const uint32_t num_kv_heads_val = num_kv_heads.value_or(num_q_heads);
    uint32_t head_dim;
    if (input_tensor_kv.has_value()) {
        TT_FATAL(input_tensor_q.padded_shape()[3] % num_q_heads == 0, "Unsupported input shape");
        TT_FATAL(input_tensor_kv.value().padded_shape()[3] % (2 * num_kv_heads_val) == 0, "Unsupported input shape");
        head_dim = input_tensor_q.padded_shape()[3] / num_q_heads;
        TT_FATAL(
            input_tensor_kv.value().padded_shape()[3] / (2 * num_kv_heads_val) == head_dim,
            "Head dims must be the same for Q and K, V");
    } else {
        TT_FATAL(
            input_tensor_q.padded_shape()[3] % (num_q_heads + 2 * num_kv_heads_val) == 0, "Unsupported input shape");
        head_dim = input_tensor_q.padded_shape()[3] / (num_q_heads + 2 * num_kv_heads_val);
    }

    return ttnn::prim::nlp_create_qkv_heads(
        queue_id,
        input_tensor_q,
        input_tensor_kv,
        num_q_heads,
        num_kv_heads,
        head_dim,
        transpose_k_heads,
        memory_config,
        optional_output_tensors);
};

std::tuple<ttnn::Tensor, ttnn::Tensor, ttnn::Tensor> NlpCreateHeadsOperation::invoke(
    const Tensor& input_tensor_q,
    const std::optional<Tensor>& input_tensor_kv,
    const uint32_t num_q_heads,
    const std::optional<uint32_t> num_kv_heads,
    const bool transpose_k_heads,
    const std::optional<MemoryConfig>& memory_config,
    std::optional<std::vector<std::optional<ttnn::Tensor>>> optional_output_tensors) {
    return invoke(
        ttnn::DefaultQueueId,
        input_tensor_q,
        input_tensor_kv,
        num_q_heads,
        num_kv_heads,
        transpose_k_heads,
        memory_config,
        std::move(optional_output_tensors));
};

}  // namespace ttnn::operations::experimental::transformer
