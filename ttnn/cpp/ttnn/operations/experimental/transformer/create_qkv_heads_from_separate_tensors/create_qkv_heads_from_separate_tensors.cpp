// SPDX-FileCopyrightText: Â© 2024 Tenstorrent Inc.
//
// SPDX-License-Identifier: Apache-2.0

#include "create_qkv_heads_from_separate_tensors.hpp"

#include <utility>
#include "device/create_qkv_heads_from_separate_tensors_device_operation.hpp"
#include "ttnn/run_operation.hpp"
#include "ttnn/operations/core/core.hpp"
#include "ttnn/common/queue_id.hpp"

namespace ttnn::operations::experimental::transformer {

std::tuple<ttnn::Tensor, ttnn::Tensor, ttnn::Tensor> CreateQKVHeadsSeparateTensorsOperation::invoke(
    QueueId queue_id,
    const Tensor& input_tensor_q,
    const Tensor& input_tensor_kv,
    const uint32_t num_q_heads,
    const std::optional<uint32_t> num_kv_heads,
    const bool transpose_k_heads,
    const std::optional<MemoryConfig>& memory_config,
    std::optional<std::array<Tensor, 3>> optional_output_tensors) {
    const uint32_t num_kv_heads_val = num_kv_heads.value_or(num_q_heads);
    TT_FATAL(
        input_tensor_q.padded_shape()[3] % num_q_heads == 0,
        "Flattened Q hidden dimension {} must be a multiple of Q heads {}",
        input_tensor_q.padded_shape()[3],
        num_q_heads);
    const uint32_t head_dim = input_tensor_q.padded_shape()[3] / (num_q_heads);
    auto optional_outputs = std::vector<std::optional<Tensor>>{};
    if (optional_output_tensors.has_value()) {
        optional_outputs = {optional_output_tensors.value().begin(), optional_output_tensors.value().end()};
    } else {
        optional_outputs = {};
    }
    auto output_tensors = tt::tt_metal::operation::run(
        CreateQKVHeadsSeparateTensorsDeviceOperation{
            num_q_heads,
            num_kv_heads_val,
            head_dim,
            transpose_k_heads,
            memory_config.value_or(input_tensor_q.memory_config())},
        {input_tensor_q, input_tensor_kv},
        {},
        optional_outputs,
        queue_id);
    return {output_tensors.at(0), output_tensors.at(1), output_tensors.at(2)};
}

std::tuple<ttnn::Tensor, ttnn::Tensor, ttnn::Tensor> CreateQKVHeadsSeparateTensorsOperation::invoke(
    const Tensor& input_tensor_q,
    const Tensor& input_tensor_kv,
    const uint32_t num_q_heads,
    const std::optional<uint32_t> num_kv_heads,
    const bool transpose_k_heads,
    const std::optional<MemoryConfig>& memory_config,
    std::optional<std::array<Tensor, 3>> optional_output_tensors) {
    return invoke(
        ttnn::DefaultQueueId,
        input_tensor_q,
        input_tensor_kv,
        num_q_heads,
        num_kv_heads,
        transpose_k_heads,
        memory_config,
        std::move(optional_output_tensors));
}

}  // namespace ttnn::operations::experimental::transformer
