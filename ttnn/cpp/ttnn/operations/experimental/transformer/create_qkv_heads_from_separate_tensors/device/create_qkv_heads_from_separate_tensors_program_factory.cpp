// SPDX-FileCopyrightText: Â© 2024 Tenstorrent Inc.
//
// SPDX-License-Identifier: Apache-2.0

#include "create_qkv_heads_from_separate_tensors_device_operation.hpp"
#include <tt-metalium/host_api.hpp>
#include <tt-metalium/constants.hpp>
#include <tt-metalium/util.hpp>

namespace ttnn::operations::experimental::transformer {

using namespace tt::constants;
using namespace tt;

static inline tt::tt_metal::operation::ProgramWithCallbacks create_qkv_separate(
    const Tensor& input_tensor_q,
    const Tensor& input_tensor_kv,
    const uint32_t num_q_heads,
    const uint32_t num_kv_heads,
    const uint32_t head_dim,
    std::vector<Tensor>& output,
    bool transpose_k) {
    const auto& q_shape = input_tensor_q.padded_shape();
    const auto& kv_shape = input_tensor_kv.padded_shape();
    auto shard_spec = input_tensor_q.shard_spec().value();
    auto all_cores = shard_spec.grid;
    auto bbox = all_cores.bounding_box();
    ShardOrientation shard_orientation = shard_spec.orientation;
    bool rm = shard_orientation == ShardOrientation::ROW_MAJOR;
    uint32_t num_h_cores = rm ? bbox.end_coord.y + 1 : bbox.end_coord.x + 1;
    uint32_t num_w_cores = rm ? bbox.end_coord.x + 1 : bbox.end_coord.y + 1;

    uint32_t q_shard_wt =
        (q_shape[3]) /
        (num_w_cores * TILE_WIDTH);  // number of tiles in width dimension  - multiple tiles per head, multiple heads
                                     // per group, multiple tensors in group, multiple groups per cores
    uint32_t q_shard_ht = (q_shape[0] * q_shape[2]) / (num_h_cores * TILE_HEIGHT);

    uint32_t k_shard_wt = (kv_shape[3] / (2 * num_w_cores * TILE_WIDTH));
    uint32_t k_shard_ht = (kv_shape[0] * kv_shape[2]) / (num_h_cores * TILE_HEIGHT);

    uint32_t per_core_q_tiles = q_shard_ht * q_shard_wt;
    uint32_t per_core_k_tiles = k_shard_ht * k_shard_wt;

    const auto q_data_format = tt_metal::datatype_to_dataformat_converter(input_tensor_q.dtype());
    const auto kv_data_format = tt_metal::datatype_to_dataformat_converter(input_tensor_kv.dtype());
    uint32_t single_tile_size = tile_size(q_data_format);

    uint32_t q_heads_per_core = num_q_heads / num_w_cores;
    uint32_t k_heads_per_core = num_kv_heads / num_w_cores;

    Program program = tt::tt_metal::CreateProgram();
    std::vector<uint32_t> reader_compile_time_args = {
        (std::uint32_t)q_shard_ht,
        (std::uint32_t)q_shard_wt,
        (std::uint32_t)k_shard_ht,
        (std::uint32_t)k_shard_wt,  // shard width for k and v individually, times two for entire kv tensor
        (std::uint32_t)q_heads_per_core,
        (std::uint32_t)k_heads_per_core,
        (std::uint32_t)head_dim / TILE_WIDTH,  // tiles per head
    };

    std::map<string, string> reader_defines;
    if (transpose_k) {
        reader_defines["TRANSPOSE_K_HEADS"] = "1";
    }
    auto reader_kernel_id = tt_metal::CreateKernel(
        program,
        "ttnn/cpp/ttnn/operations/experimental/transformer/create_qkv_heads_from_separate_tensors/device/kernels/"
        "reader_create_qkv_heads_sharded_separate.cpp",
        all_cores,
        tt_metal::ReaderDataMovementConfig(reader_compile_time_args, reader_defines));

    if (transpose_k) {
        std::vector<uint32_t> compute_args = {
            (std::uint32_t)(per_core_k_tiles),  // number of K tiles
        };
        auto compute_kernel_id = tt_metal::CreateKernel(
            program,
            "ttnn/cpp/ttnn/operations/experimental/transformer/split_query_key_value_and_split_heads/device/kernels/"
            "compute/transpose_wh_sharded.cpp",
            all_cores,
            tt_metal::ComputeConfig{.compile_args = compute_args});
    }

    uint32_t q_size = per_core_q_tiles * single_tile_size;
    uint32_t k_size = per_core_k_tiles * single_tile_size;
    uint32_t v_size = k_size;
    uint32_t kv_size = 2 * k_size;

    // qkv tensor
    auto c_in0_config = tt::tt_metal::CircularBufferConfig(q_size, {{CBIndex::c_0, q_data_format}})
                            .set_page_size(CBIndex::c_0, single_tile_size)
                            .set_globally_allocated_address(*input_tensor_q.buffer());
    auto cb_in0_id = CreateCircularBuffer(program, all_cores, c_in0_config);

    auto c_in1_config = tt::tt_metal::CircularBufferConfig(kv_size, {{CBIndex::c_1, kv_data_format}})
                            .set_page_size(CBIndex::c_1, single_tile_size)
                            .set_globally_allocated_address(*input_tensor_kv.buffer());
    auto cb_in1_id = CreateCircularBuffer(program, all_cores, c_in1_config);

    // q sharded
    auto c_out0_config = tt::tt_metal::CircularBufferConfig(q_size, {{CBIndex::c_16, q_data_format}})
                             .set_page_size(CBIndex::c_16, single_tile_size)
                             .set_globally_allocated_address(*output[0].buffer());
    auto cb_out0_id = CreateCircularBuffer(program, all_cores, c_out0_config);
    // k sharded
    auto c_out1_config = tt::tt_metal::CircularBufferConfig(k_size, {{CBIndex::c_17, kv_data_format}})
                             .set_page_size(CBIndex::c_17, single_tile_size)
                             .set_globally_allocated_address(*output[1].buffer());
    auto cb_out1_id = CreateCircularBuffer(program, all_cores, c_out1_config);
    // v sharded
    auto c_out2_config = tt::tt_metal::CircularBufferConfig(v_size, {{CBIndex::c_18, kv_data_format}})
                             .set_page_size(CBIndex::c_18, single_tile_size)
                             .set_globally_allocated_address(*output[2].buffer());
    auto cb_out2_id = CreateCircularBuffer(program, all_cores, c_out2_config);

    if (transpose_k) {
        auto c_im0_config = tt::tt_metal::CircularBufferConfig(k_size, {{CBIndex::c_24, kv_data_format}})
                                .set_page_size(CBIndex::c_24, single_tile_size);
        auto cb_im0_id = CreateCircularBuffer(program, all_cores, c_im0_config);
    }

    auto override_runtime_args_callback = [cb_in0_id, cb_in1_id, cb_out0_id, cb_out1_id, cb_out2_id](
                                              const void* operation,
                                              Program& program,
                                              const std::vector<Tensor>& input_tensors,
                                              const std::vector<std::optional<const Tensor>>& optional_input_tensors,
                                              const std::vector<Tensor>& output_tensors) {
        auto in0_buffer = input_tensors.at(0).buffer();
        auto in1_buffer = input_tensors.at(1).buffer();
        auto out0_buffer = output_tensors.at(0).buffer();
        auto out1_buffer = output_tensors.at(1).buffer();
        auto out2_buffer = output_tensors.at(2).buffer();

        UpdateDynamicCircularBufferAddress(program, cb_in0_id, *in0_buffer);
        UpdateDynamicCircularBufferAddress(program, cb_in1_id, *in1_buffer);
        UpdateDynamicCircularBufferAddress(program, cb_out0_id, *out0_buffer);
        UpdateDynamicCircularBufferAddress(program, cb_out1_id, *out1_buffer);
        UpdateDynamicCircularBufferAddress(program, cb_out2_id, *out2_buffer);
    };

    return {.program = std::move(program), .override_runtime_arguments_callback = override_runtime_args_callback};
}

/**
 * Combined QKV
 *
 * m = num_KV_heads
 * p = num_Q_heads
 * n = p/m
 * i = head index for K/V, corresponding Q group for K/V
 *
 * input: [nQi Ki Vi for i in [0, m)] repeated for the sequence length
 * dims: [B, 1, S, H] Hi = [nQi Ki Vi] for all i kv heads = ((n + 2)*head_dim)
 *
 * output: 3 separate tensors organized by heads
 * [Qs,j for j in [0, p] and s in [0,S)]
 * dims: [B, p, S, head_dim] (all nQi in the KVi group are stacked together, p = n*m)
 *
 * [Ks,i for i in [0, m) and s in [0,S)]
 * dims: [B, m, S, head_dim]
 *
 * [Vs,i for i in [0, m) and s in [0,S)]
 * dims: [B, m, S, head_dim]
 *
 * Tiles stay the same Vi,s[x:x+32] to Vi+32,s[x:x+32] stays in one tile, but now instead of nQi,s and Ki,s tiles there
 * is Vi,s-1 and Vi,s+1
 *
 * Shard across each i kv head group (width sharding) and then shard across each token s (height sharding)
 * Each block: B x [nQi Ki Vi]s (shard across flattened heads and sequence length)
 *
 * Combined batch/sequence sharding is possible too...that may best be left as an extension
 */
tt::tt_metal::operation::ProgramWithCallbacks multi_core_create_q_and_kv_heads_sharded(
    const Tensor& input_tensor_q,
    const Tensor& input_tensor_kv,
    const uint32_t num_q_heads,
    const uint32_t num_kv_heads,
    const uint32_t head_dim,
    const bool transpose_k_heads,
    std::vector<Tensor>& output,
    CoreCoord compute_with_storage_grid_size) {
    return create_qkv_separate(
        input_tensor_q, input_tensor_kv, num_q_heads, num_kv_heads, head_dim, output, transpose_k_heads);
}
}  // namespace ttnn::operations::experimental::transformer
