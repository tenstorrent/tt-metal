// SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC.
// SPDX-License-Identifier: Apache-2.0

#include "argmax_device_operation.hpp"
#include "ttnn/tensor/tensor_ops.hpp"
#include "ttnn/device_operation.hpp"
#include "argmax_utils.hpp"

using namespace tt::tt_metal;

namespace ttnn::operations::reduction::argmax {

/*
 * Generates the output shape for the reduction operation.
 * The output shape is generated by iterating over the input shape and adjusting
 * the output shape for keepdim.
 * @param input_tensor The input tensor on which reduction is performed.
 * @param dim The dimension to reduce on (optional).
 * @param keepdim Whether to keep the reduced dimension.
 * @return The output shape.
 */
ttnn::SmallVector<uint32_t> get_output_shape(const Tensor& input_tensor, const std::optional<int>& dim, bool keepdim) {
    auto input_shape = input_tensor.logical_shape();
    int rank = input_shape.size();
    ttnn::SmallVector<uint32_t> output_shape;

    // If no reduction dims are specified, we reduce all dimensions
    auto all_dim_reduce = not dim.has_value();
    auto red_dim = dim.value_or(0);
    TT_FATAL(
        (rank == 0) or ((red_dim >= -rank) and (red_dim < rank)),
        "Invalid reduction dimension {} for input tensor with rank {}",
        red_dim,
        rank);

    // Adjust negative reduction dimension to positive
    red_dim = red_dim < 0 ? red_dim + rank : red_dim;

    // Generate output shape
    // Iterate over the input shape and adjust the output shape for keepdim
    for (int d = 0; d < rank; ++d) {
        // If this is in the reduction dims, keep it only if keepdim is true
        bool is_reduction_dim = all_dim_reduce or (d == red_dim);

        if (is_reduction_dim) {
            TT_FATAL(input_shape[d] != 0, "Expected reduction dim {} to have non-zero size", d);
            if (keepdim) {
                output_shape.push_back(1);
            }
        } else {
            // If this is not a reduction dim, we keep the original size
            output_shape.push_back(input_shape[d]);
        }
    }

    return output_shape;
}

ArgMaxDeviceOperation::program_factory_t ArgMaxDeviceOperation::select_program_factory(
    const operation_attributes_t& args, const tensor_args_t& /*tensor_args*/) {
    if (args.use_multicore) {
        return program::ArgMaxMultiCoreProgramFactory{};
    }
    return program::ArgMaxSingleCoreProgramFactory{};
}

void ArgMaxDeviceOperation::validate_on_program_cache_hit(
    const operation_attributes_t& args, const tensor_args_t& tensor_args) {
    validate_on_program_cache_miss(args, tensor_args);
}

void ArgMaxDeviceOperation::validate_on_program_cache_miss(
    const operation_attributes_t& args, const tensor_args_t& tensor_args) {
    const auto& input_tensor_a = tensor_args.input;
    const auto input_layout = input_tensor_a.layout();

    TT_FATAL(
        input_tensor_a.memory_config().memory_layout() == TensorMemoryLayout::INTERLEAVED,
        "Only INTERLEAVED memory layout is supported for inputs, got {}",
        input_tensor_a.memory_config().memory_layout());

    if (input_layout == Layout::ROW_MAJOR) {
        TT_FATAL(
            input_tensor_a.dtype() == DataType::BFLOAT16 || input_tensor_a.dtype() == DataType::FLOAT32 ||
                input_tensor_a.dtype() == DataType::INT32 || input_tensor_a.dtype() == DataType::UINT32 ||
                input_tensor_a.dtype() == DataType::UINT16,
            "Only BFLOAT16, FLOAT32, INT32, UINT32, and UINT16 are supported for inputs with ROW_MAJOR layout, got {}",
            input_tensor_a.dtype());
    } else {
        TT_FATAL(
            input_tensor_a.dtype() == DataType::BFLOAT16 || input_tensor_a.dtype() == DataType::FLOAT32,
            "Only BFLOAT16, FLOAT32 are supported for inputs with TILE layout, got {}",
            input_tensor_a.dtype());

        const auto& input_shape = input_tensor_a.padded_shape();
        auto rank = input_shape.size();
        // With TILE layout, padded shape has always at least 2 dims (i.e., also for 1D input tensors).
        TT_FATAL(rank > 1, "Invalid rank {} for input tensor with TILE layout", rank);
        TT_FATAL(
            input_shape[rank - 1] % tt::constants::TILE_WIDTH == 0,
            "Last dimension {} must be divisible by TILE_WIDTH {}",
            input_shape[rank - 1],
            tt::constants::TILE_WIDTH);
        TT_FATAL(
            input_shape[rank - 2] % tt::constants::TILE_HEIGHT == 0,
            "Second-to-last dimension {} must be divisible by TILE_HEIGHT {}",
            input_shape[rank - 2],
            tt::constants::TILE_HEIGHT);
    }

    TT_FATAL(args.output_dtype == DataType::UINT32, "Only UINT32 is supported for outputs, got {}", args.output_dtype);

    TT_FATAL(
        args.output_mem_config.memory_layout() == TensorMemoryLayout::INTERLEAVED,
        "Only INTERLEAVED memory layout is supported for outputs, got {}",
        args.output_mem_config.memory_layout());

    const auto& optional_output_tensor = tensor_args.optional_output_tensor;
    if (optional_output_tensor.has_value()) {
        TT_FATAL(
            optional_output_tensor.value().dtype() == DataType::UINT32,
            "Only UINT32 is supported for outputs, got {}",
            optional_output_tensor.value().dtype());
        TT_FATAL(
            optional_output_tensor.value().memory_config().memory_layout() == TensorMemoryLayout::INTERLEAVED,
            "Only INTERLEAVED memory layout is supported for outputs, got {}",
            optional_output_tensor.value().memory_config().memory_layout());
        TT_FATAL(
            optional_output_tensor.value().layout() == Layout::ROW_MAJOR,
            "Output tensor must have ROW_MAJOR layout, got {}",
            optional_output_tensor.value().layout());
    }

    if (args.dim.has_value()) {
        const uint32_t input_rank = input_tensor_a.padded_shape().rank();
        const uint32_t normalized_dim = args.dim.value() < 0 ? args.dim.value() + input_rank : args.dim.value();

        // TODO: Add support for normalized_dim = 0, 1, 2
        TT_FATAL(
            normalized_dim == (input_rank - 1),
            "Only argmax on last dim is supported! Got dim={} (normalized={}), expected {}",
            args.dim.value(),
            normalized_dim,
            input_rank - 1);
    } else {
        TT_FATAL(input_layout != Layout::TILE, "For inputs with TILE layout, dim parameter must be specified!");
    }

    if (args.use_multicore) {
        if (args.sub_core_grids.has_value()) {
            TT_FATAL(
                args.sub_core_grids->ranges().size() <= 2,
                "Multicore argmax only supports up to 2 core grid ranges, but got {} ranges",
                args.sub_core_grids->ranges().size());
        }
        TT_FATAL(
            input_tensor_a.layout() == Layout::ROW_MAJOR,
            "Multicore argmax only supports ROW_MAJOR layout for inputs, got {}",
            input_tensor_a.layout());
    }
}

TensorSpec ArgMaxDeviceOperation::compute_output_specs(
    const operation_attributes_t& args, const tensor_args_t& tensor_args) {
    if (tensor_args.optional_output_tensor.has_value()) {
        return tensor_args.optional_output_tensor->tensor_spec();
    }

    const auto& input_tensor = tensor_args.input;
    auto output_shape = get_output_shape(input_tensor, args.dim, args.keepdim);
    return TensorSpec(
        ttnn::Shape(output_shape),
        TensorLayout(args.output_dtype, PageConfig(Layout::ROW_MAJOR), args.output_mem_config));
}

Tensor ArgMaxDeviceOperation::create_output_tensors(
    const operation_attributes_t& args, const tensor_args_t& tensor_args) {
    if (tensor_args.optional_output_tensor.has_value()) {
        return tensor_args.optional_output_tensor.value();
    }
    return create_device_tensor(compute_output_specs(args, tensor_args), tensor_args.input.device());
}

}  // namespace ttnn::operations::reduction::argmax

namespace ttnn::prim {
ttnn::Tensor argmax(
    const Tensor& input,
    tt::tt_metal::DataType output_dtype,
    std::optional<int> dim,
    bool keepdim,
    const std::optional<CoreRangeSet>& sub_core_grids,
    bool use_multicore,
    const tt::tt_metal::MemoryConfig& output_mem_config,
    std::optional<Tensor> optional_output_tensor) {
    using OperationType = ttnn::operations::reduction::argmax::ArgMaxDeviceOperation;
    return ttnn::device_operation::launch<OperationType>(
        OperationType::operation_attributes_t{
            .output_dtype = output_dtype,
            .dim = dim,
            .keepdim = keepdim,
            .sub_core_grids = sub_core_grids,
            .use_multicore = use_multicore,
            .output_mem_config = output_mem_config,
        },
        OperationType::tensor_args_t{.input = input, .optional_output_tensor = std::move(optional_output_tensor)});
}
}  // namespace ttnn::prim
