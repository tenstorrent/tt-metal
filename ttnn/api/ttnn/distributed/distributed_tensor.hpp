// SPDX-FileCopyrightText: Â© 2024 Tenstorrent Inc.
//
// SPDX-License-Identifier: Apache-2.0

#pragma once

#include <memory>

#include <tt_stl/small_vector.hpp>
#include <tt-metalium/memory_pin.hpp>

#include "tt_stl/span.hpp"
#include "ttnn/tensor/tensor.hpp"
#include "ttnn/distributed/types.hpp"

namespace ttnn::distributed {

using MeshMapperConfig = tt::tt_metal::distributed::MeshMapperConfig;
using MeshComposerConfig = tt::tt_metal::distributed::MeshComposerConfig;

// Mapper interface used for distributing a tensor onto a mesh.
class TensorToMesh {
public:
    ~TensorToMesh();
    TensorToMesh(TensorToMesh&& other) noexcept;
    TensorToMesh& operator=(TensorToMesh&& other) noexcept;
    TensorToMesh(const TensorToMesh&) = delete;
    TensorToMesh& operator=(const TensorToMesh&) = delete;

    static TensorToMesh create(const MeshDevice& mesh_device, const MeshMapperConfig& config);

    // Maps a tensor onto a mesh.
    // The input tensor is expected to be host-side tensor consisting of 1 device shard (i.e., mapped to 1x1 mesh).
    // The output tensor will be a host-side tensor mapped to a mesh of the same shape as the mesh device.
    Tensor operator()(const Tensor& tensor) const;

    // Overload that takes in a span of logical data; used in situations where the tensor object might not be
    // materialized.
    template <typename T>
    Tensor operator()(
        tt::stl::Span<T> buffer,
        const ttnn::Shape& shape,
        const tt::tt_metal::MemoryPin& buffer_pin,
        const tt::tt_metal::TensorLayout& layout,
        T pad_value = 0) const;

    tt::tt_metal::DistributedTensorConfig config() const;

private:
    class Impl;

    explicit TensorToMesh(std::unique_ptr<Impl> impl);

    std::unique_ptr<Impl> impl_;
};

// Creates an ND mesh mapper that distributes a tensor according to the `config`.
std::unique_ptr<TensorToMesh> create_mesh_mapper(MeshDevice& mesh_device, const MeshMapperConfig& config);

// Creates a mapper that replicates a tensor across all devices.
// Shorthand for specifying a MeshMapperConfig that replicates the tensor over the entire mesh.
std::unique_ptr<TensorToMesh> replicate_tensor_to_mesh_mapper(MeshDevice& mesh_device);

// Creates a mapper that shards a tensor along a single dimension.
// Shorthand for specifying a MeshMapperConfig with 1D mesh shape, and sharding the tensor along a single dimension of
// the tensor.
std::unique_ptr<TensorToMesh> shard_tensor_to_mesh_mapper(MeshDevice& mesh_device, int dim);

// Composer interface used for aggregating a tensor distributed over a mesh.
class MeshToTensor {
public:
    ~MeshToTensor();
    MeshToTensor(MeshToTensor&& other) noexcept;
    MeshToTensor& operator=(MeshToTensor&& other) noexcept;
    MeshToTensor(const MeshToTensor&) = delete;
    MeshToTensor& operator=(const MeshToTensor&) = delete;

    static MeshToTensor create(const MeshDevice& mesh_device, const MeshComposerConfig& config);

    // Composes a tensor distributed over a mesh.
    // The input tensor is expected to be distributed over a mesh of the same shape as the mesh device.
    // The output tensor will be a host-side tensor consisting of 1 device shard (i.e., mapped to 1x1 mesh).
    Tensor compose(const Tensor& tensor) const;

    // Overload that returns a pair of logical data and its shape, composed from a tensor distributed over a mesh.
    template <typename T>
    std::pair<std::vector<T>, Shape> compose(const Tensor& tensor) const;

private:
    class Impl;

    explicit MeshToTensor(std::unique_ptr<Impl> impl);

    std::unique_ptr<Impl> impl_;
};

// Creates an ND mesh composer that aggregates a tensor according to the `config`.
std::unique_ptr<MeshToTensor> create_mesh_composer(MeshDevice& mesh_device, const MeshComposerConfig& config);

// Creates a composer that concatenates a tensor across a single dimension.
// Shorthand for specifying a MeshComposerConfig with 1D mesh shape, and concatenating the tensor across a single
// dimension.
std::unique_ptr<MeshToTensor> concat_mesh_to_tensor_composer(MeshDevice& mesh_device, int dim);

// Distributes a host tensor onto multi-device configuration according to the `mapper`.
Tensor distribute_tensor(
    const Tensor& tensor,
    const TensorToMesh& mapper,
    std::optional<std::reference_wrapper<MeshDevice>> mesh_device = std::nullopt,
    ttnn::QueueId cq_id = ttnn::DefaultQueueId);

// Creates a distributed tensor from a span of logical data specified in `buffer`.
// `global_shape` must match the size of `buffer`; shapes of shards will be derived automatically based on the `mapper`,
// and the `shard_layout` will be applied subsequently. `buffer` may be re-used to create tensors directly, taking
// `buffer_pin` as the RAII to retain reference count to the object.
template <typename T>
Tensor create_distributed_tensor(
    tt::stl::Span<T> buffer,
    const ttnn::Shape& global_shape,
    const tt::tt_metal::MemoryPin& buffer_pin,
    const tt::tt_metal::TensorLayout& shard_layout,
    const TensorToMesh& mapper,
    std::optional<std::reference_wrapper<MeshDevice>> mesh_device = std::nullopt,
    ttnn::QueueId cq_id = ttnn::DefaultQueueId,
    T pad_value = 0);

// Overload for unowned spans of data.
template <typename T>
Tensor create_distributed_tensor(
    tt::stl::Span<const T> buffer,
    const ttnn::Shape& global_shape,
    const tt::tt_metal::TensorLayout& shard_layout,
    const TensorToMesh& mapper,
    std::optional<std::reference_wrapper<MeshDevice>> mesh_device = std::nullopt,
    ttnn::QueueId cq_id = ttnn::DefaultQueueId,
    T pad_value = 0);

// Aggregates a multi-device tensor into a host tensor according to the `composer`.
Tensor aggregate_tensor(const Tensor& tensor, const MeshToTensor& composer);

}  // namespace ttnn::distributed
