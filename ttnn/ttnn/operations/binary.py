# SPDX-FileCopyrightText: Â© 2024 Tenstorrent Inc.

# SPDX-License-Identifier: Apache-2.0

from typing import List, Union, Optional

import sys

import ttnn

__all__ = []


def apply_activations(tensor, activations):
    import torch

    string_to_function = {
        "relu": torch.relu,
        "gelu": torch.nn.functional.gelu,
        "silu": torch.nn.functional.silu,
    }

    if activations is not None:
        for activation in activations:
            activation_function = string_to_function[activation]
            tensor = activation_function(tensor)
    return tensor


def _golden_function(input_tensor_a, input_tensor_b, *args, activations=None, **kwargs):
    output_tensor = input_tensor_a + input_tensor_b
    return apply_activations(output_tensor, activations)


ttnn.attach_golden_function(ttnn.add, golden_function=_golden_function)
ttnn.attach_golden_function(ttnn.add_, golden_function=_golden_function)


def _golden_function(input_tensor_a, input_tensor_b, *args, activations=None, **kwargs):
    output_tensor = input_tensor_a - input_tensor_b
    return apply_activations(output_tensor, activations)


ttnn.attach_golden_function(ttnn.subtract, golden_function=_golden_function)
ttnn.attach_golden_function(ttnn.subtract_, golden_function=_golden_function)


def _golden_function(input_tensor_a, input_tensor_b, *args, activations=None, **kwargs):
    output_tensor = input_tensor_b - input_tensor_a
    return apply_activations(output_tensor, activations)


ttnn.attach_golden_function(ttnn.rsub, golden_function=_golden_function)
ttnn.attach_golden_function(ttnn.rsub_, golden_function=_golden_function)


def _golden_function(input_tensor_a, input_tensor_b, *args, activations=None, **kwargs):
    output_tensor = input_tensor_a * input_tensor_b
    return apply_activations(output_tensor, activations)


ttnn.attach_golden_function(ttnn.multiply, golden_function=_golden_function)
ttnn.attach_golden_function(ttnn.multiply_, golden_function=_golden_function)


def _golden_function(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return torch.eq(input_tensor_a, input_tensor_b)


ttnn.attach_golden_function(ttnn.eq, golden_function=_golden_function)


def _golden_function(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return torch.ne(input_tensor_a, input_tensor_b)


ttnn.attach_golden_function(ttnn.ne, golden_function=_golden_function)


def _golden_function(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return torch.gt(input_tensor_a, input_tensor_b)


ttnn.attach_golden_function(ttnn.gt, golden_function=_golden_function)


def _golden_function(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return torch.ge(input_tensor_a, input_tensor_b)


ttnn.attach_golden_function(ttnn.ge, golden_function=_golden_function)


def _golden_function(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return torch.lt(input_tensor_a, input_tensor_b)


ttnn.attach_golden_function(ttnn.lt, golden_function=_golden_function)


def _golden_function(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return torch.le(input_tensor_a, input_tensor_b)


ttnn.attach_golden_function(ttnn.le, golden_function=_golden_function)


def _golden_function(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return input_tensor_a.logical_and_(input_tensor_b)


ttnn.attach_golden_function(ttnn.logical_and_, golden_function=_golden_function)


def _golden_function(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return input_tensor_a.logical_or_(input_tensor_b)


ttnn.attach_golden_function(ttnn.logical_or_, golden_function=_golden_function)


def _golden_function(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return input_tensor_a.logical_xor_(input_tensor_b)


ttnn.attach_golden_function(ttnn.logical_xor_, golden_function=_golden_function)


def _golden_function(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return torch.ldexp(input_tensor_a, input_tensor_b)


ttnn.attach_golden_function(ttnn.ldexp, golden_function=_golden_function)
ttnn.attach_golden_function(ttnn.ldexp_, golden_function=_golden_function)


def _golden_function(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return torch.logaddexp(input_tensor_a, input_tensor_b)


ttnn.attach_golden_function(ttnn.logaddexp, golden_function=_golden_function)
ttnn.attach_golden_function(ttnn.logaddexp_, golden_function=_golden_function)


def _golden_function(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return torch.logaddexp2(input_tensor_a, input_tensor_b)


ttnn.attach_golden_function(ttnn.logaddexp2, golden_function=_golden_function)
ttnn.attach_golden_function(ttnn.logaddexp2_, golden_function=_golden_function)


def _golden_function(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return torch.divide(input_tensor_a, input_tensor_b)


ttnn.attach_golden_function(ttnn.divide, golden_function=_golden_function)
ttnn.attach_golden_function(ttnn.divide_, golden_function=_golden_function)


def _golden_function(a, b, *args, **kwargs):
    import torch

    return torch.nn.functional.gelu(torch.add(a, b))


ttnn.attach_golden_function(ttnn.bias_gelu, golden_function=_golden_function)
ttnn.attach_golden_function(ttnn.bias_gelu_, golden_function=_golden_function)


def _golden_function_squared_difference(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return torch_squared_difference(input_tensor_a, input_tensor_b)


ttnn.attach_golden_function(ttnn.squared_difference, golden_function=_golden_function_squared_difference)
ttnn.attach_golden_function(ttnn.squared_difference_, golden_function=_golden_function_squared_difference)


def _golden_function_addalpha(input_tensor_a, input_tensor_b, alpha, *args, **kwargs):
    import torch

    return torch.add(input_tensor_a, input_tensor_b, alpha=alpha)


ttnn.attach_golden_function(ttnn.addalpha, golden_function=_golden_function_addalpha)


def _golden_function_subalpha(input_tensor_a, input_tensor_b, alpha, *args, **kwargs):
    import torch

    return torch.sub(input_tensor_a, input_tensor_b, alpha=alpha)


ttnn.attach_golden_function(ttnn.subalpha, golden_function=_golden_function_subalpha)


def _golden_function_xlogy(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return torch.xlogy(input_tensor_a, input_tensor_b)


ttnn.attach_golden_function(ttnn.xlogy, golden_function=_golden_function_xlogy)


def _golden_function_hypot(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return torch.hypot(input_tensor_a, input_tensor_b)


ttnn.attach_golden_function(ttnn.hypot, golden_function=_golden_function_hypot)


def _golden_function_maximum(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return torch.maximum(input_tensor_a, input_tensor_b)


ttnn.attach_golden_function(ttnn.maximum, golden_function=_golden_function_maximum)


def _golden_function_minimum(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return torch.minimum(input_tensor_a, input_tensor_b)


ttnn.attach_golden_function(ttnn.minimum, golden_function=_golden_function_minimum)


def _golden_function_logical_xor(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return torch.logical_xor(input_tensor_a, input_tensor_b)


ttnn.attach_golden_function(ttnn.logical_xor, golden_function=_golden_function_logical_xor)


def _golden_function_logical_and(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return torch.logical_and(input_tensor_a, input_tensor_b)


ttnn.attach_golden_function(ttnn.logical_and, golden_function=_golden_function_logical_and)


def _golden_function_logical_or(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return torch.logical_or(input_tensor_a, input_tensor_b)


ttnn.attach_golden_function(ttnn.logical_or, golden_function=_golden_function_logical_or)


def _golden_function_atan2(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return torch.atan2(input_tensor_a, input_tensor_b)


ttnn.attach_golden_function(ttnn.atan2, golden_function=_golden_function_atan2)


def _golden_function_nextafter(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return torch.nextafter(input_tensor_a, input_tensor_b)


ttnn.attach_golden_function(ttnn.nextafter, golden_function=_golden_function_nextafter)


def _golden_function_isclose(input_tensor_a, input_tensor_b, *args, rtol=1e-05, atol=1e-08, equal_nan=False, **kwargs):
    import torch

    return torch.isclose(input_tensor_a, input_tensor_b, rtol=rtol, atol=atol, equal_nan=equal_nan)


ttnn.attach_golden_function(ttnn.isclose, golden_function=_golden_function_isclose)


def _golden_function_div(input_tensor_a, input_tensor_b, round_mode=None, *args, **kwargs):
    import torch

    return torch.div(input_tensor_a, input_tensor_b, rounding_mode=round_mode)


ttnn.attach_golden_function(ttnn.div, golden_function=_golden_function_div)


def _golden_function_div_no_nan(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    if isinstance(input_tensor_b, float):
        if input_tensor_b == 0:
            return torch.zeros_like(input_tensor_a)
        else:
            return input_tensor_a / input_tensor_b
    else:
        return torch.where(input_tensor_b == 0, 0, input_tensor_a / input_tensor_b)


ttnn.attach_golden_function(ttnn.div_no_nan, golden_function=_golden_function_div_no_nan)


def _golden_function_floor_div(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return torch.floor_divide(input_tensor_a, input_tensor_b)


ttnn.attach_golden_function(ttnn.floor_div, golden_function=_golden_function_floor_div)


def _golden_function_remainder(input_tensor_a, input_tensor_b, *args, device, **kwargs):
    import torch

    input_dtype = input_tensor_a.dtype
    if not torch.is_tensor(input_tensor_b):
        if input_dtype == torch.bfloat16:
            input_tensor_a = input_tensor_a.float()

    result = torch.nan_to_num(
        torch.remainder(input_tensor_a, input_tensor_b),
        nan=device.sfpu_nan(),
        posinf=device.sfpu_inf(),
        neginf=-device.sfpu_inf(),
    )

    if input_dtype == torch.bfloat16:
        result = result.bfloat16()
    return result


ttnn.attach_golden_function(ttnn.remainder, golden_function=_golden_function_remainder)


def _golden_function_fmod(input_tensor_a, input_tensor_b, *args, device, **kwargs):
    import torch

    if not torch.is_tensor(input_tensor_b):
        input_dtype = input_tensor_a.dtype
        if input_dtype == torch.bfloat16:
            input_tensor_a = input_tensor_a.float()
        result = torch.nan_to_num(
            torch.fmod(input_tensor_a, input_tensor_b),
            nan=device.sfpu_nan(),
            posinf=device.sfpu_inf(),
            neginf=-device.sfpu_inf(),
        )
        if input_dtype == torch.bfloat16:
            result = result.bfloat16()
    else:
        result = torch.fmod(input_tensor_a, input_tensor_b)

    return result


ttnn.attach_golden_function(ttnn.fmod, golden_function=_golden_function_fmod)


def torch_squared_difference(x, y, *args, **kwargs):
    import torch

    return torch.square(torch.sub(x, y))


def _golden_function_scatter(input_tensor_a, input_tensor_b, *args, **kwargs):
    input_tensor_b[0:, 0:, : input_tensor_a.shape[-2], : input_tensor_a.shape[-1]] = input_tensor_a
    return input_tensor_b


ttnn.attach_golden_function(ttnn.scatter, golden_function=_golden_function_scatter)


def _golden_function_outer(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return torch.outer(input_tensor_a.squeeze(), input_tensor_b.squeeze())


ttnn.attach_golden_function(ttnn.outer, golden_function=_golden_function_outer)


def _golden_function_polyval(input_tensor_a, coeffs, *args, **kwargs):
    result = 0.0
    for coeff in coeffs:
        result = result * input_tensor_a + coeff
    return result


ttnn.attach_golden_function(ttnn.polyval, golden_function=_golden_function_polyval)


def _golden_function_gt_(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return input_tensor_a.gt_(input_tensor_b)


ttnn.attach_golden_function(ttnn.gt_, golden_function=_golden_function_gt_)


def _golden_function_le_(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return input_tensor_a.le_(input_tensor_b)


ttnn.attach_golden_function(ttnn.le_, golden_function=_golden_function_le_)


def _golden_function_lt_(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return input_tensor_a.lt_(input_tensor_b)


ttnn.attach_golden_function(ttnn.lt_, golden_function=_golden_function_lt_)


def _golden_function_ge_(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return input_tensor_a.ge_(input_tensor_b)


ttnn.attach_golden_function(ttnn.ge_, golden_function=_golden_function_ge_)


def _golden_function_eq_(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return input_tensor_a.eq_(input_tensor_b)


ttnn.attach_golden_function(ttnn.eq_, golden_function=_golden_function_eq_)


def _golden_function_ne_(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return input_tensor_a.ne_(input_tensor_b)


ttnn.attach_golden_function(ttnn.ne_, golden_function=_golden_function_ne_)


def _golden_function_gcd(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return torch.gcd(input_tensor_a, input_tensor_b)


ttnn.attach_golden_function(ttnn.gcd, golden_function=_golden_function_gcd)


def _golden_function_lcm(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    return torch.lcm(input_tensor_a, input_tensor_b)


ttnn.attach_golden_function(ttnn.lcm, golden_function=_golden_function_lcm)


def _golden_function_prelu(input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    if not torch.is_tensor(input_tensor_b):
        input_tensor_b = torch.tensor(input_tensor_b, dtype=input_tensor_a.dtype)

    return torch.nn.functional.prelu(input_tensor_a, weight=input_tensor_b)


ttnn.attach_golden_function(ttnn.prelu, golden_function=_golden_function_prelu)


def _golden_function_logical_right_shift(input_tensor_a, shift_amt, *args, **kwargs):
    import torch

    t1_uint = input_tensor_a.to(torch.int64) & 0xFFFFFFFF
    result = (t1_uint >> shift_amt).to(torch.int32)
    return result


ttnn.attach_golden_function(ttnn.logical_right_shift, golden_function=_golden_function_logical_right_shift)

__all__ = []
