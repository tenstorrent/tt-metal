# Row Standardize - Implementation Summary

**Generated by**: generic_op_builder (automated)
**Date**: 2026-02-09
**Workflow**: Generic Op (Python-based)

---

## Operation Overview

**Name**: `row_standardize`
**Type**: Normalization operation
**Math**: `output = (x - mean_row) / sqrt(var_row + epsilon)`
**Equivalent to**: LayerNorm without learnable affine parameters (no gamma/beta)

---

## Files Created

### Python Infrastructure (3 files)

1. **`__init__.py`** (14 lines)
   - Package initialization
   - Exports `row_standardize` function

2. **`row_standardize.py`** (98 lines)
   - Entry point function
   - Input validation (rank, layout, dtype, alignment)
   - Output tensor allocation
   - Generic op invocation

3. **`row_standardize_program_descriptor.py`** (384 lines)
   - Tensor metadata extraction
   - Work distribution computation
   - Scaler and epsilon packing (bf16/f32)
   - 11 circular buffer descriptors
   - 3 kernel descriptors (reader, compute, writer)
   - Runtime args configuration

### Test Suite (1 file)

4. **`test_row_standardize.py`** (259 lines)
   - PyTorch reference implementation
   - PCC computation helper
   - Parameterized tests (11 shapes × 2 dtypes = 22 test cases)
   - Validation tests (rank, layout, dtype)
   - Edge case tests (constant rows)

### Stub Kernels (3 files)

5. **`kernels/row_standardize_reader.cpp`** (33 lines)
   - Stub reader kernel
   - Documented compile-time/runtime args
   - TODO: generate scaler/epsilon tiles, read sticks

6. **`kernels/row_standardize_compute.cpp`** (49 lines)
   - Stub compute kernel
   - Documented 8-phase pipeline
   - TODO: implement tilize->standardize->untilize

7. **`kernels/row_standardize_writer.cpp`** (26 lines)
   - Stub writer kernel
   - Documented args and responsibilities
   - TODO: write sticks to DRAM

---

## Implementation Details

### Input Requirements
- **Rank**: ≥ 2
- **Layout**: ROW_MAJOR
- **Dtype**: bfloat16 or float32
- **Last dim (W)**: Multiple of 32
- **Second-to-last dim (H)**: Multiple of 32
- **Memory**: Interleaved (on device)

### Output Specification
- **Shape**: Same as input
- **Dtype**: Same as input
- **Layout**: ROW_MAJOR
- **Memory**: Interleaved (DRAM)

### Work Distribution
- **Work unit**: One tile-row (Wt tiles spanning W dimension)
- **Total work**: `nblocks = (batch_dims × H) / 32`
- **Parallelization**: Single-core (prototype)
- **Future**: Multi-core via `split_work_to_cores()`

### Circular Buffers (11 CBs)

| CB | Index | Purpose | Capacity | Format |
|----|-------|---------|----------|--------|
| cb_rm_in | c_0 | Input RM sticks | Wt tiles | dtype |
| cb_scaler | c_1 | Reduce scaler (1/W) | 1 tile | dtype |
| cb_eps | c_2 | Epsilon scalar | 1 tile | intermed_fmt |
| cb_tilized | c_3 | Tilized input | Wt tiles | intermed_fmt |
| cb_tilized_out | c_4 | Normalized tiles | Wt tiles | intermed_fmt |
| cb_rm_out | c_16 | Output RM sticks | Wt tiles | dtype |
| cb_mean | c_24 | Row means | 1 tile | intermed_fmt |
| cb_xmm | c_25 | x - mean | Wt tiles | intermed_fmt |
| cb_xmm_sq | c_26 | (x-mean)² | Wt tiles | intermed_fmt |
| cb_var | c_27 | Row variance | 1 tile | intermed_fmt |
| cb_invstd | c_28 | rsqrt(var+eps) | 1 tile | intermed_fmt |

**Memory usage** (bfloat16): `(5×Wt + 6) × 2048 bytes`
- For W=1024 (Wt=32): ~330 KB (fits in L1)
- For W=2048 (Wt=64): ~650 KB (exceeds L1, needs large-W variant)

### Compute Pipeline (8 phases per tile-row)

1. **Tilize**: RM sticks → tiles (`compute_kernel_lib::tilize<c_0, c_3>()`)
2. **Mean reduce**: SUM with scaler 1/W → column vector (`reduce<SUM, REDUCE_ROW, WaitUpfrontNoPop>`)
3. **Subtract**: Broadcast subtract mean (`sub<COL, NoWaitNoPop>`)
4. **Square**: Element-wise square (`square<NoWaitNoPop>`)
5. **Var reduce**: SUM with scaler 1/W → column vector (`reduce<SUM, REDUCE_ROW, BulkWaitBulkPop>`)
6. **Add+Rsqrt**: Add epsilon, then rsqrt (`add_tiles_bcast_scalar` + `rsqrt_tile`)
7. **Normalize**: Multiply by invstd (`mul<COL, NoWaitNoPop>`)
8. **Untilize**: Tiles → RM sticks (`compute_kernel_lib::untilize<Wt, c_4, c_16>()`)

---

## Test Coverage

### Shape Coverage (11 test shapes)
- **2D**: (32,32), (32,64), (64,128), (128,128), (32,1024), (128,1024), (1024,32), (1024,1024)
- **3D**: (2,32,64), (4,64,128)
- **4D**: (2,4,32,64)

### Dtype Coverage
- **bfloat16**: PCC > 0.99 required
- **float32**: PCC > 0.999 required

### Validation Tests
- Rank < 2 → error
- TILE_LAYOUT → error
- Unsupported dtype → error
- Non-aligned dimensions → error

### Edge Cases
- Constant rows (zero variance) → output near zero

---

## Critical Implementation Notes

### 1. Output Tensor Position
**CRITICAL**: Output tensor MUST be last in `generic_op()` call:
```python
return ttnn.generic_op([input_tensor, output_tensor], program_descriptor)
```

### 2. Tensor Allocation
**CRITICAL**: Use positional args, NOT keyword args:
```python
ttnn.allocate_tensor_on_device(
    ttnn.Shape(output_shape),  # positional
    input_tensor.dtype,         # positional
    input_tensor.layout,        # positional
    device,                     # positional
    output_memory_config,       # positional
)
```

### 3. Compute Config
**CRITICAL**: Use `ComputeConfigDescriptor()`, not `ComputeConfig()`:
```python
config=ttnn.ComputeConfigDescriptor(
    math_fidelity=ttnn.MathFidelity.HiFi4,
    fp32_dest_acc_en=is_float32,  # Enable for float32 input
    math_approx_mode=False,
)
```

### 4. Scaler Packing
**CRITICAL**: Packed format for `generate_reduce_scaler()`:
- **bfloat16**: `(bf16 << 16 | bf16)` where bf16 is upper 16 bits of float32
- **float32**: Reinterpreted float bits as uint32

### 5. Torch Imports in Tests
**CRITICAL**: Import torch inside test functions, NOT globally (pre-commit hook requirement)

---

## Known Limitations (Prototype)

1. **Single-core only**: Multi-core distribution deferred
2. **Max W**: ~1568 for bf16, ~768 for f32 (L1 memory limit)
3. **Interleaved memory only**: Sharded memory support deferred
4. **Stub kernels**: Kernels compile but produce garbage output until implemented

---

## Next Steps

### Immediate (for ttnn-kernel-writer)
1. Implement reader kernel (scaler/epsilon generation, stick reading)
2. Implement compute kernel (8-phase pipeline with helpers)
3. Implement writer kernel (stick writing)

### Future Enhancements
1. Multi-core distribution for large tensors
2. Large-W variant (similar to softmax w_large)
3. Sharded memory support
4. Optional gamma/beta parameters (full LayerNorm)

---

## Deliverables Checklist

- [x] `__init__.py` - Package initialization
- [x] `row_standardize.py` - Entry point with validation
- [x] `row_standardize_program_descriptor.py` - ProgramDescriptor creation
- [x] `test_row_standardize.py` - Comprehensive test suite
- [x] `kernels/row_standardize_reader.cpp` - Stub kernel
- [x] `kernels/row_standardize_compute.cpp` - Stub kernel
- [x] `kernels/row_standardize_writer.cpp` - Stub kernel
- [x] Execution log maintained
- [x] Breadcrumbs logged

**Status**: COMPLETE - Ready for kernel implementation phase

---

## Usage Example

```python
import ttnn
from ttnn.operations.row_standardize import row_standardize

# Create input tensor (ROW_MAJOR layout)
torch_input = torch.randn((128, 1024))
ttnn_input = ttnn.from_torch(
    torch_input,
    dtype=ttnn.bfloat16,
    layout=ttnn.ROW_MAJOR_LAYOUT,
    device=device,
)

# Run row standardize
ttnn_output = row_standardize(ttnn_input, epsilon=1e-5)

# Convert back to torch
torch_output = ttnn.to_torch(ttnn_output)
```

---

## References

- **Spec**: `/localdev/mstaletovic/tt-metal/ttnn/ttnn/operations/row_standardize/row_standardize_spec.md`
- **Template**: `/localdev/mstaletovic/tt-metal/.claude/references/generic_op_template/`
- **Workflow**: `/localdev/mstaletovic/tt-metal/ttnn/experimental/claude_ttnn_agents/references/ttnn-generic-op-workflow.md`
- **Execution Log**: `agent_logs/generic_op_builder_execution_log.md`
- **Breadcrumbs**: `agent_logs/generic_op_builder_breadcrumbs.jsonl`
