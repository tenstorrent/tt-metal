inputs:
  device_perf_model_name:
    required: false
    default: ''
    description: "Used to specify the model for a device perf test. E2E perf tests do not set this"
  commands:
    required: true
    description: "Shell commands to run the test"
  # --- optional knobs for the summary ---
  ci_log_file:
    required: false
    default: "/work/ci_tests.log"
    description: "Where to append combined test output"
  summary_pattern:
    required: false
    # Focus on short test summary info section: header, test results, and final summary line
    default: "(.*=+.*short test summary info.*=+)|(.*\\b(PASSED|FAILED|SKIPPED|ERROR)\\b)|(.*=+.*[0-9]+.*(passed|failed|skipped|error).*=*)"
    description: "EGREP regex for lines to keep in the compact summary"
  summary_context:
    required: false
    default: "1"
    description: "grep -C context lines"
  summary_max_lines:
    required: false
    default: "600"
    description: "Cap summary length to avoid spam"
runs:
  using: composite
  steps:
    - name: Run perf commands (tee to log) + merge
      shell: bash
      env:
        TRACY_NO_INVARIANT_CHECK: 1
      run: |
        set -euo pipefail
        cd docker-job

        # E2E perf tests need git config
        if [[ -z "${{ inputs.device_perf_model_name }}" ]]; then
          git config --global --add safe.directory "$GITHUB_WORKSPACE"
        fi

        : "${CI_LOG_FILE:=${{ inputs.ci_log_file }}}"
        mkdir -p "$(dirname "$CI_LOG_FILE")"

        # run tests; keep exit code but do not stop subsequent steps
        set +e
        bash -eo pipefail -c '${{ inputs.commands }}' 2>&1 | tee -a "$CI_LOG_FILE"
        status=${PIPESTATUS[0]}
        echo "$status" > .command_status
        set -e

        # Conditional report merging based on test type
        if [[ -n "${{ inputs.device_perf_model_name }}" ]]; then
          # Device perf branch
          export DEVICE_PERF_REPORT_FILENAME="Models_Device_Perf_${{ inputs.device_perf_model_name }}_$(date +%Y_%m_%d).csv"
          python3 models/perf/merge_device_perf_results.py "$DEVICE_PERF_REPORT_FILENAME" REPORT || true
          cat "Models_Device_Perf_${{ inputs.device_perf_model_name }}_$(date +%Y_%m_%d).csv" > "Models_Device_Perf_$(date +%Y_%m_%d).csv" || true
          cat "Models_Device_Perf_$(date +%Y_%m_%d).csv" || true
          echo "device_perf_report_filename=$DEVICE_PERF_REPORT_FILENAME" >> "$GITHUB_OUTPUT"
        else
          # E2E perf branch
          env python3 models/perf/merge_perf_results.py REPORT || true
        fi

        # propagate original test exit code for this step
        exit "$(cat .command_status)"

    # ---- Generate and output compact summary to both CI and GitHub Step Summary ----
    - name: Compact test summary (output to CI and Step Summary)
      if: always()
      shell: bash
      run: |
        set -euo pipefail
        cd docker-job
        : "${CI_LOG_FILE:=${{ inputs.ci_log_file }}}"

        if [[ ! -s "$CI_LOG_FILE" ]]; then
          # No log file case
          summary_content="No aggregated log at $CI_LOG_FILE"
        else
          # Process log file
          nolog="${CI_LOG_FILE}.nocolor"
          sed -r 's/\x1B\[[0-9;]*[mK]//g' "$CI_LOG_FILE" > "$nolog"

          # Fail-first extract (short, only on failure):
          grep -nE -C 2 '(?i)(^\[gw\d+\]\s+FAILED|^FAILED\b|^ERROR\b|^E\s+[A-Z]|AssertionError|Traceback|^==\s*.*\sfailed,)' "$nolog" \
            | head -n 150 > fail-first.txt || true

          # Main compact summary (only content after "short test summary info"):
          # First, find the line number of "short test summary info"
          summary_start_line=$(grep -n "short test summary info" "$nolog" | tail -1 | cut -d: -f1 || echo "0")
          if [[ "$summary_start_line" -gt 0 ]]; then
            # Extract from that line onwards, then apply the pattern
            tail -n +$summary_start_line "$nolog" | grep -nE "${{ inputs.summary_pattern }}" \
              | head -n "${{ inputs.summary_max_lines }}" \
              | grep -v "PytestBenchmarkWarning\|pytest_benchmark" > compact-summary.txt || true
          else
            # Fallback to original behavior if no summary section found
            grep -nE "${{ inputs.summary_pattern }}" "$nolog" \
              | head -n "${{ inputs.summary_max_lines }}" \
              | grep -v "PytestBenchmarkWarning\|pytest_benchmark" > compact-summary.txt || true
          fi

          # Build summary content
          summary_content=""
          if [[ -f fail-first.txt ]]; then
            summary_content+="=== FAILURE HIGHLIGHTS ==="$'\n'
            summary_content+="$(cat fail-first.txt)"$'\n'$'\n'
          fi
          summary_content+="=== KEY LINES ==="$'\n'
          if [[ -f compact-summary.txt ]]; then
            summary_content+="$(cat compact-summary.txt)"
          else
            summary_content+="No matches for the current pattern"
          fi
        fi

        # Output to GitHub Step Summary (markdown format)
        {
          echo "### Compact test log summary"
          echo
          if [[ ! -s "$CI_LOG_FILE" ]]; then
            echo "_no aggregated log at \`$CI_LOG_FILE\`_"
          else
            if [[ -s fail-first.txt ]]; then
              echo "**Failure highlights**"
              echo
              echo '```text'
              cat fail-first.txt
              echo '```'
              echo
            fi
            echo "**Key lines**"
            echo
            if [[ -s compact-summary.txt ]]; then
              echo '```text'
              cat compact-summary.txt
              echo '```'
            else
              echo "_no matches for the current pattern_"
            fi
          fi
        } >> "$GITHUB_STEP_SUMMARY"
