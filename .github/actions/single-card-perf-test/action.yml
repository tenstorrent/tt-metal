inputs:
  device_perf_model_name:
    required: false
    default: ''
    description: "Used to specify the model for a device perf test. E2E perf tests do not set this"
  commands:
    required: true
    description: "Shell commands to run the test"
runs:
  using: "composite"
  steps:
    - name: Debug inputs
      run: |
        echo "DEBUG: device_perf_model_name='${{ inputs.device_perf_model_name }}'"
        echo "DEBUG: device_perf_model_name length=${#device_perf_model_name}"
        echo "DEBUG: commands='${{ inputs.commands }}'"
        if [ -z "${{ inputs.device_perf_model_name }}" ]; then
          echo "DEBUG: device_perf_model_name is empty - will run perf model test"
        else
          echo "DEBUG: device_perf_model_name is not empty - will run first step"
        fi
        echo "Command to execute: ${{ inputs.commands }}"
      shell: bash
      env:
        device_perf_model_name: ${{ inputs.device_perf_model_name }}
    - run: |
        cd docker-job
        set -x

        ${{ inputs.commands }}

        export DEVICE_PERF_REPORT_FILENAME=Models_Device_Perf_${{ inputs.device_perf_model_name }}_$(date +%Y_%m_%d).csv
        python3 models/perf/merge_device_perf_results.py $DEVICE_PERF_REPORT_FILENAME REPORT
        cat Models_Device_Perf_${{ inputs.device_perf_model_name }}_$(date +%Y_%m_%d).csv > Models_Device_Perf_$(date +%Y_%m_%d).csv
        cat Models_Device_Perf_$(date +%Y_%m_%d).csv
        echo "device_perf_report_filename=$DEVICE_PERF_REPORT_FILENAME" >> "$GITHUB_OUTPUT"
      if: ${{ inputs.device_perf_model_name != '' }}
      shell: bash
      env:
        TRACY_NO_INVARIANT_CHECK: 1
    - run: |
        set -x
        echo "Running perf model test (device_perf_model_name is empty)"
        echo "Command to execute: ${{ inputs.commands }}"

        ${{ inputs.commands }}
        status=$?
        env python3 models/perf/merge_perf_results.py REPORT
        exit $status

      if: ${{ inputs.device_perf_model_name == '' }}
      shell: bash
