name: "[internal] Galaxy DeepSeek tests impl"

on:
  workflow_call:
    inputs:
      docker-image:
        required: true
        type: string
      build-artifact-name:
        required: true
        type: string
      wheel-artifact-name:
        required: true
        type: string
      extra-tag:
        required: false
        type: string
        default: "in-service"
      topology:
        required: false
        type: string
        default: "topology-6u"
      test-type:
        required: false
        type: string
        default: "all"

jobs:
  generate-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.generate.outputs.matrix }}
    steps:
      - name: Generate test matrix
        id: generate
        run: |
          all_tests=$(jq -n '[
            { "name": "(Galaxy) DeepSeek unit tests", "arch": "wormhole_b0", "model": "deepseek", "test-type": "unit", "timeout": 15, "owner_id": "U03HY7MK4BT" },
            { "name": "(Galaxy) DeepSeek module tests", "arch": "wormhole_b0", "model": "deepseek", "test-type": "module", "timeout": 150, "owner_id": "U03HY7MK4BT" }
          ]')

          # Filter matrix based on test-type selection
          if [ "${{ inputs.test-type }}" = "all" ]; then
            matrix="$all_tests"
          else
            matrix=$(echo "$all_tests" | jq -c "[.[] | select(.\"test-type\" == \"${{ inputs.test-type }}\")]")
          fi

          # Fail fast if the matrix is empty (invalid test-type selected)
          if [ "$(echo "$matrix" | jq length)" -eq 0 ]; then
            echo "Error: Invalid test-type selected: '${{ inputs.test-type }}'"
            exit 1
          fi
          echo "matrix=$(echo "$matrix" | jq -c .)" >> $GITHUB_OUTPUT

  galaxy-deepseek-tests:
    needs: generate-matrix
    name: ${{ matrix.test-group.name }}
    strategy:
      fail-fast: false
      matrix:
        test-group: ${{ fromJson(needs.generate-matrix.outputs.matrix) }}
    runs-on:
      - arch-wormhole_b0
      - ${{ inputs.topology }}
      - bare-metal
      - pipeline-functional
      - ${{ inputs.extra-tag }}
    container:
      image: ${{ inputs.docker-image }}
      env:
        TT_METAL_HOME: /work
        PYTHONPATH: /work
        LD_LIBRARY_PATH: /work/build/lib
        LOGURU_LEVEL: INFO
        ARCH_NAME: ${{ matrix.test-group.arch }}
        DEEPSEEK_V3_HF_MODEL: /mnt/MLPerf/tt_dnn-models/deepseek-ai/DeepSeek-R1-0528
        DEEPSEEK_V3_CACHE: /mnt/MLPerf/tt_dnn-models/deepseek-ai/DeepSeek-R1-0528-Cache/CI
        MESH_DEVICE: TG
        # ADDED: Required for BenchmarkData to save to Superset
        CI: true
        GITHUB_ACTIONS: true
      volumes:
        - ${{ github.workspace }}/docker-job:/work # Subdir to workaround https://github.com/actions/runner/issues/691
        - /dev/hugepages-1G:/dev/hugepages-1G
        - /mnt/MLPerf:/mnt/MLPerf:io
      options: "--device /dev/tenstorrent"
    defaults:
      run:
        shell: bash
        working-directory: /work # https://github.com/actions/runner/issues/878
    steps:
      - name: ðŸ§¬ Checkout Repository
        uses: actions/checkout@v4
        with:
          submodules: recursive
          path: docker-job
      - name: â¬‡ï¸  Setup Job
        uses: ./docker-job/.github/actions/setup-job
        timeout-minutes: 10
        with:
          build-artifact-name: ${{ inputs.build-artifact-name }}
          wheel-artifact-name: ${{ inputs.wheel-artifact-name }}
      - name: Run DeepSeek unit tests
        if: ${{ matrix.test-group.test-type == 'unit' }}
        timeout-minutes: ${{ matrix.test-group.timeout }}
        run: |
          pip install -r models/demos/deepseek_v3/reference/deepseek/requirements.txt
          pytest models/demos/deepseek_v3/tests/unit --timeout 60 --durations=0
      - name: Validate weight cache
        if: ${{ matrix.test-group.test-type == 'module' }}
        timeout-minutes: 10
        run: |
          python3 models/demos/deepseek_v3/scripts/validate_weight_cache.py --root "$DEEPSEEK_V3_CACHE/tests_cache" || true

      # - name: Run DeepSeek module tests
      #   if: ${{ matrix.test-group.test-type == 'module' }}
      #   timeout-minutes: ${{ matrix.test-group.timeout }}
      #   run: |
      #     pip install -r models/demos/deepseek_v3/reference/deepseek/requirements.txt
      #     pytest models/demos/deepseek_v3/tests --ignore=models/demos/deepseek_v3/tests/unit --timeout 600  --durations=0

      - name: Run DeepSeek fused op unit tests (MLP - all ops)
        if: ${{ matrix.test-group.test-type == 'module' }}
        timeout-minutes: ${{ matrix.test-group.timeout }}
        run: |
          # MLP Fused Op Tests: eager + trace, decode + prefill, program_cache enabled
          # All tests log to Superset: run_mode, trace_mode, program_cache, PCC, ATOL, runtime

          echo "=============================================="
          echo "MLP mul fused op tests"
          echo "=============================================="
          echo "Running MLP mul (decode, eager, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/mlp/mul/test_ds_fused_mul.py::test_ds_fused_mul -k "decode and 1 and eager and program_cache and not no_program_cache" --timeout 600 --durations=0
          echo "Running MLP mul (decode, trace, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/mlp/mul/test_ds_fused_mul.py::test_ds_fused_mul -k "decode and 1 and trace and program_cache and not no_program_cache" --timeout 600 --durations=0
          echo "Running MLP mul (prefill 128, eager, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/mlp/mul/test_ds_fused_mul.py::test_ds_fused_mul -k "prefill and 128 and eager and program_cache and not no_program_cache" --timeout 600 --durations=0
          echo "Running MLP mul (prefill 128, trace, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/mlp/mul/test_ds_fused_mul.py::test_ds_fused_mul -k "prefill and 128 and trace and program_cache and not no_program_cache" --timeout 600 --durations=0

          echo "=============================================="
          echo "MLP all_gather fused op tests"
          echo "=============================================="
          echo "Running MLP all_gather (decode, eager, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/mlp/all_gather/test_ds_fused_all_gather_preff1_3.py::test_ds_fused_all_gather_preff1_3 -k "decode and 1 and eager and program_cache and not no_program_cache" --timeout 600 --durations=0
          echo "Running MLP all_gather (decode, trace, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/mlp/all_gather/test_ds_fused_all_gather_preff1_3.py::test_ds_fused_all_gather_preff1_3 -k "decode and 1 and trace and program_cache and not no_program_cache" --timeout 600 --durations=0
          echo "Running MLP all_gather (prefill 128, eager, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/mlp/all_gather/test_ds_fused_all_gather_preff1_3.py::test_ds_fused_all_gather_preff1_3 -k "prefill and 128 and eager and program_cache and not no_program_cache" --timeout 600 --durations=0
          echo "Running MLP all_gather (prefill 128, trace, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/mlp/all_gather/test_ds_fused_all_gather_preff1_3.py::test_ds_fused_all_gather_preff1_3 -k "prefill and 128 and trace and program_cache and not no_program_cache" --timeout 600 --durations=0

          echo "=============================================="
          echo "MLP ff1_3 fused op tests"
          echo "=============================================="
          echo "Running MLP ff1_3 (decode, eager, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/mlp/ff1_3/test_ds_fused_ff1_3.py::test_ds_fused_ff1_3 -k "decode and 1 and eager and program_cache and not no_program_cache and real_weights" --timeout 600 --durations=0
          echo "Running MLP ff1_3 (decode, trace, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/mlp/ff1_3/test_ds_fused_ff1_3.py::test_ds_fused_ff1_3 -k "decode and 1 and trace and program_cache and not no_program_cache and real_weights" --timeout 600 --durations=0
          echo "Running MLP ff1_3 (prefill 128, eager, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/mlp/ff1_3/test_ds_fused_ff1_3.py::test_ds_fused_ff1_3 -k "prefill and 128 and eager and program_cache and not no_program_cache and real_weights" --timeout 600 --durations=0
          echo "Running MLP ff1_3 (prefill 128, trace, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/mlp/ff1_3/test_ds_fused_ff1_3.py::test_ds_fused_ff1_3 -k "prefill and 128 and trace and program_cache and not no_program_cache and real_weights" --timeout 600 --durations=0

          echo "=============================================="
          echo "MLP ff2 fused op tests"
          echo "=============================================="
          echo "Running MLP ff2 (decode, eager, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/mlp/ff2/test_ds_fused_ff2.py::test_ds_fused_ff2 -k "decode and 1 and eager and program_cache and not no_program_cache and real_weights" --timeout 600 --durations=0
          echo "Running MLP ff2 (decode, trace, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/mlp/ff2/test_ds_fused_ff2.py::test_ds_fused_ff2 -k "decode and 1 and trace and program_cache and not no_program_cache and real_weights" --timeout 600 --durations=0
          echo "Running MLP ff2 (prefill 128, eager, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/mlp/ff2/test_ds_fused_ff2.py::test_ds_fused_ff2 -k "prefill and 128 and eager and program_cache and not no_program_cache and real_weights" --timeout 600 --durations=0
          echo "Running MLP ff2 (prefill 128, trace, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/mlp/ff2/test_ds_fused_ff2.py::test_ds_fused_ff2 -k "prefill and 128 and trace and program_cache and not no_program_cache and real_weights" --timeout 600 --durations=0

          echo "=============================================="
          echo "MLP reduce_scatter fused op tests"
          echo "=============================================="
          echo "Running MLP reduce_scatter (decode, eager, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/mlp/reduce_scatter/test_ds_fused_reduce_scatter_post_ff2.py::test_ds_fused_reduce_scatter_post_ff2 -k "decode and 1 and eager and program_cache and not no_program_cache" --timeout 600 --durations=0
          echo "Running MLP reduce_scatter (decode, trace, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/mlp/reduce_scatter/test_ds_fused_reduce_scatter_post_ff2.py::test_ds_fused_reduce_scatter_post_ff2 -k "decode and 1 and trace and program_cache and not no_program_cache" --timeout 600 --durations=0
          echo "Running MLP reduce_scatter (prefill 128, eager, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/mlp/reduce_scatter/test_ds_fused_reduce_scatter_post_ff2.py::test_ds_fused_reduce_scatter_post_ff2 -k "prefill and 128 and eager and program_cache and not no_program_cache" --timeout 600 --durations=0
          echo "Running MLP reduce_scatter (prefill 128, trace, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/mlp/reduce_scatter/test_ds_fused_reduce_scatter_post_ff2.py::test_ds_fused_reduce_scatter_post_ff2 -k "prefill and 128 and trace and program_cache and not no_program_cache" --timeout 600 --durations=0

          echo "=============================================="
          echo "All MLP fused op tests completed!"
          echo "=============================================="

          echo "=============================================="
          echo "Embedding fused op tests"
          echo "=============================================="
          echo "Running Embedding (decode, eager, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/embedding/embedding/test_ds_fused_embedding.py::test_ds_fused_embedding -k "decode and 32 and eager and program_cache and not no_program_cache and real_weights" --timeout 600 --durations=0
          echo "Running Embedding (decode, trace, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/embedding/embedding/test_ds_fused_embedding.py::test_ds_fused_embedding -k "decode and 32 and trace and program_cache and not no_program_cache and real_weights" --timeout 600 --durations=0
          echo "Running Embedding (prefill 128, eager, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/embedding/embedding/test_ds_fused_embedding.py::test_ds_fused_embedding -k "prefill and 128 and eager and program_cache and not no_program_cache and real_weights" --timeout 600 --durations=0
          echo "Running Embedding (prefill 128, trace, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/embedding/embedding/test_ds_fused_embedding.py::test_ds_fused_embedding -k "prefill and 128 and trace and program_cache and not no_program_cache and real_weights" --timeout 600 --durations=0

          echo "=============================================="
          echo "Embedding all_gather fused op tests"
          echo "=============================================="
          echo "Running Embedding all_gather (decode, eager, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/embedding/all_gather/test_ds_fused_all_gather_embedding.py::test_ds_fused_all_gather_embedding -k "decode and 32 and eager and program_cache and not no_program_cache" --timeout 600 --durations=0
          echo "Running Embedding all_gather (decode, trace, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/embedding/all_gather/test_ds_fused_all_gather_embedding.py::test_ds_fused_all_gather_embedding -k "decode and 32 and trace and program_cache and not no_program_cache" --timeout 600 --durations=0
          echo "Running Embedding all_gather (prefill 128, eager, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/embedding/all_gather/test_ds_fused_all_gather_embedding.py::test_ds_fused_all_gather_embedding -k "prefill and 128 and eager and program_cache and not no_program_cache" --timeout 600 --durations=0
          echo "Running Embedding all_gather (prefill 128, trace, program_cache)..."
          pytest models/demos/deepseek_v3/tests/fused_op_unit_tests/embedding/all_gather/test_ds_fused_all_gather_embedding.py::test_ds_fused_all_gather_embedding -k "prefill and 128 and trace and program_cache and not no_program_cache" --timeout 600 --durations=0

          echo "=============================================="
          echo "All Embedding fused op tests completed!"
          echo "=============================================="
      - uses: tenstorrent/tt-metal/.github/actions/slack-report@main
        if: ${{ failure() }}
        with:
          slack_webhook_url: ${{ secrets.SLACK_METAL_INFRA_PIPELINE_STATUS_ALERT }}
          owner: ${{ matrix.test-group.owner_id }}
      # ENABLED: Save and upload benchmark data to Superset
      - name: Save environment data
        id: save-environment-data
        if: ${{ !cancelled() }}
        run: |
          if [ -d "generated/benchmark_data" ]; then
            echo "has_benchmark_data=true" >> $GITHUB_OUTPUT
            python3 .github/scripts/data_analysis/create_benchmark_with_environment_json.py
          else
            echo "::warning::Benchmark data directory 'generated/benchmark_data' does not exist"
            echo "has_benchmark_data=false" >> $GITHUB_OUTPUT
          fi
      - name: Upload benchmark data
        if: ${{ !cancelled() && steps.save-environment-data.conclusion == 'success' && steps.save-environment-data.outputs.has_benchmark_data == 'true' }}
        uses: tenstorrent/tt-metal/.github/actions/upload-data-via-sftp@main
        with:
          ssh-private-key: ${{ secrets.SFTP_BENCHMARK_WRITER_KEY }}
          sftp-batchfile: .github/actions/upload-data-via-sftp/benchmark_data_batchfile.txt
          username: ${{ secrets.SFTP_BENCHMARK_WRITER_USERNAME }}
          hostname: ${{ secrets.SFTP_BENCHMARK_WRITER_HOSTNAME }}
          path: /work
      - uses: tenstorrent/tt-metal/.github/actions/upload-artifact-with-job-uuid@main
        timeout-minutes: 10
        if: ${{ !cancelled() }}
        with:
          prefix: "test_reports_"
      - uses: tenstorrent/tt-metal/.github/actions/cleanup@main
