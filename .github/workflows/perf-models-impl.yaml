name: "[internal] Perf models impl"

on:
  workflow_call:
    inputs:
      build-artifact-name:
        required: true
        type: string
      wheel-artifact-name:
        required: true
        type: string
      docker-image:
        required: true
        type: string
      extra-tag:
        required: false
        type: string
        default: "in-service"

jobs:
  models-perf:
    strategy:
      # Do not fail-fast because we need to ensure all tests go to completion
      # so we try not to get hanging machines
      fail-fast: false
      matrix:
        test-info: [
          {name: "N300 WH B0", arch: wormhole_b0, runs-on: ["N300", "pipeline-perf", "bare-metal", "${{ inputs.extra-tag }}"], machine-type: "bare_metal"},
        ]
        model-group:
          - name: llm_javelin
            timeout: 20
            model-list:
              - models/demos/falcon7b_common/tests
              - models/demos/wormhole/mamba/tests

          - name: cnn_javelin
            timeout: 10
            model-list:
              - models/experimental/functional_unet/tests
              - models/demos/wormhole/stable_diffusion/tests

          - name: other
            timeout: 5
            model-list:
              - models/demos/convnet_mnist/tests/
              # This is bad. These should be referencing .../tests directories
              - models/demos/bert_tiny/tests/test_performance.py
              - models/demos/squeezebert/tests/test_performance.py
              - models/demos/roberta/tests/test_performance.py

          - name: other_magic_env
            timeout: 45
            model-list:
              - models/demos/yolov10x/tests/perf/
              - models/demos/sentence_bert/tests/perf/
              # This is bad. This should be referencing .../tests directory
              - models/demos/wormhole/resnet50/tests/test_perf_e2e_resnet50.py
              - models/demos/yolov11/tests/perf/
              # This is bad. This should be referencing .../tests directory
              - models/demos/wormhole/bert_tiny/tests/test_performance.py
              - models/demos/yolov4/tests/perf/
              - models/demos/yolov7/tests/perf/
              # This is bad. This should be referencing .../tests directory
              - models/demos/wormhole/distilbert/tests/test_perf_distilbert.py
              - models/demos/segformer/tests/perf/
              - models/demos/metal_BERT_large_11/tests/

              - models/demos/yolov8s_world/tests/perf/
              - models/demos/yolov8s/tests/perf/
              - models/demos/yolov6l/tests/perf/
              - models/demos/mobilenetv2/tests/perf/
              - models/demos/vgg_unet/tests/
              - models/demos/yolov9c/tests/perf/
              - models/demos/vanilla_unet/tests/perf/
              - models/demos/yolov8x/tests/perf/
              # e2e performant test is present in .../vit/demo folder only
              - models/demos/wormhole/vit/demo/
              - models/experimental/swin_v2/tests/perf/
              - models/experimental/swin_s/tests/perf/
              - models/experimental/yolov5x/tests/perf/
              - models/experimental/vovnet/tests/perf/
              - models/demos/yolov12x/tests/perf/
              - models/demos/ufld_v2/tests/perf/
              - models/experimental/efficientnetb0/tests/perf/

    name: "${{ matrix.model-group.name }} ${{ matrix.test-info.name }}"
    runs-on: ${{ matrix.test-info.runs-on }}
    container:
      image: ${{ inputs.docker-image || 'docker-image-unresolved!' }}
      env:
        TT_METAL_HOME: /work
        PYTHONPATH: /work
        LD_LIBRARY_PATH: /work/build/lib
        ARCH_NAME: ${{ matrix.test-info.arch }}
        LOGURU_LEVEL: INFO
        GTEST_OUTPUT: xml:generated/test_reports/
        GITHUB_ACTIONS: true
      volumes:
        - ${{ github.workspace }}/docker-job:/work # Subdir to workaround https://github.com/actions/runner/issues/691
        - /dev/hugepages-1G:/dev/hugepages-1G
        - /mnt/MLPerf:/mnt/MLPerf:ro
      options: "--device /dev/tenstorrent -e TT_GH_CI_INFRA"
    defaults:
      run:
        shell: bash
        working-directory: /work # https://github.com/actions/runner/issues/878
    steps:
      - name: Setup Job
        uses: tenstorrent/tt-metal/.github/actions/setup-job@main
        timeout-minutes: 10
        with:
          build-artifact-name: ${{ inputs.build-artifact-name }}
          wheel-artifact-name: ${{ inputs.wheel-artifact-name }}
      - name: Enable Performance mode
        run: sudo cpupower frequency-set -g performance
      - name: Run performance regressions
        timeout-minutes: ${{ matrix.model-group.timeout }}
        run: |
          if [[ "${{ matrix.model-group.name }}" == "other" ]]; then
            echo "No environment variable engineering needed for other models"
          elif [[ "${{ matrix.model-group.name }}" == "tt_transformers" ]]; then
            export QWEN_DIR=/mnt/MLPerf/tt_dnn-models/qwen/Qwen2-7B-Instruct
            export FAKE_DEVICE=N150
          fi

          pytest -n auto ${{ join(matrix.model-group.model-list, ' ') }} -m models_performance_bare_metal ${{ matrix.model-group.name == 'cnn_javelin' && '--timeout=480' || '' }}

          ## Merge all the generated reports
          env python3 models/perf/merge_perf_results.py
      # TODO: Fix the pipeline before enabling notifications.
      #- uses: tenstorrent/tt-metal/.github/actions/slack-report@main
      #  if: ${{ failure() }}
      #  with:
      #    slack_webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
      - name: Check perf report exists
        id: check-perf-report
        if: ${{ !cancelled() }}
        run: |
          ls -hal
          export PERF_REPORT_FILENAME=Models_Perf_$(date +%Y_%m_%d).csv
          ls -hal $PERF_REPORT_FILENAME
          echo "perf_report_filename=$PERF_REPORT_FILENAME" >> "$GITHUB_OUTPUT"
      - name: Upload perf report
        if: ${{ !cancelled() && steps.check-perf-report.conclusion == 'success' }}
        uses: actions/upload-artifact@v4
        timeout-minutes: 10
        with:
          name: perf-report-csv-${{ matrix.model-group.name }}-${{ matrix.test-info.arch }}-${{ matrix.test-info.machine-type }}
          path: "${{ steps.check-perf-report.outputs.perf_report_filename }}"
      - uses: tenstorrent/tt-metal/.github/actions/upload-artifact-with-job-uuid@main
        timeout-minutes: 10
        if: ${{ !cancelled() }}
        with:
          path: generated/test_reports/
          prefix: "test_reports_"
      - name: Disable Performance mode
        if: always()
        run: sudo cpupower frequency-set -g ondemand
