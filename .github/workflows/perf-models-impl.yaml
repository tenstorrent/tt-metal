name: "[internal] Perf models impl"

on:
  workflow_call:
    inputs:
      build-artifact-name:
        required: true
        type: string
      wheel-artifact-name:
        required: true
        type: string
      docker-image:
        required: true
        type: string
      extra-tag:
        required: false
        type: string
        default: "in-service"

jobs:
  models-perf:
    strategy:
      # Do not fail-fast because we need to ensure all tests go to completion
      # so we try not to get hanging machines
      fail-fast: false
      matrix:
        test-info: [
          {name: "N300 WH B0", model-group: "llm_javelin", timeout: 20, arch: wormhole_b0, runs-on: ["N300", "pipeline-perf", "bare-metal", "${{ inputs.extra-tag }}"], machine-type: "bare_metal"},
          {name: "N300 WH B0", model-group: "cnn_javelin", timeout: 10, arch: wormhole_b0, runs-on: ["N300", "pipeline-perf", "bare-metal", "${{ inputs.extra-tag }}"], machine-type: "bare_metal"},
          {name: "N300 WH B0", model-group: "other", timeout: 5, arch: wormhole_b0, runs-on: ["N300", "pipeline-perf", "bare-metal", "${{ inputs.extra-tag }}"], machine-type: "bare_metal"},
          {name: "N300 WH B0", model-group: "other_magic_env", timeout: 45, arch: wormhole_b0, runs-on: ["N300", "pipeline-perf", "bare-metal", "${{ inputs.extra-tag }}"], machine-type: "bare_metal"},
          # Doesn't work on blackhole P150 {name: "P150 BH", model-group: "llm_javelin", timeout: 20, arch: blackhole, runs-on: ["P150", "pipeline-functional", "cloud-virtual-machine", "${{ inputs.extra-tag }}"], machine-type: "virtual_machine"},
          {name: "P150 BH", model-group: "cnn_javelin", timeout: 10, arch: blackhole, runs-on: ["P150", "pipeline-perf", "bare-metal", "${{ inputs.extra-tag }}"], machine-type: "bare_metal"},
          {name: "P150 BH", model-group: "other", timeout: 5, arch: blackhole, runs-on: ["P150", "pipeline-perf", "bare-metal", "${{ inputs.extra-tag }}"], machine-type: "bare_metal"},
          {name: "P150 BH", model-group: "other_mlperf_required", timeout: 5, arch: blackhole, runs-on: ["P150", "pipeline-functional", "cloud-virtual-machine", "${{ inputs.extra-tag }}"], machine-type: "virtual_machine"},
          {name: "P150 BH", model-group: "other_magic_env", timeout: 45, arch: blackhole, runs-on: ["P150", "pipeline-perf", "bare-metal", "${{ inputs.extra-tag }}"], machine-type: "bare_metal"},
        ]

    name: "${{ matrix.test-info.model-group }} ${{ matrix.test-info.name }}"
    defaults:
      run:
        shell: bash
    runs-on: ${{ matrix.test-info.runs-on }}
    env:
      DOCKER_IMAGE: ${{ inputs.docker-image }}
      DOCKER_PASSWORD: ${{ secrets.GITHUB_TOKEN }}
      DOCKER_OPTS: |
        -v /mnt/MLPerf:/mnt/MLPerf:ro
        -e TT_METAL_HOME=${{ github.workspace }}
        -e ARCH_NAME=${{ matrix.test-info.arch }}
        -e LD_LIBRARY_PATH=${{ github.workspace }}/build/lib
        -e GTEST_OUTPUT=xml:generated/test_reports/
        -e GITHUB_ACTIONS=true
        -e LOGURU_LEVEL=INFO
        ${{ (matrix.test-info.arch == 'wormhole_b0' || (matrix.test-info.arch == 'blackhole' && matrix.test-info.model-group == 'other_mlperf_required')) && '-e HF_HUB_CACHE=/mnt/MLPerf/huggingface/hub' || '' }}
      PIPELINE: model-perf
    steps:
      - name: ⬇️ Checkout
        uses: actions/checkout@v4
        with:
          submodules: recursive
      - name: ⬇️ Download Build
        uses: actions/download-artifact@v4
        timeout-minutes: 10
        with:
          name: ${{ inputs.build-artifact-name }}
      - name: Extract files
        run: tar -xvf ttm_any.tar
      - name: ⬇️ Download Wheel
        uses: actions/download-artifact@v4
        timeout-minutes: 10
        with:
          name: ${{ inputs.wheel-artifact-name }}
      - name: Run falcon7b model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/falcon7b_common/tests -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'llm_javelin' }}
      - name: Run mamba model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/wormhole/mamba/tests -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'llm_javelin' }}
      - name: Run functional_unet model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/experimental/functional_unet/tests -m models_performance_bare_metal --timeout=480
        if: ${{ !cancelled() && matrix.test-info.model-group == 'cnn_javelin' }}
      - name: Run stable_diffusion model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/docker-run
        with:
          docker_image: ${{ inputs.docker-image }}
          docker_password: ${{ secrets.GITHUB_TOKEN }}
          docker_opts: ${{ env.DOCKER_OPTS }}
          install_wheel: true
          run_args: pytest -n auto models/demos/${{ matrix.test-info.arch == 'blackhole' && 'blackhole' || 'wormhole' }}/stable_diffusion/tests -m models_performance_bare_metal --timeout=480
        if: ${{ !cancelled() && matrix.test-info.model-group == 'cnn_javelin' }}
      - name: Run covnet_mnist model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/convnet_mnist/tests/ -m models_performance_bare_metal
        if: ${{ !cancelled() && ((matrix.test-info.arch == 'wormhole_b0' && matrix.test-info.model-group == 'other') || (matrix.test-info.arch == 'blackhole' && matrix.test-info.model-group == 'other_mlperf_required')) }}
      - name: Run bert_tiny model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/bert_tiny/tests/test_performance.py -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other' }}
      - name: Run squeezebert model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/squeezebert/tests/test_performance.py -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other' }}
      - name: Run roberta model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/roberta/tests/test_performance.py -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other' }}
      # issue: https://github.com/tenstorrent/tt-metal/issues/27745
      # - name: Run yolov10x model_perf test
      #   timeout-minutes: ${{ matrix.test-info.timeout }}
      #   uses: ./.github/actions/single-card-perf-test
      #   with:
      #     commands: pytest -n auto models/demos/yolov10x/tests/perf/ -m models_performance_bare_metal
      #  if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run sentence_bert model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/sentence_bert/tests/perf/ -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run resnet50 model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/${{ matrix.test-info.arch == 'blackhole' && 'blackhole' || 'wormhole' }}/resnet50/tests/test_perf_e2e_resnet50.py -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run yolov11 model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/yolov11/tests/perf/ -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run yolov11m model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/yolov11m/tests/perf/ -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run bert_tiny model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/wormhole/bert_tiny/tests/test_performance.py -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run yolov4 model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/yolov4/tests/perf/ -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run yolov7 model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/yolov7/tests/perf/ -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run distilbert model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/wormhole/distilbert/tests/test_perf_distilbert.py -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run segformer model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/segformer/tests/perf/ -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run metal_BERT_large_11 model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/metal_BERT_large_11/tests/ -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run yolov8s_world model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/yolov8s_world/tests/perf/ -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run yolov8s model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/yolov8s/tests/perf/ -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run yolov6l model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/yolov6l/tests/perf/ -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run mobilenetv2 model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/mobilenetv2/tests/perf/ -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run vgg_unet model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/vgg_unet/tests/ -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run yolov9c model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/yolov9c/tests/perf/ -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run vanilla_unet model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/vanilla_unet/tests/perf/ -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run yolov8x model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/yolov8x/tests/perf/ -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run swin_v2 model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/experimental/swin_v2/tests/perf/ -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run swin_s model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/experimental/swin_s/tests/perf/ -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run yolov5x model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/experimental/yolov5x/tests/perf/ -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run vovnet model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/experimental/vovnet/tests/perf/ -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run yolov12x model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/yolov12x/tests/perf/ -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run ufld_v2 model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/demos/ufld_v2/tests/perf/ -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Run efficientnetb0 model_perf test
        timeout-minutes: ${{ matrix.test-info.timeout }}
        uses: ./.github/actions/single-card-perf-test
        with:
          commands: pytest -n auto models/experimental/efficientnetb0/tests/perf/ -m models_performance_bare_metal
        if: ${{ !cancelled() && matrix.test-info.model-group == 'other_magic_env' }}
      - name: Merge all the generated reports
        timeout-minutes: 10
        uses: ./.github/actions/docker-run
        with:
          docker_image: ${{ inputs.docker-image }}
          docker_password: ${{ secrets.GITHUB_TOKEN }}
          docker_opts: ${{ env.DOCKER_OPTS }}
          run_args: |
            # Merge all the generated reports
            env python3 models/perf/merge_perf_results.py CHECK
        if: ${{ !cancelled() }}

      # TODO: Fix the pipeline before enabling notifications.
      #- uses: tenstorrent/tt-metal/.github/actions/slack-report@main
      #  if: ${{ failure() }}
      #  with:
      #    slack_webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
      - name: Check perf report exists
        id: check-perf-report
        if: ${{ !cancelled() }}
        run: |
          ls -hal
          export PERF_REPORT_FILENAME=Models_Perf_$(date +%Y_%m_%d).csv
          ls -hal $PERF_REPORT_FILENAME
          echo "perf_report_filename=$PERF_REPORT_FILENAME" >> "$GITHUB_OUTPUT"
      - name: Upload perf report
        if: ${{ !cancelled() && steps.check-perf-report.conclusion == 'success' }}
        uses: actions/upload-artifact@v4
        timeout-minutes: 10
        with:
          name: perf-report-csv-${{ matrix.test-info.model-group }}-${{ matrix.test-info.arch }}-${{ matrix.test-info.machine-type }}
          path: "${{ steps.check-perf-report.outputs.perf_report_filename }}"
      - uses: tenstorrent/tt-metal/.github/actions/upload-artifact-with-job-uuid@main
        timeout-minutes: 10
        if: ${{ !cancelled() }}
        with:
          path: generated/test_reports/
          prefix: "test_reports_"
