name: "[internal] T3000 perf tests impl"

on:
  workflow_call:
    inputs:
      build-artifact-name:
        required: true
        type: string
      wheel-artifact-name:
        required: true
        type: string
      docker-image:
        required: true
        type: string
      model:
        required: false
        type: string
        default: "all"
      extra-tag:
        required: false
        type: string
        default: "in-service"

jobs:
  load-test-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.load-test-matrix.outputs.matrix }}
    env:
      TESTS_YAML_PATH: ./tests/pipeline_reorg/t3k_perf_tests.yaml
    steps:
      - name: Checkout the repository
        uses: actions/checkout@v4
        with:
          submodules: recursive
      - name: Install dependencies
        run: pip3 install PyYAML
      - name: Verify test timeouts against budget
        run: |
          set -e
          echo "Verifying that timeouts defined in tests.yaml are within the allowed limits..."
          python3 .github/scripts/utils/verify_time_budget.py \
            ${{ env.TESTS_YAML_PATH }} \
            ./.github/time_budget.yaml \
            "perf"
      - name: Load test matrix
        id: load-test-matrix
        run: |
          set -e
          # Install yq
          YQ_VERSION="v4.44.1"
          wget "https://github.com/mikefarah/yq/releases/download/${YQ_VERSION}/yq_linux_amd64" -O /usr/local/bin/yq
          chmod +x /usr/local/bin/yq
          # Check if the file exists
          if [ ! -f "$TESTS_YAML_PATH" ]; then
            echo "::error::Test matrix file not found at: $TESTS_YAML_PATH"
            exit 1
          fi
          # Sanitize line endings
          sanitized_yaml=$(sed 's/\r$//' $TESTS_YAML_PATH)
          # Convert to JSON
          json_output=$(echo "${sanitized_yaml}" | yq '. | to_json(0)')
          # Filter by model if specified
          echo "[INFO] Selected model: ${{ inputs.model }}"
          if [ "${{ inputs.model }}" = "all" ]; then
            filtered_output="$json_output"
            echo "[INFO] Loading all tests from YAML"
          else
            # Filter tests by model-name field
            # Include tests where model-name matches OR model-name doesn't exist (always-run tests)
            filtered_output=$(echo "$json_output" | jq -c --arg model "${{ inputs.model }}" '[.[] | select(.["model-name"] == $model or (.["model-name"] == null))]')
            echo "[INFO] Filtered tests by model: ${{ inputs.model }}"
          fi
          # Print the JSON output
          echo "Read contents of $TESTS_YAML_PATH and converted to JSON:"
          echo "${filtered_output}"
          echo "matrix=${filtered_output}" >> $GITHUB_OUTPUT

  t3000-perf-tests:
    needs: load-test-matrix
    strategy:
      fail-fast: false
      matrix:
        test-group: ${{ fromJson(needs.load-test-matrix.outputs.matrix) }}
    name: ${{ matrix.test-group.name }}
    runs-on:
      - arch-wormhole_b0
      - config-t3000
      - pipeline-perf
      - ${{ inputs.extra-tag }}
    container:
      image: ${{ inputs.docker-image || 'docker-image-unresolved!' }}
      env:
        TT_METAL_HOME: /work
        PYTHONPATH: /work
        LD_LIBRARY_PATH: /work/build/lib
        ARCH_NAME: wormhole_b0
        LOGURU_LEVEL: INFO
        GITHUB_ACTIONS: true
        GTEST_OUTPUT: xml:/work/generated/test_reports/
        HF_HUB_CACHE: /mnt/MLPerf/huggingface/hub
      volumes:
        - ${{ github.workspace }}/docker-job:/work # Subdir to workaround https://github.com/actions/runner/issues/691
        - /dev/hugepages-1G:/dev/hugepages-1G
        - /mnt/MLPerf:/mnt/MLPerf:ro
      options: "--device /dev/tenstorrent --privileged -v /sys:/sys"
    defaults:
      run:
        shell: bash
        working-directory: /work # https://github.com/actions/runner/issues/878
    steps:
      - name: ðŸ§¬ Checkout Repository
        uses: actions/checkout@v4
        with:
          submodules: recursive
          path: docker-job
      - name: Mark repository as safe directory
        run: |
          git config --global --add safe.directory $GITHUB_WORKSPACE
      - name: â¬‡ï¸  Setup Job
        uses: ./docker-job/.github/actions/setup-job
        timeout-minutes: 10
        with:
          build-artifact-name: ${{ inputs.build-artifact-name }}
          wheel-artifact-name: ${{ inputs.wheel-artifact-name }}
      - name: Enable Performance mode
        uses: tenstorrent/tt-metal/.github/actions/set-cpu-governor@main
        with:
          governor: performance
      - name: ${{ matrix.test-group.name }}
        timeout-minutes: ${{ matrix.test-group.timeout }}
        run: |
          source tests/scripts/t3000/run_t3000_perf_tests.sh
          echo ${{ matrix.test-group.cmd }}
          ${{ matrix.test-group.cmd }}
          env python3 models/perf/merge_perf_results.py
      - name: Save environment data
        id: save-environment-data
        if: ${{ matrix.test-group.save-perf-data && !cancelled() }}
        shell: bash
        run: |
          if [ -d "generated/benchmark_data" ]; then
            echo "has_benchmark_data=true" >> $GITHUB_OUTPUT
            python3 .github/scripts/data_analysis/create_benchmark_with_environment_json.py
          else
            echo "::warning::Benchmark data directory 'generated/benchmark_data' does not exist"
            echo "has_benchmark_data=false" >> $GITHUB_OUTPUT
          fi
      - name: Upload benchmark data
        if: ${{ matrix.test-group.save-perf-data && !cancelled() && steps.save-environment-data.conclusion == 'success' && steps.save-environment-data.outputs.has_benchmark_data == 'true' }}
        uses: ./.github/actions/upload-data-via-sftp
        with:
          ssh-private-key: ${{ secrets.SFTP_BENCHMARK_WRITER_KEY }}
          sftp-batchfile: .github/actions/upload-data-via-sftp/benchmark_data_batchfile.txt
          path: /work
          username: ${{ secrets.SFTP_BENCHMARK_WRITER_USERNAME }}
          hostname: ${{ secrets.SFTP_BENCHMARK_WRITER_HOSTNAME }}
      - name: Check perf report exists
        id: check-perf-report
        run: |
          TODAY=$(date +%Y_%m_%d)
          PERF_REPORT_FILENAME_MODELS="Models_Perf_${TODAY}.csv"
          if [ -f "$PERF_REPORT_FILENAME_MODELS" ]; then
            echo "Found Models Perf report: $PERF_REPORT_FILENAME_MODELS"
            echo "perf_report_filename=$PERF_REPORT_FILENAME_MODELS" >> "$GITHUB_OUTPUT"
          else
            echo "No Models perf report found for today."
            exit 1
          fi
      - name: Upload Models perf report
        if: ${{ steps.check-perf-report.outputs.perf_report_filename != '' }}
        uses: actions/upload-artifact@v4
        timeout-minutes: 10
        with:
          name: perf-report-csv-${{ matrix.test-group.model-type }}-wormhole_b0-${{ matrix.test-group.model-name }}-bare-metal
          path: "${{ steps.check-perf-report.outputs.perf_report_filename }}"
      - uses: tenstorrent/tt-metal/.github/actions/slack-report@main
        if: ${{ failure() }}
        with:
          slack_webhook_url: ${{ secrets.SLACK_METAL_INFRA_PIPELINE_STATUS_ALERT }}
          owner: ${{ matrix.test-group.owner_id }}
      - uses: tenstorrent/tt-metal/.github/actions/upload-artifact-with-job-uuid@main
        timeout-minutes: 10
        if: ${{ !cancelled() }}
        with:
          path: generated/test_reports/
          prefix: "test_reports_"
      - name: Disable Performance mode
        if: always()
        uses: tenstorrent/tt-metal/.github/actions/set-cpu-governor@main
        with:
          governor: ondemand
