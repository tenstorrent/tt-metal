name: "[internal] Validate installation step from instructions impl"


on:
  workflow_call:

jobs:
  install-and-run-mnist:
    name: TT-Installer and run MNIST MLP
    runs-on: ${{ format('tt-ubuntu-2204-{0}-viommu-stable', 'N150') }}
    timeout-minutes: 30
    steps:
      - name: Prepare system
        run: |
          # The runners come with docker preinstalled but we want to
          # simulate a completely blank system for our Podman install
          sudo apt-get purge -y docker-ce docker-ce-cli containerd.io

      - name: Install TT dependencies via TT-Installer
        shell: bash
        run: |
          set -euo pipefail
          timeout 60 curl -fsSL https://github.com/tenstorrent/tt-installer/releases/download/v2.1.0/install.sh -O
          chmod +x install.sh

          timeout 600 bash ./install.sh \
            --update-firmware=off \
            --install-metalium-models-container \
            --verbose \
            --reboot-option=never \
            --mode-non-interactive

      - name: Run MNIST MLP via tt-metalium-models
        shell: bash
        timeout-minutes: 10
        run: |
          set -euo pipefail
          export PATH=~/.local/bin:$PATH
          tt-metalium-models -c "python3 ttnn/tutorials/basic_python/ttnn_mlp_inference_mnist.py"

  install-and-run-falcon7b:
    name: TT-Installer and run Falcon7b Demo
    runs-on: ${{ format('tt-ubuntu-2204-{0}-viommu-stable', 'N150') }}
    timeout-minutes: 90
    steps:
      - name: Prepare system
        run: |
          # The runners come with docker preinstalled but we want to
          # simulate a completely blank system for our Podman install
          sudo apt-get purge -y docker-ce docker-ce-cli containerd.io

      - name: Install TT dependencies via TT-Installer
        shell: bash
        run: |
          set -euo pipefail
          timeout 60 curl -fsSL https://github.com/tenstorrent/tt-installer/releases/download/v2.1.0/install.sh -O
          chmod +x install.sh

          timeout 600 bash ./install.sh \
            --update-firmware=off \
            --install-metalium-models-container \
            --verbose \
            --reboot-option=never \
            --mode-non-interactive

      - name: Download Falcon 7B weights from LFC
        shell: bash
        timeout-minutes: 15
        run: |
          set -euo pipefail
          LFC_BASE="http://large-file-cache.large-file-cache.svc.cluster.local//mldata/model_checkpoints/pytorch/huggingface"
          # Download under tt_dnn-models/Falcon so once workspace is mounted at /mnt/MLPerf,
          # model_location_generator finds /mnt/MLPerf/tt_dnn-models/Falcon/tiiuae/falcon-7b-instruct
          mkdir -p tt_dnn-models/Falcon
          wget -r -nH -x --cut-dirs=5 -np --progress=dot:giga -R "index.html*" -P tt_dnn-models/Falcon "${LFC_BASE}/tiiuae/falcon-7b-instruct/"
      - name: Patch tt-metalium-models to mount workspace volume
        shell: bash
        run: |
          set -euo pipefail
          export PATH=~/.local/bin:$PATH
          SCRIPT=$(which tt-metalium-models)
          ls -al $(pwd)
          ls -al tt_dnn-models/Falcon
          sed -i '/--volume=.*hugepages/a\'$'\t''--volume=$(pwd):/mnt/MLPerf \\' "$SCRIPT"
      - name: Run Falcon7b Demo via tt-metalium-models
        shell: bash
        timeout-minutes: 60
        run: |
          set -euo pipefail
          export PATH=~/.local/bin:$PATH
          tt-metalium-models \
            -c "pytest --timeout=1200 --disable-warnings -q -s \
              --input-method=json \
              --input-path='models/demos/ttnn_falcon7b/demo/input_data.json' \
              models/demos/ttnn_falcon7b/demo/demo.py::test_demo"

  install-and-run-llama70b-t3k:
    name: TT-Installer and run T3K Llama 70B Demo
    runs-on: ["arch-wormhole_b0", "config-t3000", "in-service", "bare-metal"]
    timeout-minutes: 120
    steps:
      - name: Prepare system
        run: |
          # The runners come with docker preinstalled but we want to
          # simulate a completely blank system for our Podman install
          sudo apt-get purge -y docker-ce docker-ce-cli containerd.io

      - name: Install TT dependencies via TT-Installer
        shell: bash
        run: |
          set -euo pipefail
          timeout 60 curl -fsSL https://github.com/tenstorrent/tt-installer/releases/download/v2.1.0/install.sh -O
          chmod +x install.sh

          timeout 600 bash ./install.sh \
            --update-firmware=off \
            --install-metalium-models-container \
            --verbose \
            --reboot-option=never \
            --mode-non-interactive

      - name: Download Llama 70B weights from LFC
        shell: bash
        timeout-minutes: 30
        run: |
          set -euo pipefail
          LFC_BASE="http://yyz2-lfcache564.yyz2.tenstorrent.com/mldata/model_checkpoints/pytorch/huggingface"
          # Download under huggingface/tt_cache so when workspace is mounted at /mnt/MLPerf,
          # TT_CACHE_PATH=/mnt/MLPerf/huggingface/tt_cache/meta-llama/Llama-3.1-70B-Instruct is found
          mkdir -p huggingface/tt_cache
          wget -r -nH -x --cut-dirs=4 -np --progress=dot:giga -R "index.html*" -P huggingface/tt_cache "${LFC_BASE}/meta-llama/Llama-3.1-70B-Instruct/"

      - name: Patch tt-metalium-models to mount workspace volume
        shell: bash
        run: |
          set -euo pipefail
          export PATH=~/.local/bin:$PATH
          SCRIPT=$(which tt-metalium-models)
          if ! grep -q '$(pwd):/mnt/MLPerf' "$SCRIPT" 2>/dev/null; then
            sed -i '/--volume=.*hugepages/a\'$'\t''--volume=$(pwd):/mnt/MLPerf \\' "$SCRIPT"
          fi

      - name: Run T3K Llama 70B Demo via tt-metalium-models
        shell: bash
        timeout-minutes: 90
        run: |
          set -euo pipefail
          export PATH=~/.local/bin:$PATH
          tt-metalium-models \
            -c "export TT_CACHE_HOME=/mnt/MLPerf/huggingface/tt_cache && export HF_MODEL=meta-llama/Llama-3.1-70B-Instruct && export TT_CACHE_PATH=/mnt/MLPerf/huggingface/tt_cache/meta-llama/Llama-3.1-70B-Instruct && pytest --timeout=1800 --disable-warnings -q -s models/tt_transformers/demo/simple_text_demo.py -k 'not performance-ci-stress-1'"
