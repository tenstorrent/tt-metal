name: "ttnn - Run sweeps"

on:
  workflow_dispatch:
    inputs:
      sweep_name:
        type: choice
        description: "Which sweep module to run?"
        required: true
        default: "ALL SWEEPS (Nightly)"
        options:
          - ALL SWEEPS (Nightly)
          - ALL SWEEPS (Comprehensive)
          - ALL SWEEPS (Model Traced)
          - ALL SWEEPS (Lead Models)
          - add
          - ccl.generality.all_broadcast
          - ccl.generality.all_gather
          - ccl.generality.all_reduce
          - ccl.generality.all_to_all_combine
          - ccl.generality.all_to_all_dispatch
          - ccl.generality.all_to_all
          - ccl.generality.point_to_point
          - ccl.generality.reduce_scatter
          - composite.binary.gcd.gcd
          - composite.binary.lcm.lcm
          - conv2d.short.conv2d_short_sweep
          - conv2d.short.conv2d_ttforge_sweep
          - conv_transpose2d.short.conv_transpose2d_short_sweep
          - creation.empty.empty
          - creation.zeros.zeros
          - creation.zeros_like.zeros_like
          - data_movement.backward.concat_bw.concat_bw
          - data_movement.concat.concat_interleaved
          - data_movement.concat.concat_interleaved_n_tensors
          - data_movement.concat.concat_pytorch2
          - data_movement.concat.concat_sharded
          - data_movement.copy.copy
          - data_movement.embedding.embedding_pytorch2
          - data_movement.expand.expand_pytorch2
          - data_movement.fill.fill_pytorch2
          - data_movement.index_select.index_select_pytorch2
          - data_movement.interleaved_to_sharded.interleaved_to_sharded_e2e
          - data_movement.nonzero.nonzero
          - data_movement.pad.pad
          - data_movement.permute.permute
          - data_movement.permute.permute_pytorch2_rm
          - data_movement.permute.permute_pytorch2_tiled
          - data_movement.repeat.repeat
          - data_movement.repeat.repeat_pytorch2
          - data_movement.repeat_interleave.repeat_interleave
          - data_movement.reshape.reshape
          - data_movement.slice.slice_forge
          - data_movement.slice.slice_pytorch2_rm
          - data_movement.slice.slice_pytorch2_tiled
          - data_movement.split.split_pytorch2
          - data_movement.squeeze.squeeze_pytorch2
          - data_movement.stack.stack_pytorch2
          - data_movement.transpose.t_pytorch2
          - data_movement.transpose.transpose_forge
          - data_movement.transpose.transpose_interleaved
          - data_movement.transpose.transpose_pytorch2
          - data_movement.unsqueeze.unsqueeze_pytorch2
          - data_movement.view.view_pytorch2
          - data_movement.view.view_tt_torch
          - eltwise.binary.add.add_all_pytorch2
          - eltwise.binary.add.add_different_memory_configs
          - eltwise.binary.add.add_forge
          - eltwise.binary.add.add_llama
          - eltwise.binary.add.add_set2_pytorch2
          - eltwise.binary.atan2.atan2
          - eltwise.binary.atan2.atan2_sharded
          - eltwise.binary.bcast.bcast
          - eltwise.binary.bcast.bcast_h_sharded
          - eltwise.binary.div.div
          - eltwise.binary.div.div_forge
          - eltwise.binary.div.div_tensor_pytorch2
          - eltwise.binary.div_no_nan.div_no_nan
          - eltwise.binary.eq.eq_forge
          - eltwise.binary.eq.eq_scalar_pytorch2
          - eltwise.binary.floor_divide.floor_divide
          - eltwise.binary.floor_divide.floor_divide_pytorch2
          - eltwise.binary.fmod.fmod
          - eltwise.binary.fmod.fmod_unary
          - eltwise.binary.fmod.fmod_unary_sharded
          - eltwise.binary.ge.ge_forge
          - eltwise.binary.gt.gt_forge
          - eltwise.binary.gt.gt_scalar_pytorch2
          - eltwise.binary.hypot.hypot
          - eltwise.binary.isclose.isclose
          - eltwise.binary.isclose.isclose_sharded
          - eltwise.binary.ldexp.ldexp
          - eltwise.binary.ldexp.ldexp_sharded
          - eltwise.binary.le.le_tensor_pytorch2
          - eltwise.binary.logaddexp.logaddexp
          - eltwise.binary.logaddexp.logaddexp_sharded
          - eltwise.binary.logaddexp2.logaddexp2
          - eltwise.binary.logaddexp2.logaddexp2_sharded
          - eltwise.binary.logical_and.logical_and
          - eltwise.binary.logical_and.logical_and_
          - eltwise.binary.logical_and.logical_and_forge
          - eltwise.binary.logical_and.logical_and_output
          - eltwise.binary.logical_and.logical_and_sharded
          - eltwise.binary.logical_or.logical_or
          - eltwise.binary.logical_or.logical_or_
          - eltwise.binary.logical_or.logical_or_forge
          - eltwise.binary.logical_or.logical_or_output
          - eltwise.binary.logical_or.logical_or_sharded
          - eltwise.binary.logical_xor.logical_xor
          - eltwise.binary.logical_xor.logical_xor_
          - eltwise.binary.logical_xor.logical_xor_sharded
          - eltwise.binary.lt.lt_forge
          - eltwise.binary.lt.lt_scalar_pytorch2
          - eltwise.binary.lt.lt_tensor_pytorch2
          - eltwise.binary.multiply.mul_no_act_llama
          - eltwise.binary.multiply.mul_tensor_pytorch2
          - eltwise.binary.multiply.multiply
          - eltwise.binary.multiply.multiply_forge
          - eltwise.binary.multiply.multiply_llama
          - eltwise.binary.multiply.multiply_scalar_pytorch2
          - eltwise.binary.ne.ne_forge
          - eltwise.binary.ne.ne_scalar_pytorch2
          - eltwise.binary.polyval.polyval
          - eltwise.binary.polyval.polyval_sharded
          - eltwise.binary.remainder.remainder
          - eltwise.binary.remainder.remainder_forge
          - eltwise.binary.remainder.remainder_scalar_pytorch2
          - eltwise.binary.remainder.remainder_shape
          - eltwise.binary.remainder.remainder_unary
          - eltwise.binary.remainder.remainder_unary_sharded
          - eltwise.binary.rpow.rpow
          - eltwise.binary.rpow.rpow_sharded
          - eltwise.binary.squared_difference.squared_difference
          - eltwise.binary.squared_difference_output.squared_difference_output
          - eltwise.binary.subtract.subtract
          - eltwise.binary.subtract.subtract_forge
          - eltwise.binary.subtract.subtract_tensor_pytorch2
          - eltwise.binary.xlogy.xlogy
          - eltwise.binary_backward.add_bw.add_bw
          - eltwise.binary_backward.addalpha_bw.addalpha_bw
          - eltwise.binary_backward.div_bw.div_bw
          - eltwise.binary_backward.fmod_bw.fmod_bw
          - eltwise.binary_backward.hypot_bw.hypot_bw
          - eltwise.binary_backward.ldexp_bw.ldexp_bw
          - eltwise.binary_backward.ldexp_bw.ldexp_bw_sharded
          - eltwise.binary_backward.logaddexp2_bw.logaddexp2_bw
          - eltwise.binary_backward.logaddexp2_bw.logaddexp2_bw_sharded
          - eltwise.binary_backward.logaddexp_bw.logaddexp_bw
          - eltwise.binary_backward.logaddexp_bw.logaddexp_bw_sharded
          - eltwise.binary_backward.mul_bw.mul_bw
          - eltwise.binary_backward.remainder_bw.remainder_bw
          - eltwise.binary_backward.rsub_bw.rsub_bw
          - eltwise.binary_backward.squared_difference_bw.squared_difference_bw
          - eltwise.binary_backward.sub_bw.sub_bw
          - eltwise.binary_backward.sub_bw.sub_bw_sharded
          - eltwise.binary_backward.subalpha_bw.subalpha_bw
          - eltwise.binary_backward.xlogy_bw.xlogy_bw
          - eltwise.binary_complex.add_bw.add_bw
          - eltwise.binary_complex.div_bw.div_bw
          - eltwise.binary_complex.mul_bw.mul_bw
          - eltwise.binary_complex.sub_bw.sub_bw
          - eltwise.composite.binary.addalpha.addalpha
          - eltwise.composite.binary.maximum.maximum
          - eltwise.composite.binary.maximum.maximum_forge
          - eltwise.composite.binary.maximum.maximum_pytorch2
          - eltwise.composite.binary.maximum.maximum_sharded
          - eltwise.composite.binary.maximum.maximum_unary
          - eltwise.composite.binary.maximum.maximum_unary_sharded
          - eltwise.composite.binary.minimum.minimum
          - eltwise.composite.binary.minimum.minimum_forge
          - eltwise.composite.binary.minimum.minimum_pytorch2
          - eltwise.composite.binary.minimum.minimum_sharded
          - eltwise.composite.binary.minimum.minimum_unary
          - eltwise.composite.binary.minimum.minimum_unary_sharded
          - eltwise.composite.binary.pow.pow_pytorch2
          - eltwise.composite.binary.pow.pow_scalar_pytorch2
          - eltwise.composite.binary.pow.pow_tensor_pytorch2
          - eltwise.composite.binary.subalpha.subalpha
          - eltwise.ternary.addcdiv.addcdiv
          - eltwise.ternary.addcdiv.addcdiv_sharded
          - eltwise.ternary.addcmul.addcmul
          - eltwise.ternary.addcmul.addcmul_sharded
          - eltwise.ternary.lerp.lerp
          - eltwise.ternary.mac.mac
          - eltwise.ternary.where.where
          - eltwise.ternary.where.where_forge
          - eltwise.ternary.where.where_pytorch2
          - eltwise.ternary_backward.addcdiv_bw
          - eltwise.ternary_backward.addcmul_bw
          - eltwise.unary.abs.abs
          - eltwise.unary.abs.abs_forge
          - eltwise.unary.abs.abs_pytorch2
          - eltwise.unary.abs.abs_sharded
          - eltwise.unary.acos.acos
          - eltwise.unary.acos.acos_sharded
          - eltwise.unary.acosh.acosh
          - eltwise.unary.acosh.acosh_sharded
          - eltwise.unary.asinh.asinh
          - eltwise.unary.asinh.asinh_sharded
          - eltwise.unary.atan.atan
          - eltwise.unary.atan.atan_sharded
          - eltwise.unary.atanh.atanh
          - eltwise.unary.atanh.atanh_sharded
          - eltwise.unary.bitwise.bitwise_and.bitwise_and
          - eltwise.unary.bitwise.bitwise_and.bitwise_and_sharded
          - eltwise.unary.bitwise.bitwise_and.bitwise_and_unary
          - eltwise.unary.bitwise.bitwise_and.bitwise_and_unary_sharded
          - eltwise.unary.bitwise.bitwise_left_shift.bitwise_left_shift
          - eltwise.unary.bitwise.bitwise_left_shift.bitwise_left_shift_sharded
          - eltwise.unary.bitwise.bitwise_left_shift.bitwise_left_shift_unary
          - eltwise.unary.bitwise.bitwise_left_shift.bitwise_left_shift_unary_sharded
          - eltwise.unary.bitwise.bitwise_not.bitwise_not
          - eltwise.unary.bitwise.bitwise_not.bitwise_not_pytorch2
          - eltwise.unary.bitwise.bitwise_not.bitwise_not_sharded
          - eltwise.unary.bitwise.bitwise_or.bitwise_or
          - eltwise.unary.bitwise.bitwise_or.bitwise_or_sharded
          - eltwise.unary.bitwise.bitwise_or.bitwise_or_unary
          - eltwise.unary.bitwise.bitwise_or.bitwise_or_unary_sharded
          - eltwise.unary.bitwise.bitwise_right_shift.bitwise_right_shift
          - eltwise.unary.bitwise.bitwise_right_shift.bitwise_right_shift_sharded
          - eltwise.unary.bitwise.bitwise_right_shift.bitwise_right_shift_unary
          - eltwise.unary.bitwise.bitwise_right_shift.bitwise_right_shift_unary_sharded
          - eltwise.unary.bitwise.bitwise_xor.bitwise_xor
          - eltwise.unary.bitwise.bitwise_xor.bitwise_xor_sharded
          - eltwise.unary.bitwise.bitwise_xor.bitwise_xor_unary
          - eltwise.unary.bitwise.bitwise_xor.bitwise_xor_unary_sharded
          - eltwise.unary.cbrt.cbrt
          - eltwise.unary.ceil.ceil
          - eltwise.unary.ceil.ceil_pytorch2
          - eltwise.unary.ceil.ceil_sharded
          - eltwise.unary.clamp.clamp
          - eltwise.unary.clamp.clamp_forge
          - eltwise.unary.clamp.clamp_min_pytorch2
          - eltwise.unary.clamp.clamp_pytorch2
          - eltwise.unary.clip.clip
          - eltwise.unary.clone.clone
          - eltwise.unary.cos.cos
          - eltwise.unary.cos.cos_forge
          - eltwise.unary.cos.cos_pytorch2
          - eltwise.unary.cos.cos_sharded
          - eltwise.unary.cosh.cosh
          - eltwise.unary.cosh.cosh_sharded
          - eltwise.unary.deg2rad.deg2rad
          - eltwise.unary.deg2rad.deg2rad_sharded
          - eltwise.unary.digamma.digamma
          - eltwise.unary.digamma.digamma_sharded
          - eltwise.unary.elu.elu
          - eltwise.unary.elu.elu_pytorch2
          - eltwise.unary.elu.elu_sharded
          - eltwise.unary.eqz.eqz
          - eltwise.unary.eqz.eqz_sharded
          - eltwise.unary.erf.erf
          - eltwise.unary.erf.erf_sharded
          - eltwise.unary.erfc.erfc
          - eltwise.unary.erfc.erfc_sharded
          - eltwise.unary.erfinv.erfinv
          - eltwise.unary.erfinv.erfinv_sharded
          - eltwise.unary.exp.exp
          - eltwise.unary.exp.exp_forge
          - eltwise.unary.exp.exp_pytorch2
          - eltwise.unary.exp.exp_sharded
          - eltwise.unary.exp2.exp2
          - eltwise.unary.exp2.exp2_sharded
          - eltwise.unary.expm1.expm1
          - eltwise.unary.expm1.expm1_sharded
          - eltwise.unary.floor.floor
          - eltwise.unary.floor.floor_forge
          - eltwise.unary.floor.floor_pytorch2
          - eltwise.unary.frac.frac
          - eltwise.unary.frac.frac_sharded
          - eltwise.unary.geglu.geglu
          - eltwise.unary.gelu.gelu
          - eltwise.unary.gelu.gelu_pytorch2
          - eltwise.unary.gez.gez
          - eltwise.unary.glu.glu
          - eltwise.unary.gtz.gtz
          - eltwise.unary.hardshrink.hardshrink
          - eltwise.unary.hardshrink.hardshrink_sharded
          - eltwise.unary.hardsigmoid.hardsigmoid
          - eltwise.unary.hardsigmoid.hardsigmoid_pytorch2
          - eltwise.unary.hardsigmoid.hardsigmoid_sharded
          - eltwise.unary.hardswish.hardswish
          - eltwise.unary.hardswish.hardswish_pytorch2
          - eltwise.unary.hardswish.hardswish_sharded
          - eltwise.unary.hardtanh.hardtanh
          - eltwise.unary.hardtanh.hardtanh_pytorch2
          - eltwise.unary.hardtanh.hardtanh_sharded
          - eltwise.unary.heaviside.heaviside
          - eltwise.unary.heaviside.heaviside_sharded
          - eltwise.unary.i0.i0
          - eltwise.unary.identity.identity
          - eltwise.unary.identity.identity_sharded
          - eltwise.unary.isfinite.isfinite
          - eltwise.unary.isfinite.isfinite_sharded
          - eltwise.unary.isinf.isinf
          - eltwise.unary.isinf.isinf_sharded
          - eltwise.unary.isnan.isnan
          - eltwise.unary.isnan.isnan_sharded
          - eltwise.unary.isneginf.isneginf
          - eltwise.unary.isneginf.isneginf_sharded
          - eltwise.unary.isposinf.isposinf
          - eltwise.unary.isposinf.isposinf_sharded
          - eltwise.unary.leaky_relu.leaky_relu
          - eltwise.unary.leaky_relu.leaky_relu_pytorch2
          - eltwise.unary.leaky_relu.leaky_relu_sharded
          - eltwise.unary.lez.lez
          - eltwise.unary.lez.lez_sharded
          - eltwise.unary.lgamma.lgamma
          - eltwise.unary.lgamma.lgamma_sharded
          - eltwise.unary.log.log
          - eltwise.unary.log.log_forge
          - eltwise.unary.log.log_pytorch2
          - eltwise.unary.log.log_sharded
          - eltwise.unary.log10.log10
          - eltwise.unary.log10.log10_sharded
          - eltwise.unary.log1p.log1p
          - eltwise.unary.log1p.log1p_sharded
          - eltwise.unary.log2.log2
          - eltwise.unary.log2.log2_sharded
          - eltwise.unary.log_sigmoid.log_sigmoid
          - eltwise.unary.logical_not.logical_not
          - eltwise.unary.logical_not.logical_not_
          - eltwise.unary.logical_not.logical_not_forge
          - eltwise.unary.logical_not.logical_not_output
          - eltwise.unary.logical_not.logical_not_pytorch2
          - eltwise.unary.logical_not.logical_not_sharded
          - eltwise.unary.logit.logit
          - eltwise.unary.logit.logit_forge
          - eltwise.unary.logit.logit_sharded
          - eltwise.unary.ltz.ltz
          - eltwise.unary.mish.mish
          - eltwise.unary.mish.mish_sharded
          - eltwise.unary.multigammaln.multigammaln
          - eltwise.unary.multigammaln.multigammaln_sharded
          - eltwise.unary.neg.neg
          - eltwise.unary.neg.neg_forge
          - eltwise.unary.neg.neg_pytorch2
          - eltwise.unary.neg.neg_sharded
          - eltwise.unary.nez.nez
          - eltwise.unary.nez.nez_sharded
          - eltwise.unary.normalize_global.normalize_global
          - eltwise.unary.normalize_hw.normalize_hw
          - eltwise.unary.prelu.prelu
          - eltwise.unary.prelu.prelu_sharded
          - eltwise.unary.rad2deg.rad2deg
          - eltwise.unary.rdiv.rdiv
          - eltwise.unary.rdiv.rdiv_sharded
          - eltwise.unary.reciprocal.reciprocal
          - eltwise.unary.reciprocal.reciprocal_sharded
          - eltwise.unary.reglu.reglu
          - eltwise.unary.relu.relu
          - eltwise.unary.relu.relu_pytorch2
          - eltwise.unary.relu6.relu6
          - eltwise.unary.relu6.relu6_sharded
          - eltwise.unary.relu_max.relu_max
          - eltwise.unary.relu_max.relu_max_sharded
          - eltwise.unary.relu_min.relu_min
          - eltwise.unary.relu_min.relu_min_sharded
          - eltwise.unary.round.round_sharded
          - eltwise.unary.rsqrt.rsqrt_forge
          - eltwise.unary.rsqrt.rsqrt_pytorch2
          - eltwise.unary.rsub.rsub
          - eltwise.unary.rsub.rsub_pytorch2
          - eltwise.unary.rsub.rsub_sharded
          - eltwise.unary.selu.selu
          - eltwise.unary.selu.selu_sharded
          - eltwise.unary.sigmoid.sigmoid
          - eltwise.unary.sigmoid.sigmoid_pytorch2
          - eltwise.unary.sigmoid.sigmoid_sharded
          - eltwise.unary.sigmoid_accurate.sigmoid_accurate
          - eltwise.unary.sigmoid_accurate.sigmoid_accurate_sharded
          - eltwise.unary.sign.sign
          - eltwise.unary.silu.silu
          - eltwise.unary.silu.silu_llama
          - eltwise.unary.silu.silu_pytorch2
          - eltwise.unary.sin.sin
          - eltwise.unary.sin.sin_forge
          - eltwise.unary.sin.sin_pytorch2
          - eltwise.unary.sin.sin_sharded
          - eltwise.unary.sinh.sinh
          - eltwise.unary.sinh.sinh_sharded
          - eltwise.unary.softplus.softplus
          - eltwise.unary.softplus.softplus_sharded
          - eltwise.unary.softshrink.softshrink_sharded
          - eltwise.unary.sqrt.sqrt_forge
          - eltwise.unary.swiglu.swiglu
          - eltwise.unary.tanh.tanh
          - eltwise.unary.tanh.tanh_forge
          - eltwise.unary.tanh.tanh_pytorch2
          - eltwise.unary.tril.tril
          - eltwise.unary.tril.tril_pytorch2
          - eltwise.unary.tril.tril_sharded
          - eltwise.unary.triu.triu
          - eltwise.unary.triu.triu_sharded
          - eltwise.unary.trunc.trunc
          - eltwise.unary.trunc.trunc_sharded
          - eltwise.unary_backward.abs_bw.abs_bw
          - eltwise.unary_backward.acos_bw.acos_bw
          - eltwise.unary_backward.acos_bw.acos_bw_sharded
          - eltwise.unary_backward.acosh_bw.acosh_bw
          - eltwise.unary_backward.acosh_bw.acosh_bw_sharded
          - eltwise.unary_backward.add_bw.add_bw
          - eltwise.unary_backward.add_bw.add_bw_sharded
          - eltwise.unary_backward.assign_bw.assign_bw
          - eltwise.unary_backward.atan_bw.atan_bw
          - eltwise.unary_backward.atan_bw.atan_bw_sharded
          - eltwise.unary_backward.bias_gelu_bw.bias_gelu_bw
          - eltwise.unary_backward.celu_bw.celu_bw
          - eltwise.unary_backward.clamp_bw.clamp_bw
          - eltwise.unary_backward.clamp_bw.clamp_bw_sharded
          - eltwise.unary_backward.clip_bw.clip_bw
          - eltwise.unary_backward.clip_bw.clip_bw_sharded
          - eltwise.unary_backward.cos_bw.cos_bw
          - eltwise.unary_backward.cos_bw.cos_bw_sharded
          - eltwise.unary_backward.div_bw.div_bw
          - eltwise.unary_backward.elu_bw.elu_bw
          - eltwise.unary_backward.exp_bw.exp_bw
          - eltwise.unary_backward.exp_bw.exp_bw_sharded
          - eltwise.unary_backward.fill_bw.fill_bw
          - eltwise.unary_backward.fill_bw.fill_bw_sharded
          - eltwise.unary_backward.fill_zero_bw.fill_zero_bw
          - eltwise.unary_backward.fill_zero_bw.fill_zero_bw_sharded
          - eltwise.unary_backward.floor_bw.floor_bw
          - eltwise.unary_backward.frac_bw.frac_bw
          - eltwise.unary_backward.frac_bw.frac_bw_sharded
          - eltwise.unary_backward.hardshrink_bw
          - eltwise.unary_backward.hardsigmoid_bw.hardsigmoid_bw
          - eltwise.unary_backward.hardsigmoid_bw.hardsigmoid_bw_sharded
          - eltwise.unary_backward.hardswish_bw.hardswish_bw
          - eltwise.unary_backward.hardtanh_bw.hardtanh_bw
          - eltwise.unary_backward.hardtanh_bw.hardtanh_bw_sharded
          - eltwise.unary_backward.i0_bw.i0_bw
          - eltwise.unary_backward.i0_bw.i0_bw_sharded
          - eltwise.unary_backward.leaky_relu_bw.leaky_relu_bw
          - eltwise.unary_backward.leaky_relu_bw.leaky_relu_bw_sharded
          - eltwise.unary_backward.lgamma_bw.lgamma_bw
          - eltwise.unary_backward.lgamma_bw.lgamma_bw_sharded
          - eltwise.unary_backward.log10_bw.log10_bw
          - eltwise.unary_backward.log_bw.log_bw
          - eltwise.unary_backward.log_bw.log_bw_sharded
          - eltwise.unary_backward.log_sigmoid_bw.log_sigmoid_bw
          - eltwise.unary_backward.log_sigmoid_bw.log_sigmoid_bw_sharded
          - eltwise.unary_backward.logit_bw
          - eltwise.unary_backward.mul_bw.mul_bw
          - eltwise.unary_backward.multigammaln_bw.multigammaln_bw
          - eltwise.unary_backward.multigammaln_bw.multigammaln_bw_sharded
          - eltwise.unary_backward.neg_bw.neg_bw
          - eltwise.unary_backward.neg_bw.neg_bw_sharded
          - eltwise.unary_backward.pow_bw.pow_bw
          - eltwise.unary_backward.pow_bw.pow_bw_sharded
          - eltwise.unary_backward.rad2deg_bw.rad2deg_bw
          - eltwise.unary_backward.rad2deg_bw.rad2deg_bw_sharded
          - eltwise.unary_backward.rdiv_bw.rdiv_bw
          - eltwise.unary_backward.rdiv_bw.rdiv_bw_sharded
          - eltwise.unary_backward.relu6_bw.relu6_bw
          - eltwise.unary_backward.relu6_bw.relu6_bw_sharded
          - eltwise.unary_backward.relu_bw.relu_bw
          - eltwise.unary_backward.relu_bw.relu_bw_sharded
          - eltwise.unary_backward.round_bw.round_bw
          - eltwise.unary_backward.rpow_bw.rpow_bw
          - eltwise.unary_backward.rsqrt_bw.rsqrt_bw
          - eltwise.unary_backward.rsqrt_bw.rsqrt_bw_sharded
          - eltwise.unary_backward.selu_bw.selu_bw
          - eltwise.unary_backward.sigmoid_bw.sigmoid_bw
          - eltwise.unary_backward.silu_bw.silu_bw
          - eltwise.unary_backward.sin_bw.sin_bw
          - eltwise.unary_backward.sinh_bw.sinh_bw
          - eltwise.unary_backward.softplus_bw.softplus_bw
          - eltwise.unary_backward.softplus_bw.softplus_bw_sharded
          - eltwise.unary_backward.softshrink_bw
          - eltwise.unary_backward.sqrt_bw.sqrt_bw
          - eltwise.unary_backward.sqrt_bw.sqrt_bw_sharded
          - eltwise.unary_backward.square_bw.square_bw
          - eltwise.unary_backward.sub_bw.sub_bw
          - eltwise.unary_backward.sub_bw.sub_bw_sharded
          - eltwise.unary_backward.tan_bw.tan_bw
          - eltwise.unary_backward.tan_bw.tan_bw_sharded
          - eltwise.unary_backward.tanh_bw.tanh_bw
          - eltwise.unary_backward.tanh_bw.tanh_bw_sharded
          - eltwise.unary_backward.tanhshrink_bw.tanhshrink_bw
          - eltwise.unary_backward.threshold_bw.threshold_bw
          - eltwise.unary_backward.threshold_bw.threshold_bw_sharded
          - eltwise.unary_backward.trunc_bw.trunc_bw
          - eltwise.unary_backward.trunc_bw.trunc_bw_sharded
          - eltwise.unary_complex.angle.angle
          - eltwise.unary_complex.angle_bw.angle_bw
          - eltwise.unary_complex.conj
          - eltwise.unary_complex.conj_bw
          - eltwise.unary_complex.is_imag
          - eltwise.unary_complex.is_real
          - eltwise.unary_complex.polar.polar
          - eltwise.unary_complex.polar_bw.polar_bw
          - eltwise.unary_complex.reciprocal.reciprocal
          - eltwise.unary_complex.reciprocal.reciprocal_sharded
          - eltwise.unary_complex.reciprocal_bw
          - embedding.embedding
          - embedding_bw.embedding_bw
          - fused.layer_norm_traces
          - fused.softmax_traces
          - losses.l1_loss
          - losses.mse_loss
          - matmul.full.matmul_default_block_sharded
          - matmul.full.matmul_default_height_sharded
          - matmul.full.matmul_default_interleaved
          - matmul.full.matmul_default_width_sharded
          - matmul.generality.linear
          - matmul.generality.matmul
          - matmul.short.matmul
          - matmul.short.matmul_create_program_config
          - matmul.short.matmul_default
          - matmul.short.matmul_default_sharded
          - matmul.short.matmul_traces
          - matmul.short.matmul_user_program_config
          - matmul.short.matmul_user_program_config_mcast_1d
          - matmul.short.matmul_user_program_config_mcast_2d
          - matmul.sparse.sparse_matmul
          - matmul.sparse.batched_sparse_matmul
          - pool2d.full.max_pool2d_large_dims
          - pool2d.full.max_pool2d_params
          - pool2d.short.avg_pool2d_short_sweep
          - pool2d.short.max_pool2d_short_sweep
          - pool2d.global_avg_pool2d
          - pool2d.max_pool2d
          - normalization.batch_norm.batch_norm
          - normalization.generality.layernorm
          - normalization.generality.softmax
          - normalization.generality.softmax_fused
          - normalization.softmax.softmax
          - normalization.softmax.softmax_sharded
          - reduction.argmax.argmax
          - reduction.argmax.argmax_output
          - reduction.backward.prod_bw.prod_bw
          - reduction.cross_entropy_loss.cross_entropy_loss
          - reduction.generality.argmax
          - reduction.generality.reduction_all_ranks
          - reduction.mean.mean
          - reduction.prod
          - reduction.std.std
          - reduction.sum
          - reduction.topk.topk
          - reduction.topk.topk_output
          - reduction.traces.argmax_traces
          - reduction.traces.max_traces
          - reduction.traces.mean_traces
          - reduction.traces.sum_traces
          - reduction.traces.topk_traces
          - reduction.var.var
          - tilize
          - tilize_with_val_padding
          - transformer.attention_softmax.attention_softmax
          - transformer.attention_softmax.attention_softmax_
          - transformer.concatenate_heads.concatenate_heads
          - transformer.rotary_embedding.rotary_embedding
          - transformer.split_query_key_value_and_split_heads.split_query_key_value_and_split_heads
          - transformer.split_query_key_value_and_split_heads.split_query_key_value_and_split_heads_kv_input
          - untilize
          - untilize_with_unpadding
      suite_name:
        description: "Which sweep suite to run?"
        type: string
      skip_on_timeout:
        description: "Skip remaining tests in running suite after first timeout?"
        type: boolean
        default: true
      upload_results:
        description: "Upload sweep results to Superset?"
        type: boolean
        default: true
      measure_device_perf:
        description: "Measure device performance using device profiler (--device-perf flag)."
        type: boolean
        default: true
      measure_e2e_perf:
        description: "Measure end-to-end performance with cold/cached comparison (--perf-with-cache flag)"
        type: boolean
        default: false
      measure_memory:
        description: "Capture peak L1 memory usage using graph trace (--measure-memory flag)."
        type: boolean
        default: false
      arch:
        required: true
        type: choice
        options:
          - wormhole_b0
          - blackhole
        default: "wormhole_b0"
      runner-label:
        description: "Single sweep only"
        required: true
        type: choice
        options:
          - N150
          - N300
          - P100a
          - P150b
          - n300-llmbox
          - topology-6u
          - BH-LLMBox
          - BH-DeskBox
          - BH-LoudBox
        default: "N150"
      log-level:
        description: "Log level for sweep execution. INFO level may overwhelm runners for long sweeps."
        required: true
        type: choice
        options:
          - DEBUG
          - INFO
          - WARNING
          - CRITICAL
        default: "INFO"

  schedule:
    - cron: "30 4 * * 1,2,4,5" # This cron schedule runs the workflow at 4:30 AM UTC on Mon, Tue, Thu, Fri (nightly runs)
    - cron: "0 4 * * 3,6" # This cron schedule runs the workflow at 4:00 AM UTC on Wed/Sat (comprehensive run)
    - cron: "0 4 * * *" # This cron schedule runs the workflow at 4:00 AM UTC daily (model traced runs)
    - cron: "0 3 * * *" # This cron schedule runs the workflow at 3:00 AM UTC daily (lead model runs)
concurrency:
  group: ttnn-sweeps-${{ github.ref }}-${{ github.event.inputs.arch || 'wormhole_b0' }}-${{ github.event.inputs.runner-label || 'N150' }}-${{ github.event.inputs.sweep_name }}

jobs:
  display-workflow-options:
    name: ğŸ“‹ Display workflow options
    runs-on: ubuntu-22.04
    if: github.event_name == 'workflow_dispatch'
    steps:
      - name: Display selected workflow options
        run: |
          echo "::group::ğŸ”§ Workflow Configuration Options"
          echo "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”"
          echo "â”‚                    TTNN Run Sweeps Configuration            â”‚"
          echo "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤"
          echo "â”‚ Sweep Name:           ${{ github.event.inputs.sweep_name || 'ALL SWEEPS (Nightly)' }}"
          echo "â”‚ Suite Name:           ${{ github.event.inputs.suite_name || 'Not specified' }}"
          echo "â”‚ Skip on Timeout:      ${{ github.event.inputs.skip_on_timeout == 'true' && 'true' || 'false' }}"
          echo "â”‚ Upload Results:       ${{ github.event.inputs.upload_results == 'true' && 'true' || 'false' }}"
          echo "â”‚ Measure Device Perf:  ${{ github.event.inputs.measure_device_perf == 'true' && 'true' || 'false' }}"
          echo "â”‚ Measure E2E Perf:     ${{ github.event.inputs.measure_e2e_perf == 'true' && 'true' || 'false' }}"
          echo "â”‚ Measure Memory:       ${{ github.event.inputs.measure_memory == 'true' && 'true' || 'false' }}"
          echo "â”‚ Architecture:         ${{ github.event.inputs.arch || 'wormhole_b0' }}"
          echo "â”‚ Runner Label:         ${{ github.event.inputs['runner-label'] || 'N150' }}"
          echo "â”‚ Log Level:            ${{ github.event.inputs['log-level'] || 'INFO' }}"
          echo "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
          echo "::endgroup::"

          echo "::group::ğŸ“Š Workflow Context Information"
          echo "Run ID: ${{ github.run_id }}"
          echo "Run Number: ${{ github.run_number }}"
          echo "Run Attempt: ${{ github.run_attempt }}"
          echo "Triggered by: ${{ github.actor }}"
          echo "Repository: ${{ github.repository }}"
          echo "Branch/Ref: ${{ github.ref_name }}"
          echo "::endgroup::"
  build-artifact:
    uses: ./.github/workflows/build-artifact.yaml
    permissions:
      packages: write
    secrets: inherit
    with:
      build-wheel: true
      version: "22.04"
      tracy: ${{ github.event_name == 'schedule' || (github.event_name == 'workflow_dispatch' && github.event.inputs.measure_device_perf == 'true') }}
  ttnn-generate-sweeps:
    name: Generate Sweep Vectors ${{ matrix.arch }} ${{ matrix.runner_label }}
    needs: build-artifact
    strategy:
      fail-fast: false
      matrix:
        include:
          # Entry 1: Manual dispatch with single sweep only
          - arch: ${{ github.event.inputs.arch || 'wormhole_b0' }}
            runner_label: ${{ github.event.inputs['runner-label'] || 'N150' }}
            generate_mode: all
          # Entry 2: Scheduled/comprehensive OR manual dispatch with "ALL SWEEPS" - N150 for non-CCL modules
          - arch: ${{ github.event.inputs.arch || 'wormhole_b0' }}
            runner_label: N150
            generate_mode: non_ccl
          # Entry 3: Scheduled/comprehensive OR manual dispatch with "ALL SWEEPS" - n300-llmbox for CCL modules only
          - arch: ${{ github.event.inputs.arch || 'wormhole_b0' }}
            runner_label: n300-llmbox
            generate_mode: ccl_only
    container:
      image: ${{ needs.build-artifact.outputs.dev-docker-image }}
      env:
        TT_METAL_HOME: /work
        PYTHONPATH: /work
        LD_LIBRARY_PATH: /work/build/lib
        ARCH_NAME: ${{ matrix.arch }}
        LOGURU_LEVEL: INFO
        GITHUB_ACTIONS: true
        TRACY_NO_INVARIANT_CHECK: 1
      volumes:
        - ${{ github.workspace }}/docker-job:/work # Subdir to workaround https://github.com/actions/runner/issues/691
        - /dev/hugepages-1G:/dev/hugepages-1G
      options: "--device /dev/tenstorrent"
    defaults:
      run:
        shell: bash
        working-directory: /work # https://github.com/actions/runner/issues/878
    timeout-minutes: 30
    runs-on: >-
      ${{
        (matrix.runner_label == 'topology-6u' || matrix.runner_label == 'BH-LLMBox' || matrix.runner_label == 'BH-DeskBox' || matrix.runner_label == 'BH-LoudBox' ) && fromJSON(format('["{0}", "in-service", "bare-metal"]', matrix.runner_label))
        || format('tt-ubuntu-2204-{0}-viommu-stable', matrix.runner_label)
      }}
    steps:
      - name: ğŸ§¬ Checkout Repository
        uses: actions/checkout@v4
        with:
          submodules: recursive
          path: docker-job
      - name: â¬‡ï¸  Setup Job
        uses: ./docker-job/.github/actions/setup-job
        timeout-minutes: 10
        with:
          build-artifact-name: ${{ needs.build-artifact.outputs.build-artifact-name }}
          wheel-artifact-name: ${{ needs.build-artifact.outputs.wheel-artifact-name }}
      - name: Run ttnn sweeps generation (single sweep)
        if: |
          github.event_name == 'workflow_dispatch' &&
          github.event.inputs.sweep_name != 'ALL SWEEPS (Nightly)' &&
          github.event.inputs.sweep_name != 'ALL SWEEPS (Comprehensive)' &&
          github.event.inputs.sweep_name != 'ALL SWEEPS (Model Traced)' &&
          github.event.inputs.sweep_name != 'ALL SWEEPS (Lead Models)' &&
          matrix.generate_mode == 'all'
        run: python3 tests/sweep_framework/sweeps_parameter_generator.py --module-name ${{ github.event.inputs.sweep_name }} --tag ci-main
      - name: Run ttnn sweeps generation (all sweeps - model traced)
        if: |
          ((github.event_name == 'workflow_dispatch' && github.event.inputs.sweep_name == 'ALL SWEEPS (Model Traced)') ||
           (github.event_name == 'schedule' && github.event.schedule == '0 4 * * *')) &&
          matrix.generate_mode == 'all'
        run: python3 tests/sweep_framework/sweeps_parameter_generator.py --model-traced all --tag ci-main
      - name: Run ttnn sweeps generation (all sweeps - lead models)
        if: |
          ((github.event_name == 'workflow_dispatch' && github.event.inputs.sweep_name == 'ALL SWEEPS (Lead Models)') ||
           (github.event_name == 'schedule' && github.event.schedule == '0 3 * * *')) &&
          matrix.generate_mode == 'all'
        run: python3 tests/sweep_framework/sweeps_parameter_generator.py --model-traced lead --tag ci-main
      - name: Run ttnn sweeps generation (all sweeps)
        if: |
          ((github.event_name == 'schedule' && github.event.schedule != '0 4 * * *' && github.event.schedule != '0 3 * * *') ||
           (github.event_name == 'workflow_dispatch' && github.event.inputs.sweep_name == 'ALL SWEEPS (Nightly)') ||
           (github.event_name == 'workflow_dispatch' && github.event.inputs.sweep_name == 'ALL SWEEPS (Comprehensive)')) &&
          (matrix.generate_mode == 'non_ccl' || matrix.generate_mode == 'ccl_only')
        run: |
          # Determine which modules to skip based on generate_mode
          SKIP_MODULES_FLAG=""
          if [[ "${{ matrix.generate_mode }}" == "non_ccl" ]]; then
            # Skip all CCL modules when generating on N150
            SKIP_MODULES_FLAG="--skip-modules ccl.generality.all_broadcast,ccl.generality.all_gather,ccl.generality.all_reduce,ccl.generality.all_to_all_combine,ccl.generality.all_to_all_dispatch,ccl.generality.all_to_all,ccl.generality.point_to_point,ccl.generality.reduce_scatter"
          elif [[ "${{ matrix.generate_mode }}" == "ccl_only" ]]; then
            # Generate only CCL modules on n300-llmbox by skipping all non-CCL modules
            # Use single-line Python command to avoid YAML parsing issues with multiline strings
            NON_CCL_MODULES=$(python3 -c "import pathlib; sweeps_dir = pathlib.Path('tests/sweep_framework/sweeps'); all_modules = [str(py_file.relative_to(sweeps_dir))[:-3].replace('/', '.') for py_file in sorted(sweeps_dir.glob('**/*.py')) if '__init__' not in str(py_file.relative_to(sweeps_dir))[:-3].replace('/', '.') and not str(py_file.relative_to(sweeps_dir))[:-3].replace('/', '.').startswith('ccl.')]; print(','.join(all_modules))") || {
              echo "Error: Failed to compute non-CCL modules"
              exit 1
            }
            if [[ -z "$NON_CCL_MODULES" ]]; then
              echo "Error: No non-CCL modules found - this would skip all modules"
              exit 1
            fi
            SKIP_MODULES_FLAG="--skip-modules $NON_CCL_MODULES"
          fi

          python3 tests/sweep_framework/sweeps_parameter_generator.py --tag ci-main $SKIP_MODULES_FLAG
      - name: Upload sweep vectors
        uses: actions/upload-artifact@v4
        with:
          name: sweeps-vectors-${{ matrix.generate_mode }}
          path: /work/tests/sweep_framework/vectors_export/
          retention-days: 1
  ttnn-compute-module-matrix:
    name: Compute module matrix
    # This job creates the execution plan for parallel sweep runs by:
    # 1. Downloading all generated vector JSON files from ttnn-generate-sweeps
    # 2. Chunking them into batches based on run type (nightly=10, comprehensive/perf=3)
    # 3. Separating CCL modules for dedicated multi-chip runners (n300-llmbox)
    # 4. Determining test suite to execute (nightly, model_traced, comprehensive)
    # 5. Generating a matrix of runner configurations ie (wh/bh, runs_on, module_selector, suite_name)
    # The output matrix is consumed by ttnn-run-sweeps-batch to spawn parallel jobs
    needs: ttnn-generate-sweeps
    if: ${{ github.event_name == 'schedule' || github.event.inputs.sweep_name == 'ALL SWEEPS (Nightly)' || github.event.inputs.sweep_name == 'ALL SWEEPS (Comprehensive)' || github.event.inputs.sweep_name == 'ALL SWEEPS (Model Traced)' || github.event.inputs.sweep_name == 'ALL SWEEPS (Lead Models)' }}
    runs-on: ubuntu-22.04
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - name: Show run identifiers
        run: |
          echo "Run ID: ${{ github.run_id }}"
          echo "Run Number: ${{ github.run_number }}"
          echo "Run Attempt: ${{ github.run_attempt }}"
      - uses: actions/download-artifact@634f93cb2916e3fdff6788551b99b062d0335ce0 # v5.0
        with:
          pattern: sweeps-vectors-*
          path: /tmp/vectors-download
          merge-multiple: true
      - name: Merge vector artifacts
        shell: bash
        run: |
          # Create final vectors directory
          mkdir -p /tmp/vectors
          # Move all JSON files from subdirectories to /tmp/vectors
          find /tmp/vectors-download -name "*.json" -exec mv {} /tmp/vectors/ \;
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          clean: false
      - id: set-matrix
        shell: bash
        env:
          GITHUB_EVENT_SCHEDULE: ${{ github.event.schedule }}
          GITHUB_EVENT_NAME: ${{ github.event_name }}
          SWEEP_NAME: ${{ github.event.inputs.sweep_name }}
          MEASURE_DEVICE_PERF: ${{ github.event.inputs.measure_device_perf }}
          VECTORS_DIR: /tmp/vectors
        run: |
          matrix_json=$(python3 tests/sweep_framework/framework/compute_sweep_matrix.py)
          echo "matrix=$matrix_json" >> "$GITHUB_OUTPUT"
  ttnn-run-single-sweep:
    name: Run single sweep
    needs:
      - ttnn-generate-sweeps
      - build-artifact
    if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.sweep_name != 'ALL SWEEPS (Nightly)' && github.event.inputs.sweep_name != 'ALL SWEEPS (Comprehensive)' && github.event.inputs.sweep_name != 'ALL SWEEPS (Model Traced)' && github.event.inputs.sweep_name != 'ALL SWEEPS (Lead Models)' }}
    strategy:
      # Do not fail-fast because we need to ensure all tests go to completion
      # so we try not to get hanging machines
      fail-fast: false
      matrix:
        test-group:
          - name: ttnn sweep tests ${{ github.event.inputs['arch'] || 'wormhole_b0' }} ${{ github.event.inputs['runner-label'] || 'N150' }}
            arch: ${{ github.event.inputs['arch'] || 'wormhole_b0' }}
            runner_label: ${{ github.event.inputs['runner-label'] || 'N150' }}
    container:
      image: ${{ needs.build-artifact.outputs.dev-docker-image }}
      env:
        TT_METAL_HOME: /work
        PYTHONPATH: /work
        LD_LIBRARY_PATH: /work/build/lib
        ARCH_NAME: ${{ matrix.test-group.arch }}
        LOGURU_LEVEL: ${{ inputs.log-level }}
        TT_LOGGER_LEVEL: ${{ inputs.log-level }}
        GITHUB_ACTIONS: true
        TT_SMI_RESET_COMMAND: ${{ (matrix.test-group.runner_label == 'topology-6u') && 'tt-smi -glx_reset_auto' || 'tt-smi -r' }}
        TRACY_NO_INVARIANT_CHECK: 1
      volumes:
        - ${{ github.workspace }}/docker-job:/work # Subdir to workaround https://github.com/actions/runner/issues/691
        - /dev/hugepages-1G:/dev/hugepages-1G
        - /mnt/MLPerf:/mnt/MLPerf:ro
      options: >-
        --device /dev/tenstorrent
        --device /dev/ipmi0
        --privileged
    defaults:
      run:
        shell: bash
        working-directory: /work # https://github.com/actions/runner/issues/878
    timeout-minutes: 720
    runs-on: >-
      ${{
        (matrix.test-group.runner_label == 'topology-6u' || matrix.test-group.runner_label == 'BH-LLMBox' || matrix.test-group.runner_label == 'BH-DeskBox' || matrix.test-group.runner_label == 'BH-LoudBox') && fromJSON(format('["{0}", "in-service", "bare-metal"]', matrix.test-group.runner_label))
        || format('tt-ubuntu-2204-{0}-viommu-stable', matrix.test-group.runner_label)
      }}
    steps:
      - name: ğŸ§¬ Checkout Repository
        uses: actions/checkout@v4
        with:
          submodules: recursive
          path: docker-job
      - name: â¬‡ï¸  Setup Job
        uses: ./docker-job/.github/actions/setup-job
        timeout-minutes: 10
        with:
          build-artifact-name: ${{ needs.build-artifact.outputs.build-artifact-name }}
          wheel-artifact-name: ${{ needs.build-artifact.outputs.wheel-artifact-name }}
      - name: ğŸ“‹ Display single sweep configuration
        run: |
          echo "::group::ğŸ”§ Single Sweep Execution - Configuration"
          echo "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”"
          echo "â”‚                 Single Sweep Configuration                  â”‚"
          echo "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤"
          echo "â”‚ Target Sweep:         ${{ github.event.inputs.sweep_name }}"
          echo "â”‚ Suite Name:           ${{ github.event.inputs.suite_name || 'Default' }}"
          echo "â”‚ Architecture:         ${{ github.event.inputs.arch || 'wormhole_b0' }}"
          echo "â”‚ Runner Label:         ${{ github.event.inputs['runner-label'] || 'N150' }}"
          echo "â”‚ Log Level:            ${{ github.event.inputs['log-level'] || 'INFO' }}"
          echo "â”‚ Skip on Timeout:      ${{ github.event.inputs.skip_on_timeout == 'true' && 'true' || 'false' }}"
          echo "â”‚ Measure Device Perf:  ${{ github.event.inputs.measure_device_perf == 'true' && 'true' || 'false' }}"
          echo "â”‚ Upload Results:       ${{ github.event.inputs.upload_results == 'true' && 'true' || 'false' }}"
          echo "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
          echo "::endgroup::"
      - name: Download sweep vectors
        uses: actions/download-artifact@634f93cb2916e3fdff6788551b99b062d0335ce0 # v5.0
        with:
          pattern: sweeps-vectors-*
          path: /work/tests/sweep_framework/vectors_export
          merge-multiple: true
      - name: Run ttnn sweeps (single sweep) ${{ github.event.inputs.runner_label }} ${{ github.event.inputs.sweep_name }} ${{ github.event.inputs.suite_name }}
        run: |
          SUITE_NAME="${{ github.event.inputs.suite_name }}"

          SKIP_ON_TIMEOUT_FLAG=""
          if [[ "${{ github.event.inputs.skip_on_timeout }}" == "true" ]]; then
            SKIP_ON_TIMEOUT_FLAG="--skip-on-timeout"
          fi

          DEVICE_PERF_FLAG=""
          if [[ "${{ github.event.inputs.measure_device_perf }}" == "true" ]]; then
            DEVICE_PERF_FLAG="--device-perf"
          fi

          MEMORY_FLAG=""
          if [[ "${{ github.event.inputs.measure_memory }}" == "true" ]]; then
            MEMORY_FLAG="--measure-memory"
          fi

          E2E_PERF_FLAG=""
          if [[ "${{ github.event.inputs.measure_e2e_perf }}" == "true" ]]; then
            E2E_PERF_FLAG="--perf-with-cache"
          fi

          CMD=(
            python3 tests/sweep_framework/sweeps_runner.py
            --module-name ${{ github.event.inputs.sweep_name }}
            --vector-source vectors_export
            --result-dest results_export
            --tag ci-main
            --summary
          )

          if [[ -n "$SKIP_ON_TIMEOUT_FLAG" ]]; then
            CMD+=("$SKIP_ON_TIMEOUT_FLAG")
          fi

          if [[ -n "$DEVICE_PERF_FLAG" ]]; then
            CMD+=("$DEVICE_PERF_FLAG")
          fi

          if [[ -n "$E2E_PERF_FLAG" ]]; then
            CMD+=("$E2E_PERF_FLAG")
          fi

          if [[ -n "$SUITE_NAME" ]]; then
            CMD+=(--suite-name "$SUITE_NAME")
          fi

          if [[ -n "$MEMORY_FLAG" ]]; then
            CMD+=("$MEMORY_FLAG")
          fi

          "${CMD[@]}"
      - name: Upload sweep results
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: sweeps-run-result
          path: /work/tests/sweep_framework/results_export/
          retention-days: 1
  ttnn-run-sweeps-parallel:
    name: Run sweeps in parallel (${{ matrix.test_group_name }}, ${{ matrix.batch_display }})
    needs:
      - ttnn-generate-sweeps
      - build-artifact
      - ttnn-compute-module-matrix
    if: ${{ github.event_name == 'schedule' || github.event.inputs.sweep_name == 'ALL SWEEPS (Nightly)' || github.event.inputs.sweep_name == 'ALL SWEEPS (Comprehensive)' || github.event.inputs.sweep_name == 'ALL SWEEPS (Model Traced)' || github.event.inputs.sweep_name == 'ALL SWEEPS (Lead Models)' }}
    strategy:
      # Do not fail-fast because we need to ensure all tests go to completion
      # so we try not to get hanging machines
      fail-fast: false
      max-parallel: 20
      matrix:
        include: ${{ fromJSON(needs.ttnn-compute-module-matrix.outputs.matrix).include }}
    container:
      image: ${{ needs.build-artifact.outputs.dev-docker-image }}
      env:
        TT_METAL_HOME: /work
        PYTHONPATH: /work
        LD_LIBRARY_PATH: /work/build/lib
        ARCH_NAME: ${{ matrix.arch }}
        LOGURU_LEVEL: INFO
        GITHUB_ACTIONS: true
        TT_SMI_RESET_COMMAND: ${{ matrix.tt_smi_cmd }}
        TRACY_NO_INVARIANT_CHECK: 1
      volumes:
        - ${{ github.workspace }}/docker-job:/work # Subdir to workaround https://github.com/actions/runner/issues/691
        - /dev/hugepages-1G:/dev/hugepages-1G
        - /mnt/MLPerf:/mnt/MLPerf:ro
      options: "--device /dev/tenstorrent"
    defaults:
      run:
        shell: bash
        working-directory: /work # https://github.com/actions/runner/issues/878
    timeout-minutes: 720
    runs-on: ${{ matrix.runs_on }}
    steps:
      - name: ğŸ§¬ Checkout Repository
        uses: actions/checkout@v4
        with:
          submodules: recursive
          path: docker-job
      - name: â¬‡ï¸  Setup Job
        uses: ./docker-job/.github/actions/setup-job
        timeout-minutes: 10
        with:
          build-artifact-name: ${{ needs.build-artifact.outputs.build-artifact-name }}
          wheel-artifact-name: ${{ needs.build-artifact.outputs.wheel-artifact-name }}
      - name: ğŸ“‹ Display parallel sweep configuration
        run: |
          echo "::group::ğŸ”§ Parallel Sweeps Execution - Configuration"
          echo "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”"
          echo "â”‚              Parallel Sweeps Configuration                  â”‚"
          echo "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤"
          echo "â”‚ Run Mode:             Parallel Execution (${{ matrix.test_group_name }})"
          echo "â”‚ Batch:                ${{ matrix.batch_display }}"
          echo "â”‚ Module Selector:      ${{ matrix.module_selector }}"
          echo "â”‚ Suite Override:       ${{ matrix.suite_name || 'none' }}"
          echo "â”‚ Architecture:         ${{ matrix.arch }}"
          echo "â”‚ Runner:               ${{ toJSON(matrix.runs_on) }}"
          echo "â”‚ Skip on Timeout:      ${{ github.event_name == 'schedule' && 'true' || (github.event.inputs.skip_on_timeout == 'true' && 'true' || 'false') }}"
          echo "â”‚ Measure Device Perf:  ${{ github.event_name == 'schedule' && 'true' || (github.event.inputs.measure_device_perf == 'true' && 'true' || 'false') }}"
          echo "â”‚ Measure E2E Perf:     ${{ github.event_name != 'schedule' && github.event.inputs.measure_e2e_perf == 'true' && 'true' || 'false' }}"
          echo "â”‚ Measure Memory:       ${{ github.event_name == 'schedule' && 'true' || (github.event.inputs.measure_memory == 'true' && 'true' || 'false') }}"
          echo "â”‚ Upload Results:       ${{ github.event_name == 'schedule' && 'true' || (github.event.inputs.upload_results == 'true' && 'true' || 'false') }}"
          echo "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
          echo "::endgroup::"
      - name: Download sweep vectors
        uses: actions/download-artifact@634f93cb2916e3fdff6788551b99b062d0335ce0 # v5.0
        with:
          pattern: sweeps-vectors-*
          path: /work/tests/sweep_framework/vectors_export
          merge-multiple: true
      - name: Run ttnn sweeps (batch)
        run: |
          MODULE_SELECTOR="${{ matrix.module_selector }}"
          SUITE_NAME="${{ matrix.suite_name }}"

          # Set skip-on-timeout flag based on input (default to true for all scheduled runs)
          SKIP_ON_TIMEOUT_FLAG=""
          if [[ "${{ github.event_name }}" == "schedule" ]]; then
            # For scheduled runs: skip on timeout for both nightly and comprehensive runs
            SKIP_ON_TIMEOUT_FLAG="--skip-on-timeout"
          elif [[ "${{ github.event.inputs.skip_on_timeout }}" == "true" ]]; then
            # For manual dispatch: use the input parameter
            SKIP_ON_TIMEOUT_FLAG="--skip-on-timeout"
          fi

          # Enable device performance for scheduled runs and when requested on manual dispatch
          DEVICE_PERF_FLAG=""
          if [[ "${{ github.event_name }}" == "schedule" ]]; then
            DEVICE_PERF_FLAG="--device-perf"
          elif [[ "${{ github.event.inputs.measure_device_perf }}" == "true" ]]; then
            DEVICE_PERF_FLAG="--device-perf"
          fi

          # Enable e2e performance for manual dispatch only (not scheduled runs)
          E2E_PERF_FLAG=""
          if [[ "${{ github.event.inputs.measure_e2e_perf }}" == "true" ]]; then
            E2E_PERF_FLAG="--perf-with-cache"
          fi

          # Enable memory measurement for scheduled runs and when requested on manual dispatch
          MEMORY_FLAG=""
          if [[ "${{ github.event_name }}" == "schedule" ]]; then
            MEMORY_FLAG="--measure-memory"
          elif [[ "${{ github.event.inputs.measure_memory }}" == "true" ]]; then
            MEMORY_FLAG="--measure-memory"
          fi

          CMD=(
            python3 tests/sweep_framework/sweeps_runner.py
            --module-name "$MODULE_SELECTOR"
            --vector-source vectors_export
            --result-dest results_export
            --tag ci-main
            --summary
          )

          if [[ -n "$SKIP_ON_TIMEOUT_FLAG" ]]; then
            CMD+=("$SKIP_ON_TIMEOUT_FLAG")
          fi

          if [[ -n "$DEVICE_PERF_FLAG" ]]; then
            CMD+=("$DEVICE_PERF_FLAG")
          fi

          if [[ -n "$E2E_PERF_FLAG" ]]; then
            CMD+=("$E2E_PERF_FLAG")
          fi

          if [[ -n "$MEMORY_FLAG" ]]; then
            CMD+=("$MEMORY_FLAG")
          fi

          if [[ -n "$SUITE_NAME" ]]; then
            CMD+=(--suite-name "$SUITE_NAME")
          fi

          "${CMD[@]}"
      - name: Compute artifact suffix
        shell: bash
        run: |
          DISPLAY_LABEL="${{ matrix.batch_display }}"
          SUITE_LABEL="${{ matrix.suite_name }}"
          GROUP_LABEL="${{ matrix.test_group_name }}"
          FINGERPRINT="$GROUP_LABEL|$DISPLAY_LABEL"
          if [[ -n "$SUITE_LABEL" ]]; then
            FINGERPRINT="$FINGERPRINT|$SUITE_LABEL"
          fi
          echo "BATCH_SUFFIX=$(echo -n "$FINGERPRINT" | sha256sum | cut -d' ' -f1 | cut -c1-12)" >> $GITHUB_ENV
      - name: Upload sweep results
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: sweeps-results-${{ matrix.test_group_name }}-batch-${{ env.BATCH_SUFFIX }}
          path: /work/tests/sweep_framework/results_export/
          retention-days: 1
  ttnn-homogenize-run-result:
    name: Homogenize run result
    needs:
      - build-artifact
      - ttnn-run-sweeps-parallel
    if: ${{ !cancelled() && needs.ttnn-run-sweeps-parallel.result != 'skipped' && needs.ttnn-run-sweeps-parallel.result != 'cancelled' }}
    container:
      image: ${{ needs.build-artifact.outputs.dev-docker-image }}
      env:
        TT_METAL_HOME: /work
        PYTHONPATH: /work
        LD_LIBRARY_PATH: /work/build/lib
        LOGURU_LEVEL: INFO
        GITHUB_ACTIONS: true
      volumes:
        - ${{ github.workspace }}/docker-job:/work # Subdir to workaround https://github.com/actions/runner/issues/691
    defaults:
      run:
        shell: bash
        working-directory: /work # https://github.com/actions/runner/issues/878
    runs-on: tt-ubuntu-2204-large-stable
    steps:
      - name: ğŸ§¬ Checkout Repository
        uses: actions/checkout@v4
        with:
          submodules: recursive
          path: docker-job
      - name: â¬‡ï¸  Setup Job
        uses: ./docker-job/.github/actions/setup-job
        timeout-minutes: 10
        with:
          build-artifact-name: ${{ needs.build-artifact.outputs.build-artifact-name }}
          wheel-artifact-name: ${{ needs.build-artifact.outputs.wheel-artifact-name }}
      - name: Download all sweep results
        uses: actions/download-artifact@634f93cb2916e3fdff6788551b99b062d0335ce0 # v5.0
        with:
          pattern: sweeps-results*
          path: /work/tests/sweep_framework/results_export
          merge-multiple: true
      - name: Homogenize sweep results
        run: |
          # Determine run type based on schedule or input
          RUN_TYPE="nightly"
          if [[ "${{ github.event.schedule }}" == "0 4 * * 3,6" ]] || [[ "${{ github.event.inputs.sweep_name }}" == "ALL SWEEPS (Comprehensive)" ]]; then
            RUN_TYPE="comprehensive"
          elif [[ "${{ github.event.schedule }}" == "0 4 * * *" ]] || [[ "${{ github.event.inputs.sweep_name }}" == "ALL SWEEPS (Model Traced)" ]]; then
            RUN_TYPE="model_traced"
          elif [[ "${{ github.event.schedule }}" == "0 3 * * *" ]] || [[ "${{ github.event.inputs.sweep_name }}" == "ALL SWEEPS (Lead Models)" ]]; then
            RUN_TYPE="lead_models"
          fi
          python3 tests/sweep_framework/run_collective_update.py --run-type "$RUN_TYPE"
      - name: Upload runs
        uses: actions/upload-artifact@v4
        with:
          name: sweeps-run-result
          path: /work/tests/sweep_framework/results_export/oprun_*.json
          retention-days: 7
  ttnn-upload-results:
    name: Upload sweep results to SFTP server
    needs:
      - ttnn-homogenize-run-result
      - ttnn-run-single-sweep
    if: ${{ !cancelled() && (needs.ttnn-homogenize-run-result.result != 'skipped' || needs.ttnn-run-single-sweep.result != 'skipped') && (needs.ttnn-homogenize-run-result.result != 'cancelled' || needs.ttnn-run-single-sweep.result != 'cancelled') && (github.event_name == 'schedule' || github.event.inputs.upload_results) }}
    runs-on: ubuntu-22.04
    steps:
      - name: Checkout repository (actions only)
        uses: actions/checkout@v4
        with:
          sparse-checkout: |
            .github/actions
      - name: Download sweep results
        uses: actions/download-artifact@634f93cb2916e3fdff6788551b99b062d0335ce0 # v5.0
        with:
          name: sweeps-run-result
          path: tests/sweep_framework/results_export
      - name: List all downloaded files
        run: |
          ls -hal tests/sweep_framework/results_export
      - name: Upload via SFTP
        uses: ./.github/actions/upload-data-via-sftp
        with:
          ssh-private-key: ${{ secrets.SFTP_CICD_WRITER_KEY }}
          sftp-batchfile: .github/actions/upload-data-via-sftp/optest_batchfile.txt
          username: ml-kernel-op-test-writer
          hostname: s-dbd4b8a190fa40a4b.server.transfer.us-east-2.amazonaws.com
          path: ${{ github.workspace }}
