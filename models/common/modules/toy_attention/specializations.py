"""
Specialization registry and codegen hooks for TTTv2 attention.

The design doc calls for:
- A curated registry keyed by (shape, topology, config).
- Pre-generated specializations as the "product", with codegen as the tool.
- TTNN-only templates (no torch/HF/vLLM).

This module provides a minimal implementation of that pattern.
"""

from dataclasses import dataclass
from typing import Callable, Dict, Optional, Tuple

from .core import AttentionConfig, AttentionCore, AttentionSpec, OpConfig
from .ttnn_attention import TTNNMultiheadAttentionCore


@dataclass(frozen=True)
class AttentionFingerprint:
    """
    Immutable key capturing the specialization identity.
    """

    hidden_size: int
    num_heads: int
    head_dim: int
    grid_size: Tuple[int, int]
    shard_layout: str
    mode: str = "decode"  # e.g., "prefill" or "decode"
    flash: bool = False
    fused_qkv: bool = True


@dataclass
class SpecializationEntry:
    factory: Callable[[AttentionSpec, AttentionConfig], AttentionCore]
    source: Optional[str] = None  # human/audit friendly generated source
    notes: Optional[str] = None


SPECIALIZATION_REGISTRY: Dict[AttentionFingerprint, SpecializationEntry] = {}


def register_specialization(fingerprint: AttentionFingerprint, entry: SpecializationEntry) -> None:
    """
    Register a specialization. The registry is intentionally small and explicit.
    """
    SPECIALIZATION_REGISTRY[fingerprint] = entry


def lookup_specialization(fingerprint: AttentionFingerprint) -> Optional[SpecializationEntry]:
    return SPECIALIZATION_REGISTRY.get(fingerprint)


def emit_specialization_source(
    spec: AttentionSpec,
    config: AttentionConfig,
    fingerprint: AttentionFingerprint,
) -> str:
    """
    Produce a TTNN-only source snippet representing the specialization.

    This is a simplified illustration of the "codegen as tool" concept from the design doc.
    """

    def _fmt(op: OpConfig) -> str:
        return (
            f"OpConfig("
            f"memory_config={repr(op.memory_config)}, "
            f"program_config={repr(op.program_config)}, "
            f"compute_kernel_config={repr(op.compute_kernel_config)})"
        )

    return "\n".join(
        [
            "# Auto-generated by TTTv2 attention codegen",
            "from models.common.modules.attention.core import AttentionSpec, AttentionConfig, OpConfig",
            "from models.common.modules.attention.ttnn_attention import TTNNMultiheadAttentionCore",
            "",
            "def build():",
            f"    spec = AttentionSpec(",
            f"        hidden_size={spec.hidden_size},",
            f"        num_heads={spec.num_heads},",
            f"        head_dim={spec.head_dim},",
            f"        use_rotary_embeddings={spec.use_rotary_embeddings},",
            f"        use_sliding_window={spec.use_sliding_window},",
            f"        window_size={spec.window_size},",
            f"        dropout={spec.dropout},",
            f"    )",
            f"    config = AttentionConfig(",
            f"        qkv={_fmt(config.qkv)},",
            f"        q={_fmt(config.q)},",
            f"        k={_fmt(config.k)},",
            f"        v={_fmt(config.v)},",
            f"        attn_scores={_fmt(config.attn_scores)},",
            f"        attn_output={_fmt(config.attn_output)},",
            f"        out_proj={_fmt(config.out_proj)},",
            f"        shard_layout={repr(config.shard_layout)},",
            f"    )",
            f"    return TTNNMultiheadAttentionCore(",
            f"        spec=spec,",
            f"        config=config,",
            f"        fused_qkv={fingerprint.fused_qkv},",
            f"        use_flash={fingerprint.flash},",
            f"    )",
            "",
        ]
    )


def build_attention_core_for_runtime(
    spec: AttentionSpec,
    config: AttentionConfig,
    *,
    grid_size: Tuple[int, int],
    shard_layout: str,
    mode: str = "decode",
    flash: bool = False,
    fused_qkv: bool = True,
    allow_codegen: bool = True,
) -> AttentionCore:
    """
    Select a registered specialization or fall back to a generated one.

    This keeps runtime selection deterministic and auditable.
    """
    fingerprint = AttentionFingerprint(
        hidden_size=spec.hidden_size,
        num_heads=spec.num_heads,
        head_dim=spec.head_dim,
        grid_size=grid_size,
        shard_layout=shard_layout,
        mode=mode,
        flash=flash,
        fused_qkv=fused_qkv,
    )

    entry = lookup_specialization(fingerprint)
    if entry is not None:
        return entry.factory(spec, config)

    if not allow_codegen:
        raise KeyError(f"No specialization found for {fingerprint} and codegen is disabled.")

    # Minimal codegen path: instantiate TTNNMultiheadAttentionCore with fixed knobs.
    return TTNNMultiheadAttentionCore(
        spec=spec,
        config=config,
        fused_qkv=fused_qkv,
        use_flash=flash,
    )


# Example built-in specialization (aligned with the design doc example: wormhole_b0, grid 8x7).
WORMHOLE_B0_PREFILL = AttentionFingerprint(
    hidden_size=4096,
    num_heads=32,
    head_dim=128,
    grid_size=(8, 7),
    shard_layout="block",
    mode="prefill",
    flash=True,
    fused_qkv=True,
)


register_specialization(
    WORMHOLE_B0_PREFILL,
    SpecializationEntry(
        factory=lambda spec, config: TTNNMultiheadAttentionCore(
            spec=spec,
            config=config,
            fused_qkv=True,
            use_flash=True,
        ),
        source="inline: TTNNMultiheadAttentionCore(fused=True, flash=True)",
        notes="Llama-style prefill specialization for wormhole_b0 grid (8x7), fused QKV, flash attention.",
    ),
)
