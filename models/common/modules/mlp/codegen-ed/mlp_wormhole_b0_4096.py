# Auto-generated by TTTv2 CodeGen
from dataclasses import dataclass
from typing import Any, Callable, Optional

import ttnn


class LightweightModule:
    """LightweightModule to replace nn.Module and remove torch dependency"""

    def __call__(self, *args, **kwargs):
        return self.forward(*args, **kwargs)


@dataclass
class OpConfig:
    compute_kernel_config: Optional[Any] = None
    memory_config: Optional[Any] = None
    dtype: Optional[Any] = None
    program_config: Optional[Any] = None


@dataclass
class MLPOpConfigs:
    gate: OpConfig
    up: OpConfig
    down: OpConfig
    activation: Callable


@dataclass
class MLPWeights:
    gate_proj_weight: Any
    gate_proj_bias: Any
    up_proj_weight: Any
    up_proj_bias: Any
    down_proj_weight: Any
    down_proj_bias: Any


def forward_mlp_impl(x, weights, ops):
    """Generated from introspected function"""
    """
    Pure functional implementation of MLP forward pass.

    Args:
        x: Input tensor
        weights: Container/object with weight tensors (gate_proj, up_proj, down_proj)
        ops: Configuration object containing operation configs (gate, up, down, activation)
    """
    gate = ttnn.linear(
        x,
        weights.gate_proj_weight,
        bias=weights.gate_proj_bias,
        compute_kernel_config=ops.gate.compute_kernel_config,
        memory_config=ops.gate.memory_config,
        dtype=ops.gate.dtype,
        program_config=ops.gate.program_config,
    )
    up = ttnn.linear(
        x,
        weights.up_proj_weight,
        bias=weights.up_proj_bias,
        compute_kernel_config=ops.up.compute_kernel_config,
        memory_config=ops.up.memory_config,
        dtype=ops.up.dtype,
        program_config=ops.up.program_config,
    )
    gate = ops.activation(gate)
    intermediate = ttnn.mul(gate, up)
    output = ttnn.linear(
        intermediate,
        weights.down_proj_weight,
        bias=weights.down_proj_bias,
        compute_kernel_config=ops.down.compute_kernel_config,
        memory_config=ops.down.memory_config,
        dtype=ops.down.dtype,
        program_config=ops.down.program_config,
    )
    return output


class TTTv2MLP_wormhole_b0(LightweightModule):
    """
    Auto-generated MLP module for wormhole_b0
    Configuration:
      - Hidden size: 4096
      - Intermediate size: 11008
      - Activation: silu
    """

    def __init__(self, device, prefetcher=None):
        super().__init__()
        self.device = device
        self.hidden_size = 4096
        self.intermediate_size = 11008
        self.activation = "silu"

        # Initialize weights
        self._init_weights()
        self._init_ops_config()
        self.prefetcher = prefetcher

    def _init_weights(self):
        """Initialize projection weights"""
        import ttnn

        # Gate Projection
        gate_proj_weight = ttnn.create_weight(shape=[4096, 11008], dtype=ttnn.bfloat16, device=self.device)
        gate_proj_bias = ttnn.create_bias(shape=[11008], dtype=ttnn.bfloat16, device=self.device)

        # Up Projection
        up_proj_weight = ttnn.create_weight(shape=[4096, 11008], dtype=ttnn.bfloat16, device=self.device)
        up_proj_bias = ttnn.create_bias(shape=[11008], dtype=ttnn.bfloat16, device=self.device)

        # Down Projection
        down_proj_weight = ttnn.create_weight(shape=[11008, 4096], dtype=ttnn.bfloat16, device=self.device)
        down_proj_bias = ttnn.create_bias(shape=[4096], dtype=ttnn.bfloat16, device=self.device)

        self.weights = MLPWeights(
            gate_proj_weight=gate_proj_weight,
            gate_proj_bias=gate_proj_bias,
            up_proj_weight=up_proj_weight,
            up_proj_bias=up_proj_bias,
            down_proj_weight=down_proj_weight,
            down_proj_bias=down_proj_bias,
        )

        self.training = False

    def _init_ops_config(self):
        # Hardware-specific configuration
        compute_kernel_config = ttnn.WormholeComputeKernelConfig(
            math_fidelity=ttnn.MathFidelity.HiFi4, fp32_dest_acc_en=True, packer_l1_acc=True
        )

        # Memory configuration
        memory_config = ttnn.MemoryConfig(
            memory_layout=ttnn.TensorMemoryLayout.BLOCK_SHARDED, buffer_type=ttnn.BufferType.L1
        )

        # Op Configs
        gate_config = OpConfig(
            compute_kernel_config=compute_kernel_config, memory_config=memory_config, dtype=ttnn.bfloat16
        )
        up_config = OpConfig(
            compute_kernel_config=compute_kernel_config, memory_config=memory_config, dtype=ttnn.bfloat16
        )
        down_config = OpConfig(
            compute_kernel_config=compute_kernel_config, memory_config=memory_config, dtype=ttnn.bfloat16
        )
        self.ops = MLPOpConfigs(gate=gate_config, up=up_config, down=down_config, activation=ttnn.silu)

    def forward(self, x):
        return forward_mlp_impl(x, self.weights, self.ops)
