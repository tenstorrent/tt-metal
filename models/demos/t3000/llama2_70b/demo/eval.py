# SPDX-FileCopyrightText: Â© 2023 Tenstorrent Inc.

# SPDX-License-Identifier: Apache-2.0

import json
import os
import re
from dataclasses import dataclass
from datetime import datetime
from time import time

import datasets
import pytest
import torch
from loguru import logger
from tqdm import tqdm

from models.demos.t3000.llama2_70b.demo.demo import build_generator, construct_arg, get_sampling_func, initialize_inputs
from models.demos.t3000.llama2_70b.tt.llama_common import check_mesh_device, setup_llama_env


@dataclass
class EvalDataArgs:
    dataset: str = "wikitext"
    split: str = "test"
    config: str = "wikitext-2-raw-v1"
    stride: int = 128
    sample_len: int = 128
    num_samples: int = 128
    perplexity_score: float = 5.4


def main(args, eval_data_args):
    # Set random reproducible seed
    torch.manual_seed(0)
    model_args = args.model
    tt_args = args.tt
    data_args = args.data

    generator = build_generator(model_args, tt_args)

    # Load the model and tokenizer
    model, tokenizer = generator.model, generator.tokenizer

    # Dataset preparation
    dataset = datasets.load_dataset(
        eval_data_args.dataset, eval_data_args.config, split=eval_data_args.split, ignore_verifications=True
    )
    text = wikitext_detokenizer("\n".join(dataset["text"]))
    encodings = tokenizer.encode(text, bos=True, eos=False)  # not prepending bos

    # args for perplexity calculation
    max_length = model_args.max_seq_len
    stride = eval_data_args.stride if eval_data_args.stride else max_length
    assert stride <= max_length, "stride cannot be larger than max_length"
    seq_len = eval_data_args.sample_len
    num_samples = eval_data_args.num_samples
    max_batch_size = model_args.max_batch_size
    assert num_samples > 0, "num_samples must be greater than 0"
    assert seq_len + (num_samples - 1) * stride <= len(encodings), (
        "total length of token decoded must be less than the length of the dataset, \
        the maximum allowed num_samples is: %d"
        % (len(encodings) - seq_len)
        // stride
        + 1
    )

    perplexity = calculate_perplexity(
        model_args,
        tt_args,
        data_args,
        eval_data_args,
        model,
        tokenizer,
        seq_len,
        max_length,
        stride,
        encodings,
        num_samples,
        max_batch_size,
    )

    base_path = "models/demos/t3000/llama2_70b/scripts/llama_perplexity_runs/"
    os.makedirs(base_path, exist_ok=True)

    # Get current timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Dump perplexity and max_seq_len to a JSON file with timestamp and max_length in the file name
    filename = os.path.join(
        base_path,
        f"perplexity_{model_args.llama_version}_{model_args.implementation}_{model_args.num_layers}L_"
        f"{eval_data_args.sample_len}_{data_args.max_output_tokens}_{timestamp}.json",
    )
    result = {
        "model": model_args.llama_version,
        "perplexity": perplexity.item(),
        "seq_len": eval_data_args.sample_len,
        "gen_length": data_args.max_output_tokens,
    }
    # create folder if it doesn't exist
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    with open(filename, "w") as f:
        json.dump(result, f)

    logger.info("Perplexity: %f" % perplexity)
    if perplexity < eval_data_args.perplexity_score:
        logger.info("Perplexity is less than the threshold")
    else:
        assert False, "Perplexity is greater than the threshold"

    return perplexity


def prepare_next_input_eval(tokenizer, tokens, input_text_mask, cur_pos, next_token):
    # only replace token if prompt has already been generated
    next_token = torch.where(input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token)
    tokens[:, cur_pos] = next_token

    prev_pos = cur_pos

    return tokens, prev_pos


def run_forward(
    model_args,
    tt_args,
    data_args,
    eval_data_args,
    model,
    tokenizer,
    prompt_tokens,
    prompts,
    return_logits=False,
    return_full_logits=False,
):
    """
    return_logits: return the logits for the last token
    return_full_logits: return the logits for all tokens
    """
    assert not (return_logits and return_full_logits), "return_logits and return_full_logits cannot both be true"

    # decode arguments
    bsz = model_args.max_batch_size
    output_tokens = data_args.max_output_tokens

    sampling_func = get_sampling_func(data_args.top_k, data_args.top_p, data_args.temperature)

    total_len = min(model_args.max_kv_context_len, eval_data_args.sample_len + output_tokens)
    assert total_len <= model_args.max_kv_context_len
    assert total_len - 1 == prompt_tokens.size(1), "Prompt tokens must be of length total_len"

    # prepare inputs
    tokens, input_text_mask, _ = initialize_inputs(tokenizer, prompt_tokens, bsz, total_len)
    prev_pos = 0

    # some profiling and logging
    full_logits = []

    # Prefill up to sample_len, then decode for output_tokens-1 num tokens.
    for cur_pos in range(eval_data_args.sample_len, eval_data_args.sample_len + output_tokens):
        logger.info(f"EVAL: Inference from token {prev_pos} to {cur_pos}")
        input_tokens = tokens[:, prev_pos:cur_pos]

        if prev_pos == 0:  # Prefill
            logits = []
            for b in range(bsz):
                logits.append(model.prefill_forward_single_user(input_tokens[b : b + 1], prev_pos, b))
            logits = torch.cat(logits, dim=0)
        else:  # Decode
            logits = model.forward(input_tokens, prev_pos)

        next_logits = logits[:, -1, :]  # batch, vocab of last token
        next_token = sampling_func(next_logits)

        tokens, prev_pos = prepare_next_input_eval(tokenizer, tokens, input_text_mask, cur_pos, next_token)

        full_logits.append(logits.clone().detach())

    full_logits = torch.cat(full_logits, dim=1)
    return full_logits


def calculate_perplexity(
    model_args,
    tt_args,
    data_args,
    eval_data_args,
    model,
    tokenizer,
    seq_len,
    max_len,
    stride,
    encodings,
    num_samples,
    max_batch_size,
):
    start = time()
    eval_inputs = []
    eval_labels = []
    total_len = len(encodings)

    # construct perplexity calculation inputs
    for i, begin_loc in enumerate(range(0, total_len, stride)):
        end_loc = begin_loc + seq_len + data_args.max_output_tokens - 1
        if end_loc >= total_len:
            raise ValueError(
                "The dataset is too small to decode the number of samples requested with the given stride."
            )
        inputs = encodings[begin_loc:end_loc]
        labels = encodings[begin_loc + 1 : end_loc + 1]
        eval_inputs.append(inputs)
        eval_labels.append(labels)
        if i == num_samples - 1:
            break

    # batch perplexity calculation inputs
    dataset = torch.utils.data.TensorDataset(torch.tensor(eval_inputs), torch.tensor(eval_labels))
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=max_batch_size, shuffle=False)

    # Run PPL eval
    loss_func = torch.nn.CrossEntropyLoss(reduction="mean")
    nlls = []
    tokens_seen = 0
    with torch.no_grad():
        # Loop over the dataset
        for sample in tqdm(dataloader):
            tokens, labels = sample
            logits = run_forward(
                model_args,
                tt_args,
                data_args,
                eval_data_args,
                model=model,
                tokenizer=tokenizer,
                prompt_tokens=tokens,
                prompts=None,
                return_full_logits=True,
            )

            vocab_size = logits.shape[-1]
            logger.info(f"logits shape: {logits.shape}")
            loss = loss_func(logits.view(-1, vocab_size), labels.view(-1))
            neg_log_likelihood = loss.to("cpu").float() * logits.shape[1] * logits.shape[0]
            tokens_seen += logits.shape[1] * logits.shape[0]
            nlls.append(neg_log_likelihood)

    loss = torch.stack(nlls).sum() / tokens_seen
    ppl = torch.exp(loss)

    logger.info(f"Evaluation execution time:\t{time() - start}")
    logger.info("Loss: %f" % loss)

    return ppl


def wikitext_detokenizer(string):
    """From Megatron-DeepSpeed/tasks/zeroshot_gpt/detokenizer.py"""
    # contractions
    string = string.replace("s '", "s'")
    string = re.sub(r"/' [0-9]/", r"/'[0-9]/", string)
    # number separators
    string = string.replace(" @-@ ", "-")
    string = string.replace(" @,@ ", ",")
    string = string.replace(" @.@ ", ".")
    # punctuation
    string = string.replace(" : ", ": ")
    string = string.replace(" ; ", "; ")
    string = string.replace(" . ", ". ")
    string = string.replace(" ! ", "! ")
    string = string.replace(" ? ", "? ")
    string = string.replace(" , ", ", ")
    # double brackets
    string = re.sub(r"\(\s*([^\)]*?)\s*\)", r"(\1)", string)
    string = re.sub(r"\[\s*([^\]]*?)\s*\]", r"[\1]", string)
    string = re.sub(r"{\s*([^}]*?)\s*}", r"{\1}", string)
    string = re.sub(r"\"\s*([^\"]*?)\s*\"", r'"\1"', string)
    string = re.sub(r"'\s*([^']*?)\s*'", r"'\1'", string)
    # miscellaneous
    string = string.replace("= = = =", "====")
    string = string.replace("= = =", "===")
    string = string.replace("= =", "==")
    string = string.replace(" " + chr(176) + " ", chr(176))
    string = string.replace(" \n", "\n")
    string = string.replace("\n ", "\n")
    string = string.replace(" N ", " 1 ")
    string = string.replace(" 's", "'s")

    return string


def get_model_max_sequence_length(model):
    for c in ["n_positions", "seq_length"]:
        if hasattr(model.config, c):
            return getattr(model.config, c)
    return None


@pytest.mark.timeout(240000)
@pytest.mark.parametrize(
    "llama_version",
    (
        ("llama2"),
        ("llama3"),
    ),
)
@pytest.mark.parametrize("num_layers", (1, 2, 10, 80), ids=["1L", "2L", "10L", "80L"])
@pytest.mark.parametrize(
    "implementation, skip_model_load, n_devices",
    [
        (
            "tt",
            False,
            8,
        ),
        (
            "meta",
            False,
            8,
        ),
    ],
    ids=["tt-70b", "meta-70b"],
)
@pytest.mark.parametrize(
    "top_p, top_k, temperature",
    [
        (1, 1, 1.0),
    ],
    ids=["greedy"],
)
@pytest.mark.parametrize(  # sample_len => prefill length, num_samples => decode length
    "dataset, split, config, stride, sample_len, max_output_tokens, num_samples, perplexity_score",
    [
        ("wikitext", "test", "wikitext-2-raw-v1", 128, 128, 1, 128, 5.4),
        ("wikitext", "test", "wikitext-2-raw-v1", 128, 2048, 1, 128, 3.4313),
        ("wikitext", "test", "wikitext-2-raw-v1", 128, 128, 128, 128, 5.4),
        ("wikitext", "test", "wikitext-2-raw-v1", 128, 2048, 128, 128, 3.4313),
    ],
    ids=["wikitext-128-0", "wikitext-2k-0", "wikitext-128-128", "wikitext-2k-128"],
)
def test_LlamaModel_demo(
    # model args
    implementation,
    skip_model_load,
    num_layers,
    # Generation args
    max_output_tokens,
    top_p,
    top_k,
    temperature,
    # TT args
    t3k_mesh_device,
    n_devices,
    # Dataset args
    dataset,
    split,
    config,
    stride,
    sample_len,
    num_samples,
    perplexity_score,
    llama_version,
    use_program_cache,
):
    logger.info("Running LlamaModel perplexity test")
    ## Get model config
    model_config, ckpt_dir, tokenizer_path, cache_path = setup_llama_env(
        llama_version=llama_version,
    )

    check_mesh_device(t3k_mesh_device, model_config)

    args = construct_arg(
        implementation=implementation,
        llama_version=llama_version,
        ckpt_dir=ckpt_dir,
        tokenizer_path=tokenizer_path,
        skip_model_load=skip_model_load,
        num_layers=num_layers,
        max_output_tokens=max_output_tokens,
        top_p=top_p,
        top_k=top_k,
        temperature=temperature,
        chat=False,
        mesh_device=t3k_mesh_device,
        n_devices=n_devices,
        cache_path=cache_path,
        decode_only=False,
    )

    eval_data_args = EvalDataArgs(
        dataset=dataset,
        split=split,
        config=config,
        stride=stride,
        sample_len=sample_len,
        num_samples=num_samples,
        perplexity_score=perplexity_score,
    )

    main(args, eval_data_args)
