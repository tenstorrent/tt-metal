# SPDX-FileCopyrightText: Â© 2023 Tenstorrent Inc.

# SPDX-License-Identifier: Apache-2.0
import torch
import pytest
from loguru import logger
from tqdm import tqdm
import numpy as np

import ttnn
from ttnn import ConcatMeshToTensor
from models.demos.t3000.mixtral8x7b.tt.mixtral_common import (
    prepare_inputs_ttnn,
    get_single_rot_mat,
    cache_attention,
)
from models.demos.t3000.mixtral8x7b.tt.mixtral_model import TtTransformer
from models.demos.t3000.mixtral8x7b.tt.mixtral_embedding import TtMixtralEmbedding
from models.demos.t3000.mixtral8x7b.reference.model import Transformer
from models.demos.t3000.mixtral8x7b.reference.tokenizer import Tokenizer
from models.demos.t3000.mixtral8x7b.tt.model_config import TtModelArgs
from models.datasets.llm_dataset_utils import (
    prepare_textgen_dataset,
    prepare_textgen_dataloader,
    calculate_acc_metrics,
    verify_acc_metrics,
)


class Emb(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.emb = torch.nn.Embedding(32000, 4096)

    def forward(self, x):
        return self.emb(x)


@torch.no_grad()
def run_test_perplexity(
    mesh_device,
    batch_size,
    llm_mode,
    max_seq_len,
    num_samples,
    expected_acc_metrics,
    instruct_mode=False,  # For now, only run accuracy check for general weights
    stride=None,
    dataset_name="wikitext",
    dataset_config="wikitext-2-raw-v1",
    split="test",
):
    assert batch_size == 32, "Batch size must be 32"
    dtype = ttnn.bfloat8_b
    embed_on_host = True  # embedding and argmax on host
    seqlen = 1  # Generating one token per user at a time

    # Flag to compile the ref model and run the accuracy metrics on it
    validate_ref_model = False

    # Accuracy metrics
    running_nll, running_top1_acc, running_top5_acc = 0.0, 0.0, 0.0

    if validate_ref_model:
        ref_running_nll, ref_running_top1_acc, ref_running_top5_acc = 0.0, 0.0, 0.0

    # Load model args, weights, and tokenizer
    model_args = TtModelArgs(mesh_device, instruct=instruct_mode, max_batch_size=batch_size, max_seq_len=max_seq_len)
    tokenizer = Tokenizer(model_args.tokenizer_path)
    if instruct_mode:
        tokenizer._model.pad_id = tokenizer._model.eos_id

    model_args.n_layers = 32  # Full model

    # Prepare dataset
    logger.info("Preparing dataset...")
    dataset = prepare_textgen_dataset(dataset_name, dataset_config, split)
    if instruct_mode:
        # Pre append [INST] and post append [/INST] to the encoded prompts if instruct mode
        encodings = torch.tensor(tokenizer.encode("[INST] " + dataset + " [/INST]"))
    else:
        encodings = torch.tensor(tokenizer.encode(dataset))
    dataloader = prepare_textgen_dataloader(encodings, batch_size, max_seq_len, num_samples, stride)

    logger.info("Loading weights...")
    state_dict = torch.load(model_args.state_dict_path)
    # If not using the full model, remove the layers that are not used
    keys_dict = list(state_dict.keys())[:]
    remv = [f"layers.{i}" for i in range(model_args.n_layers, 32)]
    for k in keys_dict:
        if any([r in k for r in remv]):
            state_dict.pop(k)

    # Embedding on host
    if embed_on_host:
        embd = Emb()
        embd.load_state_dict({"emb.weight": state_dict["tok_embeddings.weight"]})

    # Load TTNN mixtral model
    logger.info("Loading weights to device...")
    tt_model = TtTransformer(
        mesh_device=mesh_device,
        state_dict=state_dict,
        args=model_args,
        layers=list(range(model_args.n_layers)),
        dtype=dtype,
        rotary_on_host=False,
        start_pos_ids=[0] * batch_size,
    )

    if validate_ref_model:
        reference_model = Transformer(args=model_args)
        reference_model.load_state_dict(state_dict)
        reference_model.eval()

    if not embed_on_host:
        tt_embds = TtMixtralEmbedding(
            mesh_device=mesh_device,
            args=model_args,
            weight_cache_path=model_args.weight_cache_path(dtype),
            state_dict=state_dict,
            dtype=ttnn.bfloat16,  # Row major layout requires bfloat16
        )

    logger.info("Finished loading weights to device.")

    # Prepare inputs for decode mode (rotary embeddings, attention mask, padding)
    current_rot_mat, rot_matrix = get_single_rot_mat(
        model_args.head_dim,
        tt_model.mesh_device,
    )

    generation_start_pos = 0

    cache_attention(
        mesh_device,
        state_dict,
        model_args,
        current_rot_mat,
        rot_matrix,
        dtype,
    )

    logger.info("Starting inference...")
    for input_ids, labels in tqdm(dataloader, desc="Evaluating batches"):
        if llm_mode == "prefill":
            # TODO Add prefill support
            assert "Prefill mode not yet supported"
        elif llm_mode == "decode":
            logits = []
            if validate_ref_model:
                ref_logits = []

            for kv_cache_len in tqdm(range(max_seq_len), desc="Decoding tokens for current batch"):
                # Convert input_id into ttnn input
                pt_decode_input = embd(input_ids[:, kv_cache_len]).view(batch_size, seqlen, -1)

                start_pos = generation_start_pos + kv_cache_len

                if embed_on_host:
                    decode_input_11BH = prepare_inputs_ttnn(
                        pt_decode_input,
                        model_args.dim,
                        tt_model.mesh_device,
                    )
                else:
                    assert "Only embedding on host is supported for now!"

                # Run ttnn mixtral model
                tt_logits = tt_model(decode_input_11BH, [start_pos] * batch_size)

                if embed_on_host:
                    # Convert ttnn tensor to torch tensor
                    pt_logits = (
                        ttnn.to_torch(tt_logits, mesh_composer=ConcatMeshToTensor(mesh_device, dim=0))[0]
                        .squeeze(1)
                        .view(32, seqlen, -1)
                        .detach()
                        .float()
                    )[:batch_size, ...]
                else:
                    assert "Only embedding on host is supported for now!"

                logits.append(pt_logits.view(-1, 1, model_args.vocab_size))

                if validate_ref_model:
                    positions = torch.LongTensor([start_pos])
                    ref_pt_logits = reference_model(pt_decode_input, positions).detach().float()
                    ref_logits.append(ref_pt_logits.view(-1, 1, model_args.vocab_size))

        logits = torch.cat(logits, dim=1)
        # Re-shape logits and labels and calculate metrics
        logits = logits.view(batch_size * max_seq_len, model_args.vocab_size)  # batch_size * max_seq_len, vocab_size
        labels = labels.view(-1)  # batch_size * max_seq_len

        # Calculate accuracy metrics
        nll, top1_acc, top5_acc = calculate_acc_metrics(logits, labels)
        running_nll += nll
        running_top1_acc += top1_acc
        running_top5_acc += top5_acc

        if validate_ref_model:
            ref_logits = torch.cat(ref_logits, dim=1)
            # Re-shape logits and labels and calculate metrics
            ref_logits = ref_logits.view(
                batch_size * max_seq_len, model_args.vocab_size
            )  # batch_size * max_seq_len, vocab_size

            # Calculate accuracy metrics
            ref_nll, ref_top1_acc, ref_top5_acc = calculate_acc_metrics(ref_logits, labels)
            ref_running_nll += ref_nll
            ref_running_top1_acc += ref_top1_acc
            ref_running_top5_acc += ref_top5_acc

    # Validate accuracy metrics against the expected values
    nll = running_nll / len(dataloader)
    ppl = np.exp(nll)
    top1_acc = running_top1_acc / len(dataloader)
    top5_acc = running_top5_acc / len(dataloader)
    logger.info(f"Negative log-likelihood: {nll:.4f}")
    logger.info(f"Perplexity: {ppl:.4f}")
    logger.info(f"Top-1 accuracy: {top1_acc:.4f}")
    logger.info(f"Top-5 accuracy: {top5_acc:.4f}")

    calculated_acc_metrics = {"ppl": ppl, "top1_acc": top1_acc, "top5_acc": top5_acc}
    verify_acc_metrics(calculated_acc_metrics, expected_acc_metrics)

    if validate_ref_model:
        ref_nll = ref_running_nll / len(dataloader)
        ref_ppl = np.exp(ref_nll)
        ref_top1_acc = ref_running_top1_acc / len(dataloader)
        ref_top5_acc = ref_running_top5_acc / len(dataloader)
        logger.info(f"Ref Negative log-likelihood: {nll:.4f}")
        logger.info(f"Ref Perplexity: {ppl:.4f}")
        logger.info(f"Ref Top-1 accuracy: {top1_acc:.4f}")
        logger.info(f"Ref Top-5 accuracy: {top5_acc:.4f}")


@pytest.mark.timeout(3600)
@pytest.mark.parametrize(
    "llm_mode, max_seq_len, num_samples, expected_ppl, expected_top1, expected_top5",
    (
        # TODO Add prefill accuracy tests
        # ("prefill", 128, 64, -, -, -),
        # ("prefill", 1024, 64, -, -, -),
        # ("prefill", 2048, 64, -, -, -),
        # ("prefill", 4096, 64, -, -, -),
        ("decode", 128, 64, 8.80, 0.52, 0.75),
        # ("decode", 1024, 64, 5.10, 0.62, 0.83),
        # ("decode", 2048, 64, 4.23, 0.64, 0.85),
        # ("decode", 4096, 32, 10.59, 0.49, 0.73),
    ),
    ids=[
        # "prefill_128",
        # "prefill_1024",
        # "prefill_2048",
        # "prefill_4096",
        "decode_128",
        # "decode_1024",
        # "decode_2048",
        # "decode_4096",
    ],
)
def test_mixtral_perplexity(
    t3k_mesh_device,
    use_program_cache,
    reset_seeds,
    llm_mode,
    max_seq_len,
    num_samples,
    expected_ppl,
    expected_top1,
    expected_top5,
):
    assert (
        llm_mode == "decode"
    ), "Only decode mode is supported for now"  # TODO Add prefill support when it reaches main

    # Adjust the batch size based on the max prefill length
    if max_seq_len >= 16 * 1024:
        batch_size = 8
    elif max_seq_len >= 8 * 1024:
        batch_size = 16
    else:
        batch_size = 32

    return run_test_perplexity(
        mesh_device=t3k_mesh_device,
        batch_size=batch_size,
        llm_mode=llm_mode,
        max_seq_len=max_seq_len,
        num_samples=num_samples,
        expected_acc_metrics={"ppl": expected_ppl, "top1_acc": expected_top1, "top5_acc": expected_top5},
    )
