# SPDX-FileCopyrightText: Â© 2023 Tenstorrent Inc.

# SPDX-License-Identifier: Apache-2.0

import os

import torch

import ttnn
from models.demos.wormhole.stable_diffusion.tt.ttnn_functional_basic_transformer_block import basic_transformer_block
from models.demos.wormhole.stable_diffusion.tt.ttnn_functional_utility_functions import (
    dealloc_input,
    permute_conv_parameters,
    pre_process_input,
)


class transformer_2d_model:
    def __init__(self, device, parameters, batch_size, input_height, input_width, compute_kernel_config):
        self.device = device
        self.compute_kernel_config = compute_kernel_config
        parameters.proj_in.weight, parameters.proj_in.bias = permute_conv_parameters(
            parameters.proj_in.weight, parameters.proj_in.bias
        )
        self.batch_size = batch_size
        self.input_height = input_height
        self.input_width = input_width
        parameters.proj_in.bias = torch.reshape(parameters.proj_in.bias, (1, 1, 1, parameters.proj_in.bias.shape[-1]))
        self.proj_in_conv_weights = ttnn.from_torch(parameters.proj_in.weight, ttnn.float32)
        self.proj_in_conv_bias = ttnn.from_torch(parameters.proj_in.bias, ttnn.float32)
        out_channels = parameters.proj_in.weight.shape[0]
        in_channels = parameters.proj_in.weight.shape[1]

        # TODO: instead of hardcoding grid size in many components, configure it in a single place
        self.grid_size = (8, 8)

        self.fallback_on_groupnorm = os.environ.get("FALLBACK_ON_GROUPNORM", "0") == "1"

        self.proj_in_in_channels = in_channels
        self.proj_in_out_channels = out_channels
        norm_num_groups = 32
        (
            self.gn_expected_input_sharded_memory_config,
            self.group_norm_core_grid,
        ) = ttnn.determine_expected_group_norm_sharded_config_and_grid_size(
            device=self.device,
            num_channels=in_channels,
            num_groups=norm_num_groups,
            input_nhw=batch_size * input_height * input_width,
            is_height_sharded=False,
        )

        if not self.fallback_on_groupnorm:
            if self.gn_expected_input_sharded_memory_config.memory_layout == ttnn.TensorMemoryLayout.BLOCK_SHARDED:
                num_cores_across_channel = self.group_norm_core_grid.y
            elif self.gn_expected_input_sharded_memory_config.memory_layout == ttnn.TensorMemoryLayout.HEIGHT_SHARDED:
                num_cores_across_channel = 1
            else:
                num_cores_across_channel = int(self.group_norm_core_grid.x * self.group_norm_core_grid.y)

            parameters.norm.weight = ttnn.create_group_norm_weight_bias_rm(
                ttnn.to_torch(parameters.norm.weight), in_channels, num_cores_across_channel
            )
            parameters.norm.bias = ttnn.create_group_norm_weight_bias_rm(
                ttnn.to_torch(parameters.norm.bias), in_channels, num_cores_across_channel
            )
            parameters.norm.weight = ttnn.from_torch(
                parameters.norm.weight,
                dtype=ttnn.bfloat16,
                layout=ttnn.ROW_MAJOR_LAYOUT,
                device=device,
                memory_config=ttnn.DRAM_MEMORY_CONFIG,
            )
            parameters.norm.bias = ttnn.from_torch(
                parameters.norm.bias,
                dtype=ttnn.bfloat16,
                layout=ttnn.ROW_MAJOR_LAYOUT,
                device=device,
                memory_config=ttnn.DRAM_MEMORY_CONFIG,
            )

            self.norm_input_mask_torch_tensor = ttnn.create_group_norm_input_mask(
                in_channels, norm_num_groups, num_cores_across_channel
            )
            self.norm_input_mask = ttnn.from_torch(
                self.norm_input_mask_torch_tensor,
                dtype=ttnn.bfloat8_b,
                layout=ttnn.TILE_LAYOUT,
                device=device,
                memory_config=ttnn.DRAM_MEMORY_CONFIG,
            )

        parameters.proj_out.weight, parameters.proj_out.bias = permute_conv_parameters(
            parameters.proj_out.weight, parameters.proj_out.bias
        )
        parameters.proj_out.bias = torch.reshape(
            parameters.proj_out.bias, (1, 1, 1, parameters.proj_out.bias.shape[-1])
        )
        self.proj_out_conv_weights = ttnn.from_torch(parameters.proj_out.weight, ttnn.float32)
        self.proj_out_conv_bias = ttnn.from_torch(parameters.proj_out.bias, ttnn.float32)
        out_channels = parameters.proj_out.weight.shape[0]
        in_channels = parameters.proj_out.weight.shape[1]

        self.output_height = ttnn.get_conv_output_dim(input_height, 1, 1, 0)
        self.output_width = ttnn.get_conv_output_dim(input_width, 1, 1, 0)
        self.proj_out_in_channels = in_channels
        self.proj_out_out_channels = out_channels
        self.blocks = [
            basic_transformer_block(device, block, seq_len=input_height * input_width)
            for block in parameters.transformer_blocks
        ]
        self.parameters = parameters

    def __call__(
        self,
        hidden_states,
        config,
        encoder_hidden_states=None,
        timestep=None,
        class_labels=None,
        cross_attention_kwargs=None,
        return_dict=True,
        num_attention_heads=16,
        attention_head_dim=None,
        in_channels=None,
        out_channels=None,
        num_layers=1,
        norm_num_groups=32,
        cross_attention_dim=None,
        attention_bias=False,
        num_vector_embeds=None,
        patch_size=None,
        num_embeds_ada_norm=None,
        use_linear_projection=False,
        only_cross_attention=False,
        upcast_attention=False,
        norm_type="layer_norm",
        eps=1e-5,
        norm_elementwise_affine: bool = True,
        output_bfloat16: bool = False,
    ):
        inner_dim = num_attention_heads * attention_head_dim
        assert norm_num_groups == 32
        is_input_continuous = (in_channels is not None) and (patch_size is None)
        is_input_vectorized = num_vector_embeds is not None
        is_input_patches = in_channels is not None and patch_size is not None
        assert (
            is_input_continuous and (not is_input_patches) and (not is_input_vectorized)
        ), "we only support continuous input."
        if norm_type == "layer_norm" and num_embeds_ada_norm is not None:
            deprecation_message = (
                f"The configuration file of this model: transformer_2d_model is outdated. `norm_type` is either not set or"
                " incorrectly set to `'layer_norm'`.Make sure to set `norm_type` to `'ada_norm'` in the config."
                " Please make sure to update the config accordingly as leaving `norm_type` might led to incorrect"
                " results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it"
                " would be very nice if you could open a Pull request for the `transformer/config.json` file"
            )
            deprecate(
                "norm_type!=num_embeds_ada_norm",
                "1.0.0",
                deprecation_message,
                standard_warn=False,
            )
            norm_type = "ada_norm"

        nhw = hidden_states.shape[2]
        if hidden_states.shape[0] == 1:
            assert nhw == self.batch_size * self.input_height * self.input_width
        batch = self.batch_size
        height = self.input_height
        width = self.input_width

        residual = hidden_states
        spilled_residual = False
        if spilled_residual:
            residual = ttnn.to_memory_config(residual, ttnn.DRAM_MEMORY_CONFIG)

        hidden_states = ttnn.to_layout(
            hidden_states,
            ttnn.ROW_MAJOR_LAYOUT,
            memory_config=hidden_states.memory_config(),
        )

        if self.fallback_on_groupnorm:
            hidden_states = ttnn.to_memory_config(hidden_states, ttnn.L1_MEMORY_CONFIG)
            hidden_states = ttnn.reshape(
                hidden_states,
                (self.batch_size, self.input_height, self.input_width, in_channels),
            )
            hidden_states = ttnn.permute(hidden_states, (0, 3, 1, 2))
            hidden_states = ttnn.operations.normalization._fallback_group_norm(
                hidden_states,
                num_groups=norm_num_groups,
                weight=self.parameters.norm.weight,
                bias=self.parameters.norm.bias,
                epsilon=eps,
            )

            hidden_states = pre_process_input(self.device, hidden_states)

        else:
            hidden_states = ttnn.reshape(
                hidden_states, (self.batch_size, 1, self.input_height * self.input_width, in_channels)
            )
            if ttnn.get_memory_config(hidden_states) != self.gn_expected_input_sharded_memory_config:
                # hidden_states = ttnn.reshard(hidden_states, self.gn_expected_input_sharded_memory_config)
                hidden_states = ttnn.to_memory_config(hidden_states, ttnn.L1_MEMORY_CONFIG)
                hidden_states = ttnn.to_memory_config(hidden_states, self.gn_expected_input_sharded_memory_config)
            hidden_states = ttnn.group_norm(
                input_tensor=hidden_states,
                num_groups=norm_num_groups,
                epsilon=eps,
                input_mask=self.norm_input_mask,
                weight=self.parameters.norm.weight,
                bias=self.parameters.norm.bias,
                memory_config=hidden_states.memory_config(),
                core_grid=self.group_norm_core_grid,
            )
        hidden_states = ttnn.reshape(
            hidden_states, (1, 1, self.batch_size * self.input_height * self.input_width, in_channels)
        )
        hidden_states = ttnn.to_memory_config(
            hidden_states, ttnn.L1_MEMORY_CONFIG
        )  # sharded to interleaved since we can't tilize block sharded
        hidden_states = ttnn.tilize(
            hidden_states,
            memory_config=hidden_states.memory_config(),
            dtype=ttnn.bfloat8_b,
        )

        # enforce same core grid on BH as we use on WH for now
        end_x = 7 if attention_head_dim == 160 else 4
        core_grid = ttnn.CoreRangeSet(
            {
                ttnn.CoreRange(
                    ttnn.CoreCoord(0, 0),
                    ttnn.CoreCoord(end_x, 7),
                ),
            }
        )
        conv_config = ttnn.Conv2dConfig(
            dtype=ttnn.bfloat8_b,
            weights_dtype=ttnn.bfloat8_b,
            activation="",
            shard_layout=ttnn.TensorMemoryLayout.BLOCK_SHARDED,
            transpose_shards=False,
            reshard_if_not_optimal=False,
            override_sharding_config=True,
            core_grid=core_grid,
        )
        compute_config = ttnn.init_device_compute_kernel_config(
            self.device.arch(),
            math_fidelity=ttnn.MathFidelity.LoFi,
            fp32_dest_acc_en=self.compute_kernel_config.fp32_dest_acc_en,
        )

        conv_kwargs = {
            "in_channels": self.proj_in_in_channels,
            "out_channels": self.proj_in_out_channels,
            "batch_size": self.batch_size,
            "input_height": self.input_height,
            "input_width": self.input_width,
            "kernel_size": (1, 1),
            "stride": (1, 1),
            "padding": (0, 0),
            "dilation": (1, 1),
            "groups": 1,
            "device": self.device,
            "conv_config": conv_config,
        }

        if not ttnn.is_tensor_storage_on_device(self.proj_in_conv_weights):
            self.proj_in_conv_weights = ttnn.prepare_conv_weights(
                weight_tensor=self.proj_in_conv_weights,
                weights_format="OIHW",
                input_memory_config=hidden_states.memory_config(),
                input_layout=hidden_states.get_layout(),
                has_bias=True,
                **conv_kwargs,
            )
            self.proj_in_conv_bias = ttnn.prepare_conv_bias(
                bias_tensor=self.proj_in_conv_bias,
                input_memory_config=hidden_states.memory_config(),
                input_layout=hidden_states.get_layout(),
                **conv_kwargs,
            )
            self.proj_in_conv_weights = ttnn.to_device(self.proj_in_conv_weights, self.device)
            self.proj_in_conv_bias = ttnn.to_device(self.proj_in_conv_bias, self.device)

        hidden_states = ttnn.conv2d(
            input_tensor=hidden_states,
            weight_tensor=self.proj_in_conv_weights,
            bias_tensor=self.proj_in_conv_bias,
            **conv_kwargs,
            compute_config=compute_config,
            return_output_dim=False,
            return_weights_and_bias=False,
        )

        inner_dim = hidden_states.shape[-1]
        # hidden_states = ttnn.to_layout(hidden_states, layout=ttnn.ROW_MAJOR_LAYOUT)
        # hidden_states = ttnn.reshape(hidden_states, (1, batch, height * width, inner_dim))

        # 2. Blocks
        for block in self.blocks:
            hidden_states = block(
                hidden_states=hidden_states,
                encoder_hidden_states=encoder_hidden_states,
                timestep=timestep,
                cross_attention_kwargs=cross_attention_kwargs,
                class_labels=class_labels,
                attention_head_dim=attention_head_dim,
                config=config,
                cross_attention_dim=cross_attention_dim,
                upcast_attention=upcast_attention,
                attention_bias=attention_bias,
                only_cross_attention=only_cross_attention,
            )

        # 3. Output
        out_channels = in_channels if out_channels is None else out_channels
        if is_input_continuous:
            if not use_linear_projection:
                conv_config = ttnn.Conv2dConfig(
                    dtype=ttnn.bfloat8_b,
                    weights_dtype=ttnn.bfloat8_b,
                    activation="",
                    shard_layout=ttnn.TensorMemoryLayout.BLOCK_SHARDED,
                    transpose_shards=False,
                )

                conv_kwargs_1 = {
                    "in_channels": self.proj_out_in_channels,
                    "out_channels": self.proj_out_out_channels,
                    "batch_size": self.batch_size,
                    "input_height": self.input_height,
                    "input_width": self.input_width,
                    "kernel_size": (1, 1),
                    "stride": (1, 1),
                    "padding": (0, 0),
                    "dilation": (1, 1),
                    "groups": 1,
                    "device": self.device,
                    "conv_config": conv_config,
                }

                if not ttnn.is_tensor_storage_on_device(self.proj_out_conv_weights):
                    self.proj_out_conv_weights = ttnn.prepare_conv_weights(
                        weight_tensor=self.proj_out_conv_weights,
                        weights_format="OIHW",
                        input_memory_config=hidden_states.memory_config(),
                        input_layout=hidden_states.get_layout(),
                        has_bias=True,
                        **conv_kwargs_1,
                    )
                    self.proj_out_conv_bias = ttnn.prepare_conv_bias(
                        bias_tensor=self.proj_out_conv_bias,
                        input_memory_config=hidden_states.memory_config(),
                        input_layout=hidden_states.get_layout(),
                        **conv_kwargs_1,
                    )
                    self.proj_out_conv_weights = ttnn.to_device(self.proj_out_conv_weights, self.device)
                    self.proj_out_conv_bias = ttnn.to_device(self.proj_out_conv_bias, self.device)
                # hidden_states = ttnn.to_memory_config(hidden_states, self.proj_out.conv.input_sharded_memory_config)
                [
                    hidden_states,
                    [_out_height, _out_width],
                    [self.proj_out_conv_weights, self.proj_out_conv_bias],
                ] = ttnn.conv2d(
                    input_tensor=hidden_states,
                    **conv_kwargs_1,
                    weight_tensor=self.proj_out_conv_weights,
                    bias_tensor=self.proj_out_conv_bias,
                    return_output_dim=True,
                    return_weights_and_bias=True,
                )

                if output_bfloat16:
                    hidden_states = dealloc_input(
                        ttnn.add,
                        hidden_states,
                        residual,
                        dtype=ttnn.bfloat16,
                        memory_config=hidden_states.memory_config(),
                    )
                else:
                    hidden_states = dealloc_input(
                        ttnn.add,
                        hidden_states,
                        residual,
                        memory_config=hidden_states.memory_config(),
                    )
            else:
                hidden_states = ttnn.to_device(hidden_states, self.device)
                hidden_states = ttnn.matmul(hidden_states, self.parameters.proj_out.weight)
                hidden_states = ttnn.add(hidden_states, self.parameters.proj_out.bias)

                hidden_states = ttnn.to_layout(hidden_states, layout=ttnn.ROW_MAJOR_LAYOUT)
                hidden_states = ttnn.reshape(hidden_states, (batch, height, width, inner_dim))

                hidden_states = ttnn.permute(hidden_states, (0, 3, 1, 2))

                hidden_states = ttnn.add(
                    hidden_states,
                    residual,
                )

        if not return_dict:
            return (hidden_states,)
        hidden_states = ttnn.reallocate(hidden_states)
        return hidden_states
