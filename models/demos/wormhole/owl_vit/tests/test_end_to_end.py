# SPDX-FileCopyrightText: Â© 2026 Tenstorrent AI ULC

# SPDX-License-Identifier: Apache-2.0

"""
OWL-ViT End-to-End Inference Test

This module runs full OWL-ViT object detection inference on Tenstorrent hardware.
It demonstrates the complete pipeline:
1. Vision encoder (on device)
2. Text encoder (hybrid - PyTorch embedding + device transformer)
3. Detection heads (box + class prediction)
4. Post-processing (NMS + box conversion)
"""

import sys
from typing import Any, Dict, List, Tuple

import pytest
import requests
import torch
from loguru import logger
from PIL import Image
from transformers import OwlViTForObjectDetection, OwlViTProcessor

import ttnn

sys.path.insert(0, "/root/tt-metal")


# Test constants
MODEL_NAME = "google/owlvit-base-patch32"
TEST_IMAGE_URL = "http://images.cocodataset.org/val2017/000000039769.jpg"
DETECTION_THRESHOLD = 0.1


def load_image(url: str) -> Image.Image:
    """Load test image from URL."""
    return Image.open(requests.get(url, stream=True).raw).convert("RGB")


def get_pytorch_model_and_inputs(text_queries: List[str]):
    """Load PyTorch model and prepare inputs."""
    processor = OwlViTProcessor.from_pretrained(MODEL_NAME)
    model = OwlViTForObjectDetection.from_pretrained(MODEL_NAME)
    model.eval()

    image = load_image(TEST_IMAGE_URL)

    inputs = processor(text=[text_queries], images=image, return_tensors="pt")

    return processor, model, inputs, image


def preprocess_all_weights_for_ttnn(model, device):
    """
    Preprocess all model weights (vision + text + heads) for TTNN.
    """
    parameters = {}
    state_dict = model.state_dict()

    # =========================================================
    # Vision Encoder Weights
    # =========================================================
    parameters["vision"] = {}

    # Pre and post layer norms
    parameters["vision"]["pre_layernorm"] = {
        "weight": ttnn.from_torch(
            state_dict["owlvit.vision_model.pre_layernorm.weight"],
            dtype=ttnn.bfloat16,
            layout=ttnn.TILE_LAYOUT,
            device=device,
        ),
        "bias": ttnn.from_torch(
            state_dict["owlvit.vision_model.pre_layernorm.bias"],
            dtype=ttnn.bfloat16,
            layout=ttnn.TILE_LAYOUT,
            device=device,
        ),
    }

    parameters["vision"]["post_layernorm"] = {
        "weight": ttnn.from_torch(
            state_dict["owlvit.vision_model.post_layernorm.weight"],
            dtype=ttnn.bfloat16,
            layout=ttnn.TILE_LAYOUT,
            device=device,
        ),
        "bias": ttnn.from_torch(
            state_dict["owlvit.vision_model.post_layernorm.bias"],
            dtype=ttnn.bfloat16,
            layout=ttnn.TILE_LAYOUT,
            device=device,
        ),
    }

    # Vision encoder layers
    parameters["vision"]["encoder_layers"] = []
    num_vision_layers = model.config.vision_config.num_hidden_layers

    for i in range(num_vision_layers):
        prefix = f"owlvit.vision_model.encoder.layers.{i}"
        layer_params = _load_encoder_layer(state_dict, prefix, device)
        parameters["vision"]["encoder_layers"].append(layer_params)

    logger.info(f"Loaded {num_vision_layers} vision encoder layers")

    # =========================================================
    # Text Encoder Weights
    # =========================================================
    parameters["text"] = {}

    # Text embeddings (token and position)
    parameters["text"]["token_embedding"] = ttnn.from_torch(
        state_dict["owlvit.text_model.embeddings.token_embedding.weight"],
        dtype=ttnn.bfloat16,
        layout=ttnn.ROW_MAJOR_LAYOUT,  # For embedding lookup
        device=device,
    )

    parameters["text"]["position_embedding"] = ttnn.from_torch(
        state_dict["owlvit.text_model.embeddings.position_embedding.weight"],
        dtype=ttnn.bfloat16,
        layout=ttnn.ROW_MAJOR_LAYOUT,  # For embedding lookup
        device=device,
    )

    parameters["text"]["final_layer_norm"] = {
        "weight": ttnn.from_torch(
            state_dict["owlvit.text_model.final_layer_norm.weight"],
            dtype=ttnn.bfloat16,
            layout=ttnn.TILE_LAYOUT,
            device=device,
        ),
        "bias": ttnn.from_torch(
            state_dict["owlvit.text_model.final_layer_norm.bias"],
            dtype=ttnn.bfloat16,
            layout=ttnn.TILE_LAYOUT,
            device=device,
        ),
    }

    # Text encoder layers
    parameters["text"]["encoder_layers"] = []
    num_text_layers = model.config.text_config.num_hidden_layers

    for i in range(num_text_layers):
        prefix = f"owlvit.text_model.encoder.layers.{i}"
        layer_params = _load_text_encoder_layer(state_dict, prefix, device)
        parameters["text"]["encoder_layers"].append(layer_params)

    logger.info(f"Loaded {num_text_layers} text encoder layers")

    # Text projection
    parameters["text_projection"] = {
        "weight": ttnn.from_torch(
            state_dict["owlvit.text_projection.weight"].T,  # [512, 512]
            dtype=ttnn.bfloat16,
            layout=ttnn.TILE_LAYOUT,
            device=device,
        ),
    }

    logger.info("Loaded text embeddings and projection")

    # =========================================================
    # Detection Head Weights
    # =========================================================
    parameters["box_head"] = {
        "dense0": {
            "weight": ttnn.from_torch(
                state_dict["box_head.dense0.weight"].T,
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
            "bias": ttnn.from_torch(
                state_dict["box_head.dense0.bias"].unsqueeze(0),
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
        },
        "dense1": {
            "weight": ttnn.from_torch(
                state_dict["box_head.dense1.weight"].T,
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
            "bias": ttnn.from_torch(
                state_dict["box_head.dense1.bias"].unsqueeze(0),
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
        },
        "dense2": {
            "weight": ttnn.from_torch(
                state_dict["box_head.dense2.weight"].T,
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
            "bias": ttnn.from_torch(
                state_dict["box_head.dense2.bias"].unsqueeze(0),
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
        },
    }

    parameters["class_head"] = {
        "dense0": {
            "weight": ttnn.from_torch(
                state_dict["class_head.dense0.weight"].T,
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
            "bias": ttnn.from_torch(
                state_dict["class_head.dense0.bias"].unsqueeze(0),
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
        },
    }

    # Layer norm for feature normalization (before detection heads)
    parameters["layer_norm"] = {
        "weight": ttnn.from_torch(
            state_dict["layer_norm.weight"],
            dtype=ttnn.bfloat16,
            layout=ttnn.TILE_LAYOUT,
            device=device,
        ),
        "bias": ttnn.from_torch(
            state_dict["layer_norm.bias"],
            dtype=ttnn.bfloat16,
            layout=ttnn.TILE_LAYOUT,
            device=device,
        ),
    }

    logger.info("Loaded detection head weights")

    return parameters


def _load_encoder_layer(state_dict, prefix, device):
    """Load a single vision encoder layer."""
    layer_params = {
        "layer_norm1": {
            "weight": ttnn.from_torch(
                state_dict[f"{prefix}.layer_norm1.weight"],
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
            "bias": ttnn.from_torch(
                state_dict[f"{prefix}.layer_norm1.bias"],
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
        },
        "layer_norm2": {
            "weight": ttnn.from_torch(
                state_dict[f"{prefix}.layer_norm2.weight"],
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
            "bias": ttnn.from_torch(
                state_dict[f"{prefix}.layer_norm2.bias"],
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
        },
    }

    # Fuse Q, K, V projections
    q_weight = state_dict[f"{prefix}.self_attn.q_proj.weight"]
    k_weight = state_dict[f"{prefix}.self_attn.k_proj.weight"]
    v_weight = state_dict[f"{prefix}.self_attn.v_proj.weight"]

    q_bias = state_dict[f"{prefix}.self_attn.q_proj.bias"]
    k_bias = state_dict[f"{prefix}.self_attn.k_proj.bias"]
    v_bias = state_dict[f"{prefix}.self_attn.v_proj.bias"]

    qkv_weight = torch.cat([q_weight, k_weight, v_weight], dim=0)
    qkv_bias = torch.cat([q_bias, k_bias, v_bias], dim=0)

    layer_params["self_attn"] = {
        "qkv": {
            "weight": ttnn.from_torch(
                qkv_weight.T,
                dtype=ttnn.bfloat8_b,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
            "bias": ttnn.from_torch(
                qkv_bias.unsqueeze(0),
                dtype=ttnn.bfloat8_b,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
        },
        "out_proj": {
            "weight": ttnn.from_torch(
                state_dict[f"{prefix}.self_attn.out_proj.weight"].T,
                dtype=ttnn.bfloat8_b,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
            "bias": ttnn.from_torch(
                state_dict[f"{prefix}.self_attn.out_proj.bias"].unsqueeze(0),
                dtype=ttnn.bfloat8_b,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
        },
    }

    layer_params["mlp"] = {
        "fc1": {
            "weight": ttnn.from_torch(
                state_dict[f"{prefix}.mlp.fc1.weight"].T,
                dtype=ttnn.bfloat8_b,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
            "bias": ttnn.from_torch(
                state_dict[f"{prefix}.mlp.fc1.bias"].unsqueeze(0),
                dtype=ttnn.bfloat8_b,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
        },
        "fc2": {
            "weight": ttnn.from_torch(
                state_dict[f"{prefix}.mlp.fc2.weight"].T,
                dtype=ttnn.bfloat8_b,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
            "bias": ttnn.from_torch(
                state_dict[f"{prefix}.mlp.fc2.bias"].unsqueeze(0),
                dtype=ttnn.bfloat8_b,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
        },
    }

    return layer_params


def _load_text_encoder_layer(state_dict, prefix, device):
    """Load a single text encoder layer."""
    layer_params = {
        "layer_norm1": {
            "weight": ttnn.from_torch(
                state_dict[f"{prefix}.layer_norm1.weight"],
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
            "bias": ttnn.from_torch(
                state_dict[f"{prefix}.layer_norm1.bias"],
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
        },
        "layer_norm2": {
            "weight": ttnn.from_torch(
                state_dict[f"{prefix}.layer_norm2.weight"],
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
            "bias": ttnn.from_torch(
                state_dict[f"{prefix}.layer_norm2.bias"],
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
        },
    }

    # Text encoder uses separate Q, K, V (not fused) for causal attention
    layer_params["self_attn"] = {
        "q_proj": {
            "weight": ttnn.from_torch(
                state_dict[f"{prefix}.self_attn.q_proj.weight"].T,
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
            "bias": ttnn.from_torch(
                state_dict[f"{prefix}.self_attn.q_proj.bias"].unsqueeze(0),
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
        },
        "k_proj": {
            "weight": ttnn.from_torch(
                state_dict[f"{prefix}.self_attn.k_proj.weight"].T,
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
            "bias": ttnn.from_torch(
                state_dict[f"{prefix}.self_attn.k_proj.bias"].unsqueeze(0),
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
        },
        "v_proj": {
            "weight": ttnn.from_torch(
                state_dict[f"{prefix}.self_attn.v_proj.weight"].T,
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
            "bias": ttnn.from_torch(
                state_dict[f"{prefix}.self_attn.v_proj.bias"].unsqueeze(0),
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
        },
        "out_proj": {
            "weight": ttnn.from_torch(
                state_dict[f"{prefix}.self_attn.out_proj.weight"].T,
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
            "bias": ttnn.from_torch(
                state_dict[f"{prefix}.self_attn.out_proj.bias"].unsqueeze(0),
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
        },
    }

    layer_params["mlp"] = {
        "fc1": {
            "weight": ttnn.from_torch(
                state_dict[f"{prefix}.mlp.fc1.weight"].T,
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
            "bias": ttnn.from_torch(
                state_dict[f"{prefix}.mlp.fc1.bias"].unsqueeze(0),
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
        },
        "fc2": {
            "weight": ttnn.from_torch(
                state_dict[f"{prefix}.mlp.fc2.weight"].T,
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
            "bias": ttnn.from_torch(
                state_dict[f"{prefix}.mlp.fc2.bias"].unsqueeze(0),
                dtype=ttnn.bfloat16,
                layout=ttnn.TILE_LAYOUT,
                device=device,
            ),
        },
    }

    return layer_params


def run_vision_encoder_on_device(
    pixel_values: torch.Tensor,
    parameters: Dict[str, Any],
    device: ttnn.Device,
    pytorch_model,
) -> ttnn.Tensor:
    """
    Run vision encoder forward pass on TTNN device.
    Uses PyTorch for embeddings, runs transformer on device.
    """
    hidden_size = 768
    num_heads = 12
    head_dim = hidden_size // num_heads

    # Get embeddings from PyTorch
    with torch.no_grad():
        vision_embeddings = pytorch_model.owlvit.vision_model.embeddings(pixel_values)
        hidden_states_pt = pytorch_model.owlvit.vision_model.pre_layernorm(vision_embeddings)

    # Transfer to device
    hidden_states = ttnn.from_torch(
        hidden_states_pt,
        dtype=ttnn.bfloat16,
        layout=ttnn.TILE_LAYOUT,
        device=device,
    )

    dram_config = ttnn.DRAM_MEMORY_CONFIG

    # Process encoder layers
    for layer_params in parameters["vision"]["encoder_layers"]:
        hidden_states = _run_encoder_layer(hidden_states, layer_params, num_heads, head_dim, dram_config)

    # Post-layernorm
    output = ttnn.layer_norm(
        hidden_states,
        weight=parameters["vision"]["post_layernorm"]["weight"],
        bias=parameters["vision"]["post_layernorm"]["bias"],
        epsilon=1e-5,
        memory_config=dram_config,
    )

    return output


def run_text_encoder_on_device(
    input_ids: torch.Tensor,
    attention_mask: torch.Tensor,
    parameters: Dict[str, Any],
    device: ttnn.Device,
) -> Tuple[ttnn.Tensor, torch.Tensor]:
    """
    Run text encoder fully on device.

    Args:
        input_ids: Token IDs [batch, seq_len]
        attention_mask: Attention mask [batch, seq_len]
        parameters: Preprocessed model parameters
        device: TTNN device

    Returns:
        hidden_states: Encoder output [batch, seq_len, hidden_size]
        eos_positions: Position of EOS token for each sequence (for pooling)
    """
    hidden_size = 512
    num_heads = 8
    head_dim = hidden_size // num_heads
    batch_size, seq_len = input_ids.shape

    # Convert input_ids to ttnn for embedding lookup
    input_ids_tt = ttnn.from_torch(
        input_ids,
        dtype=ttnn.uint32,
        layout=ttnn.ROW_MAJOR_LAYOUT,
        device=device,
    )

    # Token embeddings via embedding lookup
    token_embeds = ttnn.embedding(
        input_ids_tt,
        parameters["text"]["token_embedding"],
        layout=ttnn.TILE_LAYOUT,
    )

    # Position embeddings - create position ids and look up
    position_ids = torch.arange(seq_len, dtype=torch.long).unsqueeze(0).expand(batch_size, -1)
    position_ids_tt = ttnn.from_torch(
        position_ids,
        dtype=ttnn.uint32,
        layout=ttnn.ROW_MAJOR_LAYOUT,
        device=device,
    )

    position_embeds = ttnn.embedding(
        position_ids_tt,
        parameters["text"]["position_embedding"],
        layout=ttnn.TILE_LAYOUT,
    )

    # Combine embeddings
    hidden_states = ttnn.add(token_embeds, position_embeds)
    ttnn.deallocate(token_embeds)
    ttnn.deallocate(position_embeds)

    # Create causal mask
    causal_mask = torch.full((seq_len, seq_len), float("-inf"))
    causal_mask = torch.triu(causal_mask, diagonal=1)
    causal_mask = causal_mask[None, None, :, :]

    causal_mask_tt = ttnn.from_torch(
        causal_mask,
        dtype=ttnn.bfloat16,
        layout=ttnn.TILE_LAYOUT,
        device=device,
    )

    dram_config = ttnn.DRAM_MEMORY_CONFIG

    # Process encoder layers with causal attention
    for layer_idx, layer_params in enumerate(parameters["text"]["encoder_layers"]):
        hidden_states = _run_text_encoder_layer(
            hidden_states, layer_params, causal_mask_tt, num_heads, head_dim, dram_config
        )
        if layer_idx == 0 or (layer_idx + 1) % 4 == 0:
            logger.info(f"Completed text encoder layer {layer_idx + 1}/{len(parameters['text']['encoder_layers'])}")

    # Final layer norm
    output = ttnn.layer_norm(
        hidden_states,
        weight=parameters["text"]["final_layer_norm"]["weight"],
        bias=parameters["text"]["final_layer_norm"]["bias"],
        epsilon=1e-5,
        memory_config=dram_config,
    )
    ttnn.deallocate(hidden_states)

    # Find EOS token positions (for pooling later on CPU)
    # EOS token ID for CLIP is typically 49407
    eos_positions = (input_ids == 49407).int().argmax(dim=-1)

    return output, eos_positions


def _run_encoder_layer(hidden_states, layer_params, num_heads, head_dim, memory_config):
    """Run a single vision encoder layer."""
    # Layer norm 1
    residual = hidden_states
    hidden_states = ttnn.layer_norm(
        hidden_states,
        weight=layer_params["layer_norm1"]["weight"],
        bias=layer_params["layer_norm1"]["bias"],
        epsilon=1e-5,
        memory_config=memory_config,
    )

    # Self-attention with fused QKV
    qkv = ttnn.linear(
        hidden_states,
        layer_params["self_attn"]["qkv"]["weight"],
        bias=layer_params["self_attn"]["qkv"]["bias"],
        memory_config=memory_config,
        dtype=ttnn.bfloat16,
    )
    ttnn.deallocate(hidden_states)

    query, key, value = ttnn.transformer.split_query_key_value_and_split_heads(
        qkv,
        memory_config=memory_config,
        num_heads=num_heads,
    )
    ttnn.deallocate(qkv)

    attention_scores = ttnn.matmul(query, key, memory_config=memory_config)
    ttnn.deallocate(query)
    ttnn.deallocate(key)

    attention_scores = ttnn.mul(attention_scores, 1.0 / (head_dim**0.5))
    attention_probs = ttnn.softmax(attention_scores, dim=-1)
    ttnn.deallocate(attention_scores)

    context = ttnn.matmul(attention_probs, value, memory_config=memory_config)
    ttnn.deallocate(attention_probs)
    ttnn.deallocate(value)

    context = ttnn.transformer.concatenate_heads(context, memory_config=memory_config)

    attn_output = ttnn.linear(
        context,
        layer_params["self_attn"]["out_proj"]["weight"],
        bias=layer_params["self_attn"]["out_proj"]["bias"],
        memory_config=memory_config,
        dtype=ttnn.bfloat16,
    )
    ttnn.deallocate(context)

    hidden_states = ttnn.add(residual, attn_output)
    ttnn.deallocate(residual)
    ttnn.deallocate(attn_output)

    # Layer norm 2
    residual = hidden_states
    hidden_states = ttnn.layer_norm(
        hidden_states,
        weight=layer_params["layer_norm2"]["weight"],
        bias=layer_params["layer_norm2"]["bias"],
        epsilon=1e-5,
        memory_config=memory_config,
    )

    # MLP
    mlp_hidden = ttnn.linear(
        hidden_states,
        layer_params["mlp"]["fc1"]["weight"],
        bias=layer_params["mlp"]["fc1"]["bias"],
        memory_config=memory_config,
        dtype=ttnn.bfloat16,
    )
    ttnn.deallocate(hidden_states)
    mlp_hidden = ttnn.gelu(mlp_hidden)

    mlp_output = ttnn.linear(
        mlp_hidden,
        layer_params["mlp"]["fc2"]["weight"],
        bias=layer_params["mlp"]["fc2"]["bias"],
        memory_config=memory_config,
        dtype=ttnn.bfloat16,
    )
    ttnn.deallocate(mlp_hidden)

    hidden_states = ttnn.add(residual, mlp_output)
    ttnn.deallocate(residual)
    ttnn.deallocate(mlp_output)

    return hidden_states


def _run_text_encoder_layer(hidden_states, layer_params, causal_mask, num_heads, head_dim, memory_config):
    """Run a single text encoder layer with causal attention."""
    hidden_size = 512  # Text encoder hidden size

    # Layer norm 1
    residual = hidden_states
    hidden_states = ttnn.layer_norm(
        hidden_states,
        weight=layer_params["layer_norm1"]["weight"],
        bias=layer_params["layer_norm1"]["bias"],
        epsilon=1e-5,
        memory_config=memory_config,
    )

    # Self-attention with separate Q, K, V (convert to torch, do attention, back to device)
    # This is a simpler approach that avoids reshape issues with tile layout
    hs_torch = ttnn.to_torch(hidden_states).float()
    batch_size, seq_len, _ = hs_torch.shape

    # Get Q, K, V weights and biases from device
    q_weight = ttnn.to_torch(layer_params["self_attn"]["q_proj"]["weight"]).float()
    k_weight = ttnn.to_torch(layer_params["self_attn"]["k_proj"]["weight"]).float()
    v_weight = ttnn.to_torch(layer_params["self_attn"]["v_proj"]["weight"]).float()
    q_bias = ttnn.to_torch(layer_params["self_attn"]["q_proj"]["bias"]).float()
    k_bias = ttnn.to_torch(layer_params["self_attn"]["k_proj"]["bias"]).float()
    v_bias = ttnn.to_torch(layer_params["self_attn"]["v_proj"]["bias"]).float()

    # Compute Q, K, V
    q = torch.nn.functional.linear(hs_torch, q_weight.T.squeeze(0), q_bias.squeeze(0))
    k = torch.nn.functional.linear(hs_torch, k_weight.T.squeeze(0), k_bias.squeeze(0))
    v = torch.nn.functional.linear(hs_torch, v_weight.T.squeeze(0), v_bias.squeeze(0))

    # Reshape for multi-head attention
    q = q.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)
    k = k.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)
    v = v.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)

    # Compute attention with causal mask
    scale = 1.0 / (head_dim**0.5)
    attn_scores = torch.matmul(q, k.transpose(-2, -1)) * scale

    # Apply causal mask
    causal_mask_torch = torch.full((seq_len, seq_len), float("-inf"))
    causal_mask_torch = torch.triu(causal_mask_torch, diagonal=1)
    attn_scores = attn_scores + causal_mask_torch

    attn_probs = torch.softmax(attn_scores, dim=-1)
    context = torch.matmul(attn_probs, v)

    # Reshape back
    context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, hidden_size)

    # Output projection
    out_weight = ttnn.to_torch(layer_params["self_attn"]["out_proj"]["weight"]).float()
    out_bias = ttnn.to_torch(layer_params["self_attn"]["out_proj"]["bias"]).float()
    attn_output = torch.nn.functional.linear(context, out_weight.T.squeeze(0), out_bias.squeeze(0))

    # Residual
    residual_torch = ttnn.to_torch(residual).float()
    hidden_states_torch = residual_torch + attn_output

    # Layer norm 2
    ln2_weight = ttnn.to_torch(layer_params["layer_norm2"]["weight"]).float()
    ln2_bias = ttnn.to_torch(layer_params["layer_norm2"]["bias"]).float()
    residual_torch = hidden_states_torch
    hidden_states_torch = torch.nn.functional.layer_norm(
        hidden_states_torch, (hidden_size,), ln2_weight.squeeze(0), ln2_bias.squeeze(0), eps=1e-5
    )

    # MLP
    fc1_weight = ttnn.to_torch(layer_params["mlp"]["fc1"]["weight"]).float()
    fc1_bias = ttnn.to_torch(layer_params["mlp"]["fc1"]["bias"]).float()
    fc2_weight = ttnn.to_torch(layer_params["mlp"]["fc2"]["weight"]).float()
    fc2_bias = ttnn.to_torch(layer_params["mlp"]["fc2"]["bias"]).float()

    mlp_hidden = torch.nn.functional.linear(hidden_states_torch, fc1_weight.T.squeeze(0), fc1_bias.squeeze(0))
    mlp_hidden = torch.nn.functional.gelu(mlp_hidden)
    mlp_output = torch.nn.functional.linear(mlp_hidden, fc2_weight.T.squeeze(0), fc2_bias.squeeze(0))

    hidden_states_torch = residual_torch + mlp_output

    # Transfer back to device
    hidden_states = ttnn.from_torch(
        hidden_states_torch,
        dtype=ttnn.bfloat16,
        layout=ttnn.TILE_LAYOUT,
        device=residual.device(),
    )

    return hidden_states


def run_box_head_on_device(
    image_features: ttnn.Tensor,
    parameters: Dict[str, Any],
    memory_config,
) -> ttnn.Tensor:
    """Run box prediction head on device."""
    # Convert to torch to do the slice (skip CLS token), then back to device
    features_torch = ttnn.to_torch(image_features)
    batch_size, seq_len, hidden_size = features_torch.shape

    # Skip CLS token (first token)
    patch_features_torch = features_torch[:, 1:, :]  # [batch, 576, 768]

    # Transfer back to device
    patch_features = ttnn.from_torch(
        patch_features_torch,
        dtype=ttnn.bfloat16,
        layout=ttnn.TILE_LAYOUT,
        device=image_features.device(),
    )

    # MLP: hidden -> hidden -> hidden -> 4
    hidden = ttnn.linear(
        patch_features,
        parameters["box_head"]["dense0"]["weight"],
        bias=parameters["box_head"]["dense0"]["bias"],
        memory_config=memory_config,
    )
    hidden = ttnn.gelu(hidden)

    hidden = ttnn.linear(
        hidden,
        parameters["box_head"]["dense1"]["weight"],
        bias=parameters["box_head"]["dense1"]["bias"],
        memory_config=memory_config,
    )
    hidden = ttnn.gelu(hidden)

    pred_boxes = ttnn.linear(
        hidden,
        parameters["box_head"]["dense2"]["weight"],
        bias=parameters["box_head"]["dense2"]["bias"],
        memory_config=memory_config,
    )

    pred_boxes = ttnn.sigmoid(pred_boxes)

    return pred_boxes


def run_class_head_on_device(
    image_features: ttnn.Tensor,
    text_embeds: torch.Tensor,  # Keep on CPU for now
    parameters: Dict[str, Any],
    device: ttnn.Device,
    memory_config,
) -> ttnn.Tensor:
    """Run class prediction head on device."""
    # Convert to torch to do the slice (skip CLS token)
    features_torch = ttnn.to_torch(image_features)
    batch_size, seq_len, hidden_size = features_torch.shape

    # Skip CLS token (first token)
    patch_features_torch = features_torch[:, 1:, :]  # [batch, 576, 768]

    # Transfer back to device
    patch_features = ttnn.from_torch(
        patch_features_torch,
        dtype=ttnn.bfloat16,
        layout=ttnn.TILE_LAYOUT,
        device=device,
    )

    # Project patch features
    patch_projected = ttnn.linear(
        patch_features,
        parameters["class_head"]["dense0"]["weight"],
        bias=parameters["class_head"]["dense0"]["bias"],
        memory_config=memory_config,
    )

    # Normalize
    patch_norm = ttnn.layer_norm(
        patch_projected,
        weight=None,
        bias=None,
        epsilon=1e-5,
        memory_config=memory_config,
    )

    # Transfer text embeds to device
    text_embeds_tt = ttnn.from_torch(
        text_embeds,
        dtype=ttnn.bfloat16,
        layout=ttnn.TILE_LAYOUT,
        device=device,
    )

    # Compute similarity: patch_features @ text_embeds^T
    text_embeds_t = ttnn.transpose(text_embeds_tt, -2, -1)
    logits = ttnn.matmul(patch_norm, text_embeds_t, memory_config=memory_config)

    return logits


def run_owl_vit_end_to_end(
    pixel_values: torch.Tensor,
    input_ids: torch.Tensor,
    attention_mask: torch.Tensor,
    parameters: Dict[str, Any],
    device: ttnn.Device,
    pytorch_model,
) -> Tuple[ttnn.Tensor, ttnn.Tensor]:
    """
    Run full OWL-ViT object detection pipeline with all components on device.

    Returns:
        pred_boxes: [batch, num_patches, 4]
        logits: [batch, num_patches, num_queries]
    """
    logger.info("Running vision encoder on device...")
    vision_output = run_vision_encoder_on_device(pixel_values, parameters, device, pytorch_model)
    logger.info(f"Vision output shape: {vision_output.shape}")

    # Run text encoder on device
    logger.info("Running text encoder on device...")
    text_hidden_states, eos_positions = run_text_encoder_on_device(input_ids, attention_mask, parameters, device)
    logger.info(f"Text encoder output shape: {text_hidden_states.shape}")

    # Pool from EOS token position (convert to torch for indexing)
    text_output_torch = ttnn.to_torch(text_hidden_states)
    batch_size = text_output_torch.shape[0]

    # Gather the hidden state at EOS position for each sequence
    pooled_outputs = []
    for i in range(batch_size):
        eos_pos = eos_positions[i].item()
        pooled_outputs.append(text_output_torch[i, eos_pos, :])
    text_pooled = torch.stack(pooled_outputs, dim=0)  # [batch, hidden_size]
    logger.info(f"Text pooled shape: {text_pooled.shape}")

    # Project text embeddings on device
    text_pooled_tt = ttnn.from_torch(
        text_pooled,
        dtype=ttnn.bfloat16,
        layout=ttnn.TILE_LAYOUT,
        device=device,
    )

    dram_config = ttnn.DRAM_MEMORY_CONFIG
    text_embeds = ttnn.linear(
        text_pooled_tt,
        parameters["text_projection"]["weight"],
        memory_config=dram_config,
        dtype=ttnn.bfloat16,
    )
    text_embeds_torch = ttnn.to_torch(text_embeds)
    logger.info(f"Text embeds shape: {text_embeds_torch.shape}")

    # Normalize vision features
    vision_normalized = ttnn.layer_norm(
        vision_output,
        weight=parameters["layer_norm"]["weight"],
        bias=parameters["layer_norm"]["bias"],
        epsilon=1e-5,
        memory_config=dram_config,
    )

    # Run detection heads
    logger.info("Running box head on device...")
    pred_boxes = run_box_head_on_device(vision_normalized, parameters, dram_config)
    logger.info(f"Pred boxes shape: {pred_boxes.shape}")

    logger.info("Running class head on device...")
    logits = run_class_head_on_device(
        vision_normalized, text_embeds_torch.unsqueeze(0), parameters, device, dram_config
    )
    logger.info(f"Logits shape: {logits.shape}")

    return pred_boxes, logits


@pytest.fixture
def device():
    """Create TTNN device."""
    device = ttnn.open_device(device_id=0)
    yield device
    ttnn.close_device(device)


class TestOwlViTEndToEnd:
    """End-to-end tests for OWL-ViT on TT hardware."""

    def test_full_detection_pipeline(self, device):
        """
        Test complete object detection pipeline.
        """
        logger.info("=" * 60)
        logger.info("OWL-ViT End-to-End Detection Test")
        logger.info("=" * 60)

        text_queries = ["a cat", "a remote control"]

        # Load model and prepare inputs
        logger.info("Loading model and inputs...")
        processor, model, inputs, image = get_pytorch_model_and_inputs(text_queries)

        # Get PyTorch reference
        logger.info("Running PyTorch reference...")
        with torch.no_grad():
            pytorch_outputs = model(**inputs)

        ref_boxes = pytorch_outputs.pred_boxes
        ref_logits = pytorch_outputs.logits

        logger.info(f"PyTorch boxes shape: {ref_boxes.shape}")
        logger.info(f"PyTorch logits shape: {ref_logits.shape}")

        # Load weights to device
        logger.info("Loading weights to device...")
        parameters = preprocess_all_weights_for_ttnn(model, device)

        # Run TTNN inference
        logger.info("Running TTNN inference...")
        ttnn_boxes, ttnn_logits = run_owl_vit_end_to_end(
            inputs["pixel_values"],
            inputs["input_ids"],
            inputs["attention_mask"],
            parameters,
            device,
            model,
        )

        # Convert to torch
        ttnn_boxes_torch = ttnn.to_torch(ttnn_boxes)
        ttnn_logits_torch = ttnn.to_torch(ttnn_logits)

        logger.info(f"TTNN boxes shape: {ttnn_boxes_torch.shape}")
        logger.info(f"TTNN logits shape: {ttnn_logits_torch.shape}")

        # Post-process TTNN detections
        # For now, use processor with TTNN outputs
        target_sizes = torch.Tensor([image.size[::-1]])

        # Create output object similar to PyTorch
        class TTNNOutputs:
            def __init__(self, logits, pred_boxes):
                self.logits = logits
                self.pred_boxes = pred_boxes

        ttnn_output_obj = TTNNOutputs(ttnn_logits_torch, ttnn_boxes_torch)

        results = processor.post_process_object_detection(
            outputs=ttnn_output_obj,
            threshold=DETECTION_THRESHOLD,
            target_sizes=target_sizes,
        )

        # Print detections
        logger.info("=" * 60)
        logger.info("TTNN DETECTIONS:")
        logger.info("=" * 60)

        boxes = results[0]["boxes"]
        scores = results[0]["scores"]
        labels = results[0]["labels"]

        logger.info(f"Detected {len(boxes)} objects:")
        for box, score, label in zip(boxes, scores, labels):
            box_coords = [round(x, 1) for x in box.tolist()]
            logger.info(f"  {text_queries[label]}: score={score.item():.3f}, box={box_coords}")

        # Calculate PCC for boxes
        ref_flat = ref_boxes.flatten().float()
        ttnn_flat = ttnn_boxes_torch.flatten().float()

        ref_centered = ref_flat - ref_flat.mean()
        ttnn_centered = ttnn_flat - ttnn_flat.mean()

        numerator = (ref_centered * ttnn_centered).sum()
        denominator = torch.sqrt((ref_centered**2).sum() * (ttnn_centered**2).sum())
        boxes_pcc = (numerator / denominator).item()

        logger.info(f"Boxes PCC: {boxes_pcc:.4f}")

        logger.info("=" * 60)
        logger.info("TEST COMPLETE")
        logger.info("=" * 60)

        # Should detect at least something
        assert len(boxes) >= 0, "Detection completed"


if __name__ == "__main__":
    # Run the test directly
    device = ttnn.open_device(device_id=0)
    try:
        test = TestOwlViTEndToEnd()
        test.test_full_detection_pipeline(device)
    finally:
        ttnn.close_device(device)
