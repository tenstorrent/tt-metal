# SPDX-FileCopyrightText: Â© 2023 Tenstorrent Inc.

# SPDX-License-Identifier: Apache-2.0

import pytest
import torch
from loguru import logger

from models.demos.wormhole.mamba.reference.prefill_decode_model import Mamba
from models.demos.wormhole.mamba.tt import model_config
from models.demos.wormhole.mamba.tt.mamba_model import MambaTT
from models.demos.wormhole.mamba.tt.model_config import ModelMode
from models.perf.device_perf_utils import check_device_perf, prep_device_perf_report, run_device_perf
from models.perf.perf_utils import prep_perf_report
from models.utility_functions import disable_persistent_kernel_cache, profiler, skip_for_grayskull
from tt_metal.tools.profiler.process_model_log import get_samples_per_s


def is_nearby(actual: float, expected: float, lower_margin: float = 0.03, upper_margin: float = 0.03):
    lower_threshold = (1 - lower_margin) * expected
    upper_threshold = (1 + upper_margin) * expected
    return lower_threshold <= actual <= upper_threshold


NUM_LAYERS = 64
MARGIN = 0.05


@pytest.mark.models_performance_bare_metal
@pytest.mark.timeout(600)
@pytest.mark.parametrize(
    "model_version, mode, batch_size, sequence_length, iterations, expected_compile_time, expected_inference_time",
    (
        ("state-spaces/mamba-2.8b", ModelMode.DECODE, 32, 1, 8, 18.0, 0.110),
        ("state-spaces/mamba-2.8b", ModelMode.PREFILL, 1, 128, 8, 30.0, 0.520),
    ),
)
@pytest.mark.parametrize("device_params", [{"l1_small_size": 16384}], indirect=True)
def test_mamba_perf_e2e(
    device,
    model_version,
    mode,
    batch_size,
    sequence_length,
    iterations,
    expected_compile_time,
    expected_inference_time,
    use_program_cache,
    reset_seeds,
    get_tt_cache_path,
    is_ci_env,
):
    profiler.clear()

    logger.info(
        f"Testing end-to-end performance in {'PREFILL' if mode == ModelMode.PREFILL else 'DECODE'} mode with sequence length {sequence_length}"
    )

    logger.warning(f"Disabling persistent kernel cache due to hang on CI (#8606)")
    disable_persistent_kernel_cache()

    profiler.start(f"initialize_ref_model")
    reference_model = Mamba.from_pretrained(model_version, batch_size=batch_size)
    reference_model.args.mode = mode
    reference_model.eval()
    profiler.end(f"initialize_ref_model")

    if mode == ModelMode.DECODE:
        assert sequence_length == 1, "Sequence length must be 1 in decode mode"
        assert batch_size == 32, "Batch size must be 1 in decode mode"
    else:
        assert batch_size == 1, "Batch size must be 1 in prefill mode"

    profiler.start(f"initialize_model")
    config = model_config.create_model_config(
        batch_size, reference_model.args.d_model, mode=mode, seq_len=sequence_length
    )
    model = MambaTT(
        reference_model, device, config, tt_cache_path=get_tt_cache_path(model_version), num_layers=NUM_LAYERS
    )
    profiler.end(f"initialize_model")
    logger.info(f"Done initializing model in {profiler.get('initialize_model'):.2f} s")

    input = torch.randn((batch_size, sequence_length))
    logger.info(f"Measuring performance with input of shape {list(input.shape)}")

    logger.info(f"Compiling model with warmup run")
    profiler.start(f"inference_and_compile_time")
    model(input)
    profiler.end(f"inference_and_compile_time")

    inference_and_compile_time = profiler.get("inference_and_compile_time")
    logger.info(f"Model compiled with warmup run in {(inference_and_compile_time):.2f} s")

    logger.info(f"Running inference for {iterations} iterations")
    for idx in range(iterations):
        profiler.start("inference_time")
        profiler.start(f"inference_time_{idx}")
        model(input)
        profiler.end(f"inference_time_{idx}")
        profiler.end("inference_time")

    mean_inference_time = profiler.get("inference_time")
    inference_time = profiler.get(f"inference_time_{iterations - 1}")
    compile_time = inference_and_compile_time - inference_time
    logger.info(f"Inference time on last iterations was completed in {(inference_time * 1000.0):.2f} ms")
    logger.info(f"Mean inference time was {(mean_inference_time * 1000.0):.2f} ms")
    logger.info(f"Model compilation took {compile_time:.2f} s")

    comment = (
        f"mode-{'prefill' if mode == ModelMode.PREFILL else 'decode'}_layers-{NUM_LAYERS}_seqlen-{sequence_length}"
    )
    prep_perf_report(
        model_name=f"{model_version}",
        batch_size=batch_size,
        inference_and_compile_time=inference_and_compile_time,
        inference_time=inference_time,
        expected_compile_time=expected_compile_time,
        expected_inference_time=expected_inference_time,
        comments=comment,
    )

    lower_margin = MARGIN if is_ci_env else 1.0  # CI machines are generally slower
    upper_margin = MARGIN
    if not is_nearby(inference_time, expected_inference_time, lower_margin=lower_margin, upper_margin=upper_margin):
        logger.warning(
            f"Inference time does not match (within some margin) the expected value (was {inference_time:2f} but expected {expected_inference_time:2f})"
        )

    if not is_nearby(compile_time, expected_compile_time, lower_margin=lower_margin, upper_margin=upper_margin):
        logger.warning(
            f"Compile time does not match (within some margin) the expected value (was {compile_time:2f} but expected {expected_compile_time:2f})"
        )


@skip_for_grayskull("Requires eth connected devices to run")
@pytest.mark.timeout(600)
@pytest.mark.models_device_performance_bare_metal
@pytest.mark.parametrize(
    "batch, expected_layer_duration_ms",
    ((32, 1.619),),
)
def test_mamba_perf_device(batch, expected_layer_duration_ms):
    subdir = "ttnn_mamba"
    margin = 0.015
    command = f"pytest models/demos/wormhole/mamba/tests/test_mamba_model.py::test_device_perf[1]"
    cols = ["DEVICE FW", "DEVICE KERNEL", "DEVICE BRISC KERNEL"]

    # Convert expected perf (ms) to samples/s
    expected_layer_duration_ns = expected_layer_duration_ms * 1e6  # ms to ns
    expected_total_layer_samples_per_s = get_samples_per_s(expected_layer_duration_ns, batch)
    inference_time_key = "AVG DEVICE KERNEL SAMPLES/S"
    expected_perf_cols = {inference_time_key: expected_total_layer_samples_per_s}

    post_processed_results = run_device_perf(command, subdir, 1, cols, batch)
    logger.info(
        f"Checking device performance... Expecting {expected_total_layer_samples_per_s} samples/sec (equivalent to {expected_layer_duration_ms} ms per layer)"
    )
    expected_results = check_device_perf(post_processed_results, margin, expected_perf_cols, assert_on_fail=True)
    prep_device_perf_report(
        model_name=f"mamba-2.8b_batch_{batch}",
        batch_size=batch,
        post_processed_results=post_processed_results,
        expected_results=expected_results,
        comments="",
    )
