# SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC

# SPDX-License-Identifier: Apache-2.0

"""
TTNN DRAM Streaming Matmul Micro Op Test

Tests the simplified DRAM streaming matmul operation where:
- Input A is REPLICATED on compute cores (each core has full [M, K])
- Input B (weights) is WIDTH_SHARDED in DRAM across DRAM banks
- Output is WIDTH_SHARDED in L1 on compute cores
"""

import pytest
import torch
from loguru import logger

import ttnn
from models.demos.deepseek_v3_b1.micro_ops.dram_streaming_matmul.op import DRAMStreamingMatmul
from tests.tt_eager.python_api_testing.sweep_tests.comparison_funcs import comp_pcc


def pad_to_dram_banks(num, tile_w, lcm):
    """Pad number to be aligned with DRAM banks."""
    remainder = num % lcm
    if remainder == 0:
        return num
    padding_needed = lcm - remainder
    return num + padding_needed


@pytest.mark.parametrize("k, n", [(7168, 2048), (2048, 7168)])
@pytest.mark.parametrize("has_bias", [False, True])
@pytest.mark.parametrize("m", [32])
def test_dram_streaming_matmul(device, k, n, has_bias, m):
    """Test simplified DRAM streaming matmul.

    In the simplified version:
    - Input A is REPLICATED on compute cores (each core has full [M, K])
    - No multicast needed - each core has its own copy
    - Output is WIDTH_SHARDED across N dimension

    Supports both standard tiles (32x32) and tiny tiles (1x32) for m=1.
    """

    tile_h = m  # Tile height matches m (1 for tiny tiles, 32 for standard)
    tile_w = 32

    # Create tile object for tiny tiles when m=1
    in0_tile = ttnn.Tile([tile_h, tile_w])
    out_tile = ttnn.Tile([tile_h, tile_w])

    # Get compute cores from optimal DRAM bank assignment
    # These are the cores that will do the compute (8 on BH, 12 on WH)
    compute_cores = device.get_optimal_dram_bank_to_logical_worker_assignment(ttnn.NOC.NOC_0)
    num_cores = len(compute_cores)

    # Get number of DRAM banks (should match num_cores)
    num_banks = device.dram_grid_size().x
    assert num_cores == num_banks, f"num_cores ({num_cores}) must equal num_banks ({num_banks})"

    logger.info(f"num_compute_cores={num_cores}, num_dram_banks={num_banks}")

    n_padded = pad_to_dram_banks(n, tile_w, tile_w * num_banks)

    # Define shapes
    in0_shape = [1, 1, m, k]
    in1_shape = [1, 1, k, n]
    in1_shard_shape = [k, n_padded // num_banks]
    bias_shape = [1, 1, n]
    bias_shard_shape = [tile_h, n_padded // num_banks]

    # Calculate block dimensions
    # in0_block_w: how many K tiles to process per block
    in0_block_w = k // tile_w // num_cores

    logger.debug(f"n_padded={n_padded}, in0_block_w={in0_block_w}")

    # Create PyTorch tensors
    torch.manual_seed(42)
    in0 = torch.randn(in0_shape).bfloat16().float()
    in1 = torch.randn(in1_shape).bfloat16().float()

    # Build CoreRangeSet for specific compute cores (not bounding box)
    compute_core_grid = ttnn.CoreRangeSet(
        [ttnn.CoreRange(ttnn.CoreCoord(c.x, c.y), ttnn.CoreCoord(c.x, c.y)) for c in compute_cores]
    )

    # Input A - REPLICATED on compute cores (each core has full [M, K])
    # First replicate the tensor num_cores times along height, then HEIGHT_SHARD
    # so each core gets [M, K]
    in0_replicated = in0.repeat(1, 1, num_cores, 1)  # Shape: [1, 1, M * num_cores, K]
    in0_shard_shape_full = [m, k]  # Each core gets [M, K]
    in0_shard_spec = ttnn.ShardSpec(compute_core_grid, in0_shard_shape_full, ttnn.ShardOrientation.ROW_MAJOR)
    in0_memory_config = ttnn.MemoryConfig(ttnn.TensorMemoryLayout.HEIGHT_SHARDED, ttnn.BufferType.L1, in0_shard_spec)
    in0_t = ttnn.from_torch(
        in0_replicated,
        dtype=ttnn.bfloat16,
        layout=ttnn.TILE_LAYOUT,
        device=device,
        memory_config=in0_memory_config,
        tile=in0_tile,
    )

    # Input B (weights) - WIDTH_SHARDED in DRAM
    in1_shard_grid = ttnn.CoreCoord(device.dram_grid_size().x - 1, device.dram_grid_size().y - 1)
    in1_shard_grid = ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), in1_shard_grid)})
    in1_shard_spec = ttnn.ShardSpec(in1_shard_grid, in1_shard_shape, ttnn.ShardOrientation.ROW_MAJOR)
    in1_memory_config = ttnn.MemoryConfig(ttnn.TensorMemoryLayout.WIDTH_SHARDED, ttnn.BufferType.DRAM, in1_shard_spec)
    in1_t = ttnn.from_torch(
        in1,
        dtype=ttnn.bfloat8_b,
        layout=ttnn.TILE_LAYOUT,
        device=device,
        memory_config=in1_memory_config,
    )

    # Bias - WIDTH_SHARDED in DRAM (if needed)
    bias_t = None
    bias = None
    if has_bias:
        bias = torch.randn(bias_shape).bfloat16().float()
        bias_padded = bias.unsqueeze(2)
        bias_padded = torch.nn.functional.pad(bias_padded, (0, 0, 0, tile_h - bias_padded.size(2)), "constant", 0)
        bias_shard_grid = ttnn.CoreCoord(device.dram_grid_size().x - 1, device.dram_grid_size().y - 1)
        bias_shard_grid = ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), bias_shard_grid)})
        bias_shard_spec = ttnn.ShardSpec(bias_shard_grid, bias_shard_shape, ttnn.ShardOrientation.ROW_MAJOR)
        bias_mem_config = ttnn.MemoryConfig(
            ttnn.TensorMemoryLayout.WIDTH_SHARDED, ttnn.BufferType.DRAM, bias_shard_spec
        )
        bias_tile = ttnn.Tile([tile_h, tile_w])
        bias_t = ttnn.from_torch(
            bias_padded,
            dtype=ttnn.bfloat16,
            layout=ttnn.TILE_LAYOUT,
            device=device,
            memory_config=bias_mem_config,
            tile=bias_tile,
        )

    # Output tensor - WIDTH_SHARDED in L1 on same compute cores
    # Shard width must match in1 shard width (padded)
    output_shard_width = n_padded // num_banks
    output_shard_spec = ttnn.ShardSpec(compute_core_grid, (m, output_shard_width), ttnn.ShardOrientation.ROW_MAJOR)
    output_mem_config = ttnn.MemoryConfig(ttnn.TensorMemoryLayout.WIDTH_SHARDED, ttnn.BufferType.L1, output_shard_spec)

    torch_output_zeros = torch.zeros([1, 1, m, n]).bfloat16().float()
    ttnn_output = ttnn.from_torch(
        torch_output_zeros,
        dtype=ttnn.bfloat16,
        layout=ttnn.TILE_LAYOUT,
        device=device,
        memory_config=output_mem_config,
        tile=out_tile,
    )

    # Run DRAM streaming matmul
    logger.info(f"Running DRAM streaming matmul: m={m}, k={k}, n={n}, num_cores={num_cores}, bias={has_bias}")
    try:
        ttnn_result = DRAMStreamingMatmul.op(
            in0_t,
            in1_t,
            ttnn_output,
            bias=bias_t,
            in0_block_w=in0_block_w,
            fp32_dest_acc_en=True,
            packer_l1_acc=True,
            math_fidelity=ttnn.MathFidelity.LoFi,
            math_approx_mode=True,
        )
    except Exception as e:
        logger.error(f"DRAM streaming matmul failed: {e}")
        pytest.skip(f"Operation failed (may need API adjustments): {e}")

    # Compute PyTorch reference
    pt_out = in0 @ in1
    if has_bias:
        pt_out = pt_out + bias

    # Convert to torch for comparison
    tt_out = ttnn.to_torch(ttnn_result)

    # Verify results
    expected_pcc = 0.999
    passing, output = comp_pcc(pt_out, tt_out, expected_pcc)
    logger.info(output)
    assert passing, f"PCC check failed: {output}"
    logger.info("DRAM streaming matmul test passed!")
