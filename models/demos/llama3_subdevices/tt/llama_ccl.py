# SPDX-FileCopyrightText: Â© 2023 Tenstorrent Inc.

# SPDX-License-Identifier: Apache-2.0

import ttnn
import torch

from tests.ttnn.unit_tests.operations.ccl.test_ccl_common import (
    create_global_semaphore_with_same_address,
)
from tests.ttnn.unit_tests.operations.ccl.test_new_all_reduce import check_mesh_tensor_alloc


class TT_CCL:
    def __init__(
        self,
        mesh_device,
        model_args,
        worker_sub_device_id,
        mode="decode",
    ):
        self.mode = mode
        all_crs = ttnn.CoreRangeSet([ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(6, 9))])

        self.mesh_device = mesh_device
        self.sub_device_crs = all_crs if mode == "prefill" else model_args.sub_core_grids
        self.worker_sub_device_id = worker_sub_device_id
        self.model_config = model_args.model_config
        self.num_cbs = 2
        self.from_remote_semaphore_handles = []
        self.to_remote_semaphore_handles = []

        # Double buffered on each axis
        self.gather_semaphore_handles = [[], []]
        if mode == "prefill":
            self.from_semaphore_handles = [[], []]
            self.to_semaphore_handles = [[], []]
        for i in range(2):
            for _ in range(self.num_cbs):
                self.gather_semaphore_handles[i].append(
                    create_global_semaphore_with_same_address(self.mesh_device, self.sub_device_crs, 0)
                )
                if mode == "prefill":
                    self.from_semaphore_handles[i].append(
                        create_global_semaphore_with_same_address(self.mesh_device, self.sub_device_crs, 0)
                    )
                    self.to_semaphore_handles[i].append(
                        create_global_semaphore_with_same_address(self.mesh_device, self.sub_device_crs, 0)
                    )

        self.gather_idx = [0, 0]
        self.buffer_idx = [0, 0]
        self.reduce_scatter_buffer_idx = [0, 0]

        if mode == "decode":
            self.persistent_buffers = self.get_persistent_buffers()
            self.all_gather_buffers = self.get_all_gather_buffers()
            self.reduce_scatter_buffers = self.get_decode_reduce_scatter_buffers()
        if mode == "prefill":
            self.persistent_buffers = self.get_reduce_scatter_buffers()
            self.all_gather_buffers = self.get_prefill_all_gather_buffers()

    def reset_gather_and_buffer_idx(self):
        self.gather_idx = [0, 0]
        self.buffer_idx = [0, 0]
        self.reduce_scatter_buffer_idx = [0, 0]

    def get_all_gather_buffers(self):
        """
        Currently, this is hardcoded with llama specific shapes.

        Here are the current persistent buffers generated by this fuction:
        - SDPA: (1, 32, 32, 128)
        - LAYERNORM: (1, 1, 32, 128)
        - SAMPLING: (1, 1, 32, 128 * 1024)
        - BINARY_MUL: (1, 1, 32, 3840)

        """

        persistent_buffers = {}

        if self.model_config is None:
            return persistent_buffers

        M = 32

        # SDPA
        tt_buffer = ttnn.from_torch(
            torch.zeros((1, 32, M, 128)),
            device=self.mesh_device,
            layout=ttnn.TILE_LAYOUT,
            dtype=ttnn.bfloat16,
            memory_config=self.model_config["GATHER_USERS_MEMCFG"](list(self.mesh_device.shape)[1]),
            mesh_mapper=ttnn.ReplicateTensorToMesh(self.mesh_device),
        )
        check_mesh_tensor_alloc(tt_buffer)
        persistent_buffers["SDPA"] = tt_buffer

        # Layernorm
        grid_offset = ttnn.CoreCoord(1, 0)
        tt_stats_sharded_config = ttnn.create_sharded_memory_config(
            shape=(32, 128),
            core_grid=ttnn.CoreRangeSet([ttnn.CoreRange(grid_offset, grid_offset)]),
            strategy=ttnn.ShardStrategy.WIDTH,
            use_height_and_width_as_shard_shape=True,
        )

        tt_buffer = ttnn.from_torch(
            torch.zeros((1, 1, M, 128)),
            device=self.mesh_device,
            layout=ttnn.TILE_LAYOUT,
            dtype=ttnn.bfloat16,
            memory_config=tt_stats_sharded_config,
            mesh_mapper=ttnn.ReplicateTensorToMesh(self.mesh_device),
        )
        check_mesh_tensor_alloc(tt_buffer)
        persistent_buffers["LAYERNORM"] = tt_buffer

        tt_buffer = ttnn.from_torch(
            torch.zeros((1, 1, 32, 128 * 1024)),
            device=self.mesh_device,
            layout=ttnn.TILE_LAYOUT,
            dtype=ttnn.bfloat8_b,
            memory_config=ttnn.DRAM_MEMORY_CONFIG,
            mesh_mapper=ttnn.ReplicateTensorToMesh(self.mesh_device),
        )
        check_mesh_tensor_alloc(tt_buffer)
        persistent_buffers["SAMPLING"] = tt_buffer

        # Binary Mult + Silu
        tt_buffer = ttnn.from_torch(
            torch.zeros((1, 1, 32, 3584)),
            device=self.mesh_device,
            layout=ttnn.TILE_LAYOUT,
            dtype=ttnn.bfloat8_b,
            memory_config=self.model_config["FF2_IN_RING_MEMCFG"],
            mesh_mapper=ttnn.ReplicateTensorToMesh(self.mesh_device),
        )
        check_mesh_tensor_alloc(tt_buffer)
        persistent_buffers["BINARY_MUL"] = tt_buffer

        return persistent_buffers

    def get_persistent_buffers(self):
        """
        Currently, this is hardcoded with llama specific shapes.

        Creates double buffered persistent CCL buffers for each cluster axis.

        """

        persistent_buffers = [[], []]

        cluster_shape = (8, 4)
        M = 32
        num_cores = self.sub_device_crs.num_cores()

        # Create persistent buffers for cluster axis 0
        cluster_axis = 0
        N_per_shard = 2048 // 16 * cluster_shape[cluster_axis]  # FF2/DO
        buffer_mem_cfg = ttnn.MemoryConfig(
            ttnn.TensorMemoryLayout.WIDTH_SHARDED,
            ttnn.BufferType.L1,
            ttnn.ShardSpec(
                self.sub_device_crs,
                [M, N_per_shard],
                ttnn.ShardOrientation.ROW_MAJOR,
            ),
        )
        for _ in range(self.num_cbs):
            tt_buffer = ttnn.from_torch(
                torch.zeros((*cluster_shape, M, N_per_shard * num_cores)),
                device=self.mesh_device,
                layout=ttnn.TILE_LAYOUT,
                dtype=ttnn.bfloat16,
                memory_config=buffer_mem_cfg,
                mesh_mapper=ttnn.ShardTensor2dMesh(self.mesh_device, dims=(0, 1), mesh_shape=cluster_shape),
            )
            persistent_buffers[cluster_axis].append(tt_buffer)

        # Create persistent buffers for cluster axis 1
        cluster_axis = 1
        N_per_shard = 3840 // 24 * cluster_shape[cluster_axis]  # FF1/FF3/QKV
        buffer_mem_cfg = ttnn.MemoryConfig(
            ttnn.TensorMemoryLayout.WIDTH_SHARDED,
            ttnn.BufferType.L1,
            ttnn.ShardSpec(
                self.sub_device_crs,
                [M, N_per_shard],
                ttnn.ShardOrientation.ROW_MAJOR,
            ),
        )
        for _ in range(self.num_cbs):
            tt_buffer = ttnn.from_torch(
                torch.zeros((*cluster_shape, M, N_per_shard * num_cores)),
                device=self.mesh_device,
                layout=ttnn.TILE_LAYOUT,
                dtype=ttnn.bfloat16,
                memory_config=buffer_mem_cfg,
                mesh_mapper=ttnn.ShardTensor2dMesh(self.mesh_device, dims=(0, 1), mesh_shape=cluster_shape),
            )
            persistent_buffers[cluster_axis].append(tt_buffer)

        # Create persistent buffer for lm_head
        num_cores_after_lm_head = 32  # Use 32 cores instead of 16 to reduce L1 memory usage per core
        N_per_shard = (16 * 1024) // num_cores_after_lm_head * cluster_shape[cluster_axis]  # LM Head
        self.lm_head_buffer_mem_cfg = ttnn.MemoryConfig(
            ttnn.TensorMemoryLayout.WIDTH_SHARDED,
            ttnn.BufferType.L1,
            ttnn.ShardSpec(
                self.sub_device_crs,
                [M, N_per_shard],
                ttnn.ShardOrientation.ROW_MAJOR,
            ),
        )

        self.tt_lm_head_buffer = ttnn.from_torch(
            torch.zeros((*cluster_shape, M, N_per_shard * num_cores)),
            device=self.mesh_device,
            layout=ttnn.TILE_LAYOUT,
            dtype=ttnn.bfloat8_b,
            memory_config=ttnn.DRAM_MEMORY_CONFIG,
            mesh_mapper=ttnn.ShardTensor2dMesh(self.mesh_device, dims=(0, 1), mesh_shape=cluster_shape),
        )

        return persistent_buffers

    def get_decode_reduce_scatter_buffers(self):
        """
        Currently, this is hardcoded with llama specific shapes.

        Creates double buffered persistent CCL buffers for each cluster axis.

        """

        persistent_buffers = [[], []]

        cluster_shape = (8, 4)

        # Create persistent buffers for cluster axis 1
        cluster_axis = 1
        buffer_mem_cfg = self.model_config["REDUCE_SCATTER_INTERIM_MEMCFG"]
        for _ in range(self.num_cbs):
            tt_buffer = ttnn.from_torch(
                torch.zeros((*cluster_shape, 32, 512 * buffer_mem_cfg.shard_spec.num_cores())),
                device=self.mesh_device,
                layout=ttnn.TILE_LAYOUT,
                dtype=ttnn.bfloat8_b,
                memory_config=buffer_mem_cfg,
                mesh_mapper=ttnn.ShardTensor2dMesh(self.mesh_device, dims=(0, 1), mesh_shape=cluster_shape),
            )
            persistent_buffers[cluster_axis].append(tt_buffer)

        return persistent_buffers

    def get_prefill_reduce_scatter_buffers(self):
        """
        Currently, this is hardcoded with llama specific shapes.

        Here are the current persistent buffers generated by this fuction:
        - QKV: (1, 1, 128, 1280)
        - FF1/FF3: (1, 1, 128, 3584)
        - FF2/WO: (1, 1, 128, 2048)

        """

        persistent_buffers = {}

        if self.model_config is None:
            return persistent_buffers

        M = 128 if self.mode == "prefill" else 32
        buffers_dict = {
            "QKV": [(1, 1, 128, 1280), (1, 1, 128, 1280 // 4)],
            "WO": [(1, 1, 128, 2048), (1, 1, 128, 2048 // 8)],
            "FF1": [(1, 1, 128, 3584), (1, 1, 128, 3584 // 4)],
            "FF3": [(1, 1, 128, 3584), (1, 1, 128, 3584 // 4)],
            "FF2": [(1, 1, 128, 2048), (1, 1, 128, 2048 // 8)],
        }
        for key, shape in buffers_dict.items():
            tt_buffers = []
            for i in range(1):
                tt_buffer = ttnn.from_torch(
                    torch.zeros(shape[1]),
                    device=self.mesh_device,
                    layout=ttnn.TILE_LAYOUT,
                    dtype=ttnn.bfloat8_b,
                    memory_config=ttnn.DRAM_MEMORY_CONFIG,
                    mesh_mapper=ttnn.ReplicateTensorToMesh(self.mesh_device),
                )
                check_mesh_tensor_alloc(tt_buffer)
                tt_buffers.append(tt_buffer)
            for i in range(2):
                tt_buffer = ttnn.from_torch(
                    torch.zeros(shape[0]),
                    device=self.mesh_device,
                    layout=ttnn.TILE_LAYOUT,
                    dtype=ttnn.bfloat8_b,
                    memory_config=ttnn.DRAM_MEMORY_CONFIG,
                    mesh_mapper=ttnn.ReplicateTensorToMesh(self.mesh_device),
                )
                check_mesh_tensor_alloc(tt_buffer)
                tt_buffers.append(tt_buffer)
            for i in range(2):
                tt_buffer = ttnn.from_torch(
                    torch.zeros(shape[1]),
                    device=self.mesh_device,
                    layout=ttnn.TILE_LAYOUT,
                    dtype=ttnn.bfloat8_b,
                    memory_config=ttnn.DRAM_MEMORY_CONFIG,
                    mesh_mapper=ttnn.ReplicateTensorToMesh(self.mesh_device),
                )
                check_mesh_tensor_alloc(tt_buffer)
                tt_buffers.append(tt_buffer)
            persistent_buffers[key] = tt_buffers

        return persistent_buffers

    def get_prefill_all_gather_buffers(self):
        """
        Currently, this is hardcoded with llama specific shapes.

        Creates double buffered persistent CCL buffers for each cluster axis.

        """

        ag_persistent_buffers = {}

        M = 128 if self.mode == "prefill" else 32
        buffers_dict = {
            "QKV": [(1, 1, 128, 1280)],
            "WO": [(1, 1, 128, 2048)],
            "FF1": [(1, 1, 128, 3584)],
            "FF3": [(1, 1, 128, 3584)],
            "FF2": [(1, 1, 128, 2048)],
            "LAYERNORM": [(1, 1, 128, 128)],
            # "SAMPLING": [(1, 1, 32, 128 * 1024)]
        }
        for key, shape in buffers_dict.items():
            tt_buffer = ttnn.from_torch(
                torch.zeros(shape[0]),
                device=self.mesh_device,
                layout=ttnn.TILE_LAYOUT,
                dtype=ttnn.bfloat16 if key == "LAYERNORM" else ttnn.bfloat8_b,
                memory_config=ttnn.DRAM_MEMORY_CONFIG,
                mesh_mapper=ttnn.ReplicateTensorToMesh(self.mesh_device),
            )
            check_mesh_tensor_alloc(tt_buffer)
            ag_persistent_buffers[key] = tt_buffer

        return ag_persistent_buffers

    def line_all_reduce(
        self, input_tensor_mesh, cluster_axis, num_links, memory_config, dtype=None, lm_head=False, buffer_key=None
    ):
        if self.mode == "decode":
            if lm_head:
                persistent_buffer = self.tt_lm_head_buffer_l1
            else:
                persistent_buffer = self.persistent_buffers[cluster_axis][self.buffer_idx[cluster_axis]]

            output_tensor_mesh = ttnn.experimental.all_reduce_async(
                input_tensor_mesh,
                persistent_buffer,
                cluster_axis=cluster_axis,
                mesh_device=self.mesh_device,
                multi_device_global_semaphore=self.gather_semaphore_handles[cluster_axis][
                    self.gather_idx[cluster_axis]
                ],
                num_links=num_links,
                memory_config=memory_config,
                dtype=dtype,
                topology=ttnn.Topology.Linear,
                subdevice_id=self.worker_sub_device_id,
            )

            if lm_head:
                persistent_buffer.deallocate(True)
        else:
            if lm_head:
                ttnn_tensor_gathered = self.line_all_gather(
                    input_tensor_mesh,
                    dim=0,
                    num_links=num_links,
                    cluster_axis=cluster_axis,
                    memory_config=memory_config,
                    buffer_key=buffer_key,
                )
                ttnn_tensor_out = ttnn.experimental.fast_reduce_nc(
                    ttnn_tensor_gathered,
                    dims=[0],
                    output=None,
                    compute_kernel_config=None,
                    memory_config=memory_config,
                )
                return ttnn_tensor_out
            # ttnn.synchronize_device(self.mesh_device)
            output_tensor_scattered = self.line_reduce_scatter(
                input_tensor_mesh,
                memory_config,
                dim=3,
                cluster_axis=cluster_axis,
                num_links=num_links,
                math_op=ttnn.ReduceType.Sum,
                buffer_key=buffer_key,
            )
            # ttnn.synchronize_device(self.mesh_device)
            # Gather the scattered tensor
            output_tensor_mesh = self.line_all_gather(
                output_tensor_scattered,
                dim=3,
                cluster_axis=cluster_axis,
                memory_config=memory_config,
                num_links=num_links,
                buffer_key=buffer_key,
            )
            # ttnn.synchronize_device(self.mesh_device)

        self.gather_idx[cluster_axis] = (self.gather_idx[cluster_axis] + 1) % self.num_cbs
        self.buffer_idx[cluster_axis] = (self.buffer_idx[cluster_axis] + 1) % self.num_cbs
        return output_tensor_mesh

    def line_reduce_scatter(
        self,
        input_tensor_mesh,
        memory_config,
        cluster_axis,
        dim=3,
        num_links=1,
        math_op=ttnn.ReduceType.Sum,
        buffer_key=None,
    ):
        if self.mode == "prefill":
            persistent_buffers = self.persistent_buffers.get(buffer_key, None)

            ttnn_tensor_out = ttnn.experimental.reduce_scatter_async(
                input_tensor_mesh,
                dim,
                cluster_axis=cluster_axis,
                mesh_device=self.mesh_device,
                from_remote_multi_device_global_semaphore=self.from_semaphore_handles[cluster_axis][
                    self.gather_idx[cluster_axis]
                ],
                to_remote_multi_device_global_semaphore=self.to_semaphore_handles[cluster_axis][
                    self.gather_idx[cluster_axis]
                ],
                math_op=math_op,
                memory_config=memory_config,
                topology=ttnn.Topology.Linear,
                num_links=num_links,
                subdevice_id=self.worker_sub_device_id,
                persistent_output_tensors=persistent_buffers,
            )
            self.gather_idx[cluster_axis] = (self.gather_idx[cluster_axis] + 1) % self.num_cbs
        else:
            persistent_interim_buffer = self.reduce_scatter_buffers[cluster_axis][
                self.reduce_scatter_buffer_idx[cluster_axis]
            ]
            ttnn_tensor_out = ttnn.experimental.llama_reduce_scatter(
                input_tensor_mesh,
                persistent_interim_buffer,
                dim,
                self.gather_semaphore_handles[cluster_axis][self.gather_idx[cluster_axis]],
                self.worker_sub_device_id,
                cluster_axis=1,
                mesh_device=self.mesh_device,
                num_links=num_links,
                memory_config=memory_config,
            )
            self.gather_idx[cluster_axis] = (self.gather_idx[cluster_axis] + 1) % self.num_cbs
            self.reduce_scatter_buffer_idx[cluster_axis] = (
                self.reduce_scatter_buffer_idx[cluster_axis] + 1
            ) % self.num_cbs
        # ttnn.synchronize_device(self.mesh_device, sub_device_ids=[self.worker_sub_device_id])
        return ttnn_tensor_out

    def line_all_gather(self, input_tensor_mesh, dim, cluster_axis, memory_config, num_links=1, buffer_key=None):
        persistent_buffer = self.all_gather_buffers.get(buffer_key, None)
        # ttnn.synchronize_device(self.mesh_device, sub_device_ids=[self.worker_sub_device_id])
        ttnn_tensor_out = ttnn.experimental.all_gather_async(
            input_tensor_mesh,
            dim,
            cluster_axis=cluster_axis,
            mesh_device=self.mesh_device,
            topology=ttnn.Topology.Linear,
            multi_device_global_semaphore=self.gather_semaphore_handles[cluster_axis][self.gather_idx[cluster_axis]],
            persistent_output_tensor=persistent_buffer,
            num_links=num_links,
            memory_config=memory_config,
            subdevice_id=self.worker_sub_device_id,
        )
        self.gather_idx[cluster_axis] = (self.gather_idx[cluster_axis] + 1) % self.num_cbs
        # ttnn.synchronize_device(self.mesh_device, sub_device_ids=[self.worker_sub_device_id])
        return ttnn_tensor_out

    def line_all_reduce_host(self, input_tensor_mesh, cluster_axis, num_links, memory_config):
        dim = 3

        ##### Host side implementation #####
        rs_output_tensor_mesh = self.line_reduce_scatter_host(
            input_tensor_mesh,
            memory_config,
            dim,
            cluster_axis,
            num_links=num_links,
            math_op=ttnn.ReduceType.Sum,
        )

        output_tensor_mesh = self.line_all_gather_host(
            rs_output_tensor_mesh,
            dim,
            cluster_axis,
            memory_config,
            num_links=num_links,
        )

        return output_tensor_mesh

    def line_reduce_scatter_host(
        self, input_tensor_mesh, memory_config, dim, cluster_axis, num_links=1, math_op=ttnn.ReduceType.Sum
    ):
        ##### Host side implementation #####
        dims = [0, 1]
        dtype = input_tensor_mesh.get_dtype()
        torch_tensor_mesh = ttnn.to_torch(
            input_tensor_mesh, mesh_composer=ttnn.ConcatMesh2dToTensor(self.mesh_device, dims=dims, mesh_shape=(8, 4))
        )

        torch_tensor_mesh = torch.sum(torch_tensor_mesh, dim=cluster_axis, keepdim=True)

        dims[cluster_axis] = dim
        ttnn_tensor_out = ttnn.from_torch(
            torch_tensor_mesh,
            device=self.mesh_device,
            mesh_mapper=ttnn.ShardTensor2dMesh(self.mesh_device, dims=dims, mesh_shape=(8, 4)),
            dtype=dtype,
            memory_config=memory_config,
            layout=ttnn.TILE_LAYOUT,
        )

        return ttnn_tensor_out

    def line_all_gather_host(self, input_tensor_mesh, dim, cluster_axis, memory_config, num_links=1):
        ##### Host side implementation #####
        dims = [0, 0] if dim != 0 else [1, 1]
        dims[cluster_axis] = dim
        dtype = input_tensor_mesh.get_dtype()
        torch_tensor_mesh = ttnn.to_torch(
            input_tensor_mesh, mesh_composer=ttnn.ConcatMesh2dToTensor(self.mesh_device, dims=dims, mesh_shape=(8, 4))
        )

        dims[cluster_axis] = None
        ttnn_tensor_out = ttnn.from_torch(
            torch_tensor_mesh,
            device=self.mesh_device,
            mesh_mapper=ttnn.ShardTensor2dMesh(self.mesh_device, dims=dims, mesh_shape=(8, 4)),
            dtype=dtype,
            memory_config=memory_config,
            layout=ttnn.TILE_LAYOUT,
        )

        return ttnn_tensor_out

    def close(self):
        self.mesh_device.reset_sub_device_stall_group()


def tt_distributed_rmsnorm(
    inp,
    epsilon,
    gamma,
    mesh_device,
    compute_kernel_config,
    tt_ccl=None,
):
    # Run distributed rmsnorm part 1
    tt_stats = ttnn.rms_norm_pre_all_gather(inp, compute_kernel_config=compute_kernel_config, dtype=ttnn.bfloat16)
    padded_shape = (1, 1, inp.shape[-2], 32)

    tt_stats_gathered = tt_ccl.line_all_gather(
        tt_stats, dim=3, cluster_axis=1, num_links=1, memory_config=ttnn.DRAM_MEMORY_CONFIG, buffer_key="LAYERNORM"
    )

    # tt_stats.deallocate(True)

    # Run distributed rmsnorm part 2
    tt_out = ttnn.rms_norm_post_all_gather(
        inp, tt_stats_gathered, epsilon=epsilon, weight=gamma, compute_kernel_config=compute_kernel_config
    )

    # tt_stats_gathered.deallocate(True)
    # inp.deallocate(True)

    return tt_out, None


def tt_sharded_distributed_rmsnorm(
    inp,
    res,
    epsilon,
    gamma,
    mesh_device,
    ln_sharded_input_memcfg,
    ln_sharded_progcfg,
    ln_sharded_stats_memcfg,
    tt_ccl=None,
):
    # inp = ttnn.to_memory_config(inp, memory_config=ln_sharded_input_memcfg)

    # Run distributed rmsnorm part 1
    cluster_axis = 1
    semaphore = tt_ccl.gather_semaphore_handles[cluster_axis][tt_ccl.gather_idx[cluster_axis]]
    grid_offset = ttnn.CoreCoord(1, 0)
    persistent_buffer = tt_ccl.all_gather_buffers.get("LAYERNORM", None)
    tt_stats_sharded_config = ttnn.create_sharded_memory_config(
        shape=(32, 128),
        core_grid=ttnn.CoreRangeSet([ttnn.CoreRange(grid_offset, grid_offset)]),
        strategy=ttnn.ShardStrategy.WIDTH,
        use_height_and_width_as_shard_shape=True,
    )
    tt_stats = ttnn.fused_rms_1_1_32_8192(
        inp,
        ln_sharded_progcfg,
        cluster_axis,
        tt_ccl.mesh_device,
        semaphore,
        residual_input_tensor=res,
        num_links=1,
        memory_config=tt_stats_sharded_config,
        persistent_output_tensor=persistent_buffer,
        is_pre=True,
    )
    tt_out = ttnn.fused_rms_1_1_32_8192(
        inp,
        ln_sharded_progcfg,
        cluster_axis,
        tt_ccl.mesh_device,
        semaphore,
        epsilon=epsilon,
        weight=gamma,
        stats=tt_stats,
        is_pre=False,
    )
    ttnn.deallocate(tt_stats)
    tt_ccl.gather_idx[cluster_axis] = (tt_ccl.gather_idx[cluster_axis] + 1) % tt_ccl.num_cbs
    return tt_out, inp
