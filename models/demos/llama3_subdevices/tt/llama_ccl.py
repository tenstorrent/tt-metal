# SPDX-FileCopyrightText: Â© 2023 Tenstorrent Inc.

# SPDX-License-Identifier: Apache-2.0

import ttnn
from loguru import logger
import torch

from tests.ttnn.unit_tests.operations.ccl.test_ccl_common import (
    teardown_fabric_interface,
    create_global_semaphore_with_same_address,
)
from tests.ttnn.unit_tests.operations.ccl.test_new_all_reduce import check_mesh_tensor_alloc


class TT_CCL:
    def __init__(
        self,
        mesh_device,
        model_args,
        worker_sub_device_id,
        enable_persistent_fabric=True,
        create_persistent_fabric=True,
        teardown_persistent_fabric=True,
        mode="decode",
    ):
        self.mode = mode
        all_crs = ttnn.CoreRangeSet([ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(6, 9))])

        self.mesh_device = mesh_device
        self.sub_device_crs = all_crs if mode == "prefill" else model_args.sub_core_grids
        self.worker_sub_device_id = worker_sub_device_id
        self.enable_persistent_fabric = enable_persistent_fabric
        self.create_persistent_fabric = create_persistent_fabric
        self.teardown_persistent_fabric = teardown_persistent_fabric
        self.model_config = model_args.model_config

        if create_persistent_fabric:
            assert enable_persistent_fabric
        if teardown_persistent_fabric:
            assert enable_persistent_fabric
        if create_persistent_fabric:
            self.num_cbs = 2
            self.from_remote_semaphore_handles = []
            self.to_remote_semaphore_handles = []

            # Double buffered on each axis
            self.gather_semaphore_handles = [[], []]
            if mode == "prefill":
                self.from_semaphore_handles = [[], []]
                self.to_semaphore_handles = [[], []]
            for i in range(2):
                for _ in range(self.num_cbs):
                    self.gather_semaphore_handles[i].append(
                        create_global_semaphore_with_same_address(self.mesh_device, self.sub_device_crs, 0)
                    )
                    if mode == "prefill":
                        self.from_semaphore_handles[i].append(
                            create_global_semaphore_with_same_address(self.mesh_device, self.sub_device_crs, 0)
                        )
                        self.to_semaphore_handles[i].append(
                            create_global_semaphore_with_same_address(self.mesh_device, self.sub_device_crs, 0)
                        )

            self.gather_idx = [0, 0]
            self.buffer_idx = [0, 0]

            if mode == "decode":
                self.persistent_buffers = self.get_persistent_buffers()
                self.all_gather_buffers = self.get_all_gather_buffers()
            if mode == "prefill":
                self.persistent_buffers = self.get_reduce_scatter_buffers()
                self.all_gather_buffers = self.get_prefill_all_gather_buffers()

    def reset_gather_and_buffer_idx(self):
        self.gather_idx = [0, 0]
        self.buffer_idx = [0, 0]

    def get_all_gather_buffers(self):
        """
        Currently, this is hardcoded with llama specific shapes.

        Here are the current persistent buffers generated by this fuction:
        - SDPA: (1, 32, 32, 128)
        - LAYERNORM: (1, 1, 32, 128)
        - SAMPLING: (1, 1, 32, 128 * 1024)

        """

        persistent_buffers = {}

        if self.model_config is None:
            return persistent_buffers

        M = 32

        # SDPA
        tt_buffer = ttnn.from_torch(
            torch.zeros((1, 32, M, 128)),
            device=self.mesh_device,
            layout=ttnn.TILE_LAYOUT,
            dtype=ttnn.bfloat16,
            memory_config=self.model_config["GATHER_USERS_MEMCFG"](list(self.mesh_device.shape)[1]),
            mesh_mapper=ttnn.ReplicateTensorToMesh(self.mesh_device),
        )
        check_mesh_tensor_alloc(tt_buffer)
        persistent_buffers["SDPA"] = tt_buffer

        # Layernorm
        grid_offset = ttnn.CoreCoord(1, 0)
        tt_stats_sharded_config = ttnn.create_sharded_memory_config(
            shape=(32, 128),
            core_grid=ttnn.CoreRangeSet([ttnn.CoreRange(grid_offset, grid_offset)]),
            strategy=ttnn.ShardStrategy.WIDTH,
            use_height_and_width_as_shard_shape=True,
        )

        tt_buffer = ttnn.from_torch(
            torch.zeros((1, 1, M, 128)),
            device=self.mesh_device,
            layout=ttnn.TILE_LAYOUT,
            dtype=ttnn.bfloat16,
            memory_config=tt_stats_sharded_config,
            mesh_mapper=ttnn.ReplicateTensorToMesh(self.mesh_device),
        )
        check_mesh_tensor_alloc(tt_buffer)
        persistent_buffers["LAYERNORM"] = tt_buffer

        tt_buffer = ttnn.from_torch(
            torch.zeros((1, 1, 32, 128 * 1024)),
            device=self.mesh_device,
            layout=ttnn.TILE_LAYOUT,
            dtype=ttnn.bfloat8_b,
            memory_config=ttnn.DRAM_MEMORY_CONFIG,
            mesh_mapper=ttnn.ReplicateTensorToMesh(self.mesh_device),
        )
        check_mesh_tensor_alloc(tt_buffer)
        persistent_buffers["SAMPLING"] = tt_buffer

        return persistent_buffers

    def get_persistent_buffers(self):
        """
        Currently, this is hardcoded with llama specific shapes.

        Creates double buffered persistent CCL buffers for each cluster axis.

        """

        persistent_buffers = [[], []]

        cluster_shape = (8, 4)
        M = 32
        num_cores = self.sub_device_crs.num_cores()

        # Create persistent buffers for cluster axis 0
        cluster_axis = 0
        N_per_shard = 2048 // 16 * cluster_shape[cluster_axis]  # FF2/DO
        buffer_mem_cfg = ttnn.MemoryConfig(
            ttnn.TensorMemoryLayout.WIDTH_SHARDED,
            ttnn.BufferType.L1,
            ttnn.ShardSpec(
                self.sub_device_crs,
                [M, N_per_shard],
                ttnn.ShardOrientation.ROW_MAJOR,
            ),
        )
        for _ in range(self.num_cbs):
            tt_buffer = ttnn.from_torch(
                torch.zeros((*cluster_shape, M, N_per_shard * num_cores)),
                device=self.mesh_device,
                layout=ttnn.TILE_LAYOUT,
                dtype=ttnn.bfloat16,
                memory_config=buffer_mem_cfg,
                mesh_mapper=ttnn.ShardTensor2dMesh(self.mesh_device, dims=(0, 1), mesh_shape=cluster_shape),
            )
            persistent_buffers[cluster_axis].append(tt_buffer)

        # Create persistent buffers for cluster axis 1
        cluster_axis = 1
        N_per_shard = 3840 // 24 * cluster_shape[cluster_axis]  # FF1/FF3/QKV
        buffer_mem_cfg = ttnn.MemoryConfig(
            ttnn.TensorMemoryLayout.WIDTH_SHARDED,
            ttnn.BufferType.L1,
            ttnn.ShardSpec(
                self.sub_device_crs,
                [M, N_per_shard],
                ttnn.ShardOrientation.ROW_MAJOR,
            ),
        )
        for _ in range(self.num_cbs):
            tt_buffer = ttnn.from_torch(
                torch.zeros((*cluster_shape, M, N_per_shard * num_cores)),
                device=self.mesh_device,
                layout=ttnn.TILE_LAYOUT,
                dtype=ttnn.bfloat16,
                memory_config=buffer_mem_cfg,
                mesh_mapper=ttnn.ShardTensor2dMesh(self.mesh_device, dims=(0, 1), mesh_shape=cluster_shape),
            )
            persistent_buffers[cluster_axis].append(tt_buffer)

        # Create persistent buffer for lm_head
        num_cores_after_lm_head = 32  # Use 32 cores instead of 16 to reduce L1 memory usage per core
        N_per_shard = (16 * 1024) // num_cores_after_lm_head * cluster_shape[cluster_axis]  # LM Head
        self.lm_head_buffer_mem_cfg = ttnn.MemoryConfig(
            ttnn.TensorMemoryLayout.WIDTH_SHARDED,
            ttnn.BufferType.L1,
            ttnn.ShardSpec(
                self.sub_device_crs,
                [M, N_per_shard],
                ttnn.ShardOrientation.ROW_MAJOR,
            ),
        )

        self.tt_lm_head_buffer = ttnn.from_torch(
            torch.zeros((*cluster_shape, M, N_per_shard * num_cores)),
            device=self.mesh_device,
            layout=ttnn.TILE_LAYOUT,
            dtype=ttnn.bfloat8_b,
            memory_config=ttnn.DRAM_MEMORY_CONFIG,
            mesh_mapper=ttnn.ShardTensor2dMesh(self.mesh_device, dims=(0, 1), mesh_shape=cluster_shape),
        )

        return persistent_buffers

    def get_reduce_scatter_buffers(self):
        """
        Currently, this is hardcoded with llama specific shapes.

        Here are the current persistent buffers generated by this fuction:
        - QKV: (1, 1, 128, 1280)
        - FF1/FF3: (1, 1, 128, 3584)
        - FF2/WO: (1, 1, 128, 2048)

        """

        persistent_buffers = {}

        if self.model_config is None:
            return persistent_buffers

        M = 128 if self.mode == "prefill" else 32
        buffers_dict = {
            "QKV": [(1, 1, 128, 1280), (1, 1, 128, 1280 // 4)],
            "WO": [(1, 1, 128, 2048), (1, 1, 128, 2048 // 8)],
            "FF1": [(1, 1, 128, 3584), (1, 1, 128, 3584 // 4)],
            "FF3": [(1, 1, 128, 3584), (1, 1, 128, 3584 // 4)],
            "FF2": [(1, 1, 128, 2048), (1, 1, 128, 2048 // 8)],
        }
        for key, shape in buffers_dict.items():
            tt_buffers = []
            for i in range(1):
                tt_buffer = ttnn.from_torch(
                    torch.zeros(shape[1]),
                    device=self.mesh_device,
                    layout=ttnn.TILE_LAYOUT,
                    dtype=ttnn.bfloat8_b,
                    memory_config=ttnn.DRAM_MEMORY_CONFIG,
                    mesh_mapper=ttnn.ReplicateTensorToMesh(self.mesh_device),
                )
                check_mesh_tensor_alloc(tt_buffer)
                tt_buffers.append(tt_buffer)
            for i in range(2):
                tt_buffer = ttnn.from_torch(
                    torch.zeros(shape[0]),
                    device=self.mesh_device,
                    layout=ttnn.TILE_LAYOUT,
                    dtype=ttnn.bfloat8_b,
                    memory_config=ttnn.DRAM_MEMORY_CONFIG,
                    mesh_mapper=ttnn.ReplicateTensorToMesh(self.mesh_device),
                )
                check_mesh_tensor_alloc(tt_buffer)
                tt_buffers.append(tt_buffer)
            for i in range(2):
                tt_buffer = ttnn.from_torch(
                    torch.zeros(shape[1]),
                    device=self.mesh_device,
                    layout=ttnn.TILE_LAYOUT,
                    dtype=ttnn.bfloat8_b,
                    memory_config=ttnn.DRAM_MEMORY_CONFIG,
                    mesh_mapper=ttnn.ReplicateTensorToMesh(self.mesh_device),
                )
                check_mesh_tensor_alloc(tt_buffer)
                tt_buffers.append(tt_buffer)
            persistent_buffers[key] = tt_buffers

        return persistent_buffers

    def get_prefill_all_gather_buffers(self):
        """
        Currently, this is hardcoded with llama specific shapes.

        Creates double buffered persistent CCL buffers for each cluster axis.

        """

        ag_persistent_buffers = {}

        M = 128 if self.mode == "prefill" else 32
        buffers_dict = {
            "QKV": [(1, 1, 128, 1280)],
            "WO": [(1, 1, 128, 2048)],
            "FF1": [(1, 1, 128, 3584)],
            "FF3": [(1, 1, 128, 3584)],
            "FF2": [(1, 1, 128, 2048)],
            "LAYERNORM": [(1, 1, 128, 128)],
            # "SAMPLING": [(1, 1, 32, 128 * 1024)]
        }
        for key, shape in buffers_dict.items():
            tt_buffer = ttnn.from_torch(
                torch.zeros(shape[0]),
                device=self.mesh_device,
                layout=ttnn.TILE_LAYOUT,
                dtype=ttnn.bfloat16 if key == "LAYERNORM" else ttnn.bfloat8_b,
                memory_config=ttnn.DRAM_MEMORY_CONFIG,
                mesh_mapper=ttnn.ReplicateTensorToMesh(self.mesh_device),
            )
            check_mesh_tensor_alloc(tt_buffer)
            ag_persistent_buffers[key] = tt_buffer

        return ag_persistent_buffers

    def line_all_reduce(
        self, input_tensor_mesh, cluster_axis, num_links, memory_config, lm_head=False, buffer_key=None
    ):
        if self.mode == "decode":
            if lm_head:
                persistent_buffer = self.tt_lm_head_buffer_l1
            else:
                persistent_buffer = self.persistent_buffers[cluster_axis][self.buffer_idx[cluster_axis]]

            output_tensor_mesh = ttnn.experimental.all_reduce_async(
                input_tensor_mesh,
                persistent_buffer,
                cluster_axis=cluster_axis,
                mesh_device=self.mesh_device,
                multi_device_global_semaphore=self.gather_semaphore_handles[cluster_axis][
                    self.gather_idx[cluster_axis]
                ],
                num_links=num_links,
                memory_config=memory_config,
                topology=ttnn.Topology.Linear,
                subdevice_id=self.worker_sub_device_id,
            )

            if lm_head:
                persistent_buffer.deallocate(True)
        else:
            if self.enable_persistent_fabric:
                if lm_head:
                    ttnn_tensor_gathered = self.line_all_gather(
                        input_tensor_mesh,
                        dim=0,
                        num_links=num_links,
                        cluster_axis=cluster_axis,
                        memory_config=memory_config,
                        buffer_key=buffer_key,
                    )
                    ttnn_tensor_out = ttnn.experimental.fast_reduce_nc(
                        ttnn_tensor_gathered,
                        dims=[0],
                        output=None,
                        compute_kernel_config=None,
                        memory_config=memory_config,
                    )
                    return ttnn_tensor_out
                # ttnn.synchronize_device(self.mesh_device)
                output_tensor_scattered = self.line_reduce_scatter(
                    input_tensor_mesh,
                    memory_config,
                    dim=3,
                    cluster_axis=cluster_axis,
                    num_links=num_links,
                    math_op=ttnn.ReduceType.Sum,
                    buffer_key=buffer_key,
                )
                # ttnn.synchronize_device(self.mesh_device)
                # Gather the scattered tensor
                output_tensor_mesh = self.line_all_gather(
                    output_tensor_scattered,
                    dim=3,
                    cluster_axis=cluster_axis,
                    memory_config=memory_config,
                    num_links=num_links,
                    buffer_key=buffer_key,
                )
                # ttnn.synchronize_device(self.mesh_device)
            else:
                num_links = 1  # if cluster_axis==1 else 4
                ttnn_tensor_gathered = ttnn.all_gather(
                    input_tensor_mesh,
                    0,
                    num_links=num_links,
                    cluster_axis=cluster_axis,
                    mesh_device=self.mesh_device,
                    topology=ttnn.Topology.Linear,
                    memory_config=memory_config,
                )
                ttnn_tensor_out = ttnn.experimental.fast_reduce_nc(
                    ttnn_tensor_gathered,
                    dims=[0],
                    output=None,
                    compute_kernel_config=None,
                    memory_config=memory_config,
                )
                return ttnn_tensor_out

        self.gather_idx[cluster_axis] = (self.gather_idx[cluster_axis] + 1) % self.num_cbs
        self.buffer_idx[cluster_axis] = (self.buffer_idx[cluster_axis] + 1) % self.num_cbs
        return output_tensor_mesh

    def line_reduce_scatter(
        self,
        input_tensor_mesh,
        memory_config,
        dim,
        cluster_axis,
        num_links=1,
        math_op=ttnn.ReduceType.Sum,
        buffer_key=None,
    ):
        persistent_buffers = self.persistent_buffers.get(buffer_key, None)

        ttnn_tensor_out = ttnn.experimental.reduce_scatter_async(
            input_tensor_mesh,
            dim,
            cluster_axis=cluster_axis,
            mesh_device=self.mesh_device,
            from_remote_multi_device_global_semaphore=self.from_semaphore_handles[cluster_axis][
                self.gather_idx[cluster_axis]
            ],
            to_remote_multi_device_global_semaphore=self.to_semaphore_handles[cluster_axis][
                self.gather_idx[cluster_axis]
            ],
            math_op=math_op,
            memory_config=memory_config,
            topology=ttnn.Topology.Linear,
            num_links=num_links,
            subdevice_id=self.worker_sub_device_id,
            persistent_output_tensors=persistent_buffers,
        )
        self.gather_idx[cluster_axis] = (self.gather_idx[cluster_axis] + 1) % self.num_cbs

        # ttnn.synchronize_device(self.mesh_device, sub_device_ids=[self.worker_sub_device_id])
        return ttnn_tensor_out

    def line_all_gather(self, input_tensor_mesh, dim, cluster_axis, memory_config, num_links=1, buffer_key=None):
        if self.mode == "prefill" and not self.enable_persistent_fabric:
            num_links = 1  # if cluster_axis==1 else 4
            ttnn_tensor_out = ttnn.all_gather(
                input_tensor_mesh,
                dim,
                num_links=num_links,
                cluster_axis=cluster_axis,
                mesh_device=self.mesh_device,
                topology=ttnn.Topology.Linear,
                memory_config=memory_config,
            )
            return ttnn_tensor_out
        persistent_buffer = self.all_gather_buffers.get(buffer_key, None)
        # ttnn.synchronize_device(self.mesh_device, sub_device_ids=[self.worker_sub_device_id])
        ttnn_tensor_out = ttnn.experimental.all_gather_async(
            input_tensor_mesh,
            dim,
            cluster_axis=cluster_axis,
            mesh_device=self.mesh_device,
            topology=ttnn.Topology.Linear,
            multi_device_global_semaphore=self.gather_semaphore_handles[cluster_axis][self.gather_idx[cluster_axis]],
            persistent_output_tensor=persistent_buffer,
            num_links=num_links,
            memory_config=memory_config,
            subdevice_id=self.worker_sub_device_id,
            enable_persistent_fabric_mode=self.enable_persistent_fabric,
        )
        self.gather_idx[cluster_axis] = (self.gather_idx[cluster_axis] + 1) % self.num_cbs
        # ttnn.synchronize_device(self.mesh_device, sub_device_ids=[self.worker_sub_device_id])
        return ttnn_tensor_out

    def line_all_reduce_host(self, input_tensor_mesh, cluster_axis, num_links, memory_config):
        dim = 3

        ##### Host side implementation #####
        rs_output_tensor_mesh = self.line_reduce_scatter_host(
            input_tensor_mesh,
            memory_config,
            dim,
            cluster_axis,
            num_links=num_links,
            math_op=ttnn.ReduceType.Sum,
        )

        output_tensor_mesh = self.line_all_gather_host(
            rs_output_tensor_mesh,
            dim,
            cluster_axis,
            memory_config,
            num_links=num_links,
        )

        return output_tensor_mesh

    def line_reduce_scatter_host(
        self, input_tensor_mesh, memory_config, dim, cluster_axis, num_links=1, math_op=ttnn.ReduceType.Sum
    ):
        ##### Host side implementation #####
        dims = [0, 1]
        dtype = input_tensor_mesh.get_dtype()
        torch_tensor_mesh = ttnn.to_torch(
            input_tensor_mesh, mesh_composer=ttnn.ConcatMesh2dToTensor(self.mesh_device, dims=dims, mesh_shape=(8, 4))
        )

        torch_tensor_mesh = torch.sum(torch_tensor_mesh, dim=cluster_axis, keepdim=True)

        dims[cluster_axis] = dim
        ttnn_tensor_out = ttnn.from_torch(
            torch_tensor_mesh,
            device=self.mesh_device,
            mesh_mapper=ttnn.ShardTensor2dMesh(self.mesh_device, dims=dims, mesh_shape=(8, 4)),
            dtype=dtype,
            memory_config=memory_config,
            layout=ttnn.TILE_LAYOUT,
        )

        return ttnn_tensor_out

    def line_all_gather_host(self, input_tensor_mesh, dim, cluster_axis, memory_config, num_links=1):
        ##### Host side implementation #####
        dims = [0, 0] if dim != 0 else [1, 1]
        dims[cluster_axis] = dim
        dtype = input_tensor_mesh.get_dtype()
        torch_tensor_mesh = ttnn.to_torch(
            input_tensor_mesh, mesh_composer=ttnn.ConcatMesh2dToTensor(self.mesh_device, dims=dims, mesh_shape=(8, 4))
        )

        dims[cluster_axis] = None
        ttnn_tensor_out = ttnn.from_torch(
            torch_tensor_mesh,
            device=self.mesh_device,
            mesh_mapper=ttnn.ShardTensor2dMesh(self.mesh_device, dims=dims, mesh_shape=(8, 4)),
            dtype=dtype,
            memory_config=memory_config,
            layout=ttnn.TILE_LAYOUT,
        )

        return ttnn_tensor_out

    def close(self):
        if self.enable_persistent_fabric and self.teardown_persistent_fabric:
            logger.info("Tearing down persistent fabric interface")
            self.mesh_device.reset_sub_device_stall_group()
            teardown_fabric_interface(self.mesh_device)
            logger.info("Done tearing down persistent fabric interface")


def tt_distributed_rmsnorm(
    inp,
    epsilon,
    gamma,
    mesh_device,
    compute_kernel_config,
    tt_ccl=None,
):
    # Run distributed rmsnorm part 1
    tt_stats = ttnn.rms_norm_pre_all_gather(inp, compute_kernel_config=compute_kernel_config, dtype=ttnn.bfloat16)
    padded_shape = (1, 1, inp.shape[-2], 32)

    tt_stats_gathered = tt_ccl.line_all_gather(
        tt_stats, dim=3, cluster_axis=1, num_links=1, memory_config=ttnn.DRAM_MEMORY_CONFIG, buffer_key="LAYERNORM"
    )

    # tt_stats.deallocate(True)

    # Run distributed rmsnorm part 2
    tt_out = ttnn.rms_norm_post_all_gather(
        inp, tt_stats_gathered, epsilon=epsilon, weight=gamma, compute_kernel_config=compute_kernel_config
    )

    # tt_stats_gathered.deallocate(True)
    # inp.deallocate(True)

    return tt_out, None


def tt_sharded_distributed_rmsnorm(
    inp,
    res,
    epsilon,
    gamma,
    mesh_device,
    ln_sharded_input_memcfg,
    ln_sharded_progcfg,
    ln_sharded_stats_memcfg,
    tt_ccl=None,
):
    # inp = ttnn.to_memory_config(inp, memory_config=ln_sharded_input_memcfg)

    # Run distributed rmsnorm part 1
    tt_stats = ttnn.rms_norm_pre_all_gather(inp, residual_input_tensor=res, program_config=ln_sharded_progcfg)
    # print("tt_stats")
    # All gather stats
    # tt_stats = ttnn.all_gather(
    #     tt_stats,
    #     3,
    #     num_links=1,
    #     cluster_axis=1,
    #     mesh_device=mesh_device,
    #     memory_config=ln_sharded_stats_memcfg,
    #     topology=ttnn.Topology.Linear,
    # )
    # tt_stats_dram = ttnn.to_memory_config(tt_stats, ttnn.DRAM_MEMORY_CONFIG)
    grid_offset = ttnn.CoreCoord(1, 0)
    tt_stats_sharded_config = ttnn.create_sharded_memory_config(
        shape=(32, 128),
        core_grid=ttnn.CoreRangeSet([ttnn.CoreRange(grid_offset, grid_offset)]),
        strategy=ttnn.ShardStrategy.WIDTH,
        use_height_and_width_as_shard_shape=True,
    )

    # print("mem cfg")

    # Note: Persistent output buffer used, do not deallocate output!
    tt_global_stats_sharded = tt_ccl.line_all_gather(
        tt_stats, dim=3, cluster_axis=1, num_links=1, memory_config=tt_stats_sharded_config, buffer_key="LAYERNORM"
    )
    # ttnn.synchronize_device(tt_ccl.mesh_device, sub_device_ids=[tt_ccl.worker_sub_device_id])
    # ttnn.deallocate(tt_stats_dram)
    # print("all gather stats", tt_global_stats.shape)

    # tt_global_stats_sharded = ttnn.to_memory_config(tt_global_stats, memory_config=tt_stats_sharded_config)
    ttnn.deallocate(tt_stats)
    # print("sharded stats")

    # Run distributed rmsnorm part 2
    tt_out = ttnn.rms_norm_post_all_gather(
        inp,
        epsilon=epsilon,
        weight=gamma,
        program_config=ln_sharded_progcfg,
        stats=tt_global_stats_sharded,
    )
    # print("rmsnorm post all gather", tt_out.shape)
    return tt_out, inp
