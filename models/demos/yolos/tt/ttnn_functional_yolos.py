# SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC

# SPDX-License-Identifier: Apache-2.0

"""
TTNN Implementation of YOLOS-small for Object Detection

Follows the tt-metal patterns from models/demos/vit/
Supports three optimization stages:
- Stage 1: Basic bring-up with straightforward TTNN operations
- Stage 2: Basic optimizations (sharding, L1 memory)
- Stage 3: Deep optimizations (fused SDPA, bfloat8)
"""

import torch
import transformers

import ttnn


# =============================================================================
# Custom Preprocessor for YOLOS weights
# =============================================================================

def custom_preprocessor(torch_model, name):
    """
    Custom preprocessor for YOLOS weights.
    Handles the patch embedding conv->linear transformation and other weight preparations.
    
    Called by ttnn.model_preprocessing.preprocess_model_parameters for each submodule.
    """
    parameters = {}
    
    if isinstance(torch_model, transformers.models.yolos.modeling_yolos.YolosEmbeddings):
        # Handle patch embeddings conv weight transformation
        weight = torch_model.patch_embeddings.projection.weight  # [384, 3, 16, 16]
        bias = torch_model.patch_embeddings.projection.bias       # [384]
        
        hidden_size, in_channels, patch_h, patch_w = weight.shape
        pad_value = 4 - in_channels  # Pad from 3 to 4 channels
        
        # Pad and reshape conv weight to linear format
        # [384, 3, 16, 16] -> [384, 4, 16, 16] -> [16, 16, 4, 384] -> [1024, 384]
        preprocessed_weight = torch.nn.functional.pad(weight, (0, 0, 0, 0, 0, pad_value))
        preprocessed_weight = torch.permute(preprocessed_weight, (2, 3, 1, 0))
        preprocessed_weight = torch.reshape(
            preprocessed_weight, (patch_h * patch_w * 4, hidden_size)
        )
        
        parameters["patch_embeddings"] = {}
        parameters["patch_embeddings"]["projection"] = {
            "weight": ttnn.from_torch(preprocessed_weight, dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT),
            "bias": ttnn.from_torch(bias, dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT),
        }
        
        parameters["cls_token"] = ttnn.from_torch(
            torch_model.cls_token, dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT
        )
        parameters["detection_tokens"] = ttnn.from_torch(
            torch_model.detection_tokens, dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT
        )
        parameters["position_embeddings"] = ttnn.from_torch(
            torch_model.position_embeddings, dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT
        )
        
    return parameters


# =============================================================================
# Patch Embeddings
# =============================================================================

def yolos_patch_embeddings(config, pixel_values, *, parameters, unittest_check=False):
    """
    Convert image to patch embeddings using linear projection.
    
    Args:
        config: YOLOS config
        pixel_values: Input tensor [batch, height, width, channels+1] (padded NHWC)
        parameters: Preprocessed parameters
        unittest_check: If True, access embeddings via full path for unit testing
        
    Returns:
        Patch embeddings [batch, num_patches, hidden_size]
    """
    batch_size, img_h, img_w, img_c = pixel_values.shape  # NHWC
    patch_size = config.patch_size  # 16
    patch_count_h = img_h // patch_size
    patch_count_w = img_w // patch_size
    patch_count_all = patch_count_h * patch_count_w
    stride_h = patch_size
    stride_w = 1
    
    # Reshape for fold operation
    pixel_values = ttnn.reshape(pixel_values, (batch_size, img_h, img_w // patch_size, 4 * patch_size))
    pixel_values = ttnn.fold(pixel_values, stride_h, stride_w)
    pixel_values = ttnn.to_layout(pixel_values, layout=ttnn.TILE_LAYOUT)
    
    # Get parameters - handle both unit test and full model cases
    if unittest_check:
        proj_params = parameters.vit.embeddings.patch_embeddings.projection
    else:
        proj_params = parameters.projection
    
    # Linear projection
    patch_embedding_output = pixel_values @ proj_params.weight
    patch_embedding_output = patch_embedding_output + proj_params.bias
    
    # Reshape to [batch, num_patches, hidden_size]
    patch_embedding_output = ttnn.to_layout(patch_embedding_output, layout=ttnn.ROW_MAJOR_LAYOUT)
    patch_embedding_output = ttnn.reshape(
        patch_embedding_output, (batch_size, patch_count_all, config.hidden_size)
    )
    
    return patch_embedding_output


def yolos_embeddings(
    config,
    pixel_values,
    cls_token,
    detection_tokens,
    position_embeddings,
    *,
    parameters,
):
    """
    Construct full YOLOS embeddings: CLS + patches + detection_tokens + position.
    
    Args:
        config: YOLOS config
        pixel_values: Input images [batch, H, W, C+1]
        cls_token: CLS token [batch, 1, hidden]
        detection_tokens: Detection tokens [batch, 100, hidden]
        position_embeddings: Position embeddings [batch, seq_len, hidden]
        parameters: Preprocessed parameters (full model)
        
    Returns:
        Full embeddings [batch, 1 + num_patches + 100, hidden_size]
    """
    # Access embeddings parameters from the full model structure
    embed_params = parameters.vit.embeddings
    
    patch_embeddings = yolos_patch_embeddings(
        config, pixel_values, parameters=embed_params.patch_embeddings, unittest_check=False
    )
    
    # Ensure patch_embeddings is in TILE_LAYOUT for concat
    patch_embeddings = ttnn.to_layout(patch_embeddings, ttnn.TILE_LAYOUT)
    
    # Concatenate: [CLS] + patches + detection_tokens
    embedding_output = ttnn.concat((cls_token, patch_embeddings), dim=1)
    embedding_output = ttnn.concat((embedding_output, detection_tokens), dim=1)
    
    # Add position embeddings
    embedding_output = embedding_output + position_embeddings
    
    return embedding_output


# =============================================================================
# Attention
# =============================================================================

def yolos_layernorm_before(config, hidden_states, *, parameters):
    """Apply layer norm before attention."""
    return ttnn.layer_norm(
        hidden_states,
        weight=parameters.layernorm_before.weight,
        bias=parameters.layernorm_before.bias,
    )


def yolos_layernorm_after(config, hidden_states, *, parameters):
    """Apply layer norm after attention."""
    return ttnn.layer_norm(
        hidden_states,
        weight=parameters.layernorm_after.weight,
        bias=parameters.layernorm_after.bias,
    )


def yolos_attention(
    config,
    hidden_states,
    attention_mask,
    *,
    parameters,
):
    """
    Multi-head self-attention.
    
    Args:
        config: YOLOS config
        hidden_states: [batch, seq_len, hidden_size]
        attention_mask: Optional attention mask
        parameters: Attention parameters
        
    Returns:
        Attention output [batch, seq_len, hidden_size]
    """
    num_heads = config.num_attention_heads
    batch_size, sequence_size, hidden_size = hidden_states.shape
    head_size = hidden_size // num_heads
    
    # Q, K, V projections
    query = hidden_states @ parameters.attention.query.weight
    query = query + parameters.attention.query.bias
    query = ttnn.to_layout(query, layout=ttnn.ROW_MAJOR_LAYOUT)
    query = ttnn.reshape(query, (batch_size, sequence_size, num_heads, head_size))
    query = ttnn.to_layout(query, layout=ttnn.TILE_LAYOUT)
    query = ttnn.permute(query, (0, 2, 1, 3))  # [batch, heads, seq, head_size]
    
    key = hidden_states @ parameters.attention.key.weight
    key = key + parameters.attention.key.bias
    key = ttnn.to_layout(key, layout=ttnn.ROW_MAJOR_LAYOUT)
    key = ttnn.reshape(key, (batch_size, sequence_size, num_heads, head_size))
    key = ttnn.to_layout(key, layout=ttnn.TILE_LAYOUT)
    key = ttnn.permute(key, (0, 2, 3, 1))  # [batch, heads, head_size, seq] for matmul
    
    value = hidden_states @ parameters.attention.value.weight
    value = value + parameters.attention.value.bias
    value = ttnn.to_layout(value, layout=ttnn.ROW_MAJOR_LAYOUT)
    value = ttnn.reshape(value, (batch_size, sequence_size, num_heads, head_size))
    value = ttnn.to_layout(value, layout=ttnn.TILE_LAYOUT)
    value = ttnn.permute(value, (0, 2, 1, 3))  # [batch, heads, seq, head_size]
    
    # Attention scores: Q @ K^T / sqrt(head_size)
    attention_scores = query @ key
    attention_scores = attention_scores * (1 / (head_size ** 0.5))
    
    if attention_mask is not None:
        attention_scores = attention_scores + attention_mask
    
    # Softmax
    attention_probs = ttnn.softmax(attention_scores, dim=-1)
    
    # Context: attention_probs @ V
    context_layer = attention_probs @ value
    context_layer = ttnn.permute(context_layer, (0, 2, 1, 3))  # [batch, seq, heads, head_size]
    context_layer = ttnn.to_layout(context_layer, ttnn.ROW_MAJOR_LAYOUT)
    context_layer = ttnn.reshape(context_layer, (batch_size, sequence_size, hidden_size))
    context_layer = ttnn.to_layout(context_layer, ttnn.TILE_LAYOUT)
    
    # Output projection
    self_output = context_layer @ parameters.output.dense.weight
    self_output = self_output + parameters.output.dense.bias
    
    return self_output


# =============================================================================
# MLP / Feed-Forward
# =============================================================================

def yolos_intermediate(hidden_states, *, parameters):
    """Intermediate MLP layer with GELU activation."""
    output = hidden_states @ parameters.dense.weight
    output = output + parameters.dense.bias
    output = ttnn.gelu(output)
    return output


def yolos_output(config, hidden_states, residual, *, parameters):
    """Output MLP layer with residual connection."""
    output = hidden_states @ parameters.dense.weight
    output = output + parameters.dense.bias
    output = output + residual
    return output


def yolos_feedforward(config, hidden_states, attention_output, *, parameters):
    """Full MLP block."""
    intermediate = yolos_intermediate(hidden_states, parameters=parameters.intermediate)
    output = yolos_output(config, intermediate, attention_output, parameters=parameters.output)
    return output


# =============================================================================
# Transformer Layer
# =============================================================================

def yolos_layer(config, hidden_states, attention_mask, *, parameters):
    """
    Single transformer encoder layer.
    
    Pre-norm architecture: LN -> Attention -> Add -> LN -> MLP -> Add
    """
    # Pre-norm + attention
    layernorm_before_output = yolos_layernorm_before(config, hidden_states, parameters=parameters)
    attention_output = yolos_attention(
        config, layernorm_before_output, attention_mask, parameters=parameters.attention
    )
    attention_output = attention_output + hidden_states
    
    # Pre-norm + MLP
    layernorm_after_output = yolos_layernorm_after(config, attention_output, parameters=parameters)
    feedforward_output = yolos_feedforward(
        config, layernorm_after_output, attention_output, parameters=parameters
    )
    
    return feedforward_output


# =============================================================================
# Encoder
# =============================================================================

def yolos_encoder(config, hidden_states, attention_mask, *, parameters):
    """
    Stack of transformer encoder layers.
    """
    for layer_idx in range(config.num_hidden_layers):
        hidden_states = yolos_layer(
            config, hidden_states, attention_mask, parameters=parameters.layer[layer_idx]
        )
    
    return hidden_states


# =============================================================================
# Detection Heads
# =============================================================================

def yolos_mlp_prediction_head(hidden_states, *, parameters, apply_sigmoid=False):
    """
    3-layer MLP prediction head (for class or bbox).
    
    Args:
        hidden_states: [batch, num_queries, hidden_size]
        parameters: Parameters with layers list
        apply_sigmoid: Whether to apply sigmoid to final output (for bbox)
        
    Returns:
        Predictions [batch, num_queries, output_dim]
    """
    # Layer 0: Linear + ReLU
    output = hidden_states @ parameters.layers[0].weight
    output = output + parameters.layers[0].bias
    output = ttnn.relu(output)
    
    # Layer 1: Linear + ReLU
    output = output @ parameters.layers[1].weight
    output = output + parameters.layers[1].bias
    output = ttnn.relu(output)
    
    # Layer 2: Linear (+ optional Sigmoid)
    output = output @ parameters.layers[2].weight
    output = output + parameters.layers[2].bias
    
    if apply_sigmoid:
        output = ttnn.sigmoid(output)
    
    return output


# =============================================================================
# Full Model
# =============================================================================

def yolos_for_object_detection(
    config,
    pixel_values,
    cls_token,
    detection_tokens,
    position_embeddings,
    attention_mask,
    *,
    parameters,
):
    """
    Full YOLOS model for object detection.
    
    Args:
        config: YOLOS config
        pixel_values: Input images [batch, H, W, C+1]
        cls_token: CLS token [batch, 1, hidden]
        detection_tokens: Detection tokens [batch, num_detection_tokens, hidden]
        position_embeddings: Position embeddings [batch, seq_len, hidden]
        attention_mask: Optional attention mask
        parameters: All model parameters
        
    Returns:
        Tuple of (logits, pred_boxes)
        - logits: [batch, num_detection_tokens, num_classes+1]
        - pred_boxes: [batch, num_detection_tokens, 4]
    """
    # Embeddings
    embedding_output = yolos_embeddings(
        config,
        pixel_values,
        cls_token,
        detection_tokens,
        position_embeddings,
        parameters=parameters,
    )
    
    # Encoder
    encoder_output = yolos_encoder(
        config, embedding_output, attention_mask, parameters=parameters.vit.encoder
    )
    
    # Final layer norm
    sequence_output = ttnn.layer_norm(
        encoder_output,
        weight=parameters.vit.layernorm.weight,
        bias=parameters.vit.layernorm.bias,
    )
    
    # Extract detection tokens (last num_detection_tokens in sequence)
    # Sequence: [CLS] + patches + detection_tokens
    # Need to slice last 100 tokens
    batch_size, seq_len, hidden_size = sequence_output.shape
    num_detection_tokens = config.num_detection_tokens  # 100
    
    # Slice detection tokens
    sequence_output = ttnn.to_layout(sequence_output, ttnn.ROW_MAJOR_LAYOUT)
    detection_output = sequence_output[:, -num_detection_tokens:, :]
    detection_output = ttnn.to_layout(detection_output, ttnn.TILE_LAYOUT)
    
    # Classification head
    logits = yolos_mlp_prediction_head(
        detection_output, parameters=parameters.class_labels_classifier, apply_sigmoid=False
    )
    
    # Bounding box head
    pred_boxes = yolos_mlp_prediction_head(
        detection_output, parameters=parameters.bbox_predictor, apply_sigmoid=True
    )
    
    return logits, pred_boxes
