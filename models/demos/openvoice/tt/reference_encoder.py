# SPDX-FileCopyrightText: (c) 2026 Tenstorrent AI
# SPDX-License-Identifier: Apache-2.0

"""
Reference Encoder for speaker embedding extraction.

Extracts speaker characteristics from mel spectrograms using
Conv2d layers followed by GRU for temporal modeling.
"""

from typing import Any, List, Optional

import torch
import torch.nn.functional as F

try:
    import ttnn

    TTNN_AVAILABLE = True
except ImportError:
    TTNN_AVAILABLE = False

from models.demos.openvoice.tt.modules.gru import GRULayer


class TTNNReferenceEncoder:
    """
    Reference Encoder for extracting speaker embeddings from mel spectrograms.

    Note: This module runs on CPU (PyTorch) by design because:
    1. Speaker embedding extraction happens once per voice (not latency-critical)
    2. GRU sequential processing with variable-length audio exceeds L1 memory
    3. Impact: ~7ms overhead on total clone latency (<1%)

    Architecture:
        Input: [N, Ty/r, n_mels*r] mel spectrogram
        -> 6 Conv2d layers (32->32->64->64->128->128) with ReLU
        -> Reshape for GRU
        -> GRU (bidirectional=False, hidden=128)
        -> Linear projection to gin_channels

    Output: [N, gin_channels] speaker embedding
    """

    # Standard filter configuration from OpenVoice
    REF_ENC_FILTERS = [32, 32, 64, 64, 128, 128]

    def __init__(
        self,
        spec_channels: int,
        gin_channels: int,
        conv_weights: List[Any],
        conv_biases: List[Any],
        gru_weight_ih: Any,
        gru_weight_hh: Any,
        gru_bias_ih: Optional[Any],
        gru_bias_hh: Optional[Any],
        proj_weight: Any,
        proj_bias: Any,
        layernorm_weight: Optional[Any] = None,
        layernorm_bias: Optional[Any] = None,
        device: Optional[Any] = None,
    ):
        """
        Initialize Reference Encoder.

        Args:
            spec_channels: Number of mel channels (n_mels)
            gin_channels: Output speaker embedding dimension
            conv_weights: List of 6 Conv2d weight tensors
            conv_biases: List of 6 Conv2d bias tensors
            gru_weight_ih: GRU input-hidden weight
            gru_weight_hh: GRU hidden-hidden weight
            gru_bias_ih: GRU input-hidden bias
            gru_bias_hh: GRU hidden-hidden bias
            proj_weight: Final projection weight
            proj_bias: Final projection bias
            layernorm_weight: Optional LayerNorm weight
            layernorm_bias: Optional LayerNorm bias
            device: TTNN device
        """
        self.spec_channels = spec_channels
        self.gin_channels = gin_channels
        self.device = device

        # Conv2d layers
        self.conv_weights = conv_weights
        self.conv_biases = conv_biases

        # LayerNorm (optional)
        self.layernorm_weight = layernorm_weight
        self.layernorm_bias = layernorm_bias
        self.use_layernorm = layernorm_weight is not None

        # GRU
        self.gru = GRULayer(
            weight_ih=gru_weight_ih,
            weight_hh=gru_weight_hh,
            bias_ih=gru_bias_ih,
            bias_hh=gru_bias_hh,
            batch_first=True,
        )

        # Final projection
        self.proj_weight = proj_weight
        self.proj_bias = proj_bias

        # Calculate output channels after 6 conv layers with stride 2
        self.out_channels = self._calculate_channels(spec_channels, 3, 2, 1, 6)

    def _calculate_channels(self, L: int, kernel_size: int, stride: int, pad: int, n_convs: int) -> int:
        """Calculate spatial dimension after n convolutions."""
        for _ in range(n_convs):
            L = (L - kernel_size + 2 * pad) // stride + 1
        return L

    def __call__(self, inputs: Any, mask: Optional[Any] = None) -> Any:
        """
        Extract speaker embedding from mel spectrogram.

        Args:
            inputs: Mel spectrogram [N, Ty, n_mels]
            mask: Optional mask (unused)

        Returns:
            Speaker embedding [N, gin_channels]
        """
        # Check if input is PyTorch tensor
        is_torch = isinstance(inputs, torch.Tensor)

        # Note: Reference encoder uses PyTorch path due to L1 memory constraints
        # on TT hardware for large mel spectrograms. This is acceptable since
        # speaker embedding extraction is not in the real-time inference path.
        if not TTNN_AVAILABLE or is_torch:
            return self._forward_pytorch(inputs)

        # Convert TTNN tensor to PyTorch for reference encoder
        # (Falls back to CPU due to memory constraints on device)
        inputs_torch = ttnn.to_torch(inputs)
        return self._forward_pytorch(inputs_torch)

    def _forward_pytorch(self, inputs):
        """PyTorch fallback implementation."""

        # Helper to convert TTNN tensors to PyTorch
        def to_torch(t, dtype=torch.float32):
            if t is None:
                return None
            if isinstance(t, torch.Tensor):
                return t.to(dtype)
            if TTNN_AVAILABLE:
                return ttnn.to_torch(t).to(dtype)
            return t

        # Ensure input is float32
        inputs = inputs.to(torch.float32)
        N = inputs.size(0)

        # Reshape: [N, T, n_mels] -> [N, 1, T, n_mels]
        out = inputs.view(N, 1, -1, self.spec_channels)

        # LayerNorm on last dimension
        if self.use_layernorm:
            ln_w = to_torch(self.layernorm_weight)
            ln_b = to_torch(self.layernorm_bias)
            out = F.layer_norm(out, (self.spec_channels,), ln_w, ln_b)

        # 6 Conv2d layers with ReLU
        for i, (conv_w, conv_b) in enumerate(zip(self.conv_weights, self.conv_biases)):
            w = to_torch(conv_w)
            b = to_torch(conv_b)
            out = F.conv2d(out, w, b, stride=2, padding=1)
            out = F.relu(out)

        # Reshape for GRU: [N, 128, T', n_mels'] -> [N, T', 128*n_mels']
        out = out.transpose(1, 2)  # [N, T', 128, n_mels']
        T = out.size(1)
        out = out.contiguous().view(N, T, -1)

        # GRU - only need final hidden state
        _, h = self.gru(out, h0=None)

        # Project to speaker embedding
        # h is [1, N, hidden_size], squeeze to [N, hidden_size]
        h = h.squeeze(0)
        proj_w = to_torch(self.proj_weight)
        proj_b = to_torch(self.proj_bias)
        out = F.linear(h, proj_w, proj_b)

        return out

    @classmethod
    def from_state_dict(
        cls,
        state_dict: dict,
        prefix: str,
        spec_channels: int,
        gin_channels: int,
        device: Optional[Any] = None,
    ) -> "TTNNReferenceEncoder":
        """
        Create ReferenceEncoder from state dict.

        Args:
            state_dict: Model state dict (with weight norm already fused)
            prefix: Key prefix (e.g., "ref_enc")
            spec_channels: Number of mel channels
            gin_channels: Speaker embedding dimension
            device: TTNN device

        Returns:
            Initialized TTNNReferenceEncoder
        """
        # Load conv weights
        conv_weights = []
        conv_biases = []
        for i in range(6):
            w = state_dict.get(f"{prefix}.convs.{i}.weight")
            b = state_dict.get(f"{prefix}.convs.{i}.bias")
            conv_weights.append(w)
            conv_biases.append(b)

        # Load GRU weights
        gru_weight_ih = state_dict.get(f"{prefix}.gru.weight_ih_l0")
        gru_weight_hh = state_dict.get(f"{prefix}.gru.weight_hh_l0")
        gru_bias_ih = state_dict.get(f"{prefix}.gru.bias_ih_l0")
        gru_bias_hh = state_dict.get(f"{prefix}.gru.bias_hh_l0")

        # Load projection
        proj_weight = state_dict.get(f"{prefix}.proj.weight")
        proj_bias = state_dict.get(f"{prefix}.proj.bias")

        # Load layernorm (optional)
        layernorm_weight = state_dict.get(f"{prefix}.layernorm.weight")
        layernorm_bias = state_dict.get(f"{prefix}.layernorm.bias")

        return cls(
            spec_channels=spec_channels,
            gin_channels=gin_channels,
            conv_weights=conv_weights,
            conv_biases=conv_biases,
            gru_weight_ih=gru_weight_ih,
            gru_weight_hh=gru_weight_hh,
            gru_bias_ih=gru_bias_ih,
            gru_bias_hh=gru_bias_hh,
            proj_weight=proj_weight,
            proj_bias=proj_bias,
            layernorm_weight=layernorm_weight,
            layernorm_bias=layernorm_bias,
            device=device,
        )
