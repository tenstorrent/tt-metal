# GPT-OSS: Mixture of Experts Language Model

Inference implementation for GPT-OSS models on Tenstorrent Wormhole accelerators.

**Model Source**: [GPT-OSS on HuggingFace](https://huggingface.co/gpt-oss) (custom MoE architecture)

**Target Hardware**:
- **LoudBox**: Single Wormhole device (1Ã—8 configuration)
- **Galaxy**: Multi-device Wormhole mesh (4Ã—8 configuration)

**Current Status**: This model is under active development.
- âœ… Supported: Prefill up to sequence length 128, batch size 1, total sequence length 4096
- ðŸš§ In Progress: Extended sequence lengths, larger batch sizes

## Quick Start

```bash
# Bump up transformers version
pip install -r models/demos/gpt_oss/requirements.txt

# Set model path using HF_MODEL environment variable
export HF_MODEL="/mnt/MLPerf/tt_dnn-models/openai/gpt-oss-20b"

# Run text generation demo on Galaxy (4Ã—8 mesh)
cd tt-metal/models/demos/gpt_oss/demo
pytest text_demo.py -k "4x8 and prefill_128"
```

## Configuration

### Model Selection
```bash
# GPT-OSS-20B (faster, recommended for development)
export HF_MODEL="/mnt/MLPerf/tt_dnn-models/openai/gpt-oss-20b"

# GPT-OSS-120B (higher quality, requires more memory)
export HF_MODEL="/mnt/MLPerf/tt_dnn-models/openai/gpt-oss-120b"
```

## Testing

```bash
# Run all tests
pytest models/demos/gpt_oss/tests/unit/ -v

# Run specific test files
pytest models/demos/gpt_oss/tests/unit/test_modules.py -v     # Core components
pytest models/demos/gpt_oss/tests/unit/test_model.py -v       # Full model accuracy
```

### Test Files Overview

| File | Purpose | Tests |
|------|---------|-------|
| **`test_modules.py`** | Core MoE components | â€¢ Attention component<br>â€¢ RMSNorm<br>â€¢ TopK router<br>â€¢ Experts<br>â€¢ Full MLP pipeline<br>â€¢ Complete decoder layer |
| **`test_model.py`** | Full model integration | â€¢ End-to-end accuracy<br>â€¢ Teacher forcing<br>â€¢ Reference model comparison |
