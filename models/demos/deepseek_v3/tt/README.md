# DeepSeek Code Principles

This codebase separates model execution into three distinct stages, each of which can be run independently:
1. Convert PyTorch weights to TTNN tensor files and generate the WeightConfig
2. Generate ModelConfigs for prefill and decode modes
3. Load TTNN tensor files using WeightConfig, create a shared state using create_state, merge them with ModelPrefillConfig and ModelDecodeConfig to create a RunPrefillConfig and RunDecodeConfig, and execute the model with either of the model configs

The modules are not instantiated directly, but rather used as a namespace for the methods that define the model's behavior in prefill and decode. This is to make it easy to separate the stateful and stateless parts of the model, and allow for easy re-use of the methods.

## Weight Configuration

Generated by static method `convert_weights` on each module class. Returns a dict mapping operation names to their TTNN weight file paths:
```python
{
    "w1": "/path/to/weights/w1.input_tensor_b",
    "w2": "/path/to/weights/w2.input_tensor_b",
    "w3": "/path/to/weights/w3.input_tensor_b"
}
```

## Per-Submodule Model Configs
Generated by static methods `prefill_model_config` and `decode_model_config` on each module class. Contains operator configurations using dataclasses from `config_dataclass.py`:
```python
{
    "w1": LinearConfig(
        memory_config=ttnn.DRAM_MEMORY_CONFIG,
        program_config=matmul_program_config,
        compute_kernel_config=ttnn.experimental.tensor.CoreRangeSet(...)
    ),
    "mul_activation": MulConfig(
        memory_config=ttnn.DRAM_MEMORY_CONFIG,
        input_tensor_a_activations=[ttnn.UnaryOpType.SILU] # list because ttnn.mul expects a list
    ),
    "all_reduce": AllReduceConfig(
        cluster_axis=0,
        topology=ttnn.Topology.Ring,
        dtype=ttnn.bfloat8_b,
        dim=3,
        num_reduce_scatter_links=1,
        num_all_gather_links=1,
        use_composite=True
    )
}
```

Since prefill and decode modes typically have very different performance characteristics, we generate separate config dicts for each mode. This makes the configuration explicit and allows for easy mode-specific tuning.

## Example Usage

```python
# Stage 1: Convert weights and get weight_config (saves to disk in standard format)
weight_config = MLP1D.convert_weights(hf_config, torch_state_dict, Path("weights/mlp"), mesh_device)

# Stage 2: Generate operator configs (returns nested dicts with TTNN objects)
model_config = MLP1D.prefill_model_config(hf_config, mesh_device) # Or decode_model_config(hf_config, mesh_device) for decode

# Stage 3: Generate the runtime state of the model
model_state = MLP1D.create_state(hf_config, mesh_device)

# Stage 3: Runtime execution
run_config = MLP1D.run_config(model_config, weight_config, model_state)
output = MLP1D.forward_prefill(input_tensor, run_config) # or forward_decode(input_tensor, run_config)
```

## Module Requirements
Since the module classes are meant to only exist as namespaces for the module-specific behavior, each method to reimplement for a module is a `classmethod`. Each module should implement:

### `convert_weights(hf_config: transformers.PretrainedConfig, state_dict: dict[str, torch.Tensor], output_path: Path, mesh_device: ttnn.Device) -> WeightConfig`
- Converts PyTorch weights to TTNN format using a standard configuration:
  - dtype: `ttnn.bfloat4_b`
  - layout: `ttnn.TILE_LAYOUT`
  - memory_config: `ttnn.DRAM_MEMORY_CONFIG`
- Saves them to disk at the specified path (e.g., `w1.weight` or `w2.input_tensor_b` if you prefer)
- Handles sharding across devices appropriately (typically column sharding for projections, row sharding for down projections)
- Returns a `WeightConfig` dict mapping operation keyword parameters to their TTNN weight file paths saved using `save_and_get_path` - this MUST match the TTNN argument name, e.g. use ["w1"]["input_tensor_b"] and not ["w1"]["weight"]. See the [API docs](https://docs.tenstorrent.com/tt-metal/latest/ttnn/ttnn/api/ttnn.linear.html#ttnn-linear) for more details.

### `prefill_model_config(hf_config: PretrainedConfig, **kwargs) -> ModelPrefillConfig` and `decode_model_config(hf_config: PretrainedConfig, **kwargs) -> ModelDecodeConfig`
- Generate static operator configurations for prefill/decode mode using dataclasses
- Returns a nested dict with dataclass instances (LinearConfig, MulConfig, etc.) containing TTNN objects
- Can use helper functions like `find_prefill_grid`, `dram_shard_core_grid_for_k`
- Should generate configs that match the standard weight format used by `convert_weights`
- Should use the `FromWeightConfig` and `MeshDeviceStub` for indicating the weight tensors and providing the mesh device used in the `forward_prefill` and `forward_decode` functions (see [the section on `run_config`](#runconfig-creation) for details)

### `forward_prefill(x: ttnn.Tensor, cfg: RunPrefillConfig) -> ttnn.Tensor` and `forward_decode(x: ttnn.Tensor, cfg: RunDecodeConfig) -> ttnn.Tensor`
- Executes the layer using the provided RunConfig
- Uses clean dict expansion: `ttnn.linear(x, **cfg["w1"])`
- **Important**: `OpConfigBase` automatically filters out `None` values during `**cfg` expansion, so `None` means "use TTNN's default" rather than "pass None explicitly"
- For dynamic configs, overrides with keyword arguments: `ttnn.linear(x, program_config=self.w1_pc(**cfg["w1_pc"]), **cfg["w1"])`
- This allows clean mixing of dataclass defaults with explicit overrides without parameter conflicts
- Handles memory management (deallocations)

### (optionally) `create_state(hf_config: transformers.PretrainedConfig, mesh_device: ttnn.Device) -> ModelState`
- Good place for setting up non-weight tensors like kv_cache or dynamic program configs
- Specifies the ttnn.Device on which to load the weights with the MESH_DEVICE_STATE_DICT_KEY key

## run config creation
The `run_config(model_config: ModelPrefillConfig | ModelDecodeConfig, weight_config: WeightConfig, model_state: ModelState) -> RunPrefillConfig | RunDecodeConfig` function merges the model config, the weight config and the shared model state hierarchically over their structure. If a MESH_DEVICE_STATE_DICT_KEY key is specified somewhere in the model_state dictionary hierarchy, it will use the ttnn.MeshDevice from under that key as the device for instantiating the run_config for that hierarchy (unless said device is overriden by any of the deeper nodes). For given config items:
- if all of them are either the same container (list, tuple, dict) or a None, these containers are unified, and the matching items in these containers are merged using the same procedure
  - `OpConfigBase` subclasses are treated as dicts here, and re-wrapped in the same dataclass for `RunConfig`
  - if the container type is a dict, the containers are merged by keys. Note that, for merging the model_state in, MESH_DEVICE_STATE_DICT_KEY key is omitted to avoid name clashes with op configs which specify it as their argument.
  - if the container type is a list or a tuple, the containers have to be the same length
- if the items are not containers, then it is required that only one of them is not None.
- an exception to the above is when the items in the model config and `WeightConfig` are a `FromWeightConfig` and string path respectively. In that case, a tensor is loaded from the path stored in the `WeightConfig` onto the `ttnn.Device` specified by the model_state.
- finally, if the model config item is a `MeshDeviceStub`, it is replaced with the `ttnn.Device` specified by the model_state. This is primarily used for the ops that produce a tensor and require a `device` or a `mesh_device` argument.

## Implementation Notes
- Weight files follow a standard naming convention that matches the config keys (e.g., `w1.input_tensor_b` file for `w1` config entries)
- Runtime config is a nested dict with dataclass instances containing loaded tensors
- Each operation gets its own dataclass instance to prevent accidental weight sharing
- Dataclasses inherit from OpConfigBase which provides dict-like access for `**cfg["op"]` unpacking
- **OpConfigBase automatically filters out `None` values**: When using `**cfg["op"]`, only non-None fields are passed to TTNN functions, allowing clean parameter overrides without conflicts
- Modules should be specific rather than overly general. Different architectures or sharding strategies warrant different module implementations
- Validation happens when creating runtime config - ensuring weights exist where expected
- Use dataclasses from `config_dataclass.py` for type safety and better IDE support
- Use the `create_state` sparingly, e.g. for storing the KV-cache. Putting more model configuration into the model configs allows for more easily interpreting and modifying the model behavior.

## Design Benefits

1. **Type Safety**: Dataclasses provide clear type information and IDE support
2. **Clean Syntax**: Forward passes use `**cfg["op"]` with dataclasses that support dict-like access
3. **Separation of Concerns**: Weight conversion, operator configuration and state creation are independent
4. **Reusability**: Same weights work with different operator configurations
5. **Direct Memory Usage**: No object conversion overhead - configs use TTNN objects directly
6. **Minimal Boilerplate**: Clean forward functions with `**cfg["op"]` expansion
7. **Explicit Weight Mapping**: `WeightConfig` makes it clear which operations have weights and where they're stored
8. **Shared Reference Safety**: `RunPrefillConfig` and `RunDecodeConfig` create new dataclass instances to prevent accidental weight sharing
9. **Better Documentation**: Dataclass fields are self-documenting with type hints
