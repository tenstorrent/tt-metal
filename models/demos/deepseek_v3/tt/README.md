# DeepSeek Code Principles

This codebase separates model execution into three distinct stages, each of which can be run independently:
1. Convert PyTorch weights to TTNN tensor files and generate the WeightConfig
2. Generate ModelConfigs for prefill and decode modes
3. Load TTNN tensor files using WeightConfig and merge them with ModelPrefillConfig and ModelDecodeConfig to create a RunPrefillConfig and RunDecodeConfig
4. Execute the model using either of the model configs

The modules are not instantiated directly, but rather used as a namespace for the methods that define the model's behavior in prefill and decode. This is to make it easy to separate the stateful and stateless parts of the model, and allow for easy re-use of the methods.

## Weight Configuration

Generated by static method `convert_weights` on each module class. Returns a dict mapping operation names to their TTNN weight file paths:
```python
{
    "w1": "/path/to/weights/w1.input_tensor_b",
    "w2": "/path/to/weights/w2.input_tensor_b",
    "w3": "/path/to/weights/w3.input_tensor_b"
}
```

## Per-Submodule Model Configs

Generated by static methods `prefill_model_config` and `decode_model_config` on each module class. Contains operator configurations using dataclasses from `config_dataclass.py`:
```python
{
    "w1": LinearConfig(
        memory_config=ttnn.DRAM_MEMORY_CONFIG,
        program_config=matmul_program_config,
        compute_kernel_config=ttnn.experimental.tensor.CoreRangeSet(...)
    ),
    "mul_activation": MulConfig(
        memory_config=ttnn.DRAM_MEMORY_CONFIG,
        input_tensor_a_activations=[ttnn.UnaryOpType.SILU] # list because ttnn.mul expects a list
    ),
    "all_reduce": AllReduceConfig(
        cluster_axis=0,
        topology=ttnn.Topology.Ring,
        dtype=ttnn.bfloat8_b,
        dim=3,
        num_reduce_scatter_links=1,
        num_all_gather_links=1,
        use_composite=True
    )
}
```

Since prefill and decode modes typically have very different performance characteristics, we generate separate config dicts for each mode. This makes the configuration explicit and allows for easy mode-specific tuning.

## Example Usage

```python
# Stage 1: Convert weights and get weight_config (saves to disk in standard format)
weight_config = MLP1D.convert_weights(hf_config, torch_state_dict, Path("weights/mlp"), mesh_device)

# Stage 2: Generate operator configs (returns nested dicts with TTNN objects)
prefill_config = MLP1D.prefill_model_config(hf_config, mesh_device)
decode_config = MLP1D.decode_model_config(hf_config, mesh_device)

# Stage 3: Runtime execution
run_prefill_config, run_decode_config = MLP1D.run_config(prefill_config, decode_config, weight_config, mesh_device)
output = MLP1D.forward_prefill(input_tensor, run_prefill_config) # or forward_decode(input_tensor, run_decode_config)
```

## Module Requirements
Since the module classes are meant to only exist as namespaces for the module-specific behavior, each method to reimplement for a module is a `classmethod`. Each module should implement:

### `convert_weights(hf_config: transformers.PretrainedConfig, state_dict: dict[str, torch.Tensor], output_path: Path, mesh_device: ttnn.Device) -> WeightConfig`
- Converts PyTorch weights to TTNN format using a standard configuration:
  - dtype: `ttnn.bfloat4_b`
  - layout: `ttnn.TILE_LAYOUT`
  - memory_config: `ttnn.DRAM_MEMORY_CONFIG`
- Saves them to disk at the specified path (e.g., `w1.weight` or `w2.input_tensor_b` if you prefer)
- Handles sharding across devices appropriately (typically column sharding for projections, row sharding for down projections)
- Returns a `WeightConfig` dict mapping operation keyword parameters to their TTNN weight file paths saved using `save_and_get_path` - this MUST match the TTNN argument name, e.g. use ["w1"]["input_tensor_b"] and not ["w1"]["weight"]. See the [API docs](https://docs.tenstorrent.com/tt-metal/latest/ttnn/ttnn/api/ttnn.linear.html#ttnn-linear) for more details.

### `prefill_model_config(hf_config: PretrainedConfig, **kwargs) -> ModelPrefillConfig` and `decode_model_config(hf_config: PretrainedConfig, **kwargs) -> ModelDecodeConfig`
- Generate static operator configurations for prefill/decode mode using dataclasses
- Returns a nested dict with dataclass instances (LinearConfig, MulConfig, etc.) containing TTNN objects
- Can use helper functions like `find_prefill_grid`, `dram_shard_core_grid_for_k`
- Should generate configs that match the standard weight format used by `convert_weights`
- Should use the `FromWeightConfig` and `MeshDeviceStub` for indicating the weight tensors and providing the mesh device used in the `forward_prefill` and `forward_decode` functions (see [the section on `run_config`](#runconfig-creation) for details)

### `forward_prefill(x: ttnn.Tensor, cfg: RunPrefillConfig) -> ttnn.Tensor` and `forward_decode(x: ttnn.Tensor, cfg: RunDecodeConfig) -> ttnn.Tensor`
- Executes the layer using the provided RunConfig
- Uses clean dict expansion: `ttnn.linear(x, **cfg["w1"])`
- **Important**: `OpConfigBase` automatically filters out `None` values during `**cfg` expansion, so `None` means "use TTNN's default" rather than "pass None explicitly"
- For dynamic configs, overrides with keyword arguments: `ttnn.linear(x, program_config=self.w1_pc(**cfg["w1_pc"]), **cfg["w1"])`
- This allows clean mixing of dataclass defaults with explicit overrides without parameter conflicts
- Handles memory management (deallocations)

### (optionally) `_new_state(stateless_run_prefill_config: StatelessRunPrefillConfig, stateless_run_decode_config: StatelessRunDecodeConfig,, mesh_device: ttnn.Device) -> Any`
- Takes the model configs merged with `WeightConfig`, and the mesh_device as parameters
- Good place for setting up non-weight tensors like kv_cache or dynamic program configs

## `RunConfig` Creation
By default, the `run_config(model_prefill_config: ModelPrefillConfig, model_decode_config: ModelDecodeConfig, weight_config: WeightConfig, **kwargs) -> RunConfig` method merges each model config hierarchically over its structure with the `weight_config` and the `ModelState` returned by the `_new_state` method. For given config items:
- if all of them are either the same container (list, tuple, dict) or a None, these containers are unified, and the matching items in these containers are merged using the same procedure
  - if the container type is a dict, the containers are merged by keys
  - if the container type is a list or a tuple, the containers have to be the same length
  - `OpConfigBase` subclasses are treated as dicts here, and re-wrapped in the same dataclass for `RunConfig`
- if the items are not containers, then it is required that only one of them is not None.
- an exception to the above is when the items in the model config and `WeightConfig` are a `FromWeightConfig` and string path respectively. In that case, a tensor is loaded from the path stored in the `WeightConfig` onto the `ttnn.Device` provided as an argument to `run_config(model_config, weight_config, mesh_device)`.
- finally, if the model config item is a `MeshDeviceStub`, it is replaced with the `ttnn.Device` provided as an argument to `run_config(model_prefill_config, model_decode_config, weight_config, mesh_device)`. This is primarily used for the ops that produce a tensor and require a `device` or a `mesh_device` argument.

### Reimplementing `run_config`
Modules might require operating on several mesh devices, e.g. when running MLPs on several submeshes simultaneously. In that case, it is necessary to reimplement the `run_config`, which only takes a single `mesh_device` as an argument and loads the tensors into it without any device selection (same goes for the `MeshDeviceStub`s). In that case, it is recommended that the base `run_config` implementation is used for merging the levels of a config that span a single `ttnn.Device`, and only the higher levels of the config that require dispatching the submodules to different devices are customized.

## Implementation Notes

- Weight files follow a standard naming convention that matches the config keys (e.g., `w1.input_tensor_b` file for `w1` config entries)
- Runtime config is a nested dict with dataclass instances containing loaded tensors
- Each operation gets its own dataclass instance to prevent accidental weight sharing
- Dataclasses inherit from OpConfigBase which provides dict-like access for `**cfg["op"]` unpacking
- **OpConfigBase automatically filters out `None` values**: When using `**cfg["op"]`, only non-None fields are passed to TTNN functions, allowing clean parameter overrides without conflicts
- Modules should be specific rather than overly general. Different architectures or sharding strategies warrant different module implementations
- Validation happens when creating runtime config - ensuring weights exist where expected
- Use dataclasses from `config_dataclass.py` for type safety and better IDE support
- Use the `_new_state` sparingly, e.g. for storing the KV-cache. Putting more model configuration into the model configs allows for more easily interpreting and modifying the model behavior.

## Design Benefits

1. **Type Safety**: Dataclasses provide clear type information and IDE support
2. **Clean Syntax**: Forward passes use `**cfg["op"]` with dataclasses that support dict-like access
3. **Separation of Concerns**: Weight conversion, operator configuration and state creation are independent
4. **Reusability**: Same weights work with different operator configurations
5. **Direct Memory Usage**: No object conversion overhead - configs use TTNN objects directly
6. **Minimal Boilerplate**: Clean forward functions with `**cfg["op"]` expansion
7. **Explicit Weight Mapping**: `WeightConfig` makes it clear which operations have weights and where they're stored
8. **Shared Reference Safety**: `RunPrefillConfig` and `RunDecodeConfig` create new dataclass instances to prevent accidental weight sharing
9. **Better Documentation**: Dataclass fields are self-documenting with type hints
