{
  'norm': {
    'all_gather': AllGatherAsyncConfig(
      dim=3,
      cluster_axis=1,
      mesh_device=MeshDevice(4x8 grid, 32 devices),
      topology=<Topology.Linear: 1>,
      multi_device_global_semaphore=,
        [
                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
              ],
      persistent_output_tensor=None,
      num_links=None,
      memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
      subdevice_id=None,
      use_optimal_ccl_for_llama=None,
      barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
    ),
    'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
      epsilon=1e-06,
      weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
      bias=None,
      memory_config=None,
      program_config=None,
      compute_kernel_config=None,
      dtype=<DataType.BFLOAT16: 0>
    ),
    'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
    'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
      dtype=<DataType.BFLOAT16: 0>,
      residual_input_tensor=None,
      compute_kernel_config=None,
      program_config=None,
      memory_config=None
    ),
  },
  'hf_config': DeepseekV3Config {
  "architectures": [
    "DeepseekV3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_deepseek.DeepseekV3Config",
    "AutoModel": "modeling_deepseek.DeepseekV3Model",
    "AutoModelForCausalLM": "modeling_deepseek.DeepseekV3ForCausalLM"
  },
  "bos_token_id": 0,
  "eos_token_id": 1,
  "ep_size": 1,
  "first_k_dense_replace": 3,
  "hidden_act": "silu",
  "hidden_size": 7168,
  "initializer_range": 0.02,
  "intermediate_size": 18432,
  "kv_lora_rank": 512,
  "max_position_embeddings": 163840,
  "max_seq_len": 4096,
  "model_type": "deepseek_v3",
  "moe_intermediate_size": 2048,
  "moe_layer_freq": 1,
  "n_group": 8,
  "n_routed_experts": 256,
  "n_shared_experts": 1,
  "norm_topk_prob": true,
  "num_attention_heads": 128,
  "num_experts_per_tok": 8,
  "num_hidden_layers": 61,
  "num_key_value_heads": 128,
  "num_nextn_predict_layers": 1,
  "q_lora_rank": 1536,
  "qk_nope_head_dim": 128,
  "qk_rope_head_dim": 64,
  "quantization_config": {
    "activation_scheme": "dynamic",
    "fmt": "e4m3",
    "quant_method": "fp8",
    "weight_block_size": [
      128,
      128
    ]
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 40,
    "mscale": 1.0,
    "mscale_all_dim": 1.0,
    "original_max_position_embeddings": 4096,
    "type": "yarn"
  },
  "rope_theta": 10000,
  "routed_scaling_factor": 2.5,
  "scoring_func": "sigmoid",
  "tie_word_embeddings": false,
  "topk_group": 4,
  "topk_method": "noaux_tc",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "v_head_dim": 128,
  "vocab_size": 129280
}
,
  'embedding': {
    'all_gather': AllGatherAsyncConfig(
      dim=-1,
      cluster_axis=0,
      mesh_device=MeshDevice(4x8 grid, 32 devices),
      topology=<Topology.Linear: 1>,
      multi_device_global_semaphore=,
        [
                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f2123e52b30>,
                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79df70>,
              ],
      persistent_output_tensor=None,
      num_links=1,
      memory_config=None,
      subdevice_id=None,
      use_optimal_ccl_for_llama=None,
      barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e930>
    ),
    'embedding': EmbeddingConfig(
      weight=ttnn.Tensor(shape=Shape([129280, 224]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
      memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
      layout=<Layout.TILE: 1>
    ),
    'typecast': TypecastConfig(
      dtype=<DataType.FLOAT32: 1>,
      memory_config=None,
      sub_core_grids=None
    ),
  },
  'mlp_decoder_block': [
    {
      'mla_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla': {
        'hf_config': DeepseekV3Config {
  "architectures": [
    "DeepseekV3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_deepseek.DeepseekV3Config",
    "AutoModel": "modeling_deepseek.DeepseekV3Model",
    "AutoModelForCausalLM": "modeling_deepseek.DeepseekV3ForCausalLM"
  },
  "bos_token_id": 0,
  "eos_token_id": 1,
  "ep_size": 1,
  "first_k_dense_replace": 3,
  "hidden_act": "silu",
  "hidden_size": 7168,
  "initializer_range": 0.02,
  "intermediate_size": 18432,
  "kv_lora_rank": 512,
  "max_position_embeddings": 163840,
  "max_seq_len": 4096,
  "model_type": "deepseek_v3",
  "moe_intermediate_size": 2048,
  "moe_layer_freq": 1,
  "n_group": 8,
  "n_routed_experts": 256,
  "n_shared_experts": 1,
  "norm_topk_prob": true,
  "num_attention_heads": 128,
  "num_experts_per_tok": 8,
  "num_hidden_layers": 61,
  "num_key_value_heads": 128,
  "num_nextn_predict_layers": 1,
  "q_lora_rank": 1536,
  "qk_nope_head_dim": 128,
  "qk_rope_head_dim": 64,
  "quantization_config": {
    "activation_scheme": "dynamic",
    "fmt": "e4m3",
    "quant_method": "fp8",
    "weight_block_size": [
      128,
      128
    ]
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 40,
    "mscale": 1.0,
    "mscale_all_dim": 1.0,
    "original_max_position_embeddings": 4096,
    "type": "yarn"
  },
  "rope_theta": 10000,
  "routed_scaling_factor": 2.5,
  "scoring_func": "sigmoid",
  "tie_word_embeddings": false,
  "topk_group": 4,
  "topk_method": "noaux_tc",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "v_head_dim": 128,
  "vocab_size": 129280
}
,
        'wo_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'wkv_b2': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 512, 128]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
          ],
          'num_links': 1,
        },
        'kv_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 16, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wq_a_ag_decode': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'flash_mla_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_b': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 1536, 3072]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'flash_mla_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'wq_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=3,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wq_a2a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'q_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 48, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wo': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16384, 896]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 1536]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_b1': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 128, 512]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'wkv_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'kv_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_a2a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wkv_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
          ],
          'num_links': 1,
        },
        'wkv_a_r_decode': {
          'dims': [1],
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'output': None,
        },
        'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        'kvpe_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'flash_mla': {
          'scale': 0.07216878364870322,
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'program_config': SDPAProgramConfig(...),
          'head_dim_v': 512,
          'memory_config': MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
        },
        'wo_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
          ],
          'num_links': 1,
        },
        'kvpe_cache': ttnn.Tensor(shape=Shape([512, 1, 32, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
        'q_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wkv_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'flash_mla_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'kv_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'flash_mla_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'mesh_shape': [4, 8],
        'wq_a_rs_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
          ],
          'num_links': 1,
        },
        'q_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
      },
      'mlp': {
        'w1': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([7168, 2304]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 192]),
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
          program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
        ),
        'mul': MulConfig(
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          input_tensor_a_activations=,
            [
                        <UnaryOpType.SILU: 54>,
                      ]
        ),
        'w2': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([2304, 7168]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[2304, 608]),
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
          program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=1, ...)
        ),
        'all_gather': AllGatherAsyncConfig(
          dim=-1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'reduce_scatter_async': ReduceScatterAsyncMinimalConfig(
          dim=3,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'all_gather_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          dtype=None
        ),
        'output_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'w3': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([7168, 2304]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 192]),
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
          program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
        ),
      },
      'mla_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
      'mlp_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
    },
  ],
  'transfer_row': PointToPointConfig(
    receiver_coord=None,
    sender_coord=None,
    topology=<Topology.Linear: 1>,
    optional_output_tensor=None
  ),
  'moe_decoder_block': [
    {
      'mla_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla': {
        'hf_config': DeepseekV3Config {
  "architectures": [
    "DeepseekV3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_deepseek.DeepseekV3Config",
    "AutoModel": "modeling_deepseek.DeepseekV3Model",
    "AutoModelForCausalLM": "modeling_deepseek.DeepseekV3ForCausalLM"
  },
  "bos_token_id": 0,
  "eos_token_id": 1,
  "ep_size": 1,
  "first_k_dense_replace": 3,
  "hidden_act": "silu",
  "hidden_size": 7168,
  "initializer_range": 0.02,
  "intermediate_size": 18432,
  "kv_lora_rank": 512,
  "max_position_embeddings": 163840,
  "max_seq_len": 4096,
  "model_type": "deepseek_v3",
  "moe_intermediate_size": 2048,
  "moe_layer_freq": 1,
  "n_group": 8,
  "n_routed_experts": 256,
  "n_shared_experts": 1,
  "norm_topk_prob": true,
  "num_attention_heads": 128,
  "num_experts_per_tok": 8,
  "num_hidden_layers": 61,
  "num_key_value_heads": 128,
  "num_nextn_predict_layers": 1,
  "q_lora_rank": 1536,
  "qk_nope_head_dim": 128,
  "qk_rope_head_dim": 64,
  "quantization_config": {
    "activation_scheme": "dynamic",
    "fmt": "e4m3",
    "quant_method": "fp8",
    "weight_block_size": [
      128,
      128
    ]
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 40,
    "mscale": 1.0,
    "mscale_all_dim": 1.0,
    "original_max_position_embeddings": 4096,
    "type": "yarn"
  },
  "rope_theta": 10000,
  "routed_scaling_factor": 2.5,
  "scoring_func": "sigmoid",
  "tie_word_embeddings": false,
  "topk_group": 4,
  "topk_method": "noaux_tc",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "v_head_dim": 128,
  "vocab_size": 129280
}
,
        'wo_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'wkv_b2': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 512, 128]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
          ],
          'num_links': 1,
        },
        'kv_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 16, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wq_a_ag_decode': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'flash_mla_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_b': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 1536, 3072]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'flash_mla_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'wq_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=3,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wq_a2a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'q_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 48, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wo': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16384, 896]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 1536]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_b1': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 128, 512]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'wkv_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'kv_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_a2a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wkv_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
          ],
          'num_links': 1,
        },
        'wkv_a_r_decode': {
          'dims': [1],
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'output': None,
        },
        'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        'kvpe_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'flash_mla': {
          'scale': 0.07216878364870322,
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'program_config': SDPAProgramConfig(...),
          'head_dim_v': 512,
          'memory_config': MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
        },
        'wo_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
          ],
          'num_links': 1,
        },
        'kvpe_cache': ttnn.Tensor(shape=Shape([512, 1, 32, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
        'q_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wkv_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'flash_mla_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'kv_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'flash_mla_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'mesh_shape': [4, 8],
        'wq_a_rs_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
          ],
          'num_links': 1,
        },
        'q_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
      },
      'mlp': {
        'revert_dp': AllGatherAsyncConfig(
          dim=-2,
          cluster_axis=0,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f2123e52b30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79df70>,
                      ],
          persistent_output_tensor=None,
          num_links=3,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'shared_expert': {
          'w1': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
          'mul': MulConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            input_tensor_a_activations=,
              [
                            <UnaryOpType.SILU: 54>,
                          ]
          ),
          'w2': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([256, 7168]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[256, 608]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=1, ...)
          ),
          'all_gather': AllGatherAsyncConfig(
            dim=-1,
            cluster_axis=1,
            mesh_device=MeshDevice(4x8 grid, 32 devices),
            topology=<Topology.Linear: 1>,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                          ],
            persistent_output_tensor=None,
            num_links=1,
            memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            subdevice_id=None,
            use_optimal_ccl_for_llama=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
          ),
          'reduce_scatter_async': ReduceScatterAsyncMinimalConfig(
            dim=3,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                          ],
            num_links=1,
            persistent_output_buffers=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            intermediate_memory_config=None,
            topology=<Topology.Linear: 1>,
            subdevice_id=None,
            cluster_axis=1,
            chunks_per_sync=None,
            num_workers_per_link=None,
            num_buffers_per_channel=None
          ),
          'all_gather_reshard': ReshardConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            dtype=None
          ),
          'output_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          'w3': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
        },
        'apply_dp': {
          'mesh_shape': (4, 8),
          'memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          'cluster_axis': 0,
          'dim': -2,
        },
        'moe': [
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
        ],
      },
      'mla_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
      'mlp_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
    },
    {
      'mla_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla': {
        'hf_config': DeepseekV3Config {
  "architectures": [
    "DeepseekV3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_deepseek.DeepseekV3Config",
    "AutoModel": "modeling_deepseek.DeepseekV3Model",
    "AutoModelForCausalLM": "modeling_deepseek.DeepseekV3ForCausalLM"
  },
  "bos_token_id": 0,
  "eos_token_id": 1,
  "ep_size": 1,
  "first_k_dense_replace": 3,
  "hidden_act": "silu",
  "hidden_size": 7168,
  "initializer_range": 0.02,
  "intermediate_size": 18432,
  "kv_lora_rank": 512,
  "max_position_embeddings": 163840,
  "max_seq_len": 4096,
  "model_type": "deepseek_v3",
  "moe_intermediate_size": 2048,
  "moe_layer_freq": 1,
  "n_group": 8,
  "n_routed_experts": 256,
  "n_shared_experts": 1,
  "norm_topk_prob": true,
  "num_attention_heads": 128,
  "num_experts_per_tok": 8,
  "num_hidden_layers": 61,
  "num_key_value_heads": 128,
  "num_nextn_predict_layers": 1,
  "q_lora_rank": 1536,
  "qk_nope_head_dim": 128,
  "qk_rope_head_dim": 64,
  "quantization_config": {
    "activation_scheme": "dynamic",
    "fmt": "e4m3",
    "quant_method": "fp8",
    "weight_block_size": [
      128,
      128
    ]
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 40,
    "mscale": 1.0,
    "mscale_all_dim": 1.0,
    "original_max_position_embeddings": 4096,
    "type": "yarn"
  },
  "rope_theta": 10000,
  "routed_scaling_factor": 2.5,
  "scoring_func": "sigmoid",
  "tie_word_embeddings": false,
  "topk_group": 4,
  "topk_method": "noaux_tc",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "v_head_dim": 128,
  "vocab_size": 129280
}
,
        'wo_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'wkv_b2': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 512, 128]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
          ],
          'num_links': 1,
        },
        'kv_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 16, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wq_a_ag_decode': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'flash_mla_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_b': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 1536, 3072]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'flash_mla_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'wq_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=3,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wq_a2a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'q_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 48, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wo': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16384, 896]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 1536]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_b1': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 128, 512]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'wkv_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'kv_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_a2a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wkv_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
          ],
          'num_links': 1,
        },
        'wkv_a_r_decode': {
          'dims': [1],
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'output': None,
        },
        'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        'kvpe_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'flash_mla': {
          'scale': 0.07216878364870322,
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'program_config': SDPAProgramConfig(...),
          'head_dim_v': 512,
          'memory_config': MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
        },
        'wo_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
          ],
          'num_links': 1,
        },
        'kvpe_cache': ttnn.Tensor(shape=Shape([512, 1, 32, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
        'q_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wkv_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'flash_mla_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'kv_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'flash_mla_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'mesh_shape': [4, 8],
        'wq_a_rs_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
          ],
          'num_links': 1,
        },
        'q_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
      },
      'mlp': {
        'revert_dp': AllGatherAsyncConfig(
          dim=-2,
          cluster_axis=0,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e730>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e670>,
                      ],
          persistent_output_tensor=None,
          num_links=3,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'shared_expert': {
          'w1': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
          'mul': MulConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            input_tensor_a_activations=,
              [
                            <UnaryOpType.SILU: 54>,
                          ]
          ),
          'w2': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([256, 7168]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[256, 608]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=1, ...)
          ),
          'all_gather': AllGatherAsyncConfig(
            dim=-1,
            cluster_axis=1,
            mesh_device=MeshDevice(4x8 grid, 32 devices),
            topology=<Topology.Linear: 1>,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                          ],
            persistent_output_tensor=None,
            num_links=1,
            memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            subdevice_id=None,
            use_optimal_ccl_for_llama=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
          ),
          'reduce_scatter_async': ReduceScatterAsyncMinimalConfig(
            dim=3,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                          ],
            num_links=1,
            persistent_output_buffers=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            intermediate_memory_config=None,
            topology=<Topology.Linear: 1>,
            subdevice_id=None,
            cluster_axis=1,
            chunks_per_sync=None,
            num_workers_per_link=None,
            num_buffers_per_channel=None
          ),
          'all_gather_reshard': ReshardConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            dtype=None
          ),
          'output_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          'w3': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
        },
        'apply_dp': {
          'mesh_shape': (4, 8),
          'memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          'cluster_axis': 0,
          'dim': -2,
        },
        'moe': [
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
        ],
      },
      'mla_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
      'mlp_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
    },
    {
      'mla_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla': {
        'hf_config': DeepseekV3Config {
  "architectures": [
    "DeepseekV3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_deepseek.DeepseekV3Config",
    "AutoModel": "modeling_deepseek.DeepseekV3Model",
    "AutoModelForCausalLM": "modeling_deepseek.DeepseekV3ForCausalLM"
  },
  "bos_token_id": 0,
  "eos_token_id": 1,
  "ep_size": 1,
  "first_k_dense_replace": 3,
  "hidden_act": "silu",
  "hidden_size": 7168,
  "initializer_range": 0.02,
  "intermediate_size": 18432,
  "kv_lora_rank": 512,
  "max_position_embeddings": 163840,
  "max_seq_len": 4096,
  "model_type": "deepseek_v3",
  "moe_intermediate_size": 2048,
  "moe_layer_freq": 1,
  "n_group": 8,
  "n_routed_experts": 256,
  "n_shared_experts": 1,
  "norm_topk_prob": true,
  "num_attention_heads": 128,
  "num_experts_per_tok": 8,
  "num_hidden_layers": 61,
  "num_key_value_heads": 128,
  "num_nextn_predict_layers": 1,
  "q_lora_rank": 1536,
  "qk_nope_head_dim": 128,
  "qk_rope_head_dim": 64,
  "quantization_config": {
    "activation_scheme": "dynamic",
    "fmt": "e4m3",
    "quant_method": "fp8",
    "weight_block_size": [
      128,
      128
    ]
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 40,
    "mscale": 1.0,
    "mscale_all_dim": 1.0,
    "original_max_position_embeddings": 4096,
    "type": "yarn"
  },
  "rope_theta": 10000,
  "routed_scaling_factor": 2.5,
  "scoring_func": "sigmoid",
  "tie_word_embeddings": false,
  "topk_group": 4,
  "topk_method": "noaux_tc",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "v_head_dim": 128,
  "vocab_size": 129280
}
,
        'wo_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'wkv_b2': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 512, 128]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
          ],
          'num_links': 1,
        },
        'kv_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 16, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wq_a_ag_decode': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'flash_mla_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_b': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 1536, 3072]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'flash_mla_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'wq_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=3,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wq_a2a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'q_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 48, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wo': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16384, 896]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 1536]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_b1': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 128, 512]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'wkv_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'kv_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_a2a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wkv_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
          ],
          'num_links': 1,
        },
        'wkv_a_r_decode': {
          'dims': [1],
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'output': None,
        },
        'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        'kvpe_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'flash_mla': {
          'scale': 0.07216878364870322,
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'program_config': SDPAProgramConfig(...),
          'head_dim_v': 512,
          'memory_config': MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
        },
        'wo_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
          ],
          'num_links': 1,
        },
        'kvpe_cache': ttnn.Tensor(shape=Shape([512, 1, 32, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
        'q_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wkv_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'flash_mla_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'kv_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'flash_mla_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'mesh_shape': [4, 8],
        'wq_a_rs_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
          ],
          'num_links': 1,
        },
        'q_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
      },
      'mlp': {
        'revert_dp': AllGatherAsyncConfig(
          dim=-2,
          cluster_axis=0,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f2123e52b30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79df70>,
                      ],
          persistent_output_tensor=None,
          num_links=3,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'shared_expert': {
          'w1': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
          'mul': MulConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            input_tensor_a_activations=,
              [
                            <UnaryOpType.SILU: 54>,
                          ]
          ),
          'w2': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([256, 7168]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[256, 608]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=1, ...)
          ),
          'all_gather': AllGatherAsyncConfig(
            dim=-1,
            cluster_axis=1,
            mesh_device=MeshDevice(4x8 grid, 32 devices),
            topology=<Topology.Linear: 1>,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                          ],
            persistent_output_tensor=None,
            num_links=1,
            memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            subdevice_id=None,
            use_optimal_ccl_for_llama=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
          ),
          'reduce_scatter_async': ReduceScatterAsyncMinimalConfig(
            dim=3,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                          ],
            num_links=1,
            persistent_output_buffers=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            intermediate_memory_config=None,
            topology=<Topology.Linear: 1>,
            subdevice_id=None,
            cluster_axis=1,
            chunks_per_sync=None,
            num_workers_per_link=None,
            num_buffers_per_channel=None
          ),
          'all_gather_reshard': ReshardConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            dtype=None
          ),
          'output_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          'w3': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
        },
        'apply_dp': {
          'mesh_shape': (4, 8),
          'memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          'cluster_axis': 0,
          'dim': -2,
        },
        'moe': [
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
        ],
      },
      'mla_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
      'mlp_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
    },
    {
      'mla_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla': {
        'hf_config': DeepseekV3Config {
  "architectures": [
    "DeepseekV3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_deepseek.DeepseekV3Config",
    "AutoModel": "modeling_deepseek.DeepseekV3Model",
    "AutoModelForCausalLM": "modeling_deepseek.DeepseekV3ForCausalLM"
  },
  "bos_token_id": 0,
  "eos_token_id": 1,
  "ep_size": 1,
  "first_k_dense_replace": 3,
  "hidden_act": "silu",
  "hidden_size": 7168,
  "initializer_range": 0.02,
  "intermediate_size": 18432,
  "kv_lora_rank": 512,
  "max_position_embeddings": 163840,
  "max_seq_len": 4096,
  "model_type": "deepseek_v3",
  "moe_intermediate_size": 2048,
  "moe_layer_freq": 1,
  "n_group": 8,
  "n_routed_experts": 256,
  "n_shared_experts": 1,
  "norm_topk_prob": true,
  "num_attention_heads": 128,
  "num_experts_per_tok": 8,
  "num_hidden_layers": 61,
  "num_key_value_heads": 128,
  "num_nextn_predict_layers": 1,
  "q_lora_rank": 1536,
  "qk_nope_head_dim": 128,
  "qk_rope_head_dim": 64,
  "quantization_config": {
    "activation_scheme": "dynamic",
    "fmt": "e4m3",
    "quant_method": "fp8",
    "weight_block_size": [
      128,
      128
    ]
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 40,
    "mscale": 1.0,
    "mscale_all_dim": 1.0,
    "original_max_position_embeddings": 4096,
    "type": "yarn"
  },
  "rope_theta": 10000,
  "routed_scaling_factor": 2.5,
  "scoring_func": "sigmoid",
  "tie_word_embeddings": false,
  "topk_group": 4,
  "topk_method": "noaux_tc",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "v_head_dim": 128,
  "vocab_size": 129280
}
,
        'wo_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'wkv_b2': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 512, 128]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
          ],
          'num_links': 1,
        },
        'kv_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 16, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wq_a_ag_decode': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'flash_mla_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_b': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 1536, 3072]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'flash_mla_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'wq_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=3,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wq_a2a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'q_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 48, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wo': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16384, 896]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 1536]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_b1': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 128, 512]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'wkv_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'kv_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_a2a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wkv_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
          ],
          'num_links': 1,
        },
        'wkv_a_r_decode': {
          'dims': [1],
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'output': None,
        },
        'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        'kvpe_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'flash_mla': {
          'scale': 0.07216878364870322,
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'program_config': SDPAProgramConfig(...),
          'head_dim_v': 512,
          'memory_config': MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
        },
        'wo_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
          ],
          'num_links': 1,
        },
        'kvpe_cache': ttnn.Tensor(shape=Shape([512, 1, 32, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
        'q_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wkv_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'flash_mla_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'kv_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'flash_mla_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'mesh_shape': [4, 8],
        'wq_a_rs_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
          ],
          'num_links': 1,
        },
        'q_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
      },
      'mlp': {
        'revert_dp': AllGatherAsyncConfig(
          dim=-2,
          cluster_axis=0,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e730>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e670>,
                      ],
          persistent_output_tensor=None,
          num_links=3,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'shared_expert': {
          'w1': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
          'mul': MulConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            input_tensor_a_activations=,
              [
                            <UnaryOpType.SILU: 54>,
                          ]
          ),
          'w2': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([256, 7168]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[256, 608]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=1, ...)
          ),
          'all_gather': AllGatherAsyncConfig(
            dim=-1,
            cluster_axis=1,
            mesh_device=MeshDevice(4x8 grid, 32 devices),
            topology=<Topology.Linear: 1>,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                          ],
            persistent_output_tensor=None,
            num_links=1,
            memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            subdevice_id=None,
            use_optimal_ccl_for_llama=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
          ),
          'reduce_scatter_async': ReduceScatterAsyncMinimalConfig(
            dim=3,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                          ],
            num_links=1,
            persistent_output_buffers=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            intermediate_memory_config=None,
            topology=<Topology.Linear: 1>,
            subdevice_id=None,
            cluster_axis=1,
            chunks_per_sync=None,
            num_workers_per_link=None,
            num_buffers_per_channel=None
          ),
          'all_gather_reshard': ReshardConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            dtype=None
          ),
          'output_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          'w3': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
        },
        'apply_dp': {
          'mesh_shape': (4, 8),
          'memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          'cluster_axis': 0,
          'dim': -2,
        },
        'moe': [
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
        ],
      },
      'mla_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
      'mlp_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
    },
    {
      'mla_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla': {
        'hf_config': DeepseekV3Config {
  "architectures": [
    "DeepseekV3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_deepseek.DeepseekV3Config",
    "AutoModel": "modeling_deepseek.DeepseekV3Model",
    "AutoModelForCausalLM": "modeling_deepseek.DeepseekV3ForCausalLM"
  },
  "bos_token_id": 0,
  "eos_token_id": 1,
  "ep_size": 1,
  "first_k_dense_replace": 3,
  "hidden_act": "silu",
  "hidden_size": 7168,
  "initializer_range": 0.02,
  "intermediate_size": 18432,
  "kv_lora_rank": 512,
  "max_position_embeddings": 163840,
  "max_seq_len": 4096,
  "model_type": "deepseek_v3",
  "moe_intermediate_size": 2048,
  "moe_layer_freq": 1,
  "n_group": 8,
  "n_routed_experts": 256,
  "n_shared_experts": 1,
  "norm_topk_prob": true,
  "num_attention_heads": 128,
  "num_experts_per_tok": 8,
  "num_hidden_layers": 61,
  "num_key_value_heads": 128,
  "num_nextn_predict_layers": 1,
  "q_lora_rank": 1536,
  "qk_nope_head_dim": 128,
  "qk_rope_head_dim": 64,
  "quantization_config": {
    "activation_scheme": "dynamic",
    "fmt": "e4m3",
    "quant_method": "fp8",
    "weight_block_size": [
      128,
      128
    ]
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 40,
    "mscale": 1.0,
    "mscale_all_dim": 1.0,
    "original_max_position_embeddings": 4096,
    "type": "yarn"
  },
  "rope_theta": 10000,
  "routed_scaling_factor": 2.5,
  "scoring_func": "sigmoid",
  "tie_word_embeddings": false,
  "topk_group": 4,
  "topk_method": "noaux_tc",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "v_head_dim": 128,
  "vocab_size": 129280
}
,
        'wo_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'wkv_b2': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 512, 128]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
          ],
          'num_links': 1,
        },
        'kv_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 16, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wq_a_ag_decode': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'flash_mla_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_b': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 1536, 3072]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'flash_mla_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'wq_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=3,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wq_a2a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'q_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 48, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wo': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16384, 896]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 1536]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_b1': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 128, 512]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'wkv_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'kv_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_a2a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wkv_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
          ],
          'num_links': 1,
        },
        'wkv_a_r_decode': {
          'dims': [1],
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'output': None,
        },
        'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        'kvpe_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'flash_mla': {
          'scale': 0.07216878364870322,
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'program_config': SDPAProgramConfig(...),
          'head_dim_v': 512,
          'memory_config': MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
        },
        'wo_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
          ],
          'num_links': 1,
        },
        'kvpe_cache': ttnn.Tensor(shape=Shape([512, 1, 32, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
        'q_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wkv_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'flash_mla_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'kv_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'flash_mla_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'mesh_shape': [4, 8],
        'wq_a_rs_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
          ],
          'num_links': 1,
        },
        'q_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
      },
      'mlp': {
        'revert_dp': AllGatherAsyncConfig(
          dim=-2,
          cluster_axis=0,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f2123e52b30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79df70>,
                      ],
          persistent_output_tensor=None,
          num_links=3,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'shared_expert': {
          'w1': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
          'mul': MulConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            input_tensor_a_activations=,
              [
                            <UnaryOpType.SILU: 54>,
                          ]
          ),
          'w2': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([256, 7168]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[256, 608]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=1, ...)
          ),
          'all_gather': AllGatherAsyncConfig(
            dim=-1,
            cluster_axis=1,
            mesh_device=MeshDevice(4x8 grid, 32 devices),
            topology=<Topology.Linear: 1>,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                          ],
            persistent_output_tensor=None,
            num_links=1,
            memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            subdevice_id=None,
            use_optimal_ccl_for_llama=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
          ),
          'reduce_scatter_async': ReduceScatterAsyncMinimalConfig(
            dim=3,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                          ],
            num_links=1,
            persistent_output_buffers=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            intermediate_memory_config=None,
            topology=<Topology.Linear: 1>,
            subdevice_id=None,
            cluster_axis=1,
            chunks_per_sync=None,
            num_workers_per_link=None,
            num_buffers_per_channel=None
          ),
          'all_gather_reshard': ReshardConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            dtype=None
          ),
          'output_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          'w3': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
        },
        'apply_dp': {
          'mesh_shape': (4, 8),
          'memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          'cluster_axis': 0,
          'dim': -2,
        },
        'moe': [
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
        ],
      },
      'mla_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
      'mlp_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
    },
    {
      'mla_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla': {
        'hf_config': DeepseekV3Config {
  "architectures": [
    "DeepseekV3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_deepseek.DeepseekV3Config",
    "AutoModel": "modeling_deepseek.DeepseekV3Model",
    "AutoModelForCausalLM": "modeling_deepseek.DeepseekV3ForCausalLM"
  },
  "bos_token_id": 0,
  "eos_token_id": 1,
  "ep_size": 1,
  "first_k_dense_replace": 3,
  "hidden_act": "silu",
  "hidden_size": 7168,
  "initializer_range": 0.02,
  "intermediate_size": 18432,
  "kv_lora_rank": 512,
  "max_position_embeddings": 163840,
  "max_seq_len": 4096,
  "model_type": "deepseek_v3",
  "moe_intermediate_size": 2048,
  "moe_layer_freq": 1,
  "n_group": 8,
  "n_routed_experts": 256,
  "n_shared_experts": 1,
  "norm_topk_prob": true,
  "num_attention_heads": 128,
  "num_experts_per_tok": 8,
  "num_hidden_layers": 61,
  "num_key_value_heads": 128,
  "num_nextn_predict_layers": 1,
  "q_lora_rank": 1536,
  "qk_nope_head_dim": 128,
  "qk_rope_head_dim": 64,
  "quantization_config": {
    "activation_scheme": "dynamic",
    "fmt": "e4m3",
    "quant_method": "fp8",
    "weight_block_size": [
      128,
      128
    ]
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 40,
    "mscale": 1.0,
    "mscale_all_dim": 1.0,
    "original_max_position_embeddings": 4096,
    "type": "yarn"
  },
  "rope_theta": 10000,
  "routed_scaling_factor": 2.5,
  "scoring_func": "sigmoid",
  "tie_word_embeddings": false,
  "topk_group": 4,
  "topk_method": "noaux_tc",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "v_head_dim": 128,
  "vocab_size": 129280
}
,
        'wo_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'wkv_b2': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 512, 128]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
          ],
          'num_links': 1,
        },
        'kv_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 16, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wq_a_ag_decode': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'flash_mla_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_b': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 1536, 3072]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'flash_mla_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'wq_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=3,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wq_a2a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'q_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 48, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wo': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16384, 896]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 1536]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_b1': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 128, 512]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'wkv_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'kv_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_a2a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wkv_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
          ],
          'num_links': 1,
        },
        'wkv_a_r_decode': {
          'dims': [1],
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'output': None,
        },
        'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        'kvpe_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'flash_mla': {
          'scale': 0.07216878364870322,
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'program_config': SDPAProgramConfig(...),
          'head_dim_v': 512,
          'memory_config': MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
        },
        'wo_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
          ],
          'num_links': 1,
        },
        'kvpe_cache': ttnn.Tensor(shape=Shape([512, 1, 32, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
        'q_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wkv_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'flash_mla_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'kv_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'flash_mla_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'mesh_shape': [4, 8],
        'wq_a_rs_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
          ],
          'num_links': 1,
        },
        'q_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
      },
      'mlp': {
        'revert_dp': AllGatherAsyncConfig(
          dim=-2,
          cluster_axis=0,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e730>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e670>,
                      ],
          persistent_output_tensor=None,
          num_links=3,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'shared_expert': {
          'w1': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
          'mul': MulConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            input_tensor_a_activations=,
              [
                            <UnaryOpType.SILU: 54>,
                          ]
          ),
          'w2': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([256, 7168]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[256, 608]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=1, ...)
          ),
          'all_gather': AllGatherAsyncConfig(
            dim=-1,
            cluster_axis=1,
            mesh_device=MeshDevice(4x8 grid, 32 devices),
            topology=<Topology.Linear: 1>,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                          ],
            persistent_output_tensor=None,
            num_links=1,
            memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            subdevice_id=None,
            use_optimal_ccl_for_llama=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
          ),
          'reduce_scatter_async': ReduceScatterAsyncMinimalConfig(
            dim=3,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                          ],
            num_links=1,
            persistent_output_buffers=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            intermediate_memory_config=None,
            topology=<Topology.Linear: 1>,
            subdevice_id=None,
            cluster_axis=1,
            chunks_per_sync=None,
            num_workers_per_link=None,
            num_buffers_per_channel=None
          ),
          'all_gather_reshard': ReshardConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            dtype=None
          ),
          'output_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          'w3': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
        },
        'apply_dp': {
          'mesh_shape': (4, 8),
          'memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          'cluster_axis': 0,
          'dim': -2,
        },
        'moe': [
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
        ],
      },
      'mla_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
      'mlp_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
    },
    {
      'mla_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla': {
        'hf_config': DeepseekV3Config {
  "architectures": [
    "DeepseekV3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_deepseek.DeepseekV3Config",
    "AutoModel": "modeling_deepseek.DeepseekV3Model",
    "AutoModelForCausalLM": "modeling_deepseek.DeepseekV3ForCausalLM"
  },
  "bos_token_id": 0,
  "eos_token_id": 1,
  "ep_size": 1,
  "first_k_dense_replace": 3,
  "hidden_act": "silu",
  "hidden_size": 7168,
  "initializer_range": 0.02,
  "intermediate_size": 18432,
  "kv_lora_rank": 512,
  "max_position_embeddings": 163840,
  "max_seq_len": 4096,
  "model_type": "deepseek_v3",
  "moe_intermediate_size": 2048,
  "moe_layer_freq": 1,
  "n_group": 8,
  "n_routed_experts": 256,
  "n_shared_experts": 1,
  "norm_topk_prob": true,
  "num_attention_heads": 128,
  "num_experts_per_tok": 8,
  "num_hidden_layers": 61,
  "num_key_value_heads": 128,
  "num_nextn_predict_layers": 1,
  "q_lora_rank": 1536,
  "qk_nope_head_dim": 128,
  "qk_rope_head_dim": 64,
  "quantization_config": {
    "activation_scheme": "dynamic",
    "fmt": "e4m3",
    "quant_method": "fp8",
    "weight_block_size": [
      128,
      128
    ]
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 40,
    "mscale": 1.0,
    "mscale_all_dim": 1.0,
    "original_max_position_embeddings": 4096,
    "type": "yarn"
  },
  "rope_theta": 10000,
  "routed_scaling_factor": 2.5,
  "scoring_func": "sigmoid",
  "tie_word_embeddings": false,
  "topk_group": 4,
  "topk_method": "noaux_tc",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "v_head_dim": 128,
  "vocab_size": 129280
}
,
        'wo_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'wkv_b2': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 512, 128]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
          ],
          'num_links': 1,
        },
        'kv_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 16, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wq_a_ag_decode': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'flash_mla_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_b': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 1536, 3072]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'flash_mla_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'wq_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=3,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wq_a2a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'q_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 48, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wo': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16384, 896]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 1536]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_b1': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 128, 512]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'wkv_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'kv_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_a2a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wkv_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
          ],
          'num_links': 1,
        },
        'wkv_a_r_decode': {
          'dims': [1],
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'output': None,
        },
        'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        'kvpe_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'flash_mla': {
          'scale': 0.07216878364870322,
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'program_config': SDPAProgramConfig(...),
          'head_dim_v': 512,
          'memory_config': MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
        },
        'wo_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
          ],
          'num_links': 1,
        },
        'kvpe_cache': ttnn.Tensor(shape=Shape([512, 1, 32, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
        'q_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wkv_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'flash_mla_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'kv_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'flash_mla_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'mesh_shape': [4, 8],
        'wq_a_rs_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
          ],
          'num_links': 1,
        },
        'q_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
      },
      'mlp': {
        'revert_dp': AllGatherAsyncConfig(
          dim=-2,
          cluster_axis=0,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f2123e52b30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79df70>,
                      ],
          persistent_output_tensor=None,
          num_links=3,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'shared_expert': {
          'w1': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
          'mul': MulConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            input_tensor_a_activations=,
              [
                            <UnaryOpType.SILU: 54>,
                          ]
          ),
          'w2': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([256, 7168]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[256, 608]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=1, ...)
          ),
          'all_gather': AllGatherAsyncConfig(
            dim=-1,
            cluster_axis=1,
            mesh_device=MeshDevice(4x8 grid, 32 devices),
            topology=<Topology.Linear: 1>,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                          ],
            persistent_output_tensor=None,
            num_links=1,
            memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            subdevice_id=None,
            use_optimal_ccl_for_llama=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
          ),
          'reduce_scatter_async': ReduceScatterAsyncMinimalConfig(
            dim=3,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                          ],
            num_links=1,
            persistent_output_buffers=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            intermediate_memory_config=None,
            topology=<Topology.Linear: 1>,
            subdevice_id=None,
            cluster_axis=1,
            chunks_per_sync=None,
            num_workers_per_link=None,
            num_buffers_per_channel=None
          ),
          'all_gather_reshard': ReshardConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            dtype=None
          ),
          'output_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          'w3': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
        },
        'apply_dp': {
          'mesh_shape': (4, 8),
          'memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          'cluster_axis': 0,
          'dim': -2,
        },
        'moe': [
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
        ],
      },
      'mla_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
      'mlp_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
    },
    {
      'mla_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla': {
        'hf_config': DeepseekV3Config {
  "architectures": [
    "DeepseekV3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_deepseek.DeepseekV3Config",
    "AutoModel": "modeling_deepseek.DeepseekV3Model",
    "AutoModelForCausalLM": "modeling_deepseek.DeepseekV3ForCausalLM"
  },
  "bos_token_id": 0,
  "eos_token_id": 1,
  "ep_size": 1,
  "first_k_dense_replace": 3,
  "hidden_act": "silu",
  "hidden_size": 7168,
  "initializer_range": 0.02,
  "intermediate_size": 18432,
  "kv_lora_rank": 512,
  "max_position_embeddings": 163840,
  "max_seq_len": 4096,
  "model_type": "deepseek_v3",
  "moe_intermediate_size": 2048,
  "moe_layer_freq": 1,
  "n_group": 8,
  "n_routed_experts": 256,
  "n_shared_experts": 1,
  "norm_topk_prob": true,
  "num_attention_heads": 128,
  "num_experts_per_tok": 8,
  "num_hidden_layers": 61,
  "num_key_value_heads": 128,
  "num_nextn_predict_layers": 1,
  "q_lora_rank": 1536,
  "qk_nope_head_dim": 128,
  "qk_rope_head_dim": 64,
  "quantization_config": {
    "activation_scheme": "dynamic",
    "fmt": "e4m3",
    "quant_method": "fp8",
    "weight_block_size": [
      128,
      128
    ]
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 40,
    "mscale": 1.0,
    "mscale_all_dim": 1.0,
    "original_max_position_embeddings": 4096,
    "type": "yarn"
  },
  "rope_theta": 10000,
  "routed_scaling_factor": 2.5,
  "scoring_func": "sigmoid",
  "tie_word_embeddings": false,
  "topk_group": 4,
  "topk_method": "noaux_tc",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "v_head_dim": 128,
  "vocab_size": 129280
}
,
        'wo_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'wkv_b2': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 512, 128]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
          ],
          'num_links': 1,
        },
        'kv_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 16, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wq_a_ag_decode': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'flash_mla_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_b': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 1536, 3072]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'flash_mla_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'wq_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=3,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wq_a2a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'q_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 48, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wo': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16384, 896]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 1536]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_b1': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 128, 512]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'wkv_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'kv_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_a2a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wkv_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
          ],
          'num_links': 1,
        },
        'wkv_a_r_decode': {
          'dims': [1],
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'output': None,
        },
        'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        'kvpe_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'flash_mla': {
          'scale': 0.07216878364870322,
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'program_config': SDPAProgramConfig(...),
          'head_dim_v': 512,
          'memory_config': MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
        },
        'wo_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
          ],
          'num_links': 1,
        },
        'kvpe_cache': ttnn.Tensor(shape=Shape([512, 1, 32, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
        'q_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wkv_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'flash_mla_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'kv_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'flash_mla_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'mesh_shape': [4, 8],
        'wq_a_rs_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
          ],
          'num_links': 1,
        },
        'q_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
      },
      'mlp': {
        'revert_dp': AllGatherAsyncConfig(
          dim=-2,
          cluster_axis=0,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e730>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e670>,
                      ],
          persistent_output_tensor=None,
          num_links=3,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'shared_expert': {
          'w1': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
          'mul': MulConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            input_tensor_a_activations=,
              [
                            <UnaryOpType.SILU: 54>,
                          ]
          ),
          'w2': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([256, 7168]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[256, 608]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=1, ...)
          ),
          'all_gather': AllGatherAsyncConfig(
            dim=-1,
            cluster_axis=1,
            mesh_device=MeshDevice(4x8 grid, 32 devices),
            topology=<Topology.Linear: 1>,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                          ],
            persistent_output_tensor=None,
            num_links=1,
            memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            subdevice_id=None,
            use_optimal_ccl_for_llama=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
          ),
          'reduce_scatter_async': ReduceScatterAsyncMinimalConfig(
            dim=3,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                          ],
            num_links=1,
            persistent_output_buffers=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            intermediate_memory_config=None,
            topology=<Topology.Linear: 1>,
            subdevice_id=None,
            cluster_axis=1,
            chunks_per_sync=None,
            num_workers_per_link=None,
            num_buffers_per_channel=None
          ),
          'all_gather_reshard': ReshardConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            dtype=None
          ),
          'output_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          'w3': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
        },
        'apply_dp': {
          'mesh_shape': (4, 8),
          'memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          'cluster_axis': 0,
          'dim': -2,
        },
        'moe': [
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
        ],
      },
      'mla_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
      'mlp_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
    },
    {
      'mla_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla': {
        'hf_config': DeepseekV3Config {
  "architectures": [
    "DeepseekV3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_deepseek.DeepseekV3Config",
    "AutoModel": "modeling_deepseek.DeepseekV3Model",
    "AutoModelForCausalLM": "modeling_deepseek.DeepseekV3ForCausalLM"
  },
  "bos_token_id": 0,
  "eos_token_id": 1,
  "ep_size": 1,
  "first_k_dense_replace": 3,
  "hidden_act": "silu",
  "hidden_size": 7168,
  "initializer_range": 0.02,
  "intermediate_size": 18432,
  "kv_lora_rank": 512,
  "max_position_embeddings": 163840,
  "max_seq_len": 4096,
  "model_type": "deepseek_v3",
  "moe_intermediate_size": 2048,
  "moe_layer_freq": 1,
  "n_group": 8,
  "n_routed_experts": 256,
  "n_shared_experts": 1,
  "norm_topk_prob": true,
  "num_attention_heads": 128,
  "num_experts_per_tok": 8,
  "num_hidden_layers": 61,
  "num_key_value_heads": 128,
  "num_nextn_predict_layers": 1,
  "q_lora_rank": 1536,
  "qk_nope_head_dim": 128,
  "qk_rope_head_dim": 64,
  "quantization_config": {
    "activation_scheme": "dynamic",
    "fmt": "e4m3",
    "quant_method": "fp8",
    "weight_block_size": [
      128,
      128
    ]
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 40,
    "mscale": 1.0,
    "mscale_all_dim": 1.0,
    "original_max_position_embeddings": 4096,
    "type": "yarn"
  },
  "rope_theta": 10000,
  "routed_scaling_factor": 2.5,
  "scoring_func": "sigmoid",
  "tie_word_embeddings": false,
  "topk_group": 4,
  "topk_method": "noaux_tc",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "v_head_dim": 128,
  "vocab_size": 129280
}
,
        'wo_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'wkv_b2': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 512, 128]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
          ],
          'num_links': 1,
        },
        'kv_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 16, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wq_a_ag_decode': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'flash_mla_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_b': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 1536, 3072]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'flash_mla_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'wq_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=3,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wq_a2a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'q_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 48, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wo': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16384, 896]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 1536]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_b1': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 128, 512]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'wkv_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'kv_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_a2a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wkv_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
          ],
          'num_links': 1,
        },
        'wkv_a_r_decode': {
          'dims': [1],
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'output': None,
        },
        'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        'kvpe_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'flash_mla': {
          'scale': 0.07216878364870322,
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'program_config': SDPAProgramConfig(...),
          'head_dim_v': 512,
          'memory_config': MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
        },
        'wo_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
          ],
          'num_links': 1,
        },
        'kvpe_cache': ttnn.Tensor(shape=Shape([512, 1, 32, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
        'q_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wkv_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'flash_mla_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'kv_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'flash_mla_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'mesh_shape': [4, 8],
        'wq_a_rs_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
          ],
          'num_links': 1,
        },
        'q_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
      },
      'mlp': {
        'revert_dp': AllGatherAsyncConfig(
          dim=-2,
          cluster_axis=0,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f2123e52b30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79df70>,
                      ],
          persistent_output_tensor=None,
          num_links=3,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'shared_expert': {
          'w1': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
          'mul': MulConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            input_tensor_a_activations=,
              [
                            <UnaryOpType.SILU: 54>,
                          ]
          ),
          'w2': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([256, 7168]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[256, 608]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=1, ...)
          ),
          'all_gather': AllGatherAsyncConfig(
            dim=-1,
            cluster_axis=1,
            mesh_device=MeshDevice(4x8 grid, 32 devices),
            topology=<Topology.Linear: 1>,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                          ],
            persistent_output_tensor=None,
            num_links=1,
            memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            subdevice_id=None,
            use_optimal_ccl_for_llama=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
          ),
          'reduce_scatter_async': ReduceScatterAsyncMinimalConfig(
            dim=3,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                          ],
            num_links=1,
            persistent_output_buffers=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            intermediate_memory_config=None,
            topology=<Topology.Linear: 1>,
            subdevice_id=None,
            cluster_axis=1,
            chunks_per_sync=None,
            num_workers_per_link=None,
            num_buffers_per_channel=None
          ),
          'all_gather_reshard': ReshardConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            dtype=None
          ),
          'output_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          'w3': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
        },
        'apply_dp': {
          'mesh_shape': (4, 8),
          'memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          'cluster_axis': 0,
          'dim': -2,
        },
        'moe': [
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
        ],
      },
      'mla_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
      'mlp_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
    },
    {
      'mla_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla': {
        'hf_config': DeepseekV3Config {
  "architectures": [
    "DeepseekV3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_deepseek.DeepseekV3Config",
    "AutoModel": "modeling_deepseek.DeepseekV3Model",
    "AutoModelForCausalLM": "modeling_deepseek.DeepseekV3ForCausalLM"
  },
  "bos_token_id": 0,
  "eos_token_id": 1,
  "ep_size": 1,
  "first_k_dense_replace": 3,
  "hidden_act": "silu",
  "hidden_size": 7168,
  "initializer_range": 0.02,
  "intermediate_size": 18432,
  "kv_lora_rank": 512,
  "max_position_embeddings": 163840,
  "max_seq_len": 4096,
  "model_type": "deepseek_v3",
  "moe_intermediate_size": 2048,
  "moe_layer_freq": 1,
  "n_group": 8,
  "n_routed_experts": 256,
  "n_shared_experts": 1,
  "norm_topk_prob": true,
  "num_attention_heads": 128,
  "num_experts_per_tok": 8,
  "num_hidden_layers": 61,
  "num_key_value_heads": 128,
  "num_nextn_predict_layers": 1,
  "q_lora_rank": 1536,
  "qk_nope_head_dim": 128,
  "qk_rope_head_dim": 64,
  "quantization_config": {
    "activation_scheme": "dynamic",
    "fmt": "e4m3",
    "quant_method": "fp8",
    "weight_block_size": [
      128,
      128
    ]
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 40,
    "mscale": 1.0,
    "mscale_all_dim": 1.0,
    "original_max_position_embeddings": 4096,
    "type": "yarn"
  },
  "rope_theta": 10000,
  "routed_scaling_factor": 2.5,
  "scoring_func": "sigmoid",
  "tie_word_embeddings": false,
  "topk_group": 4,
  "topk_method": "noaux_tc",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "v_head_dim": 128,
  "vocab_size": 129280
}
,
        'wo_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'wkv_b2': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 512, 128]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
          ],
          'num_links': 1,
        },
        'kv_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 16, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wq_a_ag_decode': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'flash_mla_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_b': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 1536, 3072]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'flash_mla_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'wq_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=3,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wq_a2a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'q_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 48, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wo': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16384, 896]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 1536]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_b1': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 128, 512]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'wkv_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'kv_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_a2a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wkv_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
          ],
          'num_links': 1,
        },
        'wkv_a_r_decode': {
          'dims': [1],
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'output': None,
        },
        'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        'kvpe_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'flash_mla': {
          'scale': 0.07216878364870322,
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'program_config': SDPAProgramConfig(...),
          'head_dim_v': 512,
          'memory_config': MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
        },
        'wo_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
          ],
          'num_links': 1,
        },
        'kvpe_cache': ttnn.Tensor(shape=Shape([512, 1, 32, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
        'q_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wkv_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'flash_mla_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'kv_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'flash_mla_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'mesh_shape': [4, 8],
        'wq_a_rs_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
          ],
          'num_links': 1,
        },
        'q_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
      },
      'mlp': {
        'revert_dp': AllGatherAsyncConfig(
          dim=-2,
          cluster_axis=0,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e730>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e670>,
                      ],
          persistent_output_tensor=None,
          num_links=3,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'shared_expert': {
          'w1': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
          'mul': MulConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            input_tensor_a_activations=,
              [
                            <UnaryOpType.SILU: 54>,
                          ]
          ),
          'w2': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([256, 7168]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[256, 608]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=1, ...)
          ),
          'all_gather': AllGatherAsyncConfig(
            dim=-1,
            cluster_axis=1,
            mesh_device=MeshDevice(4x8 grid, 32 devices),
            topology=<Topology.Linear: 1>,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                          ],
            persistent_output_tensor=None,
            num_links=1,
            memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            subdevice_id=None,
            use_optimal_ccl_for_llama=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
          ),
          'reduce_scatter_async': ReduceScatterAsyncMinimalConfig(
            dim=3,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                          ],
            num_links=1,
            persistent_output_buffers=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            intermediate_memory_config=None,
            topology=<Topology.Linear: 1>,
            subdevice_id=None,
            cluster_axis=1,
            chunks_per_sync=None,
            num_workers_per_link=None,
            num_buffers_per_channel=None
          ),
          'all_gather_reshard': ReshardConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            dtype=None
          ),
          'output_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          'w3': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
        },
        'apply_dp': {
          'mesh_shape': (4, 8),
          'memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          'cluster_axis': 0,
          'dim': -2,
        },
        'moe': [
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
        ],
      },
      'mla_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
      'mlp_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
    },
    {
      'mla_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla': {
        'hf_config': DeepseekV3Config {
  "architectures": [
    "DeepseekV3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_deepseek.DeepseekV3Config",
    "AutoModel": "modeling_deepseek.DeepseekV3Model",
    "AutoModelForCausalLM": "modeling_deepseek.DeepseekV3ForCausalLM"
  },
  "bos_token_id": 0,
  "eos_token_id": 1,
  "ep_size": 1,
  "first_k_dense_replace": 3,
  "hidden_act": "silu",
  "hidden_size": 7168,
  "initializer_range": 0.02,
  "intermediate_size": 18432,
  "kv_lora_rank": 512,
  "max_position_embeddings": 163840,
  "max_seq_len": 4096,
  "model_type": "deepseek_v3",
  "moe_intermediate_size": 2048,
  "moe_layer_freq": 1,
  "n_group": 8,
  "n_routed_experts": 256,
  "n_shared_experts": 1,
  "norm_topk_prob": true,
  "num_attention_heads": 128,
  "num_experts_per_tok": 8,
  "num_hidden_layers": 61,
  "num_key_value_heads": 128,
  "num_nextn_predict_layers": 1,
  "q_lora_rank": 1536,
  "qk_nope_head_dim": 128,
  "qk_rope_head_dim": 64,
  "quantization_config": {
    "activation_scheme": "dynamic",
    "fmt": "e4m3",
    "quant_method": "fp8",
    "weight_block_size": [
      128,
      128
    ]
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 40,
    "mscale": 1.0,
    "mscale_all_dim": 1.0,
    "original_max_position_embeddings": 4096,
    "type": "yarn"
  },
  "rope_theta": 10000,
  "routed_scaling_factor": 2.5,
  "scoring_func": "sigmoid",
  "tie_word_embeddings": false,
  "topk_group": 4,
  "topk_method": "noaux_tc",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "v_head_dim": 128,
  "vocab_size": 129280
}
,
        'wo_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'wkv_b2': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 512, 128]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
          ],
          'num_links': 1,
        },
        'kv_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 16, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wq_a_ag_decode': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'flash_mla_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_b': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 1536, 3072]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'flash_mla_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'wq_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=3,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wq_a2a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'q_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 48, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wo': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16384, 896]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 1536]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_b1': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 128, 512]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'wkv_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'kv_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_a2a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wkv_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
          ],
          'num_links': 1,
        },
        'wkv_a_r_decode': {
          'dims': [1],
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'output': None,
        },
        'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        'kvpe_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'flash_mla': {
          'scale': 0.07216878364870322,
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'program_config': SDPAProgramConfig(...),
          'head_dim_v': 512,
          'memory_config': MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
        },
        'wo_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
          ],
          'num_links': 1,
        },
        'kvpe_cache': ttnn.Tensor(shape=Shape([512, 1, 32, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
        'q_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wkv_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'flash_mla_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'kv_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'flash_mla_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'mesh_shape': [4, 8],
        'wq_a_rs_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
          ],
          'num_links': 1,
        },
        'q_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
      },
      'mlp': {
        'revert_dp': AllGatherAsyncConfig(
          dim=-2,
          cluster_axis=0,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f2123e52b30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79df70>,
                      ],
          persistent_output_tensor=None,
          num_links=3,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'shared_expert': {
          'w1': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
          'mul': MulConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            input_tensor_a_activations=,
              [
                            <UnaryOpType.SILU: 54>,
                          ]
          ),
          'w2': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([256, 7168]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[256, 608]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=1, ...)
          ),
          'all_gather': AllGatherAsyncConfig(
            dim=-1,
            cluster_axis=1,
            mesh_device=MeshDevice(4x8 grid, 32 devices),
            topology=<Topology.Linear: 1>,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                          ],
            persistent_output_tensor=None,
            num_links=1,
            memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            subdevice_id=None,
            use_optimal_ccl_for_llama=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
          ),
          'reduce_scatter_async': ReduceScatterAsyncMinimalConfig(
            dim=3,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                          ],
            num_links=1,
            persistent_output_buffers=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            intermediate_memory_config=None,
            topology=<Topology.Linear: 1>,
            subdevice_id=None,
            cluster_axis=1,
            chunks_per_sync=None,
            num_workers_per_link=None,
            num_buffers_per_channel=None
          ),
          'all_gather_reshard': ReshardConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            dtype=None
          ),
          'output_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          'w3': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
        },
        'apply_dp': {
          'mesh_shape': (4, 8),
          'memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          'cluster_axis': 0,
          'dim': -2,
        },
        'moe': [
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
        ],
      },
      'mla_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
      'mlp_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
    },
    {
      'mla_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla': {
        'hf_config': DeepseekV3Config {
  "architectures": [
    "DeepseekV3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_deepseek.DeepseekV3Config",
    "AutoModel": "modeling_deepseek.DeepseekV3Model",
    "AutoModelForCausalLM": "modeling_deepseek.DeepseekV3ForCausalLM"
  },
  "bos_token_id": 0,
  "eos_token_id": 1,
  "ep_size": 1,
  "first_k_dense_replace": 3,
  "hidden_act": "silu",
  "hidden_size": 7168,
  "initializer_range": 0.02,
  "intermediate_size": 18432,
  "kv_lora_rank": 512,
  "max_position_embeddings": 163840,
  "max_seq_len": 4096,
  "model_type": "deepseek_v3",
  "moe_intermediate_size": 2048,
  "moe_layer_freq": 1,
  "n_group": 8,
  "n_routed_experts": 256,
  "n_shared_experts": 1,
  "norm_topk_prob": true,
  "num_attention_heads": 128,
  "num_experts_per_tok": 8,
  "num_hidden_layers": 61,
  "num_key_value_heads": 128,
  "num_nextn_predict_layers": 1,
  "q_lora_rank": 1536,
  "qk_nope_head_dim": 128,
  "qk_rope_head_dim": 64,
  "quantization_config": {
    "activation_scheme": "dynamic",
    "fmt": "e4m3",
    "quant_method": "fp8",
    "weight_block_size": [
      128,
      128
    ]
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 40,
    "mscale": 1.0,
    "mscale_all_dim": 1.0,
    "original_max_position_embeddings": 4096,
    "type": "yarn"
  },
  "rope_theta": 10000,
  "routed_scaling_factor": 2.5,
  "scoring_func": "sigmoid",
  "tie_word_embeddings": false,
  "topk_group": 4,
  "topk_method": "noaux_tc",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "v_head_dim": 128,
  "vocab_size": 129280
}
,
        'wo_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'wkv_b2': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 512, 128]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
          ],
          'num_links': 1,
        },
        'kv_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 16, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wq_a_ag_decode': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'flash_mla_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_b': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 1536, 3072]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'flash_mla_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'wq_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=3,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wq_a2a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'q_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 48, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wo': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16384, 896]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 1536]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_b1': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 128, 512]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'wkv_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'kv_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_a2a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wkv_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
          ],
          'num_links': 1,
        },
        'wkv_a_r_decode': {
          'dims': [1],
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'output': None,
        },
        'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        'kvpe_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'flash_mla': {
          'scale': 0.07216878364870322,
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'program_config': SDPAProgramConfig(...),
          'head_dim_v': 512,
          'memory_config': MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
        },
        'wo_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
          ],
          'num_links': 1,
        },
        'kvpe_cache': ttnn.Tensor(shape=Shape([512, 1, 32, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
        'q_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wkv_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'flash_mla_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'kv_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'flash_mla_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'mesh_shape': [4, 8],
        'wq_a_rs_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
          ],
          'num_links': 1,
        },
        'q_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
      },
      'mlp': {
        'revert_dp': AllGatherAsyncConfig(
          dim=-2,
          cluster_axis=0,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e730>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e670>,
                      ],
          persistent_output_tensor=None,
          num_links=3,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'shared_expert': {
          'w1': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
          'mul': MulConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            input_tensor_a_activations=,
              [
                            <UnaryOpType.SILU: 54>,
                          ]
          ),
          'w2': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([256, 7168]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[256, 608]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=1, ...)
          ),
          'all_gather': AllGatherAsyncConfig(
            dim=-1,
            cluster_axis=1,
            mesh_device=MeshDevice(4x8 grid, 32 devices),
            topology=<Topology.Linear: 1>,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                          ],
            persistent_output_tensor=None,
            num_links=1,
            memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            subdevice_id=None,
            use_optimal_ccl_for_llama=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
          ),
          'reduce_scatter_async': ReduceScatterAsyncMinimalConfig(
            dim=3,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                          ],
            num_links=1,
            persistent_output_buffers=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            intermediate_memory_config=None,
            topology=<Topology.Linear: 1>,
            subdevice_id=None,
            cluster_axis=1,
            chunks_per_sync=None,
            num_workers_per_link=None,
            num_buffers_per_channel=None
          ),
          'all_gather_reshard': ReshardConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            dtype=None
          ),
          'output_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          'w3': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
        },
        'apply_dp': {
          'mesh_shape': (4, 8),
          'memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          'cluster_axis': 0,
          'dim': -2,
        },
        'moe': [
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
        ],
      },
      'mla_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
      'mlp_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
    },
    {
      'mla_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla': {
        'hf_config': DeepseekV3Config {
  "architectures": [
    "DeepseekV3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_deepseek.DeepseekV3Config",
    "AutoModel": "modeling_deepseek.DeepseekV3Model",
    "AutoModelForCausalLM": "modeling_deepseek.DeepseekV3ForCausalLM"
  },
  "bos_token_id": 0,
  "eos_token_id": 1,
  "ep_size": 1,
  "first_k_dense_replace": 3,
  "hidden_act": "silu",
  "hidden_size": 7168,
  "initializer_range": 0.02,
  "intermediate_size": 18432,
  "kv_lora_rank": 512,
  "max_position_embeddings": 163840,
  "max_seq_len": 4096,
  "model_type": "deepseek_v3",
  "moe_intermediate_size": 2048,
  "moe_layer_freq": 1,
  "n_group": 8,
  "n_routed_experts": 256,
  "n_shared_experts": 1,
  "norm_topk_prob": true,
  "num_attention_heads": 128,
  "num_experts_per_tok": 8,
  "num_hidden_layers": 61,
  "num_key_value_heads": 128,
  "num_nextn_predict_layers": 1,
  "q_lora_rank": 1536,
  "qk_nope_head_dim": 128,
  "qk_rope_head_dim": 64,
  "quantization_config": {
    "activation_scheme": "dynamic",
    "fmt": "e4m3",
    "quant_method": "fp8",
    "weight_block_size": [
      128,
      128
    ]
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 40,
    "mscale": 1.0,
    "mscale_all_dim": 1.0,
    "original_max_position_embeddings": 4096,
    "type": "yarn"
  },
  "rope_theta": 10000,
  "routed_scaling_factor": 2.5,
  "scoring_func": "sigmoid",
  "tie_word_embeddings": false,
  "topk_group": 4,
  "topk_method": "noaux_tc",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "v_head_dim": 128,
  "vocab_size": 129280
}
,
        'wo_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'wkv_b2': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 512, 128]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
          ],
          'num_links': 1,
        },
        'kv_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 16, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wq_a_ag_decode': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'flash_mla_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_b': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 1536, 3072]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'flash_mla_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'wq_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=3,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wq_a2a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'q_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 48, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wo': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16384, 896]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 1536]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_b1': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 128, 512]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'wkv_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'kv_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_a2a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wkv_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
          ],
          'num_links': 1,
        },
        'wkv_a_r_decode': {
          'dims': [1],
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'output': None,
        },
        'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        'kvpe_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'flash_mla': {
          'scale': 0.07216878364870322,
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'program_config': SDPAProgramConfig(...),
          'head_dim_v': 512,
          'memory_config': MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
        },
        'wo_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
          ],
          'num_links': 1,
        },
        'kvpe_cache': ttnn.Tensor(shape=Shape([512, 1, 32, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
        'q_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wkv_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'flash_mla_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'kv_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'flash_mla_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'mesh_shape': [4, 8],
        'wq_a_rs_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
          ],
          'num_links': 1,
        },
        'q_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
      },
      'mlp': {
        'revert_dp': AllGatherAsyncConfig(
          dim=-2,
          cluster_axis=0,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f2123e52b30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79df70>,
                      ],
          persistent_output_tensor=None,
          num_links=3,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'shared_expert': {
          'w1': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
          'mul': MulConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            input_tensor_a_activations=,
              [
                            <UnaryOpType.SILU: 54>,
                          ]
          ),
          'w2': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([256, 7168]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[256, 608]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=1, ...)
          ),
          'all_gather': AllGatherAsyncConfig(
            dim=-1,
            cluster_axis=1,
            mesh_device=MeshDevice(4x8 grid, 32 devices),
            topology=<Topology.Linear: 1>,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                          ],
            persistent_output_tensor=None,
            num_links=1,
            memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            subdevice_id=None,
            use_optimal_ccl_for_llama=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
          ),
          'reduce_scatter_async': ReduceScatterAsyncMinimalConfig(
            dim=3,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                          ],
            num_links=1,
            persistent_output_buffers=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            intermediate_memory_config=None,
            topology=<Topology.Linear: 1>,
            subdevice_id=None,
            cluster_axis=1,
            chunks_per_sync=None,
            num_workers_per_link=None,
            num_buffers_per_channel=None
          ),
          'all_gather_reshard': ReshardConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            dtype=None
          ),
          'output_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          'w3': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
        },
        'apply_dp': {
          'mesh_shape': (4, 8),
          'memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          'cluster_axis': 0,
          'dim': -2,
        },
        'moe': [
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
        ],
      },
      'mla_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
      'mlp_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
    },
    {
      'mla_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla': {
        'hf_config': DeepseekV3Config {
  "architectures": [
    "DeepseekV3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_deepseek.DeepseekV3Config",
    "AutoModel": "modeling_deepseek.DeepseekV3Model",
    "AutoModelForCausalLM": "modeling_deepseek.DeepseekV3ForCausalLM"
  },
  "bos_token_id": 0,
  "eos_token_id": 1,
  "ep_size": 1,
  "first_k_dense_replace": 3,
  "hidden_act": "silu",
  "hidden_size": 7168,
  "initializer_range": 0.02,
  "intermediate_size": 18432,
  "kv_lora_rank": 512,
  "max_position_embeddings": 163840,
  "max_seq_len": 4096,
  "model_type": "deepseek_v3",
  "moe_intermediate_size": 2048,
  "moe_layer_freq": 1,
  "n_group": 8,
  "n_routed_experts": 256,
  "n_shared_experts": 1,
  "norm_topk_prob": true,
  "num_attention_heads": 128,
  "num_experts_per_tok": 8,
  "num_hidden_layers": 61,
  "num_key_value_heads": 128,
  "num_nextn_predict_layers": 1,
  "q_lora_rank": 1536,
  "qk_nope_head_dim": 128,
  "qk_rope_head_dim": 64,
  "quantization_config": {
    "activation_scheme": "dynamic",
    "fmt": "e4m3",
    "quant_method": "fp8",
    "weight_block_size": [
      128,
      128
    ]
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 40,
    "mscale": 1.0,
    "mscale_all_dim": 1.0,
    "original_max_position_embeddings": 4096,
    "type": "yarn"
  },
  "rope_theta": 10000,
  "routed_scaling_factor": 2.5,
  "scoring_func": "sigmoid",
  "tie_word_embeddings": false,
  "topk_group": 4,
  "topk_method": "noaux_tc",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "v_head_dim": 128,
  "vocab_size": 129280
}
,
        'wo_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'wkv_b2': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 512, 128]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
          ],
          'num_links': 1,
        },
        'kv_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 16, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wq_a_ag_decode': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'flash_mla_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_b': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 1536, 3072]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'flash_mla_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'wq_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=3,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wq_a2a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'q_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 48, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wo': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16384, 896]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 1536]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_b1': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 128, 512]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'wkv_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'kv_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_a2a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wkv_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
          ],
          'num_links': 1,
        },
        'wkv_a_r_decode': {
          'dims': [1],
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'output': None,
        },
        'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        'kvpe_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'flash_mla': {
          'scale': 0.07216878364870322,
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'program_config': SDPAProgramConfig(...),
          'head_dim_v': 512,
          'memory_config': MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
        },
        'wo_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
          ],
          'num_links': 1,
        },
        'kvpe_cache': ttnn.Tensor(shape=Shape([512, 1, 32, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
        'q_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wkv_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'flash_mla_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'kv_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'flash_mla_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'mesh_shape': [4, 8],
        'wq_a_rs_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
          ],
          'num_links': 1,
        },
        'q_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
      },
      'mlp': {
        'revert_dp': AllGatherAsyncConfig(
          dim=-2,
          cluster_axis=0,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e730>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e670>,
                      ],
          persistent_output_tensor=None,
          num_links=3,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'shared_expert': {
          'w1': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
          'mul': MulConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            input_tensor_a_activations=,
              [
                            <UnaryOpType.SILU: 54>,
                          ]
          ),
          'w2': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([256, 7168]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[256, 608]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=1, ...)
          ),
          'all_gather': AllGatherAsyncConfig(
            dim=-1,
            cluster_axis=1,
            mesh_device=MeshDevice(4x8 grid, 32 devices),
            topology=<Topology.Linear: 1>,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                          ],
            persistent_output_tensor=None,
            num_links=1,
            memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            subdevice_id=None,
            use_optimal_ccl_for_llama=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
          ),
          'reduce_scatter_async': ReduceScatterAsyncMinimalConfig(
            dim=3,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                          ],
            num_links=1,
            persistent_output_buffers=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            intermediate_memory_config=None,
            topology=<Topology.Linear: 1>,
            subdevice_id=None,
            cluster_axis=1,
            chunks_per_sync=None,
            num_workers_per_link=None,
            num_buffers_per_channel=None
          ),
          'all_gather_reshard': ReshardConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            dtype=None
          ),
          'output_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          'w3': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
        },
        'apply_dp': {
          'mesh_shape': (4, 8),
          'memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          'cluster_axis': 0,
          'dim': -2,
        },
        'moe': [
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          None,
        ],
      },
      'mla_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
      'mlp_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
    },
    {
      'mla_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla': {
        'hf_config': DeepseekV3Config {
  "architectures": [
    "DeepseekV3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_deepseek.DeepseekV3Config",
    "AutoModel": "modeling_deepseek.DeepseekV3Model",
    "AutoModelForCausalLM": "modeling_deepseek.DeepseekV3ForCausalLM"
  },
  "bos_token_id": 0,
  "eos_token_id": 1,
  "ep_size": 1,
  "first_k_dense_replace": 3,
  "hidden_act": "silu",
  "hidden_size": 7168,
  "initializer_range": 0.02,
  "intermediate_size": 18432,
  "kv_lora_rank": 512,
  "max_position_embeddings": 163840,
  "max_seq_len": 4096,
  "model_type": "deepseek_v3",
  "moe_intermediate_size": 2048,
  "moe_layer_freq": 1,
  "n_group": 8,
  "n_routed_experts": 256,
  "n_shared_experts": 1,
  "norm_topk_prob": true,
  "num_attention_heads": 128,
  "num_experts_per_tok": 8,
  "num_hidden_layers": 61,
  "num_key_value_heads": 128,
  "num_nextn_predict_layers": 1,
  "q_lora_rank": 1536,
  "qk_nope_head_dim": 128,
  "qk_rope_head_dim": 64,
  "quantization_config": {
    "activation_scheme": "dynamic",
    "fmt": "e4m3",
    "quant_method": "fp8",
    "weight_block_size": [
      128,
      128
    ]
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 40,
    "mscale": 1.0,
    "mscale_all_dim": 1.0,
    "original_max_position_embeddings": 4096,
    "type": "yarn"
  },
  "rope_theta": 10000,
  "routed_scaling_factor": 2.5,
  "scoring_func": "sigmoid",
  "tie_word_embeddings": false,
  "topk_group": 4,
  "topk_method": "noaux_tc",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "v_head_dim": 128,
  "vocab_size": 129280
}
,
        'wo_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'wkv_b2': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 512, 128]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
          ],
          'num_links': 1,
        },
        'kv_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 16, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wq_a_ag_decode': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'flash_mla_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_b': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 1536, 3072]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'flash_mla_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'wq_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=3,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wq_a2a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'q_norm': RMSNormConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 48, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          residual_input_tensor=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi)
        ),
        'wo': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16384, 896]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wq_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 1536]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_b1': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 16, 128, 512]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'wkv_a_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'wkv_a': LinearConfig(
          input_tensor_b=ttnn.Tensor(shape=Shape([1, 896, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          compute_kernel_config=None,
          program_config=None
        ),
        'kv_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wq_a2a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'wkv_a_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
          ],
          'num_links': 1,
        },
        'wkv_a_r_decode': {
          'dims': [1],
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'output': None,
        },
        'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        'kvpe_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'flash_mla': {
          'scale': 0.07216878364870322,
          'compute_kernel_config': ComputeKernelConfig(math_fidelity=HiFi4),
          'program_config': SDPAProgramConfig(...),
          'head_dim_v': 512,
          'memory_config': MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
        },
        'wo_ag_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
          ],
          'num_links': 1,
        },
        'kvpe_cache': ttnn.Tensor(shape=Shape([512, 1, 32, 576]), dtype=DataType.BFLOAT8_B, memory=INTERLEAVED_DRAM),
        'q_rope_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=HEIGHT_SHARDED, buffer=L1),
          dtype=None
        ),
        'wkv_a_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'flash_mla_rs_decode': ReduceScatterAsyncMinimalConfig(
          dim=1,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                      ],
          num_links=1,
          persistent_output_buffers=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          intermediate_memory_config=None,
          topology=<Topology.Linear: 1>,
          subdevice_id=None,
          cluster_axis=1,
          chunks_per_sync=None,
          num_workers_per_link=None,
          num_buffers_per_channel=None
        ),
        'kv_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
        'flash_mla_ag_decode': AllGatherAsyncConfig(
          dim=1,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=1,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'mesh_shape': [4, 8],
        'wq_a_rs_prefill': {
          'barrier_semaphore': <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
          'multi_device_global_semaphore': [
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
          ],
          'num_links': 1,
        },
        'q_rope_out_reshard': ReshardConfig(
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          dtype=None
        ),
      },
      'mlp': {
        'revert_dp': AllGatherAsyncConfig(
          dim=-2,
          cluster_axis=0,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f2123e52b30>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79df70>,
                      ],
          persistent_output_tensor=None,
          num_links=3,
          memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'shared_expert': {
          'w1': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
          'mul': MulConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            input_tensor_a_activations=,
              [
                            <UnaryOpType.SILU: 54>,
                          ]
          ),
          'w2': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([256, 7168]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[256, 608]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=1, ...)
          ),
          'all_gather': AllGatherAsyncConfig(
            dim=-1,
            cluster_axis=1,
            mesh_device=MeshDevice(4x8 grid, 32 devices),
            topology=<Topology.Linear: 1>,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                          ],
            persistent_output_tensor=None,
            num_links=1,
            memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            subdevice_id=None,
            use_optimal_ccl_for_llama=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>
          ),
          'reduce_scatter_async': ReduceScatterAsyncMinimalConfig(
            dim=3,
            multi_device_global_semaphore=,
              [
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                            <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                          ],
            num_links=1,
            persistent_output_buffers=None,
            barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            intermediate_memory_config=None,
            topology=<Topology.Linear: 1>,
            subdevice_id=None,
            cluster_axis=1,
            chunks_per_sync=None,
            num_workers_per_link=None,
            num_buffers_per_channel=None
          ),
          'all_gather_reshard': ReshardConfig(
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            dtype=None
          ),
          'output_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          'w3': LinearConfig(
            input_tensor_b=ttnn.Tensor(shape=Shape([7168, 256]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 32]),
            memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
            compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
            program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
          ),
        },
        'apply_dp': {
          'mesh_shape': (4, 8),
          'memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
          'cluster_axis': 0,
          'dim': -2,
        },
        'moe': [
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6442b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644270>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644030>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79ea30>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac79e9f0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          {
            'moe_experts': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w1_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'w3_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 7168, 2048]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
              'mul_experts': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=,
                  [
                                    <UnaryOpType.SILU: 54>,
                                  ]
              ),
              'num_experts_per_device': 8,
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'w2_experts': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 8, 2048, 7168]), dtype=DataType.BFLOAT4_B, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
                program_config=None
              ),
            },
            'num_experts_per_tok': 8,
            'num_dispatch_devices': 4,
            'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_dispatch_metadata_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
            'final_output_reduce_scatter': ReduceScatterAsyncMinimalConfig(
              dim=3,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6440b0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644070>,
                              ],
              num_links=1,
              persistent_output_buffers=None,
              barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              intermediate_memory_config=None,
              topology=<Topology.Linear: 1>,
              subdevice_id=None,
              cluster_axis=1,
              chunks_per_sync=None,
              num_workers_per_link=None,
              num_buffers_per_channel=None
            ),
            'all_to_all_dispatch': AllToAllDispatchConfig(
              cluster_axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>,
              subdevice_id=None
            ),
            'topk_weights_repeat': RepeatConfig(
              repeat_dims=Shape([7168, 1, 1, 1])
            ),
            'num_experts_per_device': 8,
            'moe_gate': {
              'input_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'reshape_active_experts': ReshapeConfig(
                shape=(1, 1, -1, 256)
              ),
              'topk_expert_groups': TopKConfig(
                k=4,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback_config': TopKFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>,
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                use_bitonic_sort=True
              ),
              'linear_fallback_config': LinearFallbackConfig(
                mesh_device=MeshDevice(4x8 grid, 32 devices),
                dtype=<DataType.BFLOAT16: 0>
              ),
              'reshape_scores': ReshapeConfig(
                shape=(1, -1, 8, 32)
              ),
              'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
              'mul_scores_with_mask': MulConfig(
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                input_tensor_a_activations=None
              ),
              'scatter_top_expert_groups': ScatterConfig(
                input=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                dim=3,
                src=ttnn.Tensor(shape=Shape([1, 1, 1, 4]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM)
              ),
              'add_norm_eps': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_within_expert_groups': TopKConfig(
                k=2,
                dim=-1,
                largest=True,
                sorted=True
              ),
              'topk_fallback': False,
              'multiply_expert_scale': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 8]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'reshape_group_mask': ReshapeConfig(
                shape=(1, -1, 8, 1)
              ),
              'gate_proj': LinearConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 7168, 256]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                compute_kernel_config=ComputeKernelConfig(math_fidelity=HiFi2),
                program_config=None
              ),
              'mesh_device': MeshDevice(4x8 grid, 32 devices),
              'linear_fallback': False,
              'add_score_correction_bias': BinaryOpConfig(
                input_tensor_b=ttnn.Tensor(shape=Shape([1, 1, 1, 256]), dtype=DataType.FLOAT32, memory=INTERLEAVED_DRAM),
                memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
                dtype=<DataType.BFLOAT16: 0>,
                activation=None
              ),
              'topk_experts': TopKConfig(
                k=8,
                dim=-1,
                largest=True,
                sorted=True
              ),
            },
            'output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'activations_repeat': RepeatConfig(
              repeat_dims=Shape([1, 8, 1, 1])
            ),
            'all_to_all_combine_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'all_to_all_combine': AllToAllCombineConfig(
              axis=0,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              num_links=3,
              topology=<Topology.Linear: 1>
            ),
            'revert_tp': AllGatherAsyncConfig(
              dim=-1,
              cluster_axis=1,
              mesh_device=MeshDevice(4x8 grid, 32 devices),
              topology=<Topology.Linear: 1>,
              multi_device_global_semaphore=,
                [
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                              ],
              persistent_output_tensor=None,
              num_links=1,
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              subdevice_id=None,
              use_optimal_ccl_for_llama=None,
              barrier_semaphore=None
            ),
            'all_to_all_dispatch_output_memory_config': MemoryConfig(layout=INTERLEAVED, buffer=L1),
            'expert_mapping_tensors': ttnn.Tensor(shape=Shape([1, 1, 256, 32]), dtype=DataType.UINT16, memory=INTERLEAVED_DRAM),
            'hidden_size': 7168,
            'device': MeshDevice(4x8 grid, 32 devices),
            'mul_experts_output_with_weights': MulConfig(
              memory_config=MemoryConfig(layout=INTERLEAVED, buffer=L1),
              input_tensor_a_activations=None
            ),
            'num_devices': 32,
          },
          None,
        ],
      },
      'mla_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
      'mlp_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
        dtype=None
      ),
      'mla_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm': {
        'all_gather': AllGatherAsyncConfig(
          dim=3,
          cluster_axis=1,
          mesh_device=MeshDevice(4x8 grid, 32 devices),
          topology=<Topology.Linear: 1>,
          multi_device_global_semaphore=,
            [
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                        <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
                      ],
          persistent_output_tensor=None,
          num_links=None,
          memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
          subdevice_id=None,
          use_optimal_ccl_for_llama=None,
          barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
        ),
        'rms_norm_post_all_gather': RMSNormPostAllGatherConfig(
          epsilon=1e-06,
          weight=ttnn.Tensor(shape=Shape([1, 1, 28, 32]), dtype=DataType.BFLOAT16, memory=INTERLEAVED_DRAM),
          bias=None,
          memory_config=None,
          program_config=None,
          compute_kernel_config=None,
          dtype=<DataType.BFLOAT16: 0>
        ),
        'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        'rms_norm_pre_all_gather': RMSNormPreAllGatherConfig(
          dtype=<DataType.BFLOAT16: 0>,
          residual_input_tensor=None,
          compute_kernel_config=None,
          program_config=None,
          memory_config=None
        ),
      },
      'mlp_norm_reshard': ReshardConfig(
        memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
        dtype=None
      ),
    },
  ],
  'lm_head': {
    'all_gather': AllGatherAsyncConfig(
      dim=-1,
      cluster_axis=1,
      mesh_device=MeshDevice(4x8 grid, 32 devices),
      topology=<Topology.Linear: 1>,
      multi_device_global_semaphore=,
        [
                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441f0>,
                <ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac6441b0>,
              ],
      persistent_output_tensor=None,
      num_links=1,
      memory_config=MemoryConfig(layout=INTERLEAVED, buffer=DRAM),
      subdevice_id=None,
      use_optimal_ccl_for_llama=None,
      barrier_semaphore=<ttnn._ttnn.global_semaphore.global_semaphore object at 0x7f1fac644230>
    ),
    'mesh_scatter': {
      'mesh_shape': (4, 8),
      'scatter_idx': (
        0,
        None,
      ),
    },
    'input_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
    'linear': LinearConfig(
      input_tensor_b=ttnn.Tensor(shape=Shape([7168, 4040]), dtype=DataType.BFLOAT4_B, memory=WIDTH_SHARDED_DRAM, sharded[7168, 352]),
      memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
      compute_kernel_config=ComputeKernelConfig(math_fidelity=LoFi),
      program_config=MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig(in0_block_w=4, ...)
    ),
    'output_memory_config': MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
  },
  'mesh_shape': [4, 8],
  'norm_reshard': ReshardConfig(
    memory_config=MemoryConfig(layout=WIDTH_SHARDED, buffer=L1),
    dtype=None
  ),
  'metalayer_padding_map': [
    [False, False, False, False],
    [False, False, False, False],
    [False, False, False, False],
    [False, False, False, False],
    [False, False, False, False],
    [False, False, False, False],
    [False, False, False, False],
    [False, False, False, False],
    [False, False, False, False],
    [False, False, False, False],
    [False, False, False, False],
    [False, False, False, False],
    [False, False, False, False],
    [False, False, False, True],
    [False, False, False, True],
  ],
}