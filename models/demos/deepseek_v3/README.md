# DeepSeek-V3

## Platforms:
    Galaxy (WH)

## Introduction
This codebase was designed for the model: [deepseek-ai/DeepSeek-R1-0528](https://huggingface.co/deepseek-ai/DeepSeek-R1-0528) but should also work with other DeepseekV3 models such as:

- [deepseek-ai/DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)
- [deepseek-ai/DeepSeek-V3](https://huggingface.co/deepseek-ai/DeepSeek-V3)

## Prerequisites
- Cloned [tt-metal repository](https://github.com/tenstorrent/tt-metal) for source code
- Installed: [TT-Metalium™ / TT-NN™](https://github.com/tenstorrent/tt-metal/blob/main/INSTALLING.md)

## How to Run

If you are not running on Tenstorrent internal infrastructure, you need to set the following environment variables:

- `DEEPSEEK_V3_CACHE`: Path to a directory where cached data such as converted weights and test inputs/outputs will be stored.

- `DEEPSEEK_V3_HF_MODEL`: Path to a directory containing the DeepSeek-V3 Hugging Face model weights. Defaults to `models/demos/deepseek_v3/reference`. Download the model from Hugging Face set this to the model directory.

These variables are used in scripts for generating test data and running tests.

This codebase separates model execution into three distinct stages, each of which can be run independently:
1. Convert PyTorch weights to TTNN tensor files and generate the WeightConfig
2. Generate ModelConfigs for prefill and decode modes
3. Load TTNN tensor files using WeightConfig, create a shared state using create_state, merge them with ModelPrefillConfig and ModelDecodeConfig to create a RunPrefillConfig and RunDecodeConfig, and execute the model with either of the model configs

The modules are not instantiated directly, but rather used as a namespace for the methods that define the model's behavior in prefill and decode. This is to make it easy to separate the stateful and stateless parts of the model, and allow for easy re-use of the methods.

### Weight Configuration
Generated by static method `convert_weights` on each module class. Returns a dict mapping operation names to their TTNN weight file paths:
```python
{
    "w1": "/path/to/weights/w1.input_tensor_b",
    "w2": "/path/to/weights/w2.input_tensor_b",
    "w3": "/path/to/weights/w3.input_tensor_b"
}
```

### Per-Submodule Model Configs
Generated by static methods `prefill_model_config` and `decode_model_config` on each module class. Contains operator configurations using dataclasses from `config_dataclass.py`:
```python
{
    "w1": LinearConfig(
        memory_config=ttnn.DRAM_MEMORY_CONFIG,
        program_config=matmul_program_config,
        compute_kernel_config=ttnn.experimental.tensor.CoreRangeSet(...)
    ),
    "mul_activation": MulConfig(
        memory_config=ttnn.DRAM_MEMORY_CONFIG,
        input_tensor_a_activations=[ttnn.UnaryOpType.SILU] # list because ttnn.mul expects a list
    ),
    "all_reduce": AllReduceConfig(
        cluster_axis=0,
        topology=ttnn.Topology.Ring,
        dtype=ttnn.bfloat8_b,
        dim=3,
        num_reduce_scatter_links=1,
        num_all_gather_links=1,
        use_composite=True
    )
}
```

### Example Usage
```python
# Stage 1: Convert weights and get weight_config (saves to disk in standard format)
weight_config = MLP1D.convert_weights(hf_config, torch_state_dict, Path("weights/mlp"), mesh_device)

# Stage 2: Generate operator configs (returns nested dicts with TTNN objects)
model_config = MLP1D.prefill_model_config(hf_config, mesh_device) # Or decode_model_config(hf_config, mesh_device) for decode

# Stage 3: Generate the runtime state of the model
model_state = MLP1D.create_state(hf_config, mesh_device)

# Stage 3: Runtime execution
run_config = MLP1D.run_config(model_config, weight_config, model_state)
output = MLP1D.forward_prefill(input_tensor, run_config) # or forward_decode(input_tensor, run_config)
```

## Details
###  Folder Contents
- [reference](./reference): Reference model code from HuggingFace, cleaned up and extracted submodule code etc.
- [tests](./tests): pytests for submodules
- [tt](./tt): ttnn submodule code

## Test Results CSV (opt-in)

You can opt-in to collecting a CSV with one row per test invocation for later analysis.

- Enable by providing a path:
  - CLI: `pytest models/demos/deepseek_v3/tests --results-csv /path/to/results.csv`
  - Env: `TT_TEST_RESULTS_CSV=/path/to/results.csv`
- Behavior: appends to the file; writes a header only if the file is new/empty.
- Implementation: pytest plugin at `models/demos/deepseek_v3/utils/results_csv_plugin.py` loaded via `conftest.py`.

Columns captured per test invocation:

- `test_nodeid`: Fully-qualified pytest node id.
- `module`: High-level module name (e.g., `MoEGate`, `MLA1D`, `MoE`, `Model1D`, `LMHead`, `Embedding1D`, `DecoderBlock`, `RMSNorm`).
- `mode`, `seq_len`, `batch_size`: Test parameters when available.
- `hf_max_seq_len`: From the `hf_config` fixture when present.
- `mesh_shape`, `mesh_num_devices`: From the `mesh_device` fixture when present.
- `weights_type`, `module_path`: Recorded if used in the test.
- `metrics_json`: JSON array of measured metrics (each entry includes a `type` like `pcc`/`allclose`/`ulp`, and values/messages).
- `outcome`: `passed`/`failed`/`skipped`.
- `duration_sec`: Wallclock runtime for the test body.
- `error`: Full traceback text when failed (including timeouts), safely quoted in CSV.
- Run metadata: `run_id`, `session_timestamp`, `hostname`, `git_commit`, `ttnn_version`.
- Timestamps: `start_timestamp`, `end_timestamp` per test.

Notes

- This is disabled by default. If neither `--results-csv` nor `TT_TEST_RESULTS_CSV` is set, no CSV is produced and the plugin does nothing.
- Metrics are captured by intercepting calls to `comp_pcc`, `comp_allclose`, and `comp_ulp` in `models.common.utility_functions` (and the `models.utility_functions` alias). If a test computes multiple metrics, all are recorded in order in `metrics_json`.
- The `module` column is inferred from parameterized class fixtures (when used) or otherwise from the test filename.

## TTNN Op Tracing (opt-in)

Capture every `ttnn` (and `ttnn.experimental`) op call made during module tests to a JSONL file so you can auto‑generate op‑level tests with real shapes and configs.

- Enable by providing a path:
  - CLI: `pytest models/demos/deepseek_v3/tests --ops-jsonl /path/to/op_calls.jsonl`
  - Env: `TT_OP_RESULTS_JSONL=/path/to/op_calls.jsonl`
- Format: JSON Lines; one record per op call, including:
  - Linking: `run_id`, `test_nodeid`, inferred `module`, per‑test `op_index`.
  - Op: `op_name` (e.g., `ttnn.linear`), `duration_ms`, timestamps.
  - Inputs: Tensor shapes/dtypes and memory configs.
  - Configs: Serialized op configs/program configs/memory configs present in kwargs.
  - Output tensor metadata; errors if the op raised.
- Implementation: pytest plugin at `models/demos/deepseek_v3/utils/op_capture_plugin.py` uses an import hook to wrap `ttnn` callables.

### Replaying a single op

Use the helper to reconstruct and execute a captured op call with random inputs of the recorded shapes/configs.

- Programmatic:
  - `from models.demos.deepseek_v3.utils.op_replay import replay_op_record`
  - `result = replay_op_record(record_dict, mesh_device)`
- CLI (requires hardware):
  - `python -m models.demos.deepseek_v3.utils.op_replay --jsonl /path/to/op_calls.jsonl --index 0`
  - The helper opens a default mesh (1xN or 4x8) and prints the output shape.

The helper reconstructs:
- Random torch inputs → TTNN tensors with recorded dtype/layout/memory_config and sharding dims.
- Config objects (memory/program configs, dataclasses) via the shared serializer.
- MeshDevice is not loaded from JSON; the helper uses the active device you provide (or default in the CLI).

Notes

- This is disabled by default; when not enabled it adds zero overhead.
- No source changes required; the plugin wraps `ttnn` via an import hook and also retrofits already‑imported `ttnn` modules.
- Serializer module: `models/demos/deepseek_v3/utils/serialize_configs.py` provides `to_jsonable` and `from_jsonable` for config objects and placeholders (TensorRef, MeshDevice).

### Generating pytest op tests from JSONL

Create reproducible pytest tests from captured JSONL:

- CLI:
  - `python -m models.demos.deepseek_v3.utils.generate_op_tests \
      --jsonl /path/to/op_calls.jsonl \
      --out-dir models/demos/deepseek_v3/tests/op_repros \
      --tests-per-group 1`
  - This writes `test_generated_ops.py` that replays a small sample per op signature using `replay_op_record` and the existing `mesh_device` fixture from `conftest.py`.
  - Run them: `pytest models/demos/deepseek_v3/tests/op_repros/test_generated_ops.py -q`
