"""
TTTv2 Source Code Generation Example

This demonstrates how TTTv2 modules can generate optimized TTNN code
for specific model implementations and target devices.
"""

from dataclasses import dataclass
from typing import Dict, Optional, Tuple


@dataclass
class LinearConfig:
    """Configuration for a Linear layer"""

    in_features: int
    out_features: int
    bias: bool = True
    activation: Optional[str] = None


@dataclass
class HardwareProfile:
    """Hardware-specific optimization parameters"""

    device_name: str
    compute_grid: Tuple[int, int]
    l1_memory_size: int
    dram_bandwidth: int
    supports_fp32_dest_acc: bool
    optimal_tile_size: int
    max_cores: int


class TTTLinearCodeGenerator:
    """Generate optimized Linear layer code for specific hardware"""

    def __init__(self, hw_profile: HardwareProfile):
        self.hw = hw_profile

    def generate(self, config: LinearConfig, model_name: str) -> str:
        """Generate optimized Linear layer implementation"""

        # Calculate optimal sharding based on hardware
        shard_config = self._calculate_optimal_sharding(config)

        # Generate the code
        code = f'''"""
Auto-generated Linear layer for {model_name} on {self.hw.device_name}
Generated by TTTv2 Code Generator
"""

import torch
import ttnn
from typing import Optional


class {model_name}Linear_{config.in_features}x{config.out_features}:
    """Optimized Linear layer for {model_name}"""

    def __init__(self, device):
        self.device = device

        # Pre-computed optimal configurations for {self.hw.device_name}
        self.compute_kernel_config = ttnn.WormholeComputeKernelConfig(
            math_fidelity=ttnn.MathFidelity.HiFi4,
            math_approx_mode=True,
            fp32_dest_acc_en={self.hw.supports_fp32_dest_acc},
            packer_l1_acc=True
        )

        # Optimized program configuration
        self.program_config = ttnn.MatmulMultiCoreReuseMultiCastProgramConfig(
            compute_with_storage_grid_size=({shard_config["grid_x"]}, {shard_config["grid_y"]}),
            in0_block_w={shard_config["in0_block_w"]},
            out_subblock_h={shard_config["out_subblock_h"]},
            out_subblock_w={shard_config["out_subblock_w"]},
            per_core_M={shard_config["per_core_M"]},
            per_core_N={shard_config["per_core_N"]},
            transpose_mcast=False,
            fused_activation={self._get_fused_activation(config.activation)}
        )

        # Memory configuration
        self.memory_config = ttnn.MemoryConfig(
            memory_layout=ttnn.TensorMemoryLayout.BLOCK_SHARDED,
            buffer_type=ttnn.BufferType.L1
        )

    def prepare_weights(self, weight: torch.Tensor, bias: Optional[torch.Tensor] = None):
        """Prepare and optimize weights for this specific configuration"""

        # Convert to tilized layout optimal for this hardware
        weight_ttnn = ttnn.from_torch(
            weight,
            device=self.device,
            layout=ttnn.TILE_LAYOUT,
            memory_config=self.memory_config,
            dtype=ttnn.bfloat16
        )

        # Pre-transpose if beneficial
        if {shard_config["needs_transpose"]}:
            weight_ttnn = ttnn.transpose(weight_ttnn, -2, -1)

        bias_ttnn = None
        if bias is not None and {config.bias}:
            bias_ttnn = ttnn.from_torch(
                bias,
                device=self.device,
                layout=ttnn.TILE_LAYOUT,
                memory_config=self.memory_config,
                dtype=ttnn.bfloat16
            )

        return weight_ttnn, bias_ttnn

    def forward(self, x: ttnn.Tensor, weight: ttnn.Tensor, bias: Optional[ttnn.Tensor] = None):
        """Optimized forward pass"""

        # Input preparation if needed
        if x.memory_config() != self.memory_config:
            x = ttnn.to_memory_config(x, self.memory_config)

        # Execute optimized linear operation
        output = ttnn.linear(
            x,
            weight,
            bias=bias,
            compute_kernel_config=self.compute_kernel_config,
            program_config=self.program_config,
            memory_config=self.memory_config,
            dtype=ttnn.bfloat16
        )

        return output

    def get_resource_usage(self) -> Dict[str, int]:
        """Return estimated resource usage"""
        return {{
            "l1_memory_bytes": {shard_config["l1_usage"]},
            "active_cores": {shard_config["grid_x"] * shard_config["grid_y"]},
            "dram_bandwidth_gbps": {shard_config["bandwidth_estimate"]}
        }}


# Convenience function for model builders
def create_linear_{config.in_features}x{config.out_features}(device):
    """Factory function for creating optimized Linear layer"""
    return {model_name}Linear_{config.in_features}x{config.out_features}(device)
'''

        return code

    def _calculate_optimal_sharding(self, config: LinearConfig) -> Dict:
        """Calculate optimal sharding configuration for the hardware"""

        # Simplified optimization logic
        m = config.out_features
        n = config.in_features
        k = config.in_features

        # Calculate optimal grid usage
        total_cores = self.hw.compute_grid[0] * self.hw.compute_grid[1]

        # Try to balance M and N dimensions across cores
        grid_x = min(self.hw.compute_grid[0], (m + 31) // 32)
        grid_y = min(self.hw.compute_grid[1], (n + 31) // 32)

        # Calculate per-core work
        per_core_M = (m + grid_x - 1) // grid_x
        per_core_N = (n + grid_y - 1) // grid_y

        # Tile sizes based on hardware
        tile_size = self.hw.optimal_tile_size

        return {
            "grid_x": grid_x,
            "grid_y": grid_y,
            "in0_block_w": min(2, k // tile_size),
            "out_subblock_h": min(4, per_core_M // tile_size),
            "out_subblock_w": min(2, per_core_N // tile_size),
            "per_core_M": per_core_M,
            "per_core_N": per_core_N,
            "needs_transpose": n > m,  # Simplified heuristic
            "l1_usage": self._estimate_l1_usage(per_core_M, per_core_N, k),
            "bandwidth_estimate": self._estimate_bandwidth(m, n, k),
        }

    def _estimate_l1_usage(self, per_core_M: int, per_core_N: int, k: int) -> int:
        """Estimate L1 memory usage per core"""
        # Simplified estimation
        bytes_per_element = 2  # bfloat16
        tiles_per_core = (per_core_M * per_core_N * k) // (32 * 32)
        return tiles_per_core * 32 * 32 * bytes_per_element

    def _estimate_bandwidth(self, m: int, n: int, k: int) -> float:
        """Estimate DRAM bandwidth requirement in GB/s"""
        # Simplified estimation
        total_bytes = (m * k + k * n + m * n) * 2  # bfloat16
        time_seconds = 0.001  # Assume 1ms compute time
        return (total_bytes / 1e9) / time_seconds

    def _get_fused_activation(self, activation: Optional[str]) -> str:
        """Convert activation to TTNN fused activation string"""
        if activation == "gelu":
            return "ttnn.UnaryOpType.GELU"
        elif activation == "relu":
            return "ttnn.UnaryOpType.RELU"
        else:
            return "None"


class TTTModelCodeGenerator:
    """High-level code generator for entire models"""

    def __init__(self, model_name: str, hw_profile: HardwareProfile):
        self.model_name = model_name
        self.hw_profile = hw_profile
        self.generated_modules = []

    def add_linear(self, name: str, config: LinearConfig):
        """Add a Linear layer to the model"""
        generator = TTTLinearCodeGenerator(self.hw_profile)
        code = generator.generate(config, f"{self.model_name}_{name}")
        self.generated_modules.append({"name": name, "type": "linear", "code": code, "config": config})

    def generate_model_file(self) -> str:
        """Generate complete model file with all modules"""

        imports = '''"""
Auto-generated model implementation for {model_name}
Target device: {device}
Generated by TTTv2 Code Generator
"""

import torch
import ttnn
from typing import Dict, Optional, List

'''

        # Combine all module codes
        all_code = imports.format(model_name=self.model_name, device=self.hw_profile.device_name)

        for module in self.generated_modules:
            all_code += "\n\n" + module["code"]

        # Add model class that uses all modules
        model_class = f'''

class {self.model_name}Model:
    """Complete model implementation using generated modules"""

    def __init__(self, device):
        self.device = device
        self.modules = {{}}
        '''

        for module in self.generated_modules:
            if module["type"] == "linear":
                config = module["config"]
                model_class += f"""
        self.modules["{module["name"]}"] = create_linear_{config.in_features}x{config.out_features}(device)"""

        model_class += '''

    def get_total_resource_usage(self) -> Dict[str, int]:
        """Calculate total resource usage across all modules"""
        total_l1 = 0
        total_cores = 0
        max_bandwidth = 0

        for name, module in self.modules.items():
            usage = module.get_resource_usage()
            total_l1 += usage["l1_memory_bytes"]
            total_cores = max(total_cores, usage["active_cores"])
            max_bandwidth = max(max_bandwidth, usage["dram_bandwidth_gbps"])

        return {
            "total_l1_bytes": total_l1,
            "max_active_cores": total_cores,
            "peak_bandwidth_gbps": max_bandwidth
        }
'''

        all_code += model_class

        return all_code


# Example usage
if __name__ == "__main__":
    # Define hardware profile for Wormhole B0
    wh_b0 = HardwareProfile(
        device_name="wormhole_b0",
        compute_grid=(8, 7),
        l1_memory_size=1024 * 1024,  # 1MB
        dram_bandwidth=200,  # GB/s
        supports_fp32_dest_acc=True,
        optimal_tile_size=32,
        max_cores=56,
    )

    # Create model generator
    model_gen = TTTModelCodeGenerator("Llama2_7B", wh_b0)

    # Add layers with specific configurations
    model_gen.add_linear("q_proj", LinearConfig(4096, 4096, bias=False))
    model_gen.add_linear("k_proj", LinearConfig(4096, 1024, bias=False))
    model_gen.add_linear("v_proj", LinearConfig(4096, 1024, bias=False))
    model_gen.add_linear("o_proj", LinearConfig(4096, 4096, bias=False))
    model_gen.add_linear("mlp_up", LinearConfig(4096, 11008, bias=False, activation="gelu"))
    model_gen.add_linear("mlp_down", LinearConfig(11008, 4096, bias=False))

    # Generate complete model file
    generated_code = model_gen.generate_model_file()

    print("Generated code preview:")
    print("=" * 80)
    print(generated_code[:2000] + "\n... [truncated] ...")
