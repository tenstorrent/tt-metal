# SPDX-FileCopyrightText: © 2024 Tenstorrent Inc.
# SPDX-License-Identifier: Apache-2.0

"""
TTNN implementation of DVAE (Discrete Variational Autoencoder) for audio reconstruction.

DVAE reconstructs mel spectrograms from discrete audio tokens using:
- Encoder: ConvNeXt-style convolutional blocks
- Quantizer: GFSQ (Grouped Residual Finite Scalar Quantization)
- Decoder: ConvNeXt-style deconvolutional blocks
- Output: Mel spectrogram reconstruction
"""

import torch
import ttnn
import numpy as np
from typing import Tuple, Optional
from loguru import logger

try:
    from .common import (
        get_weights_memory_config,
        get_activations_memory_config,
        torch_to_ttnn,
        ttnn_to_torch,
    )
except ImportError:
    from common import (
        get_weights_memory_config,
        get_activations_memory_config,
        torch_to_ttnn,
        ttnn_to_torch,
    )


class TtnnDVAE:
    """
    TTNN implementation of DVAE for audio reconstruction.

    Architecture:
        - Encoder: Downsampling convolutions + ConvNeXt blocks
        - Quantizer: GFSQ (simplified for TTNN compatibility)
        - Decoder: Upsampling convolutions + ConvNeXt blocks
        - Output: Mel spectrogram reconstruction
    """

    def __init__(
        self,
        device: ttnn.Device,
        num_encoder_layers: int = 2,
        num_decoder_layers: int = 2,
        hidden_dim: int = 256,
        num_mel_bins: int = 100,
    ):
        self.device = device
        self.num_encoder_layers = num_encoder_layers
        self.num_decoder_layers = num_decoder_layers
        self.hidden_dim = hidden_dim
        self.num_mel_bins = num_mel_bins

        # Initialize component lists
        self.encoder_conv_in = []
        self.encoder_blocks = []
        self.encoder_conv_out = None
        self.downsample_conv = []
        self.decoder_conv_in = []
        self.decoder_blocks = []
        self.out_conv = None
        self.coef = None

        logger.info(
            f"TtnnDVAE initialized: {num_encoder_layers} encoder layers, "
            f"{num_decoder_layers} decoder layers, hidden_dim={hidden_dim}"
        )

    def load_weights(self, weights_dict: dict):
        """
        Load weights from PyTorch state dict.

        Args:
            weights_dict: Dictionary containing weight tensors with keys:
                - 'coef': Quantizer coefficient
                - 'downsample_conv.0.weight', 'downsample_conv.0.bias': Downsampling convs
                - 'encoder.conv_in.*': Encoder input convolutions
                - 'encoder.decoder_block.{i}.*': Encoder ConvNeXt blocks
                - 'encoder.conv_out.*': Encoder output convolution
                - 'decoder.conv_in.*': Decoder input convolutions
                - 'decoder.decoder_block.{i}.*': Decoder ConvNeXt blocks
                - 'out_conv.*': Final output convolution
        """
        logger.info("Loading DVAE weights...")

        # Quantizer coefficient
        self.coef = torch_to_ttnn(
            weights_dict['coef'],
            self.device,
            memory_config=get_weights_memory_config(),
        )

        # Downsampling convolutions - need ROW_MAJOR layout for conv2d
        self.downsample_conv = [
            {
                'weight': torch_to_ttnn(
                    weights_dict['downsample_conv.0.weight'],
                    self.device,
                    layout=ttnn.ROW_MAJOR_LAYOUT,  # Conv2d requires ROW_MAJOR
                    memory_config=get_weights_memory_config(),
                ),
                'bias': None,  # Disable bias for testing
            },
            {
                'weight': torch_to_ttnn(
                    weights_dict['downsample_conv.2.weight'],
                    self.device,
                    layout=ttnn.ROW_MAJOR_LAYOUT,  # Conv2d requires ROW_MAJOR
                    memory_config=get_weights_memory_config(),
                ),
                'bias': None,  # Disable bias for testing
            },
        ]

        # Encoder input convolution - need ROW_MAJOR layout for conv2d
        self.encoder_conv_in = [
            {
                'weight': torch_to_ttnn(
                    weights_dict['encoder.conv_in.0.weight'],
                    self.device,
                    layout=ttnn.ROW_MAJOR_LAYOUT,
                    memory_config=get_weights_memory_config(),
                ),
                'bias': None,  # Disable bias for testing
            },
            {
                'weight': torch_to_ttnn(
                    weights_dict['encoder.conv_in.2.weight'],
                    self.device,
                    layout=ttnn.ROW_MAJOR_LAYOUT,
                    memory_config=get_weights_memory_config(),
                ),
                'bias': None,  # Disable bias for testing
            },
        ]

        # Encoder ConvNeXt blocks
        self.encoder_blocks = []
        for i in range(self.num_encoder_layers):
            block_weights = {
                'dwconv_weight': torch_to_ttnn(
                    weights_dict[f'encoder.decoder_block.{i}.dwconv.weight'],
                    self.device,
                    memory_config=get_weights_memory_config(),
                ),
                'dwconv_bias': torch_to_ttnn(
                    weights_dict[f'encoder.decoder_block.{i}.dwconv.bias'],
                    self.device,
                    memory_config=get_weights_memory_config(),
                ),
                'norm_weight': torch_to_ttnn(
                    weights_dict[f'encoder.decoder_block.{i}.norm.weight'],
                    self.device,
                    memory_config=get_weights_memory_config(),
                ),
                'norm_bias': torch_to_ttnn(
                    weights_dict[f'encoder.decoder_block.{i}.norm.bias'],
                    self.device,
                    memory_config=get_weights_memory_config(),
                ),
                'pwconv1_weight': torch_to_ttnn(
                    weights_dict[f'encoder.decoder_block.{i}.pwconv1.weight'],
                    self.device,
                    memory_config=get_weights_memory_config(),
                ),
                'pwconv1_bias': torch_to_ttnn(
                    weights_dict[f'encoder.decoder_block.{i}.pwconv1.bias'],
                    self.device,
                    memory_config=get_weights_memory_config(),
                ),
                'pwconv2_weight': torch_to_ttnn(
                    weights_dict[f'encoder.decoder_block.{i}.pwconv2.weight'],
                    self.device,
                    memory_config=get_weights_memory_config(),
                ),
                'pwconv2_bias': torch_to_ttnn(
                    weights_dict[f'encoder.decoder_block.{i}.pwconv2.bias'],
                    self.device,
                    memory_config=get_weights_memory_config(),
                ),
            }
            self.encoder_blocks.append(block_weights)

        # Encoder output convolution
        # Encoder output - need ROW_MAJOR layout for conv2d
        self.encoder_conv_out = torch_to_ttnn(
            weights_dict['encoder.conv_out.weight'],
            self.device,
            layout=ttnn.ROW_MAJOR_LAYOUT,
            memory_config=get_weights_memory_config(),
        )

        # Decoder input convolution - need ROW_MAJOR layout for conv2d
        self.decoder_conv_in = [
            {
                'weight': torch_to_ttnn(
                    weights_dict['decoder.conv_in.0.weight'],
                    self.device,
                    layout=ttnn.ROW_MAJOR_LAYOUT,
                    memory_config=get_weights_memory_config(),
                ),
                'bias': None,  # Disable bias for testing
            },
            {
                'weight': torch_to_ttnn(
                    weights_dict['decoder.conv_in.2.weight'],
                    self.device,
                    layout=ttnn.ROW_MAJOR_LAYOUT,
                    memory_config=get_weights_memory_config(),
                ),
                'bias': None,  # Disable bias for testing
            },
        ]

        # Decoder ConvNeXt blocks
        self.decoder_blocks = []
        for i in range(self.num_decoder_layers):
            block_weights = {
                'dwconv_weight': torch_to_ttnn(
                    weights_dict[f'decoder.decoder_block.{i}.dwconv.weight'],
                    self.device,
                    memory_config=get_weights_memory_config(),
                ),
                'dwconv_bias': torch_to_ttnn(
                    weights_dict[f'decoder.decoder_block.{i}.dwconv.bias'],
                    self.device,
                    memory_config=get_weights_memory_config(),
                ),
                'norm_weight': torch_to_ttnn(
                    weights_dict[f'decoder.decoder_block.{i}.norm.weight'],
                    self.device,
                    memory_config=get_weights_memory_config(),
                ),
                'norm_bias': torch_to_ttnn(
                    weights_dict[f'decoder.decoder_block.{i}.norm.bias'],
                    self.device,
                    memory_config=get_weights_memory_config(),
                ),
                'pwconv1_weight': torch_to_ttnn(
                    weights_dict[f'decoder.decoder_block.{i}.pwconv1.weight'],
                    self.device,
                    memory_config=get_weights_memory_config(),
                ),
                'pwconv1_bias': torch_to_ttnn(
                    weights_dict[f'decoder.decoder_block.{i}.pwconv1.bias'],
                    self.device,
                    memory_config=get_weights_memory_config(),
                ),
                'pwconv2_weight': torch_to_ttnn(
                    weights_dict[f'decoder.decoder_block.{i}.pwconv2.weight'],
                    self.device,
                    memory_config=get_weights_memory_config(),
                ),
                'pwconv2_bias': torch_to_ttnn(
                    weights_dict[f'decoder.decoder_block.{i}.pwconv2.bias'],
                    self.device,
                    memory_config=get_weights_memory_config(),
                ),
            }
            self.decoder_blocks.append(block_weights)

        # Output convolution - need ROW_MAJOR layout for conv2d
        self.out_conv = {
            'weight': torch_to_ttnn(
                weights_dict['out_conv.weight'],
                self.device,
                layout=ttnn.ROW_MAJOR_LAYOUT,
                memory_config=get_weights_memory_config(),
            ),
            'bias': None,  # Output conv typically has no bias in DVAE
        }

        logger.info("✅ DVAE weights loaded")

    def __call__(self, mel_spectrogram: ttnn.Tensor) -> ttnn.Tensor:
        """
        Forward pass of DVAE.

        Args:
            mel_spectrogram: Input mel spectrogram in NHWC format [batch_size, 1, time_steps, num_mel_bins]

        Returns:
            ttnn.Tensor: Reconstructed mel spectrogram in NHWC format [batch_size, 1, time_steps, num_mel_bins]
        """
        # Input is already in NHWC format: [batch, H=1, W=time_steps, C=mel_bins]
        x = mel_spectrogram

        # Encoder
        encoded = self._encode(x)

        # Quantization (simplified - bypass quantization for reconstruction testing)
        # In full implementation, this would apply GFSQ quantization
        quantized = encoded

        # Decoder
        reconstructed = self._decode(quantized)

        return reconstructed

    def _encode(self, x: ttnn.Tensor) -> ttnn.Tensor:
        """
        Encoder forward pass.
        Input x: [batch, H=1, W=time_steps, C=mel_bins] (NHWC format)
        """
        # Create conv config for encoder
        conv_config = ttnn.Conv2dConfig(
            weights_dtype=ttnn.bfloat16,
            output_layout=ttnn.ROW_MAJOR_LAYOUT,
            deallocate_activation=False,
        )

        # Create compute config
        compute_config = ttnn.init_device_compute_kernel_config(
            self.device.arch(),
            math_approx_mode=True,
            math_fidelity=ttnn.MathFidelity.HiFi4,
            fp32_dest_acc_en=False,
            packer_l1_acc=False,
        )

        # Skip coefficient for testing basic conv operations
        # coef_expanded = ttnn.unsqueeze(self.coef, dim=3)  # [1, mel_bins, 1, 1]
        # x = ttnn.multiply(x, coef_expanded)

        # Downsampling convolutions (2D conv)
        # Input: [batch, 1, time_steps, mel_bins] (NHWC)
        for i, conv in enumerate(self.downsample_conv):
            if i == 0:
                # First conv: weight [512, mel_bins, 1, 3]
                # Input: [batch, 1, time_steps, mel_bins], Output: [batch, 1, time_steps, 512]
                x = ttnn.conv2d(
                    input_tensor=x,
                    weight_tensor=conv['weight'],
                    bias_tensor=conv['bias'],
                    in_channels=self.num_mel_bins,
                    out_channels=512,
                    device=self.device,
                    batch_size=x.shape[0],
                    input_height=x.shape[1],  # 1
                    input_width=x.shape[2],   # time_steps
                    kernel_size=(1, 3),
                    stride=(1, 1),
                    padding=(0, 0, 1, 1),  # (top, bottom, left, right)
                    conv_config=conv_config,
                    compute_config=compute_config,
                    groups=1,
                    memory_config=get_activations_memory_config(),
                )
            else:
                # Second conv: weight [512, 512, 1, 4], with stride
                # Input: [batch, 1, time_steps, 512], Output: [batch, 1, time_steps//2, 512]
                x = ttnn.conv2d(
                    input_tensor=x,
                    weight_tensor=conv['weight'],
                    bias_tensor=conv['bias'],
                    in_channels=512,
                    out_channels=512,
                    device=self.device,
                    batch_size=x.shape[0],
                    input_height=x.shape[1],  # 1
                    input_width=x.shape[2],   # time_steps
                    kernel_size=(1, 4),
                    stride=(1, 2),  # Downsampling in width
                    padding=(0, 0, 1, 1),  # (top, bottom, left, right)
                    conv_config=conv_config,
                    compute_config=compute_config,
                    groups=1,
                    memory_config=get_activations_memory_config(),
                )
            x = ttnn.relu(x)

        # Encoder input convolutions
        for i, conv in enumerate(self.encoder_conv_in):
            if i == 0:
                # [64, 512, 1, 3]
                x = ttnn.conv2d(
                    input_tensor=x,
                    weight_tensor=conv['weight'],
                    bias_tensor=conv['bias'],
                    in_channels=512,
                    out_channels=64,
                    device=self.device,
                    batch_size=x.shape[0],
                    input_height=1,
                    input_width=x.shape[3],
                    kernel_size=(1, 3),
                    stride=(1, 1),
                    padding=(0, 1),
                    groups=1,
                    memory_config=get_activations_memory_config(),
                )
            else:
                # [hidden_dim, 64, 1, 3]
                x = ttnn.conv2d(
                    input_tensor=x,
                    weight_tensor=conv['weight'],
                    bias_tensor=conv['bias'],
                    in_channels=64,
                    out_channels=self.hidden_dim,
                    device=self.device,
                    batch_size=x.shape[0],
                    input_height=1,
                    input_width=x.shape[3],
                    kernel_size=(1, 3),
                    stride=(1, 1),
                    padding=(0, 1),
                    groups=1,
                    memory_config=get_activations_memory_config(),
                )
            x = ttnn.relu(x)

        # Encoder ConvNeXt blocks (temporarily disabled for testing)
        # for block_weights in self.encoder_blocks:
        #     x = self._convnext_block(x, block_weights)

        # Encoder output (1x1 conv)
        x = ttnn.conv2d(
            input_tensor=x,
            weight_tensor=self.encoder_conv_out,
            bias_tensor=None,
            in_channels=self.hidden_dim,
            out_channels=1024,
            device=self.device,
            batch_size=x.shape[0],
            input_height=1,
            input_width=x.shape[3],
            kernel_size=(1, 1),
            stride=(1, 1),
            padding=(0, 0),
            groups=1,
            memory_config=get_activations_memory_config(),
        )

        return x

    def _decode(self, x: ttnn.Tensor) -> ttnn.Tensor:
        """
        Decoder forward pass.
        """
        # Decoder input convolutions
        for i, conv in enumerate(self.decoder_conv_in):
            if i == 0:
                # [64, 1024, 1, 3]
                x = ttnn.conv2d(
                    input_tensor=x,
                    weight_tensor=conv['weight'],
                    bias_tensor=conv['bias'],
                    in_channels=1024,
                    out_channels=64,
                    device=self.device,
                    batch_size=x.shape[0],
                    input_height=1,
                    input_width=x.shape[3],
                    kernel_size=(1, 3),
                    stride=(1, 1),
                    padding=(0, 1),
                    groups=1,
                    memory_config=get_activations_memory_config(),
                )
            else:
                # [hidden_dim, 64, 1, 3]
                x = ttnn.conv2d(
                    input_tensor=x,
                    weight_tensor=conv['weight'],
                    bias_tensor=conv['bias'],
                    in_channels=64,
                    out_channels=self.hidden_dim,
                    device=self.device,
                    batch_size=x.shape[0],
                    input_height=1,
                    input_width=x.shape[3],
                    kernel_size=(1, 3),
                    stride=(1, 1),
                    padding=(0, 1),
                    groups=1,
                    memory_config=get_activations_memory_config(),
                )
            x = ttnn.relu(x)

        # Decoder ConvNeXt blocks (temporarily disabled for testing)
        # for block_weights in self.decoder_blocks:
        #     x = self._convnext_block(x, block_weights)

        # Output convolution
        x = ttnn.conv2d(
            input_tensor=x,
            weight_tensor=self.out_conv['weight'],
            bias_tensor=self.out_conv['bias'],
            in_channels=self.hidden_dim,
            out_channels=self.num_mel_bins,
            device=self.device,
            batch_size=x.shape[0],
            input_height=1,
            input_width=x.shape[3],
            kernel_size=(1, 3),
            stride=(1, 1),
            padding=(0, 1),
            groups=1,
            memory_config=get_activations_memory_config(),
        )

        return x

    def _convnext_block(self, x: ttnn.Tensor, weights: dict) -> ttnn.Tensor:
        """
        ConvNeXt block implementation for 2D tensors.

        Args:
            x: Input tensor [batch, channels, 1, time_steps]
            weights: Dictionary containing block weights

        Returns:
            ttnn.Tensor: Output tensor [batch, channels, 1, time_steps]
        """
        residual = x

        # Depthwise conv: (1, 7) kernel with groups=channels for depthwise
        x = ttnn.conv2d(
            input_tensor=x,
            weight_tensor=weights['dwconv']['weight'],
            bias_tensor=weights['dwconv']['bias'],
            in_channels=x.shape[1],  # channels
            out_channels=x.shape[1],  # same as input channels
            device=self.device,
            batch_size=x.shape[0],
            input_height=1,
            input_width=x.shape[3],
            kernel_size=(1, 7),
            stride=(1, 1),
            padding=(0, 3),
            groups=x.shape[1],  # depthwise: groups = channels
            memory_config=get_activations_memory_config(),
        )

        # LayerNorm: Need to reshape for LayerNorm, then reshape back
        # [B, C, 1, T] -> [B, T, C] for LayerNorm, then back
        x_reshaped = ttnn.permute(x, (0, 3, 1, 2))  # [B, T, C, 1]
        x_reshaped = ttnn.squeeze(x_reshaped, dim=3)  # [B, T, C]

        # Apply LayerNorm
        x_reshaped = ttnn.layer_norm(
            x_reshaped,
            weight=weights['norm']['weight'],
            bias=weights['norm']['bias'],
            memory_config=get_activations_memory_config(),
        )

        # Pointwise conv 1: expand channels (Linear layer)
        x_reshaped = ttnn.linear(
            x_reshaped,
            weights['pwconv1']['weight'],
            bias=weights['pwconv1']['bias'],
            memory_config=get_activations_memory_config(),
        )
        x_reshaped = ttnn.gelu(x_reshaped)

        # Pointwise conv 2: reduce channels back (Linear layer)
        x_reshaped = ttnn.linear(
            x_reshaped,
            weights['pwconv2']['weight'],
            bias=weights['pwconv2']['bias'],
            memory_config=get_activations_memory_config(),
        )

        # Reshape back: [B, T, C] -> [B, C, 1, T]
        x_reshaped = x_reshaped.unsqueeze(3)  # [B, T, C, 1]
        x = ttnn.permute(x_reshaped, (0, 2, 3, 1))  # [B, C, 1, T]

        # Residual connection
        x = ttnn.add(x, residual, memory_config=get_activations_memory_config())

        return x
