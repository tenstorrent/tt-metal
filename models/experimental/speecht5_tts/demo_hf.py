#!/usr/bin/env python3
# SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC

# SPDX-License-Identifier: Apache-2.0

"""
Clean HuggingFace SpeechT5 Demo - Text-to-Speech Generation

This demo showcases the standard HuggingFace SpeechT5 pipeline.
Generates high-quality speech from text input using HF models.

Features:
- Full HF implementation (Encoder + Decoder + Postnet)
- Clean, production-ready code
- Performance benchmarking
"""

import torch
import soundfile as sf
from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan
from datasets import load_dataset


def generate_speech_hf(
    text,
    speaker_embeddings,
    processor,
    model,
    vocoder,
    max_steps=100,
    return_stats=False,
):
    """
    Generate speech using standard HuggingFace SpeechT5 pipeline.

    Args:
        text: Input text string
        speaker_embeddings: Speaker embedding tensor
        processor: SpeechT5Processor
        model: SpeechT5ForTextToSpeech
        vocoder: SpeechT5HifiGan
        max_steps: Maximum number of generation steps (default: 100)
        return_stats: If True, return statistics along with speech (default: False)

    Returns:
        torch.Tensor: Generated audio waveform (if return_stats=False)
        tuple: (speech, stats_dict) if return_stats=True
    """

    import time

    # Process input text
    generation_start = time.time()
    inputs = processor(text=text, return_tensors="pt")

    # Generate speech using HF model
    speech = model.generate_speech(
        inputs["input_ids"],
        speaker_embeddings,
        vocoder=vocoder,
    )

    generation_time = time.time() - generation_start
    audio_duration = len(speech.squeeze()) / 16000.0

    if return_stats:
        # Calculate basic stats (HF doesn't provide detailed timing breakdown)
        stats = {
            "generation_time": generation_time,
            "audio_duration": audio_duration,
            "audio_samples": len(speech.squeeze()),
            "sample_rate": 16000,
            "rtf": generation_time / audio_duration if audio_duration > 0 else 0,  # Real-time factor
        }
        return speech, stats
    else:
        return speech


def main():
    """Main demo function."""

    import argparse

    parser = argparse.ArgumentParser(description="HuggingFace SpeechT5 TTS Demo")
    parser.add_argument(
        "texts", nargs="+", help="Input text(s) to convert to speech. Each text will generate a separate audio file."
    )
    parser.add_argument(
        "--output_dir", default=".", help="Output directory for audio files (default: current directory)"
    )
    parser.add_argument("--max_steps", type=int, default=100, help="Maximum number of generation steps (default: 100)")

    args = parser.parse_args()

    # Create output directory if it doesn't exist
    import os

    os.makedirs(args.output_dir, exist_ok=True)

    try:
        print("ðŸ”¥ Loading HuggingFace SpeechT5 models...")
        print("   This may take a moment to download and load the models")

        # Load models
        processor = SpeechT5Processor.from_pretrained("microsoft/speecht5_tts")
        model = SpeechT5ForTextToSpeech.from_pretrained("microsoft/speecht5_tts")
        vocoder = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan")

        # Load speaker embeddings
        embeddings_dataset = load_dataset("Matthijs/cmu-arctic-xvectors", split="validation")
        speaker_embeddings = torch.tensor(embeddings_dataset[7306]["xvector"]).unsqueeze(0)

        print("âœ… Models loaded successfully!")

        # Warm-up phase (optional for HF)
        print("ðŸ”¥ Performing warm-up generation...")
        warmup_start_time = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None
        warmup_end_time = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None

        if torch.cuda.is_available():
            warmup_start_time.record()

        warmup_text = "Hi there. How are you? What are you doing?"
        warmup_speech = generate_speech_hf(
            warmup_text,
            speaker_embeddings,
            processor,
            model,
            vocoder,
            max_steps=args.max_steps,
        )

        if torch.cuda.is_available():
            warmup_end_time.record()
            torch.cuda.synchronize()
            warmup_duration = warmup_start_time.elapsed_time(warmup_end_time) / 1000.0
        else:
            warmup_duration = 0.1  # Dummy timing for CPU

        print(f"âœ… Warm-up completed in {warmup_duration:.1f}s (generated {len(warmup_speech)} samples)")

        # Generate speech for each input text
        results = []
        for i, text in enumerate(args.texts, 1):
            # Generate filename from text (sanitize for filesystem)
            safe_text = "".join(c for c in text[:50] if c.isalnum() or c in (" ", "-", "_")).rstrip()
            if not safe_text:
                safe_text = f"speech_{i}"
            safe_text = safe_text.replace(" ", "_")
            output_file = os.path.join(args.output_dir, f"speech_hf_{safe_text}.wav")

            print(f"\nðŸŽµ Generating speech for text {i}: '{text}'")

            # Time the generation
            generation_start = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None
            generation_end = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None

            if torch.cuda.is_available():
                generation_start.record()

            speech, generation_stats = generate_speech_hf(
                text,
                speaker_embeddings,
                processor,
                model,
                vocoder,
                max_steps=args.max_steps,
                return_stats=True,
            )

            if torch.cuda.is_available():
                generation_end.record()
                torch.cuda.synchronize()
                generation_time = generation_start.elapsed_time(generation_end) / 1000.0
            else:
                generation_time = generation_stats["generation_time"]

            # Update stats with CUDA timing if available
            generation_stats["generation_time"] = generation_time

            # Save audio
            sf.write(output_file, speech.squeeze().detach().numpy(), samplerate=16000)
            audio_duration = len(speech.squeeze()) / 16000.0

            # Store results
            result = {
                "text": text,
                "output_file": output_file,
                "generation_time": generation_time,
                "audio_duration": audio_duration,
                "audio_samples": len(speech.squeeze()),
                "rtf": generation_time / audio_duration if audio_duration > 0 else 0,
            }
            results.append(result)

            print(f"   âœ… Saved to: {output_file}")
            print(".1f")
            print(".1f")

        # Display summary table
        print("\n" + "=" * 120)
        print("ðŸ“Š HUGGINGFACE SPEECHT5 INFERENCE SUMMARY")
        print("=" * 120)
        print(f"{'#':<4} {'Text':<25} {'Time(s)':<10} {'Audio(s)':<10} {'RTF':<8} {'Samples':<10}")
        print("-" * 100)

        total_generation_time = 0
        total_audio_duration = 0
        total_samples = 0

        for i, result in enumerate(results, 1):
            truncated_text = result["text"][:20] + "..." if len(result["text"]) > 23 else result["text"]
            print(
                f"{i:<4} {truncated_text:<25} {result['generation_time']:<10.3f} {result['audio_duration']:<10.1f} {result['rtf']:<8.2f} {result['audio_samples']:<10}"
            )
            total_generation_time += result["generation_time"]
            total_audio_duration += result["audio_duration"]
            total_samples += result["audio_samples"]

        print("-" * 100)
        print(
            f"{'TOTAL':<4} {'':<25} {total_generation_time:<10.3f} {total_audio_duration:<10.1f} {'-':<8} {total_samples:<10}"
        )

        print(f"\nðŸ“ˆ Overall Statistics:")
        print(f"   â€¢ Total inference time: {total_generation_time:.3f}s")
        print(f"   â€¢ Total audio duration: {total_audio_duration:.3f}s")
        print(f"   â€¢ Total samples generated: {total_samples}")
        print(f"   â€¢ Average RTF: {total_generation_time/total_audio_duration:.3f} (real-time factor)")
        print(f"   â€¢ Average audio duration: {total_audio_duration/len(results):.3f}s per text")

        print("\nâœ… Demo completed successfully!")

    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback

        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
