# SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC
#
# SPDX-License-Identifier: Apache-2.0

"""
MaskFormer heads for mask logits and class predictions.

The heads consume transformer decoder outputs and produce:

* Semantic class logits per query (including the ``no_object`` token).
* Mask embeddings projected back to the pixel decoder feature space.
* Final mask logits generated by combining mask embeddings with pixel features.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Optional, Tuple, Any, Dict

import torch
import warnings

try:
    from models.common.utility_functions import tt_to_torch_tensor, is_blackhole
except ModuleNotFoundError:  # pragma: no cover - optional when running outside repo context
    tt_to_torch_tensor = None

    def is_blackhole() -> bool:
        return False


try:
    from transformers.models.maskformer.modeling_maskformer import MaskformerMLPPredictionHead
except ModuleNotFoundError:  # pragma: no cover - optional dependency
    MaskformerMLPPredictionHead = None

from .backbone_swin import DEFAULT_TT_DTYPE
from .tt_configs import build_heads_program_configs
from .ttnn_compat import ttnn, require_ttnn
from .weights import extract_heads_state


@dataclass
class MaskFormerHeadsConfig:
    """Configuration for segmentation heads."""

    num_classes: int = 150
    hidden_dim: int = 256
    mask_dim: int = 256


class MaskFormerHeads:
    """Bundle of TT-NN projection heads for MaskFormer outputs."""

    def __init__(
        self,
        config: MaskFormerHeadsConfig,
        device: Optional[object],
        *,
        dtype: Optional[object] = DEFAULT_TT_DTYPE,
    ) -> None:
        self.config = config
        if device is not None and ttnn is None:
            require_ttnn("allocate MaskFormer heads on a TT device")
        self.device = device
        self.dtype = dtype
        self._class_predictor: Optional[torch.nn.Linear] = None
        self._mask_embedder: Optional[torch.nn.Module] = None
        # TTNN-prepared weight tensors for on-device execution
        self._tt_class_weight = None
        self._tt_class_bias = None
        self._tt_mlp_w1 = None
        self._tt_mlp_b1 = None
        self._tt_mlp_w2 = None
        self._tt_mlp_b2 = None
        self._tt_mlp_w3 = None
        self._tt_mlp_b3 = None
        if MaskformerMLPPredictionHead is not None:
            self._class_predictor = torch.nn.Linear(config.hidden_dim, config.num_classes + 1)
            self._mask_embedder = MaskformerMLPPredictionHead(
                input_dim=config.hidden_dim,
                hidden_dim=config.hidden_dim,
                output_dim=config.mask_dim,
                num_layers=3,
            )
        self._torch_state: Dict[str, Any] = {}

    def _make_compute_kernel_config(self):
        """Select an appropriate compute kernel config for the current arch."""

        if ttnn is None:
            raise RuntimeError("TT-NN runtime is required to construct compute kernel configs for MaskFormer heads.")
        ComputeConfigClass = ttnn.WormholeComputeKernelConfig
        try:
            if is_blackhole() and hasattr(ttnn, "types") and hasattr(ttnn.types, "BlackholeComputeKernelConfig"):
                ComputeConfigClass = ttnn.types.BlackholeComputeKernelConfig  # type: ignore[assignment]
        except Exception:
            # Conservatively fall back to Wormhole config on detection errors.
            pass
        return ComputeConfigClass(
            math_fidelity=ttnn.MathFidelity.HiFi2,
            math_approx_mode=False,
            fp32_dest_acc_en=True,
            packer_l1_acc=False,
        )

    def forward(
        self,
        decoder_outputs: torch.Tensor,
        pixel_embeddings: torch.Tensor,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Compute class and mask logits.

        Prefer TTNN path when device/weights are ready; otherwise, fall back to HF/torch.
        """

        # TTNN execution path
        if self.device is not None and ttnn is not None and self._tt_class_weight is not None:
            heads_cfg = build_heads_program_configs(
                num_queries=int(decoder_outputs.shape[1]), hidden_dim=self.config.hidden_dim
            )
            act_mem = heads_cfg.activation_memory or ttnn.DRAM_MEMORY_CONFIG

            # decoder_outputs: [B, Q, D]
            # pixel_embeddings: [B, Cmask, H, W]
            # Convert inputs to TT tensors if needed
            if isinstance(decoder_outputs, torch.Tensor):
                dec_tt = ttnn.from_torch(
                    decoder_outputs.detach().contiguous(),
                    dtype=self.dtype or DEFAULT_TT_DTYPE,
                    layout=ttnn.TILE_LAYOUT,
                    device=self.device,
                    memory_config=act_mem,
                )
            else:
                dec_tt = decoder_outputs
                if dec_tt.get_layout() != ttnn.TILE_LAYOUT:
                    dec_tt = ttnn.to_layout(dec_tt, ttnn.TILE_LAYOUT)
            if isinstance(pixel_embeddings, torch.Tensor):
                pix_nhwc = pixel_embeddings.detach().contiguous().permute(0, 2, 3, 1)
                pix_tt = ttnn.from_torch(
                    pix_nhwc,
                    dtype=self.dtype or DEFAULT_TT_DTYPE,
                    layout=ttnn.ROW_MAJOR_LAYOUT,
                    device=self.device,
                    memory_config=act_mem,
                )
            else:
                pix_tt = pixel_embeddings
                if pix_tt.get_layout() != ttnn.ROW_MAJOR_LAYOUT:
                    pix_tt = ttnn.to_layout(pix_tt, ttnn.ROW_MAJOR_LAYOUT)

            matmul_cfg = self._make_compute_kernel_config()
            linear_kwargs = {}
            matmul_kwargs = {}
            if heads_cfg.core_grid is not None:
                linear_kwargs["core_grid"] = heads_cfg.core_grid
                matmul_kwargs["core_grid"] = heads_cfg.core_grid
            if heads_cfg.matmul is not None:
                linear_kwargs["program_config"] = heads_cfg.matmul
                matmul_kwargs["program_config"] = heads_cfg.matmul

            def _linear(x, w, b=None, activation=None):
                if hasattr(ttnn, "linear"):
                    try:
                        return ttnn.linear(
                            x,
                            w,
                            b,
                            activation=activation,
                            compute_kernel_config=matmul_cfg,
                            dtype=self.dtype or DEFAULT_TT_DTYPE,
                            **linear_kwargs,
                        )
                    except Exception:
                        pass
                y = ttnn.matmul(x, w, transpose_b=True, compute_kernel_config=matmul_cfg, **matmul_kwargs)
                if b is not None:
                    y = ttnn.add(y, b)
                if activation is not None:
                    y = ttnn.relu(y)
                return y

            # Class logits: [B, Q, D] x [NumCls+1, D]^T -> [B, Q, NumCls+1]
            class_logits_tt = _linear(dec_tt, self._tt_class_weight, self._tt_class_bias)

            # Mask embedder MLP: three linears with ReLU
            x = _linear(dec_tt, self._tt_mlp_w1, self._tt_mlp_b1, activation="relu")
            x = _linear(x, self._tt_mlp_w2, self._tt_mlp_b2, activation="relu")
            mask_embeddings_tt = _linear(x, self._tt_mlp_w3, self._tt_mlp_b3, activation=None)

            # Compute mask logits via batched matmul: [B,Q,Cm] x [B,Cm,H*W] -> [B,Q,H*W] -> [B,Q,H,W]
            B = int(pix_tt.shape[0])
            H = int(pix_tt.shape[1])
            W = int(pix_tt.shape[2])
            C = int(pix_tt.shape[3])
            pix_seq = ttnn.reshape(pix_tt, (B, H * W, C))
            pix_seq_t = ttnn.to_layout(pix_seq, ttnn.TILE_LAYOUT)
            logits_seq = ttnn.matmul(
                mask_embeddings_tt, pix_seq_t, transpose_b=True, compute_kernel_config=matmul_cfg, **matmul_kwargs
            )
            logits_rm = ttnn.to_layout(logits_seq, ttnn.ROW_MAJOR_LAYOUT)
            logits_hw = ttnn.reshape(logits_rm, (B, int(logits_rm.shape[1]), H, W))

            # Return torch tensors for downstream post-processing
            class_logits = self._ensure_torch_tensor(logits_to_torch(class_logits_tt))
            mask_logits = self._ensure_torch_tensor(logits_to_torch(logits_hw))
            return class_logits, mask_logits

        # Fallback path (HF/torch)
        if self._class_predictor is None or self._mask_embedder is None:
            raise NotImplementedError("TT-NN heads pending; install transformers/torch for CPU fallback execution.")

        decoder_outputs = self._ensure_torch_tensor(decoder_outputs)
        pixel_embeddings = self._ensure_torch_tensor(pixel_embeddings)

        class_logits = self._class_predictor(decoder_outputs)
        mask_embeddings = self._mask_embedder(decoder_outputs)  # [B, Q, mask_dim]
        mask_logits = torch.einsum("bqc,bchw->bqhw", mask_embeddings, pixel_embeddings)
        return class_logits, mask_logits

    def load_weights(self, weights: Dict[str, object]) -> None:
        if self._class_predictor is None or self._mask_embedder is None:
            return

        state = extract_heads_state(weights)
        torch_state = {name: self._ensure_torch_tensor(tensor) for name, tensor in state.items()}
        class_state = {
            k[len("class_predictor.") :]: v for k, v in torch_state.items() if k.startswith("class_predictor.")
        }
        missing, unexpected = self._class_predictor.load_state_dict(class_state, strict=False)
        if missing or unexpected:
            warnings.warn(
                f"Class predictor weight load mismatch. Missing: {missing[:5]} Unexpected: {unexpected[:5]}",
                RuntimeWarning,
            )

        mask_state = {k[len("mask_embedder.") :]: v for k, v in torch_state.items() if k.startswith("mask_embedder.")}
        missing, unexpected = self._mask_embedder.load_state_dict(mask_state, strict=False)
        if missing or unexpected:
            warnings.warn(
                f"Mask embedder weight load mismatch. Missing: {missing[:5]} Unexpected: {unexpected[:5]}",
                RuntimeWarning,
            )
        # Prepare TTNN weights for on-device execution if a device is present
        if self.device is not None and ttnn is not None:

            def _to_tt_linear(w: torch.Tensor, b: Optional[torch.Tensor]):
                wt = ttnn.from_torch(
                    w.detach().contiguous(),
                    dtype=self.dtype or DEFAULT_TT_DTYPE,
                    layout=ttnn.TILE_LAYOUT,
                    device=self.device,
                    memory_config=ttnn.L1_MEMORY_CONFIG,
                )
                bt = None
                if b is not None:
                    b_reshaped = b.detach().contiguous().view(1, 1, -1)
                    bt = ttnn.from_torch(
                        b_reshaped,
                        dtype=self.dtype or DEFAULT_TT_DTYPE,
                        layout=ttnn.ROW_MAJOR_LAYOUT,
                        device=self.device,
                        memory_config=ttnn.L1_MEMORY_CONFIG,
                    )
                    bt = ttnn.to_layout(bt, ttnn.TILE_LAYOUT)
                return wt, bt

            # Class predictor
            cw = self._class_predictor.weight
            cb = self._class_predictor.bias
            self._tt_class_weight, self._tt_class_bias = _to_tt_linear(cw, cb)

            # MLP: 3 PredictionBlock layers; take their inner Linear at index 0
            def _extract_linear(module):
                if hasattr(module, "_modules") and "0" in module._modules:
                    return module._modules["0"]
                # Fallback: try common attributes
                for attr in ("linear", "fc", "proj"):
                    if hasattr(module, attr):
                        return getattr(module, attr)
                raise AttributeError("Unable to locate inner Linear layer in PredictionBlock")

            l0 = _extract_linear(self._mask_embedder.layers[0])
            l1 = _extract_linear(self._mask_embedder.layers[1])
            l2 = _extract_linear(self._mask_embedder.layers[2])
            self._tt_mlp_w1, self._tt_mlp_b1 = _to_tt_linear(l0.weight, l0.bias)
            self._tt_mlp_w2, self._tt_mlp_b2 = _to_tt_linear(l1.weight, l1.bias)
            self._tt_mlp_w3, self._tt_mlp_b3 = _to_tt_linear(l2.weight, l2.bias)

    @classmethod
    def from_huggingface(
        cls,
        weights: Dict[str, object],
        *,
        config: MaskFormerHeadsConfig,
        device: Optional[object] = None,
    ) -> "MaskFormerHeads":
        heads = cls(config=config, device=device)
        heads.load_weights(weights)
        return heads

    def _ensure_torch_tensor(self, tensor: Any) -> torch.Tensor:
        if isinstance(tensor, torch.Tensor):
            return tensor
        if tt_to_torch_tensor is not None:
            try:
                return tt_to_torch_tensor(tensor)
            except Exception:
                pass
        if hasattr(tensor, "to_torch"):
            return tensor.to_torch()
        if hasattr(tensor, "cpu"):
            return torch.tensor(tensor.cpu().numpy())
        if isinstance(tensor, (list, tuple)):
            return torch.tensor(tensor)
        raise TypeError(f"Unsupported tensor type for conversion to torch: {type(tensor)!r}")


def logits_to_torch(tensor):
    """Helper to convert a TTNN tensor to torch, preserving floats."""
    if hasattr(ttnn, "to_torch"):
        try:
            return ttnn.to_torch(tensor)
        except Exception:
            pass
    # Fallback best-effort
    if hasattr(tensor, "to_torch"):
        return tensor.to_torch()
    if hasattr(tensor, "cpu"):
        import torch as _torch

        return _torch.tensor(tensor.cpu().numpy())
    raise TypeError("Unsupported TTNN tensor conversion to torch.")
