# SPDX-FileCopyrightText: © 2025 Tenstorrent AI ULC

# SPDX-License-Identifier: Apache-2.0

import statistics
import pytest
import torch
import ttnn
from loguru import logger
from models.common.utility_functions import is_blackhole
from models.perf.benchmarking_utils import BenchmarkProfiler, BenchmarkData

from ....pipelines.mochi.pipeline_mochi import MochiPipeline as TTMochiPipeline


@pytest.mark.parametrize(
    "model_name, image_w, image_h, guidance_scale, num_inference_steps, num_frames",
    [
        ("genmo/mochi-1-preview", 848, 480, 3.5, 50, 168),
    ],
)
@pytest.mark.parametrize(
    "mesh_device, sp_axis, tp_axis, vae_mesh_shape, vae_sp_axis, vae_tp_axis, topology, num_links",
    [
        # VAE mesh shape = (1, 8) is more memory efficient.
        [(2, 4), 0, 1, (1, 8), 0, 1, ttnn.Topology.Linear, 1],
        [(4, 8), 1, 0, (4, 8), 0, 1, ttnn.Topology.Linear, 4],  # note sp <-> tp switch for VAE for memory efficiency.
    ],
    ids=[
        "dit_2x4sp0tp1_vae_1x8sp0tp1",
        "dit_4x8sp1tp0_vae_4x8sp0tp1",
    ],
    indirect=["mesh_device"],
)
@pytest.mark.parametrize(
    "device_params",
    [{"fabric_config": ttnn.FabricConfig.FABRIC_1D}],
    indirect=True,
)
@pytest.mark.parametrize("use_cache", [True, False], ids=["yes_use_cache", "no_use_cache"])
def test_mochi_pipeline_performance(
    *,
    mesh_device: ttnn.MeshDevice,
    model_name: str,
    image_w: int,
    image_h: int,
    guidance_scale: float,
    num_inference_steps: int,
    num_frames: int,
    sp_axis: int,
    tp_axis: int,
    vae_mesh_shape: tuple,
    vae_sp_axis: int,
    vae_tp_axis: int,
    topology: ttnn.Topology,
    num_links: int,
    use_cache: bool,
    is_ci_env: bool,
    galaxy_type: str,
) -> None:
    """Performance test for Mochi pipeline with detailed timing analysis."""

    benchmark_profiler = BenchmarkProfiler()

    # Skip 4U.
    if galaxy_type == "4U":
        # NOTE: Pipelines fail if a performance test is skipped without providing a benchmark output.
        if is_ci_env:
            with benchmark_profiler("run", iteration=0):
                pass

            benchmark_data = BenchmarkData()
            benchmark_data.save_partial_run_json(
                benchmark_profiler,
                run_type="empty_run",
                ml_model_name="empty_run",
            )
        pytest.skip("4U is not supported for this test")

    logger.info(f"  Image size: {image_w}x{image_h}")
    logger.info(f"  Guidance scale: {guidance_scale}")
    logger.info(f"  Inference steps: {num_inference_steps}")
    logger.info(f"  Number frames: {num_frames}")

    sp_factor = tuple(mesh_device.shape)[sp_axis]
    tp_factor = tuple(mesh_device.shape)[tp_axis]

    logger.info(
        f"Creating TT Mochi pipeline with DiT mesh device shape {mesh_device.shape}, VAE mesh device shape {vae_mesh_shape}"
    )
    logger.info(f"DiT SP axis: {sp_axis}, TP axis: {tp_axis}")
    logger.info(f"VAE SP axis: {vae_sp_axis}, TP axis: {tp_axis}")

    tt_pipe = TTMochiPipeline.create_pipeline(mesh_device=mesh_device, checkpoint_name=model_name, use_cache=use_cache)

    # Use a generator for deterministic results.
    generator = torch.Generator("cpu").manual_seed(0)

    # Test prompts
    prompts = [
        """A close-up of a beautiful butterfly landing on a flower, wings gently moving in the breeze.""",
        """A neon-lit alley in a sprawling cyberpunk metropolis at night, rain-slick streets reflecting glowing holograms, dense atmosphere, flying cars in the sky, people in high-tech streetwear — ultra-detailed, cinematic lighting, 4K""",
        """A colossal whale floating through a desert sky like a blimp, casting a long shadow over sand dunes, people in ancient robes watching in awe, golden hour lighting, dreamlike color palette — surrealism, concept art, Greg Rutkowski style""",
        """A Roman general standing on a battlefield at dawn, torn red cape blowing in the wind, distant soldiers forming ranks, painterly brushwork in the style of Caravaggio, chiaroscuro lighting, epic composition""",
        """An epic, high-definition cinematic shot of a rustic snowy cabin glowing warmly at dusk, nestled in a serene winter landscape. Surrounded by gentle snow-covered pines and delicate falling snowflakes — captured in a rich, atmospheric, wide-angle scene with deep cinematic depth and warmth.""",
    ]

    # Warmup run (not timed)
    logger.info("Running warmup iteration...")

    with benchmark_profiler("run", iteration=0):
        frames = tt_pipe(
            prompts[0],
            num_inference_steps=2,  # Small number of steps to reduce test time.
            guidance_scale=guidance_scale,
            num_frames=num_frames,
            height=image_h,
            width=image_w,
            generator=generator,
        ).frames[0]

    logger.info(f"Warmup completed in {benchmark_profiler.get_duration('run', 0):.2f}s")

    # Validate output
    assert frames is not None, "No frames were generated by the TT pipeline"
    assert len(frames) > 0, "Empty frames list generated by the TT pipeline"

    # Check frame properties
    first_frame = frames[0]
    logger.info(f"TT Pipeline generated {len(frames)} frames, first frame size: {first_frame.size}")

    # Optional: Export to video file
    try:
        from diffusers.utils import export_to_video

        export_to_video(frames, "tt_mochi_test_output.mp4", fps=30)
        logger.info("TT Pipeline video exported to tt_mochi_test_output.mp4")
    except ImportError:
        logger.info("Could not export video - diffusers.utils.export_to_video not available")
    except AttributeError as e:
        logger.info(f"AttributeError: {e}")

    # Performance measurement runs
    logger.info("Running performance measurement iterations...")
    num_perf_runs = 1  # For now use 1 prompt to minimize test time.

    # Optional Tracy profiling (if available)
    profiler = None
    try:
        from tracy import Profiler

        profiler = Profiler()
        profiler.enable()
        logger.info("Tracy profiling enabled")
    except ImportError:
        logger.info("Tracy profiler not available, continuing without profiling")

    try:
        for i in range(num_perf_runs):
            logger.info(f"Performance run {i+1}/{num_perf_runs}...")

            # Run pipeline with different prompt
            prompt_idx = (i + 1) % len(prompts)
            with benchmark_profiler("run", iteration=i):
                frames = tt_pipe(
                    prompts[prompt_idx],
                    num_inference_steps=num_inference_steps,
                    guidance_scale=guidance_scale,
                    num_frames=num_frames,
                    height=image_h,
                    width=image_w,
                    generator=generator,
                    profiler=benchmark_profiler,
                    profiler_iteration=i,
                ).frames[0]

            logger.info(f"  Run {i+1} completed in {benchmark_profiler.get_duration('run', i):.2f}s")

    finally:
        if profiler:
            profiler.disable()
            logger.info("Tracy profiling disabled")

    # Calculate statistics
    text_encoder_times = [benchmark_profiler.get_duration("encoder", i) for i in range(num_perf_runs)]
    denoising_times = [benchmark_profiler.get_duration("denoising", i) for i in range(num_perf_runs)]
    vae_times = [benchmark_profiler.get_duration("vae", i) for i in range(num_perf_runs)]
    total_times = [benchmark_profiler.get_duration("run", i) for i in range(num_perf_runs)]

    # Report results
    cfg_factor = tt_pipe.parallel_config.cfg_parallel.factor
    sp_factor = tt_pipe.parallel_config.sequence_parallel.factor
    tp_factor = tt_pipe.parallel_config.tensor_parallel.factor

    print("\n" + "=" * 80)
    print("MOCHI PERFORMANCE RESULTS")
    print("=" * 80)
    print(f"Model: {model_name}")
    print(f"Image Size: {image_w}x{image_h}")
    print(f"Guidance Scale: {guidance_scale}")
    print(f"Inference Steps: {num_inference_steps}")
    print(f"DiT Configuration: cfg={cfg_factor}, sp={sp_factor}, tp={tp_factor}")
    print(f"Mesh Shape: {mesh_device.shape}")
    print(f"VAE Mesh Shape: {vae_mesh_shape}")
    print(f"Topology: {topology}")
    print("-" * 80)

    def print_stats(name, times):
        if not times:
            print(f"{name:25} | No data available")
            return
        mean_time = statistics.mean(times)
        std_time = statistics.stdev(times) if len(times) > 1 else 0
        min_time = min(times)
        max_time = max(times)
        print(
            f"{name:25} | Mean: {mean_time:8.4f}s | Std: {std_time:8.4f}s | Min: {min_time:8.4f}s | Max: {max_time:8.4f}s"
        )

    print_stats("Text Encoding", text_encoder_times)
    print_stats("Denoising", denoising_times)
    print_stats("VAE Decoding", vae_times)
    print_stats("Total Pipeline", total_times)
    print("-" * 80)

    # Validate performance
    measurements = {
        "encoder": statistics.mean(text_encoder_times),
        "denoising": statistics.mean(denoising_times),
        "vae": statistics.mean(vae_times),
        "total": statistics.mean(total_times),
    }
    if tuple(mesh_device.shape) == (2, 4) and vae_mesh_shape == (1, 8):
        expected_metrics = {
            "encoder": 8.0,
            "denoising": 1320,
            "vae": 55,
            "total": 1385,
        }
    elif tuple(mesh_device.shape) == (4, 8) and vae_mesh_shape == (4, 8):
        expected_metrics = {
            "encoder": 8.0,
            "denoising": 400,
            "vae": 22,
            "total": 430,
        }
    else:
        assert False, f"Unknown mesh device for performance comparison: {mesh_device}"

    if is_ci_env:
        # In CI, dump a performance report
        benchmark_data = BenchmarkData()
        for iteration in range(num_perf_runs):
            for step_name in ["encoder", "denoising", "vae", "run"]:
                benchmark_data.add_measurement(
                    profiler=benchmark_profiler,
                    iteration=iteration,
                    step_name=step_name,
                    name=step_name,
                    value=benchmark_profiler.get_duration(step_name, iteration),
                    target=expected_metrics["total" if step_name == "run" else step_name],
                )
        device_name_map = {
            (1, 4): "BH_QB",
            (2, 4): "WH_T3K",
            (4, 8): "BH_GLX" if is_blackhole() else "WH_GLX",
        }
        benchmark_data.save_partial_run_json(
            benchmark_profiler,
            run_type=device_name_map[tuple(mesh_device.shape)],
            ml_model_name="Mochi",
            batch_size=1,
            config_params={
                "width": image_w,
                "height": image_h,
                "num_frames": num_frames,
                "num_steps": num_inference_steps,
                "sp_factor": sp_factor,
                "tp_factor": tp_factor,
                "topology": str(topology),
                "num_links": num_links,
                "fsdp": True,
            },
        )

    pass_perf_check = True
    assert_msgs = []
    for k in expected_metrics.keys():
        if measurements[k] > expected_metrics[k]:
            assert_msgs.append(
                f"Warning: {k} is outside of the tolerance range. Expected: {expected_metrics[k]}, Actual: {measurements[k]}"
            )
            pass_perf_check = False

    assert pass_perf_check, "\n".join(assert_msgs)

    logger.info("Performance test completed successfully!")
