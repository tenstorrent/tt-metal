# models/experimental/tt_dit/tests/models/sd35/test_performance_sd35_medium.py

# SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC
# SPDX-License-Identifier: Apache-2.0

import statistics
import pytest
import ttnn
from loguru import logger
from models.perf.benchmarking_utils import BenchmarkProfiler, BenchmarkData

from ....pipelines.stable_diffusion_35_medium.pipeline_stable_diffusion_35_medium import (
    StableDiffusion3MediumPipeline as TTSD35MediumPipeline,
)
from ....parallel.config import DiTParallelConfig, ParallelFactor


@pytest.mark.parametrize(
    "model_name, image_w, image_h, guidance_scale, num_inference_steps",
    [
        ("stabilityai/stable-diffusion-3.5-medium", 1024, 1024, 4.5, 40),
    ],
)
@pytest.mark.parametrize(
    "mesh_device, sp_axis, tp_axis, num_links",
    [
        # N150 configurations - single card
        [(1, 1), 0, 1, 1],  # Single device, no parallelism
        [(1, 2), 0, 1, 1],  # Tensor parallel on axis 1
        [(1, 2), 1, 0, 1],  # Sequence parallel on axis 1
    ],
    ids=[
        "1x1",
        "1x2sp0tp1",
        "1x2sp1tp0",
    ],
    indirect=["mesh_device"],
)
@pytest.mark.parametrize("device_params", [{"fabric_config": ttnn.FabricConfig.FABRIC_1D}], indirect=True)
@pytest.mark.parametrize("use_cache", [True, False], ids=["yes_use_cache", "no_use_cache"])
def test_sd35_medium_pipeline_performance(
    *,
    mesh_device: ttnn.MeshDevice,
    model_name: str,
    image_w: int,
    image_h: int,
    guidance_scale: float,
    num_inference_steps: int,
    sp_axis: int,
    tp_axis: int,
    num_links: int,
    use_cache: bool,
    is_ci_env: bool,
) -> None:
    """Performance test for SD3.5 Medium pipeline on N150 with detailed timing analysis."""

    benchmark_profiler = BenchmarkProfiler()

    logger.info(f"  Image size: {image_w}x{image_h}")
    logger.info(f"  Guidance scale: {guidance_scale}")
    logger.info(f"  Inference steps: {num_inference_steps}")

    sp_factor = tuple(mesh_device.shape)[sp_axis]
    tp_factor = tuple(mesh_device.shape)[tp_axis]

    logger.info(f"Creating TT SD3.5 Medium pipeline with mesh device shape {mesh_device.shape}")
    logger.info(f"SP axis: {sp_axis}, TP axis: {tp_axis}")

    # Create parallel config for N150
    parallel_config = DiTParallelConfig(
        cfg_parallel=ParallelFactor(factor=1, mesh_axis=0),  # No CFG parallel on N150
        tensor_parallel=ParallelFactor(factor=tp_factor, mesh_axis=tp_axis),
        sequence_parallel=ParallelFactor(factor=sp_factor, mesh_axis=sp_axis),
    )

    # Create the TT SD3.5 Medium pipeline
    tt_pipe = TTSD35MediumPipeline(
        mesh_device=mesh_device,
        enable_t5_text_encoder=False,  # Disable T5 for N150 to save memory
        guidance_cond=1,  # Single condition for N150
        parallel_config=parallel_config,
        num_links=num_links,
        height=image_h,
        width=image_w,
        model_checkpoint_path=model_name,
        use_cache=use_cache,
    )

    # Test prompts
    prompts = [
        "A beautiful landscape with mountains and a lake",
        "A cute cat sitting on a windowsill",
        "A futuristic city skyline at sunset",
    ]

    # Set up timing collector
    tt_pipe.timing_collector = TimingCollector()

    # First run to warm up and verify functionality
    logger.info("Running initial validation...")
    with benchmark_profiler("run", iteration=0):
        images = tt_pipe.run_single_prompt(
            prompt=prompts[0],
            negative_prompt="blurry, low quality",
            num_inference_steps=num_inference_steps,
            seed=42,
        )

    # Validate output
    assert len(images) > 0, "Empty image list generated by the TT pipeline"

    first_image = images[0]
    logger.info(f"TT Pipeline generated image, size: {first_image.size}")

    # Performance measurement runs
    logger.info("Running performance measurement iterations...")
    all_timings = []
    num_perf_runs = 3  # Multiple runs for better statistics

    try:
        for i in range(num_perf_runs):
            logger.info(f"Performance run {i+1}/{num_perf_runs}...")

            # Use different prompt for each run
            prompt_idx = (i + 1) % len(prompts)

            with benchmark_profiler("run", iteration=i):
                images = tt_pipe.run_single_prompt(
                    prompt=prompts[prompt_idx],
                    negative_prompt="blurry, low quality",
                    num_inference_steps=num_inference_steps,
                    seed=42 + i,  # Different seed for variety
                )

            # Collect timing data
            timing_data = tt_pipe.timing_collector.get_timing_data()
            all_timings.append(
                {
                    "text_encoder": timing_data.clip_encoding_time + timing_data.t5_encoding_time,
                    "denoising": sum(timing_data.denoising_step_times),
                    "vae": timing_data.vae_decoding_time,
                    "total": timing_data.total_time,
                }
            )
            logger.info(f"  Run {i+1} completed in {timing_data.total_time:.2f}s")

    except Exception as e:
        logger.error(f"Performance test failed: {e}")
        raise

    # Calculate statistics
    text_encoder_times = [t["text_encoder"] for t in all_timings]
    denoising_times = [t["denoising"] for t in all_timings]
    vae_times = [t["vae"] for t in all_timings]
    total_times = [t["total"] for t in all_timings]

    def print_stats(name, times):
        if times:
            logger.info(f"{name}: mean={statistics.mean(times):.2f}s, " f"min={min(times):.2f}s, max={max(times):.2f}s")

    print("-" * 80)
    print("SD3.5 Medium Pipeline Performance Statistics")
    print("-" * 80)
    print_stats("Text Encoding", text_encoder_times)
    print_stats("Denoising", denoising_times)
    print_stats("VAE Decoding", vae_times)
    print_stats("Total Pipeline", total_times)
    print("-" * 80)

    # Validate that we got reasonable results
    assert len(all_timings) == num_perf_runs, f"Expected {num_perf_runs} timing results, got {len(all_timings)}"
    assert all(t["total"] > 0 for t in all_timings), "All runs should have positive total time"

    # Performance validation for N150
    measurements = {
        "text_encoding_time": statistics.mean(text_encoder_times),
        "denoising_time": statistics.mean(denoising_times),
        "vae_decoding_time": statistics.mean(vae_times),
        "total_time": statistics.mean(total_times),
    }

    # Set reasonable performance targets for N150
    if tuple(mesh_device.shape) == (1, 1):
        expected_metrics = {
            "text_encoding_time": 3.0,  # CLIP only, no T5
            "denoising_time": 45.0,  # Medium model on single device
            "vae_decoding_time": 2.0,  # Fast VAE decoding
            "total_time": 50.0,  # Total pipeline time
        }
    elif tuple(mesh_device.shape) in [(1, 2)]:
        expected_metrics = {
            "text_encoding_time": 3.0,
            "denoising_time": 30.0,  # Faster with parallelism
            "vae_decoding_time": 2.0,
            "total_time": 35.0,
        }
    else:
        assert False, f"Unknown mesh device for performance comparison: {mesh_device}"

    # Validate performance meets expectations
    for metric, expected in expected_metrics.items():
        actual = measurements[metric]
        logger.info(f"{metric}: {actual:.2f}s (expected < {expected:.2f}s)")
        assert actual < expected, f"{metric} too slow: {actual:.2f}s > {expected:.2f}s"

    # Save benchmark data
    benchmark_data = BenchmarkData()
    benchmark_data.save_performance_json(
        benchmark_profiler,
        run_type="sd35_medium_performance",
        ml_model_name="stable-diffusion-3.5-medium",
        measurements=measurements,
        device_config={
            "mesh_shape": tuple(mesh_device.shape),
            "sp_axis": sp_axis,
            "tp_axis": tp_axis,
            "use_cache": use_cache,
        },
    )


@pytest.mark.parametrize(
    "mesh_device, sp_axis, tp_axis, num_links",
    [
        [(1, 1), 0, 1, 1],  # Minimal configuration for functional test
    ],
    ids=["1x1"],
    indirect=["mesh_device"],
)
@pytest.mark.parametrize("device_params", [{"fabric_config": ttnn.FabricConfig.FABRIC_1D}], indirect=True)
def test_sd35_medium_pipeline_functional(
    *,
    mesh_device: ttnn.MeshDevice,
    sp_axis: int,
    tp_axis: int,
    num_links: int,
) -> None:
    """Functional test for SD3.5 Medium pipeline - validates correctness without performance checks."""

    parallel_config = DiTParallelConfig(
        cfg_parallel=ParallelFactor(factor=1, mesh_axis=0),
        tensor_parallel=ParallelFactor(factor=1, mesh_axis=tp_axis),
        sequence_parallel=ParallelFactor(factor=1, mesh_axis=sp_axis),
    )

    # Create pipeline with minimal configuration
    tt_pipe = TTSD35MediumPipeline(
        mesh_device=mesh_device,
        enable_t5_text_encoder=False,
        guidance_cond=1,
        parallel_config=parallel_config,
        num_links=num_links,
        height=512,  # Smaller image for faster test
        width=512,
        model_checkpoint_path="stabilityai/stable-diffusion-3.5-medium",
        use_cache=False,
    )

    # Test with a simple prompt
    images = tt_pipe.run_single_prompt(
        prompt="A red circle on a white background",
        negative_prompt="",
        num_inference_steps=10,  # Few steps for speed
        seed=123,
    )

    # Basic validation
    assert len(images) == 1, "Should generate exactly one image"
    assert images[0].size == (512, 512), f"Image size should be 512x512, got {images[0].size}"

    # Save test image for visual inspection (optional)
    if logger.level <= "DEBUG":
        images[0].save("test_sd35_medium_output.png")
        logger.info("Test image saved to test_sd35_medium_output.png")
