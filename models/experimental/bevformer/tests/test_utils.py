"""
Comprehensive test utilities for comparing ttnn and PyTorch tensor outputs.
Provides detailed statistical analysis and comparison functions with sparsity awareness.
"""

import torch
import ttnn
import numpy as np
from typing import Tuple, Dict, Any, Optional
import logging

logger = logging.getLogger(__name__)


def convert_ttnn_to_torch(ttnn_tensor) -> torch.Tensor:
    """Convert ttnn tensor to PyTorch tensor for comparison."""
    if isinstance(ttnn_tensor, torch.Tensor):
        return ttnn_tensor
    return ttnn.to_torch(ttnn_tensor)


def analyze_sparsity(
    tensor: torch.Tensor, zero_threshold: float = 1e-8, block_size: Optional[int] = None, tensor_name: str = "tensor"
) -> Dict[str, Any]:
    """
    Comprehensive sparsity analysis of a tensor.

    Args:
        tensor: Input tensor to analyze
        zero_threshold: Values below this threshold are considered zero
        block_size: Size for block sparsity analysis (e.g., 4 for 4x4 blocks)
        tensor_name: Name for logging

    Returns:
        Dictionary containing sparsity statistics
    """
    if tensor.numel() == 0:
        return {"error": "Empty tensor"}

    tensor_flat = tensor.float().flatten()
    total_elements = tensor_flat.numel()

    # Basic sparsity
    near_zero_mask = torch.abs(tensor_flat) <= zero_threshold
    num_zeros = near_zero_mask.sum().item()
    num_nonzeros = total_elements - num_zeros
    sparsity_ratio = num_zeros / total_elements

    # Value distribution analysis
    nonzero_values = tensor_flat[~near_zero_mask]

    stats = {
        "tensor_name": tensor_name,
        "shape": tuple(tensor.shape),
        "total_elements": total_elements,
        "zero_threshold": zero_threshold,
        # Basic sparsity metrics
        "num_zeros": num_zeros,
        "num_nonzeros": num_nonzeros,
        "sparsity_ratio": sparsity_ratio,
        "density_ratio": 1.0 - sparsity_ratio,
        # Nonzero value statistics
        "nonzero_min": nonzero_values.min().item() if num_nonzeros > 0 else 0.0,
        "nonzero_max": nonzero_values.max().item() if num_nonzeros > 0 else 0.0,
        "nonzero_mean": nonzero_values.mean().item() if num_nonzeros > 0 else 0.0,
        "nonzero_std": nonzero_values.std().item() if num_nonzeros > 0 else 0.0,
        "nonzero_median": nonzero_values.median().item() if num_nonzeros > 0 else 0.0,
    }

    # Effective rank analysis (for 2D tensors)
    if tensor.dim() == 2 and num_nonzeros > 0:
        try:
            # Compute SVD to get effective rank
            U, S, V = torch.svd(tensor)
            # Effective rank: number of singular values above threshold
            significant_sv = (S > zero_threshold).sum().item()
            total_sv = S.numel()

            # Spectral metrics
            spectral_norm = S.max().item()
            frobenius_norm = torch.norm(tensor, "fro").item()
            nuclear_norm = S.sum().item()

            # Rank-based sparsity
            rank_sparsity = 1.0 - (significant_sv / total_sv)

            stats.update(
                {
                    "effective_rank": significant_sv,
                    "max_rank": total_sv,
                    "rank_sparsity": rank_sparsity,
                    "spectral_norm": spectral_norm,
                    "frobenius_norm": frobenius_norm,
                    "nuclear_norm": nuclear_norm,
                    "condition_number": (S.max() / S[S > zero_threshold].min()).item() if significant_sv > 1 else 1.0,
                }
            )
        except Exception as e:
            logger.warning(f"SVD analysis failed for {tensor_name}: {e}")
            stats.update({"effective_rank": None, "svd_error": str(e)})

    # Block sparsity analysis
    if block_size is not None and tensor.dim() >= 2:
        try:
            h, w = tensor.shape[-2], tensor.shape[-1]
            if h >= int(block_size) and w >= int(block_size):
                # Reshape into blocks
                blocks_h = h // int(block_size)
                blocks_w = w // int(block_size)

                # Extract blocks
                tensor_2d = tensor.view(-1, h, w)  # Flatten batch dimensions
                blocks = tensor_2d[:, : blocks_h * int(block_size), : blocks_w * int(block_size)]
                blocks = blocks.view(tensor_2d.shape[0], blocks_h, int(block_size), blocks_w, int(block_size))
                blocks = blocks.permute(0, 1, 3, 2, 4).contiguous()
                blocks = blocks.view(-1, int(block_size), int(block_size))

                # Analyze block sparsity
                block_norms = torch.norm(blocks.view(blocks.shape[0], -1), dim=1)
                zero_blocks = (block_norms <= zero_threshold).sum().item()
                total_blocks = blocks.shape[0]
                block_sparsity = zero_blocks / total_blocks

                stats.update(
                    {
                        "block_size": block_size,
                        "total_blocks": total_blocks,
                        "zero_blocks": zero_blocks,
                        "block_sparsity": block_sparsity,
                        "structured_sparsity_ratio": block_sparsity,
                    }
                )
        except Exception as e:
            logger.warning(f"Block sparsity analysis failed for {tensor_name}: {e}")
            stats.update({"block_sparsity_error": str(e)})

    # Pattern analysis for attention-like tensors
    if tensor.dim() >= 3:  # Likely attention tensor [batch, heads, seq, seq] or similar
        try:
            # Analyze sparsity across different dimensions
            batch_sparsity = []
            head_sparsity = []

            if tensor.dim() == 4:  # [batch, heads, seq, seq]
                for b in range(tensor.shape[0]):
                    batch_mask = torch.abs(tensor[b]) <= zero_threshold
                    batch_sparsity.append(batch_mask.float().mean().item())

                for h in range(tensor.shape[1]):
                    head_mask = torch.abs(tensor[:, h]) <= zero_threshold
                    head_sparsity.append(head_mask.float().mean().item())

                stats.update(
                    {
                        "batch_sparsity_mean": np.mean(batch_sparsity),
                        "batch_sparsity_std": np.std(batch_sparsity),
                        "head_sparsity_mean": np.mean(head_sparsity),
                        "head_sparsity_std": np.std(head_sparsity),
                        "attention_pattern_analysis": True,
                    }
                )
        except Exception as e:
            logger.warning(f"Attention pattern analysis failed for {tensor_name}: {e}")

    return stats


def analyze_attention_sparsity(
    attention_weights: torch.Tensor, zero_threshold: float = 1e-8, tensor_name: str = "attention_weights"
) -> Dict[str, Any]:
    """
    Specialized sparsity analysis for attention weight tensors.

    Args:
        attention_weights: Attention weight tensor [batch, heads, seq_len, seq_len]
        zero_threshold: Threshold for considering values as zero
        tensor_name: Name for logging

    Returns:
        Dictionary with attention-specific sparsity metrics
    """
    base_stats = analyze_sparsity(attention_weights, zero_threshold, tensor_name=tensor_name)

    if attention_weights.dim() < 3:
        return base_stats

    # Attention-specific analysis
    attention_stats = {}

    try:
        # Assume last two dimensions are [seq_len, seq_len]
        seq_len = attention_weights.shape[-1]

        # Diagonal attention analysis
        if attention_weights.shape[-2] == attention_weights.shape[-1]:
            # Extract diagonal elements
            diag_elements = torch.diagonal(attention_weights, dim1=-2, dim2=-1)
            diag_mask = torch.abs(diag_elements) <= zero_threshold
            diagonal_sparsity = diag_mask.float().mean().item()

            # Extract off-diagonal elements
            mask = ~torch.eye(seq_len, dtype=torch.bool, device=attention_weights.device)
            off_diag_elements = attention_weights[..., mask].view(*attention_weights.shape[:-2], -1)
            off_diag_mask = torch.abs(off_diag_elements) <= zero_threshold
            off_diagonal_sparsity = off_diag_mask.float().mean().item()

            attention_stats.update(
                {
                    "diagonal_sparsity": diagonal_sparsity,
                    "off_diagonal_sparsity": off_diagonal_sparsity,
                    "diagonal_dominance": diagonal_sparsity < off_diagonal_sparsity,
                }
            )

        # Local vs global attention analysis
        if seq_len > 8:  # Only for reasonably sized sequences
            # Local attention (within distance 2)
            local_mask = torch.zeros_like(attention_weights, dtype=torch.bool)
            for i in range(seq_len):
                start = max(0, i - 2)
                end = min(seq_len, i + 3)
                local_mask[..., i, start:end] = True

            local_elements = attention_weights[local_mask]
            local_sparsity_mask = torch.abs(local_elements) <= zero_threshold
            local_sparsity = local_sparsity_mask.float().mean().item()

            # Global attention (rest)
            global_elements = attention_weights[~local_mask]
            global_sparsity_mask = torch.abs(global_elements) <= zero_threshold
            global_sparsity = global_sparsity_mask.float().mean().item()

            attention_stats.update(
                {
                    "local_attention_sparsity": local_sparsity,
                    "global_attention_sparsity": global_sparsity,
                    "locality_preference": local_sparsity < global_sparsity,
                }
            )

        # Entropy-based attention pattern analysis
        if attention_weights.min() >= 0:  # Valid probability distribution
            # Compute entropy for each attention head
            eps = 1e-12
            log_weights = torch.log(attention_weights + eps)
            entropy = -(attention_weights * log_weights).sum(dim=-1)  # Entropy per query

            attention_stats.update(
                {
                    "attention_entropy_mean": entropy.mean().item(),
                    "attention_entropy_std": entropy.std().item(),
                    "attention_entropy_min": entropy.min().item(),
                    "attention_entropy_max": entropy.max().item(),
                }
            )

    except Exception as e:
        logger.warning(f"Attention-specific analysis failed: {e}")
        attention_stats["attention_analysis_error"] = str(e)

    # Merge with base stats
    base_stats.update(attention_stats)
    return base_stats


def compare_sparsity_patterns(
    tensor1: torch.Tensor,
    tensor2: torch.Tensor,
    zero_threshold: float = 1e-8,
    tensor_names: Tuple[str, str] = ("tensor1", "tensor2"),
) -> Dict[str, Any]:
    """
    Compare sparsity patterns between two tensors.

    Args:
        tensor1: First tensor (reference)
        tensor2: Second tensor (comparison)
        zero_threshold: Threshold for zero detection
        tensor_names: Names for the tensors

    Returns:
        Dictionary with sparsity comparison metrics
    """
    if tensor1.shape != tensor2.shape:
        return {"error": f"Shape mismatch: {tensor1.shape} vs {tensor2.shape}"}

    # Individual sparsity analysis
    stats1 = analyze_sparsity(tensor1, zero_threshold, tensor_name=tensor_names[0])
    stats2 = analyze_sparsity(tensor2, zero_threshold, tensor_name=tensor_names[1])

    # Pattern comparison
    flat1 = tensor1.float().flatten()
    flat2 = tensor2.float().flatten()

    mask1 = torch.abs(flat1) <= zero_threshold
    mask2 = torch.abs(flat2) <= zero_threshold

    # Sparsity pattern overlap
    both_zero = (mask1 & mask2).sum().item()
    either_zero = (mask1 | mask2).sum().item()
    only_tensor1_zero = (mask1 & ~mask2).sum().item()
    only_tensor2_zero = (~mask1 & mask2).sum().item()

    pattern_overlap = both_zero / either_zero if either_zero > 0 else 1.0

    # Jaccard similarity of sparsity patterns (intersection over union)
    jaccard_similarity = both_zero / either_zero if either_zero > 0 else 1.0

    # Alternative similarity: intersection over smaller set
    smaller_zero_count = min(mask1.sum().item(), mask2.sum().item())
    intersection_similarity = both_zero / smaller_zero_count if smaller_zero_count > 0 else 1.0

    # Sparsity difference
    sparsity_diff = abs(stats1["sparsity_ratio"] - stats2["sparsity_ratio"])

    comparison_stats = {
        "tensor_names": tensor_names,
        "individual_stats": {tensor_names[0]: stats1, tensor_names[1]: stats2},
        # Pattern comparison
        "pattern_overlap": pattern_overlap,
        "jaccard_similarity": jaccard_similarity,
        "intersection_similarity": intersection_similarity,
        "sparsity_difference": sparsity_diff,
        "both_zero_elements": both_zero,
        "either_zero_elements": either_zero,
        "only_tensor1_zero": only_tensor1_zero,
        "only_tensor2_zero": only_tensor2_zero,
        "total_elements": flat1.numel(),
        # Relative sparsity metrics
        "sparsity_ratio_1": stats1["sparsity_ratio"],
        "sparsity_ratio_2": stats2["sparsity_ratio"],
        "sparsity_correlation": "higher" if stats1["sparsity_ratio"] < stats2["sparsity_ratio"] else "lower",
    }

    return comparison_stats


def print_sparsity_analysis(
    tensor: torch.Tensor,
    zero_threshold: float = 1e-8,
    block_size: Optional[int] = None,
    tensor_name: str = "tensor",
    show_detailed: bool = True,
) -> None:
    """
    Print comprehensive sparsity analysis.

    Args:
        tensor: Tensor to analyze
        zero_threshold: Zero threshold
        block_size: Block size for structured sparsity analysis
        tensor_name: Name for display
        show_detailed: Whether to show detailed statistics
    """
    print(f"\nüï≥Ô∏è  SPARSITY ANALYSIS: {tensor_name}")
    print("=" * 60)

    stats = analyze_sparsity(tensor, zero_threshold, block_size, tensor_name)

    if "error" in stats:
        print(f"‚ùå Error: {stats['error']}")
        return

    # Basic sparsity info
    print(f"üìä BASIC SPARSITY:")
    print(f"  Shape: {stats['shape']}")
    print(f"  Total elements: {stats['total_elements']:,}")
    print(f"  Zero threshold: {stats['zero_threshold']:.2e}")
    print(f"  Zero elements: {stats['num_zeros']:,} ({stats['sparsity_ratio']:.1%})")
    print(f"  Non-zero elements: {stats['num_nonzeros']:,} ({stats['density_ratio']:.1%})")

    # Sparsity categorization
    sparsity_level = stats["sparsity_ratio"]
    if sparsity_level < 0.1:
        sparsity_category = "Dense"
        icon = "üü¶"
    elif sparsity_level < 0.5:
        sparsity_category = "Moderately Sparse"
        icon = "üü®"
    elif sparsity_level < 0.9:
        sparsity_category = "Sparse"
        icon = "üüß"
    else:
        sparsity_category = "Extremely Sparse"
        icon = "üü•"

    print(f"  Category: {icon} {sparsity_category}")

    # Non-zero value statistics
    if stats["num_nonzeros"] > 0:
        print(f"\nüìà NON-ZERO VALUE STATISTICS:")
        print(f"  Range: [{stats['nonzero_min']:.6f}, {stats['nonzero_max']:.6f}]")
        print(f"  Mean: {stats['nonzero_mean']:.6f}")
        print(f"  Median: {stats['nonzero_median']:.6f}")
        print(f"  Std: {stats['nonzero_std']:.6f}")

    # Rank analysis for 2D tensors
    if "effective_rank" in stats and stats["effective_rank"] is not None:
        print(f"\nüî¢ RANK ANALYSIS (2D tensors):")
        print(f"  Effective rank: {stats['effective_rank']}/{stats['max_rank']}")
        print(f"  Rank utilization: {(stats['effective_rank']/stats['max_rank']):.1%}")
        print(f"  Condition number: {stats['condition_number']:.2f}")
        if show_detailed:
            print(f"  Spectral norm: {stats['spectral_norm']:.6f}")
            print(f"  Frobenius norm: {stats['frobenius_norm']:.6f}")
            print(f"  Nuclear norm: {stats['nuclear_norm']:.6f}")

    # Block sparsity
    if "block_sparsity" in stats:
        print(f"\nüß± BLOCK SPARSITY ({stats['block_size']}x{stats['block_size']} blocks):")
        print(f"  Total blocks: {stats['total_blocks']:,}")
        print(f"  Zero blocks: {stats['zero_blocks']:,} ({stats['block_sparsity']:.1%})")
        print(f"  Structured sparsity: {stats['structured_sparsity_ratio']:.1%}")

    # Attention pattern analysis
    if "attention_pattern_analysis" in stats:
        print(f"\nüéØ ATTENTION PATTERN ANALYSIS:")
        print(f"  Batch sparsity: {stats['batch_sparsity_mean']:.1%} ¬± {stats['batch_sparsity_std']:.1%}")
        print(f"  Head sparsity: {stats['head_sparsity_mean']:.1%} ¬± {stats['head_sparsity_std']:.1%}")

    # Attention-specific metrics
    if "diagonal_sparsity" in stats:
        print(f"\nüîç ATTENTION-SPECIFIC METRICS:")
        print(f"  Diagonal sparsity: {stats['diagonal_sparsity']:.1%}")
        print(f"  Off-diagonal sparsity: {stats['off_diagonal_sparsity']:.1%}")
        print(f"  Diagonal dominance: {'‚úÖ' if stats['diagonal_dominance'] else '‚ùå'}")

        if "local_attention_sparsity" in stats:
            print(f"  Local attention sparsity: {stats['local_attention_sparsity']:.1%}")
            print(f"  Global attention sparsity: {stats['global_attention_sparsity']:.1%}")
            print(f"  Locality preference: {'‚úÖ' if stats['locality_preference'] else '‚ùå'}")

        if "attention_entropy_mean" in stats:
            print(f"  Attention entropy: {stats['attention_entropy_mean']:.3f} ¬± {stats['attention_entropy_std']:.3f}")

    print("=" * 60)


def compare_tensor_shapes(torch_tensor: torch.Tensor, ttnn_tensor, tensor_name: str = "tensor") -> bool:
    """
    Compare shapes of torch and ttnn tensors.

    Args:
        torch_tensor: Reference PyTorch tensor
        ttnn_tensor: ttnn tensor or converted torch tensor
        tensor_name: Name for logging

    Returns:
        True if shapes match, False otherwise
    """
    ttnn_as_torch = convert_ttnn_to_torch(ttnn_tensor)

    torch_shape = torch_tensor.shape
    ttnn_shape = ttnn_as_torch.shape

    shapes_match = torch_shape == ttnn_shape

    logger.info(f"{tensor_name} shape comparison:")
    logger.info(f"  PyTorch shape: {torch_shape}")
    logger.info(f"  ttnn shape:    {ttnn_shape}")
    logger.info(f"  Match: {shapes_match}")

    return shapes_match


def compute_error_statistics(torch_tensor: torch.Tensor, ttnn_tensor, tensor_name: str = "tensor") -> Dict[str, float]:
    """
    Compute comprehensive error statistics between torch and ttnn tensors.

    Args:
        torch_tensor: Reference PyTorch tensor
        ttnn_tensor: ttnn tensor to compare
        tensor_name: Name for logging

    Returns:
        Dictionary containing error statistics
    """
    ttnn_as_torch = convert_ttnn_to_torch(ttnn_tensor)

    # Ensure tensors have same shape
    if torch_tensor.shape != ttnn_as_torch.shape:
        raise ValueError(f"Shape mismatch: torch={torch_tensor.shape}, ttnn={ttnn_as_torch.shape}")

    # Convert to float32 for precision in calculations
    torch_flat = torch_tensor.float().flatten()
    ttnn_flat = ttnn_as_torch.float().flatten()

    # Absolute error
    abs_error = torch.abs(torch_flat - ttnn_flat)

    # Relative error (avoid division by zero)
    torch_abs = torch.abs(torch_flat)
    rel_error = torch.where(
        torch_abs > 1e-12,  # Avoid division by very small numbers
        abs_error / torch_abs,
        abs_error,  # Use absolute error when reference is near zero
    )

    # Pearson Correlation Coefficient
    pcc = torch.corrcoef(torch.stack([torch_flat, ttnn_flat]))[0, 1].item()
    if torch.isnan(torch.tensor(pcc)):
        pcc = 0.0  # Handle case where one tensor is constant

    # Compute statistics
    stats = {
        # Basic tensor info
        "num_elements": torch_flat.numel(),
        "torch_dtype": str(torch_tensor.dtype),
        "ttnn_dtype": str(ttnn_as_torch.dtype),
        # Absolute error statistics
        "abs_error_min": abs_error.min().item(),
        "abs_error_max": abs_error.max().item(),
        "abs_error_mean": abs_error.mean().item(),
        "abs_error_std": abs_error.std().item(),
        "abs_error_median": abs_error.median().item(),
        # Relative error statistics
        "rel_error_min": rel_error.min().item(),
        "rel_error_max": rel_error.max().item(),
        "rel_error_mean": rel_error.mean().item(),
        "rel_error_std": rel_error.std().item(),
        "rel_error_median": rel_error.median().item(),
        # Correlation
        "pcc": pcc,
        # Value range statistics
        "torch_min": torch_flat.min().item(),
        "torch_max": torch_flat.max().item(),
        "torch_mean": torch_flat.mean().item(),
        "torch_std": torch_flat.std().item(),
        "ttnn_min": ttnn_flat.min().item(),
        "ttnn_max": ttnn_flat.max().item(),
        "ttnn_mean": ttnn_flat.mean().item(),
        "ttnn_std": ttnn_flat.std().item(),
        # Percentile-based error analysis
        "abs_error_p95": torch.quantile(abs_error, 0.95).item(),
        "abs_error_p99": torch.quantile(abs_error, 0.99).item(),
        "rel_error_p95": torch.quantile(rel_error, 0.95).item(),
        "rel_error_p99": torch.quantile(rel_error, 0.99).item(),
    }

    # Add sparsity analysis
    try:
        torch_sparsity = analyze_sparsity(torch_tensor, tensor_name=f"{tensor_name}_torch")
        ttnn_sparsity = analyze_sparsity(ttnn_as_torch, tensor_name=f"{tensor_name}_ttnn")
        sparsity_comparison = compare_sparsity_patterns(
            torch_tensor, ttnn_as_torch, tensor_names=(f"{tensor_name}_torch", f"{tensor_name}_ttnn")
        )

        # Add key sparsity metrics to main stats
        stats.update(
            {
                "torch_sparsity_ratio": torch_sparsity["sparsity_ratio"],
                "ttnn_sparsity_ratio": ttnn_sparsity["sparsity_ratio"],
                "sparsity_difference": sparsity_comparison["sparsity_difference"],
                "sparsity_pattern_similarity": sparsity_comparison["jaccard_similarity"],
                "torch_density_ratio": torch_sparsity["density_ratio"],
                "ttnn_density_ratio": ttnn_sparsity["density_ratio"],
            }
        )

        # Store detailed sparsity stats separately
        stats["detailed_sparsity"] = {
            "torch_sparsity": torch_sparsity,
            "ttnn_sparsity": ttnn_sparsity,
            "comparison": sparsity_comparison,
        }

    except Exception as e:
        logger.warning(f"Sparsity analysis failed for {tensor_name}: {e}")
        stats["sparsity_analysis_error"] = str(e)

    return stats


def print_detailed_comparison(
    torch_tensor: torch.Tensor,
    ttnn_tensor,
    tensor_name: str = "tensor",
    show_histograms: bool = False,
    show_sparsity: bool = True,
) -> None:
    """
    Print detailed comparison statistics between torch and ttnn tensors.

    Args:
        torch_tensor: Reference PyTorch tensor
        ttnn_tensor: ttnn tensor to compare
        tensor_name: Name for logging
        show_histograms: Whether to show error distribution histograms
        show_sparsity: Whether to show sparsity analysis
    """
    print(f"\n{'='*60}")
    print(f"DETAILED COMPARISON: {tensor_name}")
    print(f"{'='*60}")

    # Shape comparison
    if not compare_tensor_shapes(torch_tensor, ttnn_tensor, tensor_name):
        print("‚ùå Shape mismatch - cannot proceed with detailed comparison")
        return

    # Error statistics
    try:
        stats = compute_error_statistics(torch_tensor, ttnn_tensor, tensor_name)

        print(f"\nüìä TENSOR INFO:")
        print(f"  Shape: {torch_tensor.shape}")
        print(f"  Elements: {stats['num_elements']:,}")
        print(f"  PyTorch dtype: {stats['torch_dtype']}")
        print(f"  ttnn dtype: {stats['ttnn_dtype']}")

        print(f"\nüìà VALUE RANGES:")
        print(
            f"  PyTorch: [{stats['torch_min']:.6f}, {stats['torch_max']:.6f}] (mean: {stats['torch_mean']:.6f}, std: {stats['torch_std']:.6f})"
        )
        print(
            f"  ttnn:    [{stats['ttnn_min']:.6f}, {stats['ttnn_max']:.6f}] (mean: {stats['ttnn_mean']:.6f}, std: {stats['ttnn_std']:.6f})"
        )

        # Sparsity summary
        if show_sparsity and "torch_sparsity_ratio" in stats:
            print(f"\nüï≥Ô∏è  SPARSITY SUMMARY:")
            print(f"  PyTorch sparsity: {stats['torch_sparsity_ratio']:.1%}")
            print(f"  ttnn sparsity:    {stats['ttnn_sparsity_ratio']:.1%}")
            print(f"  Sparsity difference: {stats['sparsity_difference']:.1%}")
            print(f"  Pattern similarity: {stats['sparsity_pattern_similarity']:.3f}")

            # Categorize sparsity level
            avg_sparsity = (stats["torch_sparsity_ratio"] + stats["ttnn_sparsity_ratio"]) / 2
            if avg_sparsity < 0.1:
                sparsity_category = "üü¶ Dense"
            elif avg_sparsity < 0.5:
                sparsity_category = "üü® Moderately Sparse"
            elif avg_sparsity < 0.9:
                sparsity_category = "üüß Sparse"
            else:
                sparsity_category = "üü• Extremely Sparse"
            print(f"  Overall category: {sparsity_category}")

        print(f"\nüéØ CORRELATION:")
        print(f"  Pearson CC: {stats['pcc']:.8f}")

        print(f"\nüìè ABSOLUTE ERROR:")
        print(f"  Min:    {stats['abs_error_min']:.2e}")
        print(f"  Max:    {stats['abs_error_max']:.2e}")
        print(f"  Mean:   {stats['abs_error_mean']:.2e}")
        print(f"  Median: {stats['abs_error_median']:.2e}")
        print(f"  Std:    {stats['abs_error_std']:.2e}")
        print(f"  95th percentile: {stats['abs_error_p95']:.2e}")
        print(f"  99th percentile: {stats['abs_error_p99']:.2e}")

        print(f"\nüìä RELATIVE ERROR (as fraction, e.g., 0.01 = 1%):")
        print(f"  Min:    {stats['rel_error_min']:.2e} ({stats['rel_error_min']*100:.2e}%)")
        print(f"  Max:    {stats['rel_error_max']:.2e} ({stats['rel_error_max']*100:.2e}%)")
        print(f"  Mean:   {stats['rel_error_mean']:.2e} ({stats['rel_error_mean']*100:.2e}%)")
        print(f"  Median: {stats['rel_error_median']:.2e} ({stats['rel_error_median']*100:.2e}%)")
        print(f"  Std:    {stats['rel_error_std']:.2e}")
        print(f"  95th percentile: {stats['rel_error_p95']:.2e} ({stats['rel_error_p95']*100:.2e}%)")
        print(f"  99th percentile: {stats['rel_error_p99']:.2e} ({stats['rel_error_p99']*100:.2e}%)")

        if show_histograms:
            print_error_histograms(torch_tensor, ttnn_tensor, tensor_name)

        # Detailed sparsity analysis if available and requested
        if show_sparsity and "detailed_sparsity" in stats:
            detailed_sparsity = stats["detailed_sparsity"]

            # Quick sparsity comparison insights
            print(f"\nüîç SPARSITY INSIGHTS:")

            # Check if sparsity patterns are preserved
            similarity = detailed_sparsity["comparison"]["jaccard_similarity"]
            if similarity > 0.95:
                pattern_status = "‚úÖ Excellent pattern preservation"
            elif similarity > 0.8:
                pattern_status = "‚úÖ Good pattern preservation"
            elif similarity > 0.5:
                pattern_status = "‚ö†Ô∏è Moderate pattern preservation"
            else:
                pattern_status = "‚ùå Poor pattern preservation"
            print(f"  {pattern_status} ({similarity:.3f})")

            # Check for rank preservation in 2D tensors
            torch_sparsity = detailed_sparsity["torch_sparsity"]
            ttnn_sparsity = detailed_sparsity["ttnn_sparsity"]

            if "effective_rank" in torch_sparsity and torch_sparsity["effective_rank"] is not None:
                torch_rank = torch_sparsity["effective_rank"]
                if "effective_rank" in ttnn_sparsity and ttnn_sparsity["effective_rank"] is not None:
                    ttnn_rank = ttnn_sparsity["effective_rank"]
                    rank_diff = abs(torch_rank - ttnn_rank)
                    if rank_diff <= 1:
                        rank_status = "‚úÖ Rank preserved"
                    elif rank_diff <= 3:
                        rank_status = "‚ö†Ô∏è Rank slightly changed"
                    else:
                        rank_status = "‚ùå Rank significantly changed"
                    print(f"  {rank_status} (torch: {torch_rank}, ttnn: {ttnn_rank})")

    except Exception as e:
        print(f"‚ùå Error computing statistics: {e}")

    print(f"{'='*60}")


def print_error_histograms(
    torch_tensor: torch.Tensor, ttnn_tensor, tensor_name: str = "tensor", bins: int = 10
) -> None:
    """
    Print simple ASCII histograms of error distributions.

    Args:
        torch_tensor: Reference PyTorch tensor
        ttnn_tensor: ttnn tensor to compare
        tensor_name: Name for display
        bins: Number of histogram bins
    """
    try:
        ttnn_as_torch = convert_ttnn_to_torch(ttnn_tensor)

        torch_flat = torch_tensor.float().flatten()
        ttnn_flat = ttnn_as_torch.float().flatten()
        abs_error = torch.abs(torch_flat - ttnn_flat).numpy()

        # Create histogram
        counts, bin_edges = np.histogram(abs_error, bins=bins)

        print(f"\nüìä ABSOLUTE ERROR HISTOGRAM ({tensor_name}):")
        max_count = max(counts) if max(counts) > 0 else 1

        for i in range(len(counts)):
            bin_start = bin_edges[i]
            bin_end = bin_edges[i + 1]
            count = counts[i]
            percentage = (count / len(abs_error)) * 100

            # Create bar representation
            bar_length = int((count / max_count) * 30)
            bar = "‚ñà" * bar_length

            print(f"  [{bin_start:.2e}, {bin_end:.2e}): {bar} {count:>6} ({percentage:5.1f}%)")

    except Exception as e:
        print(f"‚ùå Error creating histogram: {e}")


def check_with_tolerances(
    torch_tensor: torch.Tensor,
    ttnn_tensor,
    pcc_threshold: float = 0.99,
    abs_error_threshold: float = 1e-2,
    rel_error_threshold: float = 0.05,
    max_error_ratio: float = 0.01,
    tensor_name: str = "tensor",
) -> Tuple[bool, Dict[str, Any]]:
    """
    Comprehensive tolerance-based checking with multiple criteria.

    Args:
        torch_tensor: Reference PyTorch tensor
        ttnn_tensor: ttnn tensor to compare
        pcc_threshold: Minimum acceptable Pearson correlation coefficient
        abs_error_threshold: Maximum acceptable absolute error (mean)
        rel_error_threshold: Maximum acceptable relative error (mean)
        max_error_ratio: Maximum acceptable ratio of elements with high error
        tensor_name: Name for logging

    Returns:
        (passed, results_dict) - passed is True if all criteria met
    """
    results = {
        "tensor_name": tensor_name,
        "pcc_threshold": pcc_threshold,
        "abs_error_threshold": abs_error_threshold,
        "rel_error_threshold": rel_error_threshold,
        "max_error_ratio": max_error_ratio,
        "passed": False,
        "individual_checks": {},
        "stats": {},
    }

    try:
        # Shape check
        shapes_match = compare_tensor_shapes(torch_tensor, ttnn_tensor, tensor_name)
        results["individual_checks"]["shapes_match"] = shapes_match

        if not shapes_match:
            return False, results

        # Compute statistics
        stats = compute_error_statistics(torch_tensor, ttnn_tensor, tensor_name)
        results["stats"] = stats

        # Individual tolerance checks
        pcc_pass = stats["pcc"] >= pcc_threshold
        abs_error_pass = stats["abs_error_mean"] <= abs_error_threshold
        rel_error_pass = stats["rel_error_mean"] <= rel_error_threshold

        # High error element ratio check
        ttnn_as_torch = convert_ttnn_to_torch(ttnn_tensor)
        torch_flat = torch_tensor.float().flatten()
        ttnn_flat = ttnn_as_torch.float().flatten()
        abs_error = torch.abs(torch_flat - ttnn_flat)
        high_error_elements = (abs_error > abs_error_threshold).sum().item()
        high_error_ratio = high_error_elements / torch_flat.numel()
        error_ratio_pass = high_error_ratio <= max_error_ratio

        results["individual_checks"].update(
            {
                "pcc_pass": pcc_pass,
                "abs_error_pass": abs_error_pass,
                "rel_error_pass": rel_error_pass,
                "error_ratio_pass": error_ratio_pass,
                "high_error_ratio": high_error_ratio,
            }
        )

        # Overall pass/fail
        all_pass = all([shapes_match, pcc_pass, abs_error_pass, rel_error_pass, error_ratio_pass])

        results["passed"] = all_pass

        # Log results
        print(f"\nüß™ TOLERANCE CHECK RESULTS ({tensor_name}):")
        print(f"  Shapes match: {'‚úÖ' if shapes_match else '‚ùå'}")
        print(f"  PCC ‚â• {pcc_threshold}: {'‚úÖ' if pcc_pass else '‚ùå'} (actual: {stats['pcc']:.6f})")
        print(
            f"  Abs error ‚â§ {abs_error_threshold}: {'‚úÖ' if abs_error_pass else '‚ùå'} (actual: {stats['abs_error_mean']:.6f})"
        )
        print(
            f"  Rel error ‚â§ {rel_error_threshold}: {'‚úÖ' if rel_error_pass else '‚ùå'} (actual: {stats['rel_error_mean']:.6f})"
        )
        print(
            f"  High error ratio ‚â§ {max_error_ratio}: {'‚úÖ' if error_ratio_pass else '‚ùå'} (actual: {high_error_ratio:.6f})"
        )
        print(f"  OVERALL: {'‚úÖ PASSED' if all_pass else '‚ùå FAILED'}")

        return all_pass, results

    except Exception as e:
        logger.error(f"Error in tolerance checking: {e}")
        results["error"] = str(e)
        return False, results


def save_comparison_report(
    torch_tensor: torch.Tensor, ttnn_tensor, tensor_name: str, output_path: str, **kwargs
) -> None:
    """
    Save detailed comparison report to file.

    Args:
        torch_tensor: Reference PyTorch tensor
        ttnn_tensor: ttnn tensor to compare
        tensor_name: Name for the tensor
        output_path: Path to save the report
        **kwargs: Additional parameters for tolerance checking
    """
    try:
        with open(output_path, "w") as f:
            # Redirect print output to file
            import sys

            original_stdout = sys.stdout
            sys.stdout = f

            print(f"COMPARISON REPORT: {tensor_name}")
            print(f"Generated at: {torch.utils.data.get_worker_info()}")  # Timestamp placeholder
            print("=" * 80)

            # Detailed comparison
            print_detailed_comparison(torch_tensor, ttnn_tensor, tensor_name, show_histograms=True)

            # Tolerance checking
            passed, results = check_with_tolerances(torch_tensor, ttnn_tensor, tensor_name=tensor_name, **kwargs)

            # Restore stdout
            sys.stdout = original_stdout

        logger.info(f"Comparison report saved to: {output_path}")

    except Exception as e:
        logger.error(f"Error saving comparison report: {e}")


# Legacy compatibility functions
def check_with_pcc(torch_tensor: torch.Tensor, ttnn_tensor, pcc: float = 0.99) -> Tuple[bool, str]:
    """
    Legacy PCC checking function for backward compatibility.

    Args:
        torch_tensor: Reference tensor
        ttnn_tensor: Tensor to compare
        pcc: PCC threshold

    Returns:
        (passed, message)
    """
    try:
        ttnn_as_torch = convert_ttnn_to_torch(ttnn_tensor)

        if torch_tensor.shape != ttnn_as_torch.shape:
            return False, f"Shape mismatch: {torch_tensor.shape} vs {ttnn_as_torch.shape}"

        stats = compute_error_statistics(torch_tensor, ttnn_tensor)
        actual_pcc = stats["pcc"]

        passed = actual_pcc >= pcc
        message = f"PCC: {actual_pcc:.6f} (threshold: {pcc})"

        return passed, message

    except Exception as e:
        return False, f"Error computing PCC: {e}"


def print_sparsity_summary(tensor: torch.Tensor, tensor_name: str = "tensor", zero_threshold: float = 1e-8) -> None:
    """
    Print a concise sparsity summary for a single tensor.

    Args:
        tensor: Tensor to analyze
        tensor_name: Name for display
        zero_threshold: Threshold for zero detection
    """
    stats = analyze_sparsity(tensor, zero_threshold, tensor_name=tensor_name)

    if "error" in stats:
        print(f"‚ùå Sparsity analysis error for {tensor_name}: {stats['error']}")
        return

    # Categorize sparsity
    sparsity_ratio = stats["sparsity_ratio"]
    if sparsity_ratio < 0.1:
        category = "üü¶ Dense"
    elif sparsity_ratio < 0.5:
        category = "üü® Moderately Sparse"
    elif sparsity_ratio < 0.9:
        category = "üüß Sparse"
    else:
        category = "üü• Extremely Sparse"

    print(
        f"üï≥Ô∏è  {tensor_name}: {category} ({sparsity_ratio:.1%} sparse, {stats['num_zeros']:,}/{stats['total_elements']:,} zeros)"
    )

    # Add rank info if available
    if "effective_rank" in stats and stats["effective_rank"] is not None:
        print(
            f"   Rank: {stats['effective_rank']}/{stats['max_rank']} (utilization: {(stats['effective_rank']/stats['max_rank']):.1%})"
        )

    # Add attention-specific info if available
    if "diagonal_sparsity" in stats:
        print(
            f"   Attention patterns: diag {stats['diagonal_sparsity']:.1%}, off-diag {stats['off_diagonal_sparsity']:.1%}"
        )


def analyze_model_sparsity(model_outputs: Dict[str, torch.Tensor], zero_threshold: float = 1e-8) -> Dict[str, Any]:
    """
    Analyze sparsity patterns across multiple model outputs (e.g., attention weights, features).

    Args:
        model_outputs: Dictionary of tensor_name -> tensor
        zero_threshold: Threshold for zero detection

    Returns:
        Dictionary with aggregated sparsity statistics
    """
    results = {
        "zero_threshold": zero_threshold,
        "tensor_count": len(model_outputs),
        "individual_stats": {},
        "aggregate_stats": {},
    }

    sparsity_ratios = []
    total_elements = 0
    total_zeros = 0

    print(f"\nüîç MODEL SPARSITY ANALYSIS")
    print("=" * 50)

    for tensor_name, tensor in model_outputs.items():
        stats = analyze_sparsity(tensor, zero_threshold, tensor_name)
        results["individual_stats"][tensor_name] = stats

        if "error" not in stats:
            sparsity_ratios.append(stats["sparsity_ratio"])
            total_elements += stats["total_elements"]
            total_zeros += stats["num_zeros"]

            # Print concise summary
            print_sparsity_summary(tensor, tensor_name, zero_threshold)

    # Aggregate statistics
    if sparsity_ratios:
        results["aggregate_stats"] = {
            "mean_sparsity": np.mean(sparsity_ratios),
            "std_sparsity": np.std(sparsity_ratios),
            "min_sparsity": np.min(sparsity_ratios),
            "max_sparsity": np.max(sparsity_ratios),
            "median_sparsity": np.median(sparsity_ratios),
            "overall_sparsity": total_zeros / total_elements,
            "total_elements": total_elements,
            "total_zeros": total_zeros,
        }

        print(f"\nüìä AGGREGATE STATISTICS:")
        print(f"  Overall sparsity: {results['aggregate_stats']['overall_sparsity']:.1%}")
        print(
            f"  Mean tensor sparsity: {results['aggregate_stats']['mean_sparsity']:.1%} ¬± {results['aggregate_stats']['std_sparsity']:.1%}"
        )
        print(
            f"  Range: [{results['aggregate_stats']['min_sparsity']:.1%}, {results['aggregate_stats']['max_sparsity']:.1%}]"
        )
        print(f"  Total elements analyzed: {total_elements:,}")

    print("=" * 50)
    return results


# Example usage function
def example_usage():
    """Example of how to use the comparison utilities with sparsity analysis."""
    print("üöÄ EXAMPLE USAGE OF ENHANCED COMPARISON UTILITIES")
    print("=" * 60)

    # Create example tensors with different sparsity patterns
    torch.manual_seed(42)  # For reproducible results

    # Dense tensor
    torch_ref = torch.randn(2, 64, 64)
    torch_test = torch_ref + 0.01 * torch.randn_like(torch_ref)  # Add small noise

    # Sparse attention-like tensor
    attention_weights = torch.rand(1, 8, 32, 32)
    attention_weights = torch.softmax(attention_weights, dim=-1)

    # Make it sparser by zeroing out small values
    attention_mask = attention_weights > 0.1
    attention_sparse = attention_weights * attention_mask.float()

    # Create TTNN version with slight differences
    attention_ttnn = attention_sparse + 0.005 * torch.randn_like(attention_sparse)
    attention_ttnn = torch.clamp(attention_ttnn, min=0.0)  # Keep non-negative

    print("\nüß™ BASIC COMPARISON WITH SPARSITY:")

    # Basic detailed comparison with sparsity
    print_detailed_comparison(torch_ref, torch_test, "dense_example", show_sparsity=True)

    print("\nüéØ ATTENTION TENSOR SPARSITY ANALYSIS:")

    # Detailed sparsity analysis for attention weights
    print_sparsity_analysis(attention_sparse, tensor_name="attention_weights", show_detailed=True)

    print("\nüìä ATTENTION TENSOR COMPARISON:")

    # Compare attention tensors
    print_detailed_comparison(attention_sparse, attention_ttnn, "attention_comparison", show_sparsity=True)

    print("\nüèóÔ∏è  MODEL SPARSITY OVERVIEW:")

    # Model-wide sparsity analysis
    model_outputs = {
        "dense_features": torch_ref,
        "attention_weights": attention_sparse,
        "output_projection": torch.randn(64, 128) * (torch.rand(64, 128) > 0.3),  # 70% sparse
    }

    model_analysis = analyze_model_sparsity(model_outputs)

    print("\nüîß TOLERANCE CHECKING WITH SPARSITY:")

    # Tolerance checking
    passed, results = check_with_tolerances(
        torch_ref,
        torch_test,
        pcc_threshold=0.95,
        abs_error_threshold=0.02,
        rel_error_threshold=0.1,
        tensor_name="example_tensor",
    )

    print("\nüìã SPARSITY COMPARISON BETWEEN TENSORS:")

    # Direct sparsity comparison
    sparsity_comp = compare_sparsity_patterns(
        attention_sparse, attention_ttnn, tensor_names=("sparse_attention", "ttnn_attention")
    )

    print(f"  Pattern similarity: {sparsity_comp['jaccard_similarity']:.3f}")
    print(f"  Sparsity difference: {sparsity_comp['sparsity_difference']:.1%}")

    # Legacy PCC check
    pcc_passed, pcc_msg = check_with_pcc(torch_ref, torch_test, pcc=0.95)
    print(f"\nüìà Legacy PCC check: {pcc_passed}, {pcc_msg}")

    print(f"\n‚úÖ Example completed! Use these functions to analyze your BEVFormer attention patterns.")
    print("   Key functions:")
    print("   - print_sparsity_analysis(): Detailed sparsity analysis of single tensor")
    print("   - analyze_model_sparsity(): Overview of sparsity across multiple tensors")
    print("   - print_detailed_comparison(): Enhanced comparison with sparsity metrics")
    print("   - compare_sparsity_patterns(): Direct sparsity pattern comparison")


if __name__ == "__main__":
    example_usage()
