# SPDX-FileCopyrightText: Â© 2026 Tenstorrent AI ULC
# SPDX-License-Identifier: Apache-2.0

import sys
import csv
import time
import builtins
import torch
import ttnn
import pytest
import numpy as np
from tqdm import tqdm
from pathlib import Path
from datetime import datetime
from scipy.stats import pearsonr
from transformers import PretrainedConfig

# --- Hardware Module Imports ---
from models.experimental.tt_symbiote.modules.linear import TTNNLinear
from models.experimental.tt_symbiote.modules.normalization import TTNNLayerNorm
from models.experimental.tt_symbiote.modules.activation import TTNNSilu
from models.experimental.tt_symbiote.utils.device_management import set_device

# Global telemetry state
MODULE_PERF_RECORDS = []
ttnn.CONFIG.enable_model_cache = True

# =============================================================================
# 0. METRICS & TELEMETRY
# =============================================================================


def calculate_pcc(golden, hw):
    """Calculates Pearson Correlation Coefficient for parity validation."""
    if golden is None or hw is None:
        return 0.0
    g = golden.detach().cpu().to(torch.float32).flatten().numpy()
    h = hw.detach().cpu().to(torch.float32).flatten().numpy()
    if np.all(g == g[0]) and np.all(h == h[0]):
        return 1.0
    corr, _ = pearsonr(g, h)
    return corr


class ModuleTimer:
    """Records hardware execution latency per layer."""

    def __init__(self, name):
        self.name = name

    def __enter__(self):
        self.start = time.perf_counter()

    def __exit__(self, type, value, traceback):
        duration = (time.perf_counter() - self.start) * 1000  # ms
        MODULE_PERF_RECORDS.append((self.name, duration))


# =============================================================================
# 1. GLOBAL ENVIRONMENT PATCHING (CRITICAL FOR COLLECTION)
# =============================================================================

import transformers.modeling_flash_attention_utils as fa_utils
import transformers.utils.import_utils as import_utils


def dummy_flash_attn(*args, **kwargs):
    return args[0] if args else None


# Satisfaction of Flash Attention dependencies
fa_utils._lazy_imports = lambda x: (
    dummy_flash_attn,
    dummy_flash_attn,
    lambda x, *a, **k: (x, None, None, None),
    lambda x, *a, **k: x,
)
import_utils.is_flash_attn_2_available = lambda: True
import_utils.is_flash_attn_available = lambda: True

# Builtin injection for namespace compatibility
builtins_to_inject = {
    "flash_attn_func": dummy_flash_attn,
    "flash_attn_varlen_func": dummy_flash_attn,
    "_flash_supports_window_size": False,
    "_flash_supports_softcap": False,
    "flash_241": False,
    "deterministic_g": False,
    "_use_top_left_mask": False,
}
for name, val in builtins_to_inject.items():
    setattr(builtins, name, val)

# Force hardware-optimized attention paths in PretrainedConfig
orig_config_init = PretrainedConfig.__init__


def patched_config_init(self, *args, **kwargs):
    orig_config_init(self, *args, **kwargs)
    flags = {
        "_attn_implementation": "flash_attention_2",
        "_attn_implementation_autoset": False,
        "_attn_implementation_internal": "flash_attention_2",
        "initializer_range": getattr(self, "initializer_range", 0.02),
    }
    for attr, val in flags.items():
        setattr(self, attr, val)


PretrainedConfig.__init__ = patched_config_init

# Extend ttnn.Tensor with PyTorch-style API methods
ttnn.Tensor.__len__ = lambda self: self.shape[0]
ttnn.Tensor.dim = lambda self: len(self.shape)
ttnn.Tensor.size = lambda self, dim=None: self.shape[dim] if dim is not None else self.shape
ttnn.Tensor.to = lambda self, *args, **kwargs: self

original_ttnn_reshape = ttnn.reshape


def robust_ttnn_reshape(self, *shape):
    target = list(shape[0]) if len(shape) == 1 and isinstance(shape[0], (list, tuple, torch.Size)) else list(shape)
    if len(target) > 4:
        return ttnn.from_torch(self.to_torch().reshape(target), dtype=self.dtype, device=self.device())
    return original_ttnn_reshape(self, target)


ttnn.Tensor.reshape = robust_ttnn_reshape

# =============================================================================
# 2. HARDWARE COMPATIBILITY WRAPPERS
# =============================================================================

for cls_name in ["Siglip2Model", "Siglip2ForImageClassification"]:
    if not hasattr(builtins, cls_name):
        setattr(builtins, cls_name, type(cls_name, (object,), {}))


def make_torch_compatible(cls):
    """Bridges TTNN modules for weight loading and profiling."""
    if not hasattr(cls, "state_dict"):
        cls.state_dict = lambda self, *args, **kwargs: {}

    def _load_shim(self, SD, prefix, *args, **kwargs):
        for target, variants in {"weight": ["weight", "W"], "bias": ["bias", "b"]}.items():
            for variant in variants:
                if prefix + variant in SD:
                    setattr(
                        self,
                        target,
                        torch.nn.Parameter(SD[prefix + variant].to("cpu").to(torch.bfloat16), requires_grad=False),
                    )
                    break

    cls._load_from_state_dict = _load_shim

    orig_call = cls.__call__

    def timed_call(self, *args, **kwargs):
        name = getattr(self, "module_path", self.__class__.__name__)
        with ModuleTimer(name):
            return orig_call(self, *args, **kwargs)

    cls.__call__ = timed_call

    for attr in ["parameters", "modules", "buffers", "children", "named_children"]:
        if not hasattr(cls, attr):
            setattr(cls, attr, lambda self, *a, **k: iter([]))
    if not hasattr(cls, "apply"):
        cls.apply = lambda self, fn: (fn(self), self)[1]
    return cls


for hardware_cls in [TTNNLinear, TTNNLayerNorm, TTNNSilu]:
    make_torch_compatible(hardware_cls)

# =============================================================================
# 3. REPO PATHING & GR00T IMPORT
# =============================================================================

repo_root = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(repo_root))
sys.path.insert(0, str(repo_root / "models/experimental/tt_symbiote/groot"))
modules_path = repo_root / "models/experimental/tt_symbiote/modules"
if str(modules_path) not in sys.path:
    sys.path.insert(0, str(modules_path))

from gr00t.model.gr00t_n1d6.gr00t_n1d6_tens import Gr00tN1d6

Gr00tN1d6._supports_flash_attn_2 = True

# =============================================================================
# 4. TEST CASE EXECUTION
# =============================================================================


@pytest.mark.parametrize("device_params", [{"l1_small_size": 245760}], indirect=True)
def test_gr00t_inference(device):
    torch_dtype, model_id = torch.bfloat16, "nvidia/GR00T-N1.6-3B"
    torch.manual_seed(42)

    print(f"\n[INIT] Loading model: {model_id}")
    model = Gr00tN1d6.from_pretrained(model_id, dtype=torch_dtype, trust_remote_code=True)
    model.eval()

    # Robust Input Pipeline: Force text tokens to avoid Size(0) errors
    image_token_id = 32000
    input_ids = torch.zeros((1, 256), dtype=torch.long)
    input_ids[0, :10] = torch.arange(1, 11)
    input_ids[0, 10 : 10 + 64] = image_token_id

    inputs = {
        "input_ids": input_ids,
        "attention_mask": torch.ones(1, 256, dtype=torch.long),
        "pixel_values": torch.randn(1, 1, 3, 224, 224, dtype=torch_dtype),
        "state": torch.randn(1, 1, model.config.max_state_dim, dtype=torch_dtype),
        "embodiment_id": torch.zeros(1, dtype=torch.long),
    }

    print("[CPU] Running Golden Reference...")
    with torch.no_grad():
        golden_output = model(**inputs)

    print("[TTNN] Performing hardware surgery...")
    model.perform_surgery()
    for name, module in model.named_modules():
        module.module_path = name

    set_device(model, device)

    print("[TTNN] Preparing weights for device...")
    for _, module in tqdm(model.named_modules(), desc="Weight Prep"):
        if hasattr(module, "preprocess_weights"):
            module.preprocess_weights()
            module.move_weights_to_device()

    MODULE_PERF_RECORDS.clear()
    print("[DEVICE] Triggering Hardware Inference on Wormhole...")
    with torch.no_grad():
        hw_output = model(**inputs)

    final_pcc = calculate_pcc(golden_output["action"], hw_output["action"])

    # CSV Generation
    csv_path = repo_root / f"gr00t_parity_perf_{datetime.now():%Y%m%d_%H%M%S}.csv"
    with open(csv_path, mode="w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["Module Path", "Wall Clock Time (ms)", "Status", "Total Model PCC"])
        for i, (path, duration) in enumerate(MODULE_PERF_RECORDS):
            status = "Slowing Down" if duration > 15 else "Healthy"
            row = [path, f"{duration:.4f}", status]
            if i == 0:
                row.append(f"{final_pcc:.6f}")
            writer.writerow(row)

    print("\n" + "=" * 80)
    print(f"REPORT GENERATED: {csv_path}")
    print(f"FINAL ACTION HEAD PCC: {final_pcc:.6f}")
    print("=" * 80)

    assert final_pcc > 0.95, f"PCC threshold failure! Got {final_pcc}"
    print("\nInference and Parity completed successfully!")
