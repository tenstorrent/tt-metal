# SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC
# SPDX-License-Identifier: Apache-2.0

from __future__ import annotations

from dataclasses import dataclass
from typing import Optional
from enum import Enum

import ttnn
from loguru import logger

from models.experimental.panoptic_deeplab.tt.common import from_torch_fast


class SliceMode(Enum):
    """Enumeration for different slicing modes"""

    NONE = "none"
    CHANNEL = "channel"
    HEIGHT = "height"
    WIDTH = "width"


@dataclass
class SliceConfig:
    """Configuration for manual slicing control"""

    mode: SliceMode = SliceMode.NONE
    num_slices: int = 1

    def __post_init__(self):
        if self.mode != SliceMode.NONE and self.num_slices <= 1:
            raise ValueError(f"{self.mode.value} slicing requires num_slices > 1")


@dataclass
class TtConv2dParameters:
    weight: ttnn.Tensor
    bias: ttnn.Tensor | None
    device: ttnn.MeshDevice
    dilation: tuple[int, int] = (1, 1)
    slice_config: SliceConfig = None

    def __post_init__(self):
        if self.slice_config is None:
            self.slice_config = SliceConfig()

    @classmethod
    def from_torch(
        cls,
        state: dict[str, any],
        *,
        device: ttnn.MeshDevice,
        dtype: ttnn.DataType | None = None,
        slice_config: SliceConfig | None = None,
    ) -> TtConv2dParameters:
        mesh_mapper = ttnn.ReplicateTensorToMesh(device) if isinstance(device, ttnn.MeshDevice) else None

        if slice_config is None:
            slice_config = SliceConfig()

        # Convert torch tensors to ttnn tensors
        weight = from_torch_fast(state["weight"], dtype=dtype, mesh_mapper=mesh_mapper)
        bias = None
        if "bias" in state and state["bias"] is not None:
            bias = from_torch_fast(state["bias"].reshape((1, 1, 1, -1)), dtype=dtype, mesh_mapper=mesh_mapper)

        return cls(
            weight=weight,
            bias=bias,
            device=device,
            dilation=state.get("dilation", (1, 1)),
            slice_config=slice_config,
        )

    @classmethod
    def from_preprocessed_parameters(
        cls,
        parameters,
        *,
        device: ttnn.MeshDevice,
        dtype: ttnn.DataType | None = None,
        slice_config: SliceConfig | None = None,
    ) -> TtConv2dParameters:
        """
        Create TtConv2dParameters from preprocessed TTNN parameters.

        Args:
            parameters: Preprocessed parameters containing TTNN tensors
            device: TTNN device
            dtype: Data type (not used since tensors are already preprocessed)
            slice_config: Slicing configuration

        Returns:
            TtConv2dParameters instance
        """
        if slice_config is None:
            slice_config = SliceConfig()

        # Extract weight and bias directly as they're already TTNN tensors
        weight = parameters["weight"]
        bias = parameters.get("bias", None)

        return cls(
            weight=weight,
            bias=bias,
            device=device,
            dilation=(1, 1),  # Default, can be overridden
            slice_config=slice_config,
        )

    @property
    def in_channels(self) -> int:
        return self.weight.shape[1]

    @property
    def out_channels(self) -> int:
        return self.weight.shape[0]

    @property
    def kernel_size(self) -> tuple[int, int]:
        return self.weight.shape[-2], self.weight.shape[-1]


class TtConv2d:
    """
    TTNN Conv2d wrapper with explicit manual control over slicing strategies.

    Supports the following slicing modes:
    - Channel slicing: Splits input channels across multiple conv operations
    - Height slicing: Uses ttnn spatial slicing along height dimension
    - Width slicing: Uses ttnn spatial slicing along width dimension

    Note: Height and width slicing cannot be used simultaneously.

    Usage examples:

    # Create parameters with channel slicing
    slice_config = SliceConfig(mode=SliceMode.CHANNEL, num_slices=4)
    params = TtConv2dParameters.from_torch(state_dict, device=device, slice_config=slice_config)
    conv = TtConv2d(params, stride=(2, 2), padding=(1, 1))

    # Or with spatial slicing
    slice_config = SliceConfig(mode=SliceMode.WIDTH, num_slices=2)
    params = TtConv2dParameters.from_torch(state_dict, device=device, slice_config=slice_config)

    # Using convenience methods
    conv = TtConv2d.create_with_channel_slicing(params, num_slices=4, stride=(2, 2))

    # Spatial slicing (height OR width, not both)
    conv = TtConv2d.create_with_width_slicing(params, num_slices=2)
    # OR
    conv = TtConv2d.create_with_height_slicing(params, num_slices=2)

    # No slicing
    conv = TtConv2d.create(params)

    Limitations of DRAM slicing (slice_count > 1), taken from https://github.com/tenstorrent/tt-metal/pull/19686:
    - Only works with activations of dtype BFloat16.
    - No logic to check if preprocessed weights can be safely reused. This is okay if all slices are
      approximately the same size.
    - Slice output is interleaved. This needs an additional interleaved to sharded op.
    - Doesn't support prepared weights.
    """

    def __init__(
        self,
        parameters: TtConv2dParameters,
        *,
        stride: tuple[int, int] = (1, 1),
        padding: tuple[int, int] = (0, 0),
    ) -> None:
        self._stride = stride
        self._padding = padding

        self._in_channels = parameters.in_channels
        self._out_channels = parameters.out_channels
        self._kernel_size = parameters.kernel_size

        self._dilation = parameters.dilation
        self._slice_config = parameters.slice_config
        self._weight = parameters.weight
        self._bias = parameters.bias
        self._device = parameters.device

        # Caching for sliced weights and biases
        self._weight_slices = []
        self._bias_slices = []

    def _get_conv_config(self) -> ttnn.Conv2dConfig:
        """Create default conv2d configuration"""
        return ttnn.Conv2dConfig(
            weights_dtype=ttnn.bfloat8_b,
            shard_layout=None,
            deallocate_activation=False,
            enable_act_double_buffer=False,
            enable_weights_double_buffer=False,
            output_layout=ttnn.TILE_LAYOUT,
            activation=None,
            transpose_shards=False,
            in_place=False,
            enable_kernel_stride_folding=False,
            full_inner_dim=False,
            act_block_h_override=32,
            config_tensors_in_dram=True,
        )

    def _get_compute_kernel_config(self) -> ttnn.WormholeComputeKernelConfig:
        """Create LoFi compute kernel configuration for optimal performance"""
        return ttnn.WormholeComputeKernelConfig(
            math_fidelity=ttnn.MathFidelity.LoFi,
            math_approx_mode=True,
            fp32_dest_acc_en=False,
            packer_l1_acc=False,
        )

    def _create_spatial_slice_config(self) -> Optional[ttnn.Conv2dSliceConfig]:
        """Create spatial slice configuration based on slice_config"""
        if self._slice_config.mode == SliceMode.HEIGHT:
            return ttnn.Conv2dSliceConfig(
                slice_type=ttnn.Conv2dDRAMSliceHeight, num_slices=self._slice_config.num_slices
            )
        elif self._slice_config.mode == SliceMode.WIDTH:
            return ttnn.Conv2dSliceConfig(
                slice_type=ttnn.Conv2dDRAMSliceWidth, num_slices=self._slice_config.num_slices
            )
        return ttnn.Conv2dL1FullSliceConfig

    def _perform_channel_slicing(
        self,
        x: ttnn.Tensor,
        batch_size: int,
        height: int,
        width: int,
        conv_config: ttnn.Conv2dConfig,
        memory_config: ttnn.MemoryConfig | None = None,
    ) -> tuple[ttnn.Tensor, list[int]]:
        """Perform channel slicing convolution"""
        logger.trace(
            f"TtConv2d channel slicing - input shape: {x.shape}, slices: {self._slice_config.num_slices}, kernel: {self._kernel_size}, stride: {self._stride}, padding: {self._padding}, memory_config: {memory_config}"
        )

        assert (
            self._in_channels % self._slice_config.num_slices == 0
        ), f"Input channels ({self._in_channels}) must be divisible by num_slices ({self._slice_config.num_slices})"

        split_in_channels = self._in_channels // self._slice_config.num_slices

        # We need this for ttnn.add at the end to apply bias
        if not ttnn.is_tensor_storage_on_device(self._bias):
            self._bias = ttnn.to_device(self._bias, self._device)

        # Create input slices
        input_slices = []
        for i in range(self._slice_config.num_slices):
            start_idx = i * split_in_channels
            end_idx = (i + 1) * split_in_channels
            slice_tensor = ttnn.slice(x, [0, 0, 0, start_idx], [batch_size, height, width, end_idx])
            input_slices.append(slice_tensor)

        # Create weight slices (cached)
        if not self._weight_slices:
            if ttnn.is_tensor_storage_on_device(self._weight):
                # Weights are on device - use TTNN slicing
                logger.debug(
                    f"TtConv2d channel slicing: using device-based TTNN slicing for {self._slice_config.num_slices} slices"
                )
                for i in range(self._slice_config.num_slices):
                    start_idx = i * split_in_channels
                    end_idx = (i + 1) * split_in_channels
                    weight_slice = ttnn.slice(
                        self._weight,
                        [0, start_idx, 0, 0],
                        [self._out_channels, end_idx, self._kernel_size[0], self._kernel_size[1]],
                    )
                    self._weight_slices.append(weight_slice)
            else:
                # Weights are on host - convert to torch, slice, then convert back to TTNN
                logger.debug(
                    f"TtConv2d channel slicing: using host-based PyTorch slicing for {self._slice_config.num_slices} slices"
                )
                torch_weight = ttnn.to_torch(self._weight)
                for i in range(self._slice_config.num_slices):
                    start_idx = i * split_in_channels
                    end_idx = (i + 1) * split_in_channels
                    torch_slice = torch_weight[:, start_idx:end_idx, :, :]
                    # Convert back to TTNN but keep on host (don't send to device)
                    weight_slice = ttnn.from_torch(torch_slice, dtype=self._weight.dtype, layout=self._weight.layout)
                    self._weight_slices.append(weight_slice)

        accumulated_output = None
        output_height, output_width = 0, 0

        # Process each channel slice
        for i in range(self._slice_config.num_slices):
            logger.trace(
                f"TtConv2d processing channel slice {i+1}/{self._slice_config.num_slices}, input_slice shape: {input_slices[i].shape}, weight_slice shape: {self._weight_slices[i].shape}"
            )
            input_slice = input_slices[i]
            output_slice, [output_height, output_width], [self._weight_slices[i], bias_tmp] = ttnn.conv2d(
                input_tensor=input_slice,
                weight_tensor=self._weight_slices[i],
                bias_tensor=None,
                in_channels=split_in_channels,
                out_channels=self._out_channels,
                device=self._device,
                kernel_size=list(self._kernel_size),
                stride=list(self._stride),
                padding=list(self._padding),
                batch_size=batch_size,
                input_height=height,
                input_width=width,
                dilation=list(self._dilation),
                return_output_dim=True,
                return_weights_and_bias=True,
                conv_config=conv_config,
                memory_config=memory_config,
                dtype=ttnn.bfloat8_b,
                compute_config=self._get_compute_kernel_config(),
                slice_config=ttnn.Conv2dL1FullSliceConfig,
            )
            output_slice = ttnn.move(output_slice)
            if i == 0:
                accumulated_output = ttnn.to_memory_config(output_slice, ttnn.DRAM_MEMORY_CONFIG)
            else:
                accumulated_output = ttnn.add(
                    output_slice, accumulated_output, memory_config=memory_config, output_tensor=accumulated_output
                )
                output_slice.deallocate(True)

        # Add bias if present
        if self._bias is not None:
            # Ensure bias has the same layout and dtype as accumulated_output
            bias_tensor = self._bias
            if bias_tensor.layout != ttnn.TILE_LAYOUT or bias_tensor.dtype != accumulated_output.dtype:
                bias_tensor = ttnn.to_layout(bias_tensor, ttnn.TILE_LAYOUT, dtype=accumulated_output.dtype)
            accumulated_output = ttnn.add(accumulated_output, bias_tensor, output_tensor=accumulated_output)

        final_shape = [batch_size, output_height, output_width, self._out_channels]
        logger.trace(
            f"TtConv2d channel slicing complete - output shape: {final_shape}, memory_config: {accumulated_output.memory_config()}"
        )
        return accumulated_output, final_shape

    def _perform_standard_conv(
        self,
        x: ttnn.Tensor,
        batch_size: int,
        height: int,
        width: int,
        conv_config: ttnn.Conv2dConfig,
        memory_config: ttnn.MemoryConfig | None = None,
        spatial_slice_config: Optional[ttnn.Conv2dSliceConfig] = None,
    ) -> tuple[ttnn.Tensor, list[int]]:
        """Perform standard convolution with optional spatial slicing"""
        logger.trace(
            f"TtConv2d standard conv - input shape: {x.shape}, kernel: {self._kernel_size}, stride: {self._stride}, padding: {self._padding}, spatial_slice_config: {spatial_slice_config}, memory_config: {memory_config}"
        )
        output, [output_height, output_width], [prepared_weight, prepared_bias] = ttnn.conv2d(
            input_tensor=x,
            weight_tensor=self._weight,
            bias_tensor=self._bias,
            in_channels=self._in_channels,
            out_channels=self._out_channels,
            device=self._device,
            kernel_size=list(self._kernel_size),
            stride=list(self._stride),
            padding=list(self._padding),
            batch_size=batch_size,
            input_height=height,
            input_width=width,
            dilation=list(self._dilation),
            return_output_dim=True,
            return_weights_and_bias=True,
            conv_config=conv_config,
            memory_config=memory_config,
            slice_config=spatial_slice_config,
            dtype=ttnn.bfloat8_b,
            compute_config=self._get_compute_kernel_config(),
        )

        # Update prepared weights and bias
        self._weight = prepared_weight
        self._bias = prepared_bias

        shape = [batch_size, output_height, output_width, self._out_channels]
        logger.trace(
            f"TtConv2d standard conv complete - output shape: {shape}, memory_config: {output.memory_config()}"
        )
        return output, shape

    def _call_without_reshape(
        self,
        x: ttnn.Tensor,
        *,
        conv_config: ttnn.Conv2dConfig | None = None,
        memory_config: ttnn.MemoryConfig | None = None,
    ) -> tuple[ttnn.Tensor, list[int]]:
        (batch_size, height, width, _) = x.shape

        # Use provided conv_config or create default
        if conv_config is None:
            conv_config = self._get_conv_config()

        # Determine slicing strategy based on slice_config
        if self._slice_config.mode == SliceMode.CHANNEL:
            return self._perform_channel_slicing(x, batch_size, height, width, conv_config, memory_config)
        else:
            # Handle spatial slicing (height, width, or both)
            spatial_slice_config = self._create_spatial_slice_config()
            return self._perform_standard_conv(
                x, batch_size, height, width, conv_config, memory_config, spatial_slice_config
            )

    def __call__(
        self,
        x: ttnn.Tensor,
        memory_config: ttnn.MemoryConfig | None = None,
        conv_config: Optional[ttnn.Conv2dConfig] = None,
    ) -> ttnn.Tensor:
        x, shape = self._call_without_reshape(x, memory_config=memory_config, conv_config=conv_config)

        x = ttnn.to_memory_config(x, ttnn.DRAM_MEMORY_CONFIG)

        # tiled tensors need to be rank 3 for reshape
        x = ttnn.squeeze(x, dim=0)
        shape = [shape[1], shape[2], shape[3]]
        x = ttnn.reshape(x, shape)
        x = ttnn.unsqueeze(x, dim=0)

        return x

    @property
    def in_channels(self) -> int:
        return self._in_channels

    @property
    def out_channels(self) -> int:
        return self._out_channels

    @property
    def kernel_size(self) -> tuple[int, int]:
        return self._kernel_size

    @classmethod
    def create_with_channel_slicing(
        cls,
        parameters: TtConv2dParameters,
        num_slices: int,
        *,
        stride: tuple[int, int] = (1, 1),
        padding: tuple[int, int] = (0, 0),
    ) -> "TtConv2d":
        """Create TtConv2d with channel slicing configuration."""
        parameters.slice_config = SliceConfig(mode=SliceMode.CHANNEL, num_slices=num_slices)
        return cls(parameters, stride=stride, padding=padding)

    @classmethod
    def create_with_height_slicing(
        cls,
        parameters: TtConv2dParameters,
        num_slices: int,
        *,
        stride: tuple[int, int] = (1, 1),
        padding: tuple[int, int] = (0, 0),
    ) -> "TtConv2d":
        """Create TtConv2d with height slicing configuration."""
        parameters.slice_config = SliceConfig(mode=SliceMode.HEIGHT, num_slices=num_slices)
        return cls(parameters, stride=stride, padding=padding)

    @classmethod
    def create_with_width_slicing(
        cls,
        parameters: TtConv2dParameters,
        num_slices: int,
        *,
        stride: tuple[int, int] = (1, 1),
        padding: tuple[int, int] = (0, 0),
    ) -> "TtConv2d":
        """Create TtConv2d with width slicing configuration."""
        parameters.slice_config = SliceConfig(mode=SliceMode.WIDTH, num_slices=num_slices)
        return cls(parameters, stride=stride, padding=padding)

    @classmethod
    def create(
        cls,
        parameters: TtConv2dParameters,
        *,
        stride: tuple[int, int] = (1, 1),
        padding: tuple[int, int] = (0, 0),
    ) -> "TtConv2d":
        """Create TtConv2d without any slicing."""
        parameters.slice_config = SliceConfig(mode=SliceMode.NONE)
        return cls(parameters, stride=stride, padding=padding)
