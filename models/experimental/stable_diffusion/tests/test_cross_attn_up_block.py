# SPDX-FileCopyrightText: Â© 2023 Tenstorrent Inc.

# SPDX-License-Identifier: Apache-2.0

from typing import Optional

import torch.nn as nn
import torch
from diffusers import StableDiffusionPipeline
from loguru import logger
import pytest


import ttnn
from models.utility_functions import (
    torch_to_tt_tensor,
    tt_to_torch_tensor,
    torch_to_tt_tensor_rm,
    is_wormhole_b0,
    is_blackhole,
)
from models.utility_functions import comp_pcc, comp_allclose_and_pcc
from models.experimental.stable_diffusion.tt.unet_2d_blocks import TtCrossAttnUpBlock2D
from models.experimental.stable_diffusion.tt.experimental_ops import UseDeviceConv


# low PCC for value 2, 3: 0.9851282356324425 etc.
@pytest.mark.skipif(is_wormhole_b0() or is_blackhole(), reason="Unsupported on WH and BH")
@pytest.mark.skip(reason="Test is failing, see issue #7536")
@pytest.mark.parametrize("index", [1, 2, 3])
def test_run_cross_attn_up_block_real_input_inference(device, index, model_location_generator):
    pipe = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4", torch_dtype=torch.float32)
    unet = pipe.unet
    unet.eval()
    state_dict = unet.state_dict()

    dir_path = model_location_generator("tensor_files", model_subdir="StableDiffusion")
    attr_path = f"{dir_path}/CrossAttnUpBlock2D_inp__attr__block_{index}.pt"
    attention_mask_path = f"{dir_path}/CrossAttnUpBlock2D_inp__attention_mask__block_{index}.pt"
    cross_attn_kwargs_path = f"{dir_path}/CrossAttnUpBlock2D_inp__cross_attention_kwargs__block_{index}.pt"
    emb_path = f"{dir_path}/CrossAttnUpBlock2D_inp__emb__block_{index}.pt"
    encoder_hidden_states_path = f"{dir_path}/CrossAttnUpBlock2D_inp__encoder_hidden_states__block_{index}.pt"
    sample_path = f"{dir_path}/CrossAttnUpBlock2D_inp__sample__block_{index}.pt"
    res_samples_path = f"{dir_path}/CrossAttnUpBlock2D_inp__res_samples__block_{index}.pt"
    upsample_size_path = f"{dir_path}/CrossAttnUpBlock2D_inp__upsample_size__block_{index}.pt"

    map_location = torch.device("cpu")
    sample = torch.load(sample_path, map_location=map_location)
    emb = torch.load(emb_path, map_location=map_location)
    encoder_hidden_states = torch.load(encoder_hidden_states_path, map_location=map_location)
    attention_mask = torch.load(attention_mask_path, map_location=map_location)
    cross_attention_kwargs = torch.load(cross_attn_kwargs_path, map_location=map_location)
    res_samples = torch.load(res_samples_path, map_location=map_location)
    upsample_size = torch.load(upsample_size_path, map_location=map_location)

    kwargs = torch.load(attr_path)
    base_address = f"up_blocks.{index}"
    up_block = pipe.unet.up_blocks[index]

    torch_output = up_block(
        hidden_states=sample,
        temb=emb,
        res_hidden_states_tuple=res_samples,
        encoder_hidden_states=encoder_hidden_states,
        attention_mask=attention_mask,
        cross_attention_kwargs=cross_attention_kwargs,
        upsample_size=upsample_size,
    )

    tt_cross_attn_up_block = TtCrossAttnUpBlock2D(**kwargs, state_dict=state_dict, base_address=base_address)

    tt_sample = torch_to_tt_tensor_rm(sample, device, put_on_device=False)
    tt_emb = torch_to_tt_tensor_rm(emb, device, put_on_device=False)
    tt_encoder_hidden_states = torch_to_tt_tensor_rm(encoder_hidden_states, device, put_on_device=False)

    tt_output = tt_cross_attn_up_block(
        hidden_states=tt_sample,
        temb=tt_emb,
        res_hidden_states_tuple=res_samples,
        encoder_hidden_states=tt_encoder_hidden_states,
        attention_mask=attention_mask,
        cross_attention_kwargs=cross_attention_kwargs,
    )

    tt_output = tt_to_torch_tensor(tt_output)

    passing = comp_pcc(torch_output, tt_output, pcc=0.98)
    logger.info(comp_allclose_and_pcc(tt_output, torch_output))

    assert passing[0], passing[1:]
    logger.info(f"PASSED {passing[1]}")


# test_run_cross_attn_up_block_inference_new(1)
# low PCC for on device = 0.90
@pytest.mark.skip(reason="Test not run and failing")
def test_run_cross_attn_up_block_inference(device):
    # setup pytorch model
    pipe = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4", torch_dtype=torch.float32)
    unet = pipe.unet
    unet.eval()
    state_dict = unet.state_dict()

    test = "test1"

    if test == "test1":
        in_channels = 640
        out_channels = 1280
        temb_channels = 1280
        prev_output_channel = 1280
        dropout = 0.0
        num_layers = 3
        resnet_eps = 1e-05
        resnet_time_scale_shift = "default"
        resnet_act_fn = "silu"
        resnet_groups = 32
        resnet_pre_norm = True
        attn_num_head_channels = 8
        cross_attention_dim = 768
        output_scale_factor = 1.0
        add_upsample = True
        dual_cross_attention = False
        use_linear_projection = False
        only_cross_attention = False
        upcast_attention = False
        ##### end of cross att up blck #####
        hidden_states_shape = torch.Size([2, 1280, 16, 16])
        sample = torch.randn(hidden_states_shape)

        temb_shape = torch.Size([1, 1, 2, 1280])
        emb = torch.randn(temb_shape)

        res0 = torch.Size([2, 640, 16, 16])
        res1 = torch.Size([2, 1280, 16, 16])
        res2 = torch.Size([2, 1280, 16, 16])
        res_samples = (torch.randn(res0), torch.randn(res1), torch.randn(res2))

        encoder_hidden_states_shape = torch.Size([1, 2, 77, 768])
        encoder_hidden_states = torch.randn(encoder_hidden_states_shape)

        cross_attention_kwargs = None
        upsample_size = None
        attention_mask = None
        base_address = "up_blocks.1"
        up_block = pipe.unet.up_blocks[1]

    torch_output = up_block(
        hidden_states=sample,
        temb=emb.squeeze(0).squeeze(0),
        res_hidden_states_tuple=res_samples,
        encoder_hidden_states=encoder_hidden_states.squeeze(0),
        attention_mask=attention_mask,
        cross_attention_kwargs=cross_attention_kwargs,
        upsample_size=upsample_size,
    )

    tt_cross_attn_up_block = TtCrossAttnUpBlock2D(
        in_channels=in_channels,
        out_channels=out_channels,
        temb_channels=temb_channels,
        prev_output_channel=prev_output_channel,
        dropout=dropout,
        num_layers=num_layers,
        resnet_eps=resnet_eps,
        resnet_time_scale_shift=resnet_time_scale_shift,
        resnet_act_fn=resnet_act_fn,
        resnet_groups=resnet_groups,
        resnet_pre_norm=resnet_pre_norm,
        attn_num_head_channels=attn_num_head_channels,
        cross_attention_dim=cross_attention_dim,
        output_scale_factor=output_scale_factor,
        add_upsample=add_upsample,
        dual_cross_attention=dual_cross_attention,
        use_linear_projection=use_linear_projection,
        only_cross_attention=only_cross_attention,
        upcast_attention=upcast_attention,
        state_dict=state_dict,
        base_address="up_blocks.1",
    )

    tt_sample = torch_to_tt_tensor_rm(sample, device, put_on_device=False)
    tt_emb = torch_to_tt_tensor_rm(emb, device, put_on_device=False)
    tt_encoder_hidden_states = torch_to_tt_tensor_rm(encoder_hidden_states, device, put_on_device=False)

    tt_output = tt_cross_attn_up_block(
        hidden_states=tt_sample,
        temb=tt_emb,
        res_hidden_states_tuple=res_samples,
        encoder_hidden_states=tt_encoder_hidden_states,
        attention_mask=attention_mask,
        cross_attention_kwargs=cross_attention_kwargs,
    )

    ttnn.synchronize_device(device)

    tt_output = tt_to_torch_tensor(tt_output)

    passing = comp_pcc(torch_output, tt_output, pcc=0.90)  # was 0.97 before
    logger.info(comp_allclose_and_pcc(tt_output, torch_output))

    assert passing[0], passing[1:]
    logger.info(f"PASSED {passing[1]}")
