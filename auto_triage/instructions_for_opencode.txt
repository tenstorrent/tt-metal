Auto Triage Playbook (LLM-Friendly)
==================================

Mission
-------
You are diagnosing a single failing subjob in a GitHub Actions workflow that is currently red on `main`. Your job is to identify the exact commit that introduced the deterministic failure.

High-Level Workflow
-------------------
1. Read `auto_triage/data/boundaries_summary.json` (or `subjob_runs.json`) to understand the ordered run history.
2. Download logs for candidate runs, starting from the oldest failing run whose logs still exist.
3. Confirm the first deterministic failure by comparing consecutive runs.
4. Once determinism is proven, analyze commits between the last good run and the first deterministic bad run.
5. Select the most likely culprit commit, note credible alternatives, and document everything in `auto_triage/output/`.

Key Data Files (under `auto_triage/data/`)
-----------------------------------------
- `boundaries_summary.json`: includes the last successful run (`run_number` 0) plus every subsequent failing run. Each entry has job URL, job ID, commit SHA, and run_number. Use `runs` for the full timeline or `failed_runs` for failures only.
- `subjob_runs.json`: the same `runs` array in isolation, handy for quick chronological reference.

Deterministic Failure Rules
---------------------------
- A run is deterministic only if the **same test failure** is observed in two consecutive runs (unless it is the most recent failing run, which you may assume is deterministic).
- Ignore failures caused by infrastructure issues (runner disconnects, OOM before tests, GitHub outages, setup steps). The failing logs must show the actual test failure.
- Do **not** analyze any commits until you have identified the first deterministic failure with concrete evidence from logs.
- Document why the chosen run is deterministic in your final explanation.

Understanding the Example Scenario
----------------------------------
Runs A–E happen in order. A passes. B and C fail due to flaky infrastructure. D and E both fail with the same test assertion. The first deterministic failure is therefore run D, and the culprit commit lies between runs A and D. Only after confirming this pattern can you analyze commits in that interval.

Log Download Policy
-------------------
- Use `auto_triage/get_logs.sh <job_url> [output_dir]`.
  ```
  auto_triage/get_logs.sh https://github.com/tenstorrent/tt-metal/actions/runs/<run_id>/job/<job_id>
  ```
  Output defaults to `auto_triage/logs/job_<job_id>/` with `full/` archives and, when possible, `job_specific/` subsets.
- Always attempt to download the **oldest available failing run**. If logs are missing or `get_logs.sh` errors out, stop chasing older runs and switch to the newest run that still has logs (Edge Case A guidance below).
- If the newest run fails before tests execute, try the next-newest run, repeating until you find a run where the test actually ran.
- If you cannot find the test log within the archive, assume that run was non-deterministic (test never executed).
- Never run `find_boundaries.sh`; it has already been executed.

Edge Case A (Expired Logs)
--------------------------
- Trigger: `get_logs.sh` fails for the oldest failing run because logs expired.
- Response: stop searching for older logs. Use the most recent run that has logs to understand the current deterministic failure.
- Deliverables: only `auto_triage/output/explanation.md`, explicitly noting that logs expired and providing the best root-cause hypothesis plus remediation guidance. Do **not** analyze commits in this case.

Efficiency + Log Parsing
------------------------
- Logs are large. Prefer `grep`, small helper scripts, or search terms like `FAILED`, `AssertionError`, `summary`.
- Once you find an error signature, reuse it (via `grep`) when checking other logs so you never manually re-scan for the same string twice.
- Always explain your thought process before running commands.

Toolbox Reference
-----------------
1. **Download logs**
   ```
   auto_triage/get_logs.sh <job_url> [output_dir]
   ```
2. **List commits between a known good commit and the first deterministic bad commit**
   ```
   auto_triage/download_data_between_commits.sh <good_commit> <bad_commit> [output_json]
   ```
   Default output: `auto_triage/data/commit_info.json` (includes Copilot summaries, author, etc.).
3. **Changed files for a specific commit**
   ```
   auto_triage/get_changed_files.sh <commit> [output_json]
   ```
   Default output: `auto_triage/data/commit_<commit>_files.json`.
4. **Full diff for a commit**
   ```
   auto_triage/get_commit_diff.sh <commit> [output_patch]
   ```
   Default output: `auto_triage/data/commit_<commit>_diff.patch`.

Commit Analysis Requirements
----------------------------
- Only begin commit analysis after identifying the first deterministic failing run.
- Use `download_data_between_commits.sh` to gather commit metadata (especially Copilot summaries) for the range between the last passing run and the first deterministic failing run.
- The final explanation must cite the Copilot overview (or other metadata) that justifies why a commit is suspect.

Outputs (when **not** in Edge Case A)
-------------------------------------
Write the following files under `auto_triage/output/`:
- `chosen_commit.json`: commit SHA and justification for the most likely culprit.
- `alternatives.json`: other plausible commits with short rationales.
- `explanation.md`: a detailed report covering:
  - Which job/run was the first deterministic failure and why.
  - The commit window examined and how it was derived.
  - The repeated error signature.
  - Which Copilot summaries and file/diff details you relied on.
  - Fix-forward guidance beyond “revert the commit”.
- Contact guidance:
  - Identify the most relevant engineer(s) to loop in. If a culprit commit is named, list the author first.
  - Otherwise, use `git log`, `git blame`, and `.github/CODEOWNERS` to find owners of the failing files/tests.
  - For each suggested contact, include a GitHub profile URL (e.g., `https://github.com/<handle>`). If you know the real name, show both name and handle.
  - Mention these contacts, with hyperlinks, explicitly in `explanation.md`.

Outputs (Edge Case A)
---------------------
- Only produce `auto_triage/output/explanation.md`.
- State that logs expired and provide best-effort debugging guidance referencing the most recent logs that still exist.
- Still include a “Who to contact” section using `git log`, `git blame`, and `.github/CODEOWNERS` to identify likely owners for the failing area, and provide a GitHub URL for each handle.

Final Reminders
---------------
- Stay deterministic: every conclusion must be backed by logs or data.
- Be concise but explicit about reasoning.
- Efficiency matters—avoid redundant work.
- Accuracy is critical; developers act on this analysis.
- Always recommend specific people to contact, prioritizing commit authors and file owners identified via `git log`, `git blame`, and `.github/CODEOWNERS`, and share their GitHub URLs for easy follow-up.
