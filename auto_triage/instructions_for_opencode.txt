Auto Triage Playbook (LLM-Friendly)
==================================

Mission
-------
Diagnose the current failing GitHub Actions subjob on `main`. Depending on the evidence, you must choose **exactly one** of the three cases below and produce the required outputs.

Workspace Layout
----------------
- You are running inside `.auto_triage/` and all helper scripts are already staged here.
- The full `tt-metal` repository has been mirrored into `./workspace/`. Read any source files from there.
- Stay in this directory. Run helper scripts as `./get_logs.sh`, `./download_data_between_commits.sh`, etc.
- Data files live in `./auto_triage/data/` (also available via the `./data/` symlink), logs in `./auto_triage/logs/` (also exposed via `./logs/`), and outputs must go into `./auto_triage/output/` (`./output/`).
- Never reference parent directories (e.g., `../`). Doing so will trigger permission denials.
- Always read `./data/subjob_runs.json` (symlinked to `./auto_triage/data/subjob_runs.json`) for the list of recent runs (success vs failure). `boundaries_summary.json` is metadata only; do not rely on it for determinism checks.

-Case Matrix
------------
- **Case 1 – Failure attributable to a specific commit**
  - You have enough evidence to tie the observed failure to a particular commit (or tiny set) within a ≤100 commit window, and `download_data_between_commits.sh` succeeds. Matching failures across runs strengthens the case but is **not** required; a single failing run plus a compelling diff/copilot summary is sufficient.
- **Case 2 – Deterministic failure, culprit commit unknown**
  - The failure is deterministic but you cannot identify the exact commit (expired logs, >100 commits so the download script aborts, or other data gaps). Focus on root-cause instructions rather than naming a commit.
- **Case 3 – Failure unrelated to tt-metal changes**
  - Evidence shows the issue is probably external (infrastructure, flaky machines, mismatched dependencies, or commits that clearly cannot affect the failing test). Use this only when you have exhausted reasonable tt-metal hypotheses.

You must justify the selected case explicitly in `./output/explanation.md`.

Shared Workflow (All Cases)
---------------------------
1. Inspect `./data/boundaries_summary.json` (or `./data/subjob_runs.json`) to understand the ordered history and identify the last successful run and subsequent failures. The higher the run_number the newer the run.
2. **Log downloads:**
   - Start by downloading:
     - The **oldest failing run** (first failure after the last success) and
     - The **two most recent failing runs**.
   - If there are fewer than three failures total, download all of them.
- If the oldest failing run cannot be downloaded (logs expired or `get_logs.sh` errors), treat this as a strong signal toward Case 2. Continue the investigation using the most recent logs you can obtain, but remain open to switching to Case 3 if those logs indicate the job never ran the test or otherwise point to external causes.
   - If the downloaded errors match, rely on that signature. If they differ, continue with the most recent error unless you need an extra run to resolve ambiguity. Avoid additional downloads unless absolutely necessary (they are expensive).
- If you truly need additional evidence, you may download more runs, but justify the cost and stop as soon as you have enough data.
3. Determine whether the failure is likely caused by tt-metal changes. Matching errors across runs help establish determinism, but a single run can still justify Case 1 if the commit evidence is strong. **Never** assume determinism just because a run is the latest; always explain your rationale.
4. If you can map the failure to code changes (even from a single run), proceed toward Case 1. If you believe the failure is deterministic but cannot identify a culprit commit, you are heading toward Case 2. If tt-metal seems unrelated, that’s Case 3.
5. Contact planning: while you work, note commit authors, file owners (`git log`, `git blame`, `.github/CODEOWNERS`), and capture GitHub profile URLs for everyone you may need to ping.
6. Run `./download_data_between_commits.sh` before analyzing commits. If it prints `BATCH_COUNT=<n>`, call `./download_data_between_commits_batch.sh <good_commit> <bad_commit> <batch_index>` for each batch index `0..n-1` (using the same output JSON so results append) before continuing.
7. In your final report, include links to the last successful job run, the first failing job run, and every commit between them (use the compare URL and `commit_info.json`).

Failure Evidence Rules
----------------------
- Repeated identical **test** errors across runs strongly indicate determinism but are not strictly required for Case 1. Use your judgment: if code diffs clearly explain the failure, you may proceed with a single run.
- Treat infrastructure/setup issues (runner disconnects, missing deps, timeouts before tests) as non-deterministic unless commits point directly to them.
- If log archives lack the actual test output, assume that specific run is non-deterministic.
- Always document the failure signature and why you believe it is (or is not) attributable to tt-metal code.

Toolbox Reference
-----------------
1. **Download logs**
   ```
   ./get_logs.sh <job_url> [output_dir]
   ```
2. **List commits between the last success and first deterministic failure**
   ```
   ./download_data_between_commits.sh <good_commit> <bad_commit> [output_json]
   ```
   Creates/updates `./data/commit_info.json`. If more than 30 commits are detected it will report `BATCH_COUNT` and expect the batch script below to be called.
3. **Download commit metadata in batches (when requested)**
   ```
   ./download_data_between_commits_batch.sh <good_commit> <bad_commit> <batch_index> [output_json]
   ```
   Processes up to 30 commits per batch and appends to the specified JSON (run for indices `0..BATCH_COUNT-1`).
4. **Changed files for a commit**
   ```
   ./get_changed_files.sh <commit> [output_json]
   ```
5. **Full diff for a commit**
   ```
   ./get_commit_diff.sh <commit> [output_patch]
   ```

Case 1 – Deterministic Failure With Identified Commit
-----------------------------------------------------
Trigger this case when:
- You have two matching test failures that prove determinism.
- The commit window between last success and first deterministic failure has ≤100 commits (script succeeds).
- A specific commit (or very small set) clearly explains the failure via Copilot overview, file diffs, etc.

Actions:
1. Run `./download_data_between_commits.sh <good> <bad>` to gather metadata.
2. Use Copilot summaries, changed files, and diffs to pick the most likely culprit. Record alternatives if the evidence is close.
3. Prepare outputs under `./output/`:
   - `chosen_commit.json`
   - `alternatives.json`
   - `explanation.md` covering:
     - Why the failure is deterministic.
     - The commit window, Copilot summaries referenced, and supporting diffs.
     - Detailed fix-forward suggestions (not just “revert”).
     - A “Who to contact” section listing names + GitHub handles + profile URLs, prioritizing the culprit commit author and relevant file owners (per `git log`, `git blame`, `.github/CODEOWNERS`).
     - Direct links to the last successful job, the first failing job, each commit in the window, and the suspected culprit commit.

Case 2 – Deterministic Failure But Commit Unknown
-------------------------------------------------
Use this case when a deterministic failure exists but you cannot determine the commit because:
- The oldest failing run’s logs cannot be retrieved (expired or `get_logs.sh` error) and the remaining evidence still points to a tt-metal regression rather than an external issue.
- The commit window exceeds 100 commits (script exits with “too many commits. cannot download”).
- Metadata is insufficient to map the failure to a specific change.

Actions:
1. Explain clearly why the commit cannot be identified (expired logs, excessive commit span, missing data, etc.).
2. Describe the failure signature, affected files/tests, and best-effort hypotheses on where the bug likely lives in the codebase.
3. Produce **only** `./output/explanation.md` (no JSON outputs). Include:
   - Determinism evidence.
   - Guidance for engineers on how to debug/fix the issue (files to inspect, test commands, configs to tweak).
   - A contact list (names + handles + GitHub URLs) derived from `git log`, `git blame`, `.github/CODEOWNERS`, prioritizing likely owners of the failing areas.

Case 3 – Failure Likely Outside tt-metal
----------------------------------------
Choose this case when tt-metal commits cannot reasonably explain the failure. Indicators include:
- Every failure since the last success appears non-deterministic or infrastructure-related (runner crashes, networking, flaky hardware).
- The failing runs all show identical errors, but the intervening commits only touch unrelated code paths (e.g., Python docs while the job is a C++ kernel test).
- The mismatch between failure signature and available commits leads you to conclude that the issue likely sits outside tt-metal (e.g., flaky machine, external dependency regression).
- You could not download the oldest failing run, and the remaining logs show the test never executed or failed for reasons clearly unrelated to tt-metal.

Actions:
1. Document why tt-metal is likely not at fault:
   - Summaries of the failures you inspected.
   - Commit analysis showing no plausible offending changes.
   - Any external/environmental clues from the logs.
2. Produce `./output/explanation.md` with:
   - A clear statement that this is Case 3.
   - Suggestions for next steps (check runners, dependencies, hardware, upstream repos, etc.).
   - A contact list (names + handles + GitHub URLs) for people who can help investigate the suspected area (e.g., infra owners, CODEOWNERS of the test, commit authors touching tooling).

Contact Guidance (All Cases)
----------------------------
- Always recommend specific engineers to loop in. Order of preference:
  1. Identified culprit commit author(s) (Case 1).
  2. Owners of files or tests implicated in the failure (from `git log`, `git blame`, `.github/CODEOWNERS`).
  3. Infra/test maintainers if Case 3 points outside tt-metal.
- Provide both the best-known real name (if visible) and the GitHub handle, plus a hyperlink `https://github.com/<handle>`.
- Include this section in `explanation.md` for every case.

Final Reminders
---------------
- Do not assume a failure is deterministic without evidence.
- Avoid downloading more logs than necessary; each download costs money.
- When log downloads fail, document the attempt and reason before moving on.
- Keep reasoning concise but explicit, and always tie conclusions back to logs or commit data.
- Accuracy matters—engineers will act on your findings. Choose the correct case and justify it thoroughly.
- If there are **no failing runs** after reading `boundaries_summary.json`, write `./output/explanation.md` stating that the triage run was invoked in error (pipeline currently passing) and exit without further analysis.
