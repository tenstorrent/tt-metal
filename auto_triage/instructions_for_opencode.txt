Auto Triage Playbook (LLM-Friendly)
==================================

Mission
-------
Diagnose the current failing GitHub Actions subjob on `main`. Depending on the evidence, you must choose **exactly one** of the three cases below and produce the required outputs.

Workspace Layout
----------------
- You are running inside `.auto_triage/` and all helper scripts are already staged here.
- The full `tt-metal` repository has been mirrored into `./workspace/`. Read any source files from there.
- Stay in this directory. Run helper scripts as `./get_logs.sh`, `./download_data_between_commits.sh`, etc.
- Data files live in `./auto_triage/data/` (also available via the `./data/` symlink), logs/annotations live in `./auto_triage/logs/` (`./logs/`), and outputs must go into `./auto_triage/output/` (`./output/`). Each job’s annotations (if downloaded) reside at `./logs/job_<job_id>/annotations.json`.
- Never reference parent directories (e.g., `../`). Doing so will trigger permission denials.
- Always read `./data/subjob_runs.json` (symlinked to `./auto_triage/data/subjob_runs.json`) for the list of recent runs (success vs failure). `boundaries_summary.json` is metadata only; do not rely on it for determinism checks.

-Case Matrix
------------
- **Case 1 – Failure attributable to a specific commit**
  - You have enough evidence to tie the observed failure to a particular commit (or tiny set) within a ≤200 commit window (the scripts can download metadata in batches). Matching failures across runs strengthens the case but is **not** required; a single failing run plus a compelling diff/copilot summary is sufficient.
- **Case 2 – Deterministic failure, culprit commit unknown**
  - The failure is deterministic but you cannot identify the exact commit (expired logs, >200 commits so the download script aborts, or other data gaps). Focus on root-cause instructions rather than naming a commit.
- **Case 3 – Failure unrelated to tt-metal changes**
  - Evidence shows the issue is probably external (infrastructure, flaky machines, mismatched dependencies, or commits that clearly cannot affect the failing test). Use this only when you have exhausted reasonable tt-metal hypotheses.

You must justify the selected case explicitly in `./output/explanation.md`.

Shared Workflow (All Cases)
---------------------------
1. Inspect `./data/boundaries_summary.json` (or `./data/subjob_runs.json`) to understand the ordered history and identify the last successful run and subsequent failures. The higher the run_number the newer the run.
2. **Run-level evidence collection:**
   - Start by downloading:
     - The **oldest failing run** (first failure after the last success) and
     - The **two most recent failing runs**.
   - If there are fewer than three failures total, download all of them.
   - Before pulling logs for any run, execute `./get_annotations.sh <job_url> [output_json]`. Use annotations only when they include actionable details (file/line, specific assert, inputs). If annotations are missing or generic (e.g., just exit codes, restating the test name, or vague asserts), immediately fall back to `./get_logs.sh` for that run to gather real context.
- If the oldest failing run cannot be downloaded (logs expired or `get_logs.sh` errors), treat this as a strong signal toward Case 2. Continue the investigation using the most recent logs you can obtain, but remain open to switching to Case 3 if those logs indicate the job never ran the test or otherwise point to external causes.
   - If the downloaded errors match, rely on that signature. If they differ, continue with the most recent error unless you need an extra run to resolve ambiguity. Avoid additional downloads unless absolutely necessary (they are expensive).
- If you truly need additional evidence, you may download more runs, but justify the cost and stop as soon as you have enough data.
3. **Ignore setup failures:** if the job never reached the actual test (e.g., runner issues, environment setup failure, infra outage), discard that run entirely—it must not influence determinism or commit ranges. Only runs that executed the test suite belong in your analysis.
4. Determine whether the failure is likely caused by tt-metal changes. Matching errors across runs help establish determinism, but a single run can still justify Case 1 if the commit evidence is strong. **Never** assume determinism just because a run is the latest; always explain your rationale.
4. If you can map the failure to code changes (even from a single run), proceed toward Case 1. If you believe the failure is deterministic but cannot identify a culprit commit, you are heading toward Case 2. If tt-metal seems unrelated, that’s Case 3.
5. Contact planning: while you work, note commit authors, file owners (`git log`, `git blame`, `.github/CODEOWNERS`), and capture GitHub profile URLs for everyone you may need to ping.
6. Run `./download_data_between_commits.sh` before analyzing commits. If it prints `BATCH_COUNT=<n>`, call `./download_data_between_commits_batch.sh <good_commit> <bad_commit> <batch_index>` for each batch index `0..n-1`. Let both scripts use their default output (`./data/commit_info.json`) so each batch automatically appends to the same file.
7. In your final report, include links to the last successful job run, the first failing job run, and every commit between them (use the compare URL and `commit_info.json`).

Failure Evidence Rules
----------------------
- Repeated identical **test** errors across runs strongly indicate determinism but are not strictly required for Case 1. Use your judgment: if code diffs clearly explain the failure, you may proceed with a single run.
- Treat infrastructure/setup issues (runner disconnects, missing deps, timeouts before tests) as non-deterministic and exclude them entirely from the run history. Only consider runs where the tests actually executed.
- Anchor on the **most recent** run that executed the tests: its failure signature is the reference. Earlier runs only count as the start of a deterministic failure if they show the exact same test-level error.
- If there is only one test-executing failure, or no earlier run repeats the same error, treat that lone failure as the entire signal.
- If log archives lack the actual test output, assume that specific run is non-deterministic.
- Always document the failure signature and why you believe it is (or is not) attributable to tt-metal code.

Toolbox Reference
-----------------
1. **Fetch job annotations (preferred)**
   ```
   ./get_annotations.sh <job_url> [output_json]
   ```
   Saves the run’s annotations (if any) under `./logs/job_<id>/annotations.json`. Use this before touching logs.
2. **Download logs (fallback when annotations have no signal)**
   ```
   ./get_logs.sh <job_url> [output_dir]
   ```
3. **List commits between the last success and first deterministic failure**
   ```
   ./download_data_between_commits.sh <good_commit> <bad_commit> [output_json]
   ```
   Creates/updates `./data/commit_info.json`. If more than 30 commits are detected it will report `BATCH_COUNT` and expect the batch script below to be called.
4. **Download commit metadata in batches (when requested)**
   ```
   ./download_data_between_commits_batch.sh <good_commit> <bad_commit> <batch_index> [output_json]
   ```
   Processes up to 30 commits per batch and appends to the specified JSON (run for indices `0..BATCH_COUNT-1`).
5. **Changed files for a commit**
   ```
   ./get_changed_files.sh <commit> [output_json]
   ```
6. **Full diff for a commit**
   ```
   ./get_commit_diff.sh <commit> [output_patch]
   ```

Case 1 – Deterministic Failure With Identified Commit
-----------------------------------------------------
Trigger this case when:
- You have two matching test failures that prove determinism.
- The commit window between last success and first deterministic failure has ≤200 commits (batches cover anything above 30).
- A specific commit (or very small set) clearly explains the failure via Copilot overview, file diffs, etc.

Actions:
1. Run `./download_data_between_commits.sh <good> <bad>` to gather metadata.
2. Use Copilot summaries, changed files, and diffs to pick the most likely culprit. Record alternatives if the evidence is close.
3. Prepare outputs under `./output/`:
   - `chosen_commit.json`
   - `alternatives.json`
   - `explanation.md` covering:
     - Why the failure is deterministic.
     - The commit window, Copilot summaries referenced, and supporting diffs.
     - Exact code changes (file path + before/after hunk) from the suspected commit that plausibly trigger the observed error, with a clear explanation tying the change to the failure signature. At minimum: cite the old code (pre-commit), the new code (post-commit), and describe how the behavior change produces the observed test failure.
     - Detailed fix-forward suggestions (not just “revert”).
     - A “Who to contact” section listing names + GitHub handles + profile URLs, prioritizing the culprit commit author and relevant file owners (per `git log`, `git blame`, `.github/CODEOWNERS`).
     - Direct links to the last successful job, the first failing job, each commit in the window, and the suspected culprit commit.

Case 2 – Deterministic Failure But Commit Unknown
-------------------------------------------------
Use this case when a deterministic failure exists but you cannot determine the commit because:
- The oldest failing run’s logs cannot be retrieved (expired or `get_logs.sh` error) and the remaining evidence still points to a tt-metal regression rather than an external issue.
- The commit window exceeds 200 commits (script exits with “too many commits. cannot download”).
- Metadata is insufficient to map the failure to a specific change.

Actions:
1. Explain clearly why the commit cannot be identified (expired logs, excessive commit span, missing data, etc.).
2. Describe the failure signature, affected files/tests, and best-effort hypotheses on where the bug likely lives in the codebase.
3. Produce **only** `./output/explanation.md` (no JSON outputs). Include:
   - Determinism evidence and the exact error message text (or stack trace snippet) observed in the relevant job(s).
   - Code references from the repository (file path + failing code block) that most likely cause or influence the failure, even if no specific culprit commit is known. Compare old vs. current behavior when possible.
   - Guidance for engineers on how to debug/fix the issue (files to inspect, test commands, configs to tweak).
   - A contact list (names + handles + GitHub URLs) derived from `git log`, `git blame`, `.github/CODEOWNERS`, prioritizing likely owners of the failing areas.

Case 3 – Failure Likely Outside tt-metal
----------------------------------------
Choose this case when tt-metal commits cannot reasonably explain the failure. Indicators include:
- Every failure since the last success appears non-deterministic or infrastructure-related (runner crashes, networking, flaky hardware).
- The failing runs all show identical errors, but the intervening commits only touch unrelated code paths (e.g., Python docs while the job is a C++ kernel test).
- The mismatch between failure signature and available commits leads you to conclude that the issue likely sits outside tt-metal (e.g., flaky machine, external dependency regression).
- You could not download the oldest failing run, and the remaining logs show the test never executed or failed for reasons clearly unrelated to tt-metal.

Actions:
1. Document why tt-metal is likely not at fault:
   - Summaries of the failures you inspected and the exact error messages observed.
   - Commit analysis showing no plausible offending changes.
   - Any external/environmental clues from the logs.
2. Produce `./output/explanation.md` with:
   - A clear statement that this is Case 3.
   - Suggestions for next steps (check runners, dependencies, hardware, upstream repos, etc.).
   - References to any code/config files inspected to justify that tt-metal itself is not at fault (even if only to show they remained unchanged).
   - A contact list (names + handles + GitHub URLs) for people who can help investigate the suspected area (e.g., infra owners, CODEOWNERS of the test, commit authors touching tooling).

Contact Guidance (All Cases)
----------------------------
- Always recommend specific engineers to loop in. Order of preference:
  1. Identified culprit commit author(s) (Case 1).
  2. Owners of files or tests implicated in the failure (from `git log`, `git blame`, `.github/CODEOWNERS`).
  3. Infra/test maintainers if Case 3 points outside tt-metal.
- Provide both the best-known real name (if visible) and the GitHub handle, plus a hyperlink `https://github.com/<handle>`.
- Include this section in `explanation.md` for every case.

Final Reminders
---------------
- Do not assume a failure is deterministic without evidence.
- Avoid downloading more logs than necessary; each download costs money.
- When log downloads fail, document the attempt and reason before moving on.
- Keep reasoning concise but explicit, and always tie conclusions back to logs or commit data.
- Accuracy matters—engineers will act on your findings. Choose the correct case and justify it thoroughly.
- If there are **no failing runs** after reading `boundaries_summary.json`, write `./output/explanation.md` stating that the triage run was invoked in error (pipeline currently passing) and exit without further analysis.
