# Understanding nvtop and Building Monitoring Tools for Tenstorrent

This directory contains documentation and implementations related to monitoring Tenstorrent devices, inspired by nvtop's architecture.

## Quick Navigation

- **[NVTOP_ARCHITECTURE_GUIDE.md](./NVTOP_ARCHITECTURE_GUIDE.md)** - Complete guide on how nvtop works and how to build similar tools
- **[IMPLEMENTATION_COMPARISON.md](./IMPLEMENTATION_COMPARISON.md)** - Comparison of sysfs vs UMD telemetry approaches
- **[TT_SMI_UMD_TELEMETRY_GUIDE.md](./TT_SMI_UMD_TELEMETRY_GUIDE.md)** - Original guide on using UMD APIs

## Files in This Directory

### Current Implementation
- `tt_smi.cpp` - Snapshot monitoring tool (like nvidia-smi)
- `allocation_server_poc.cpp` - Memory tracking server

### Enhanced Implementation (New)
- `tt_smi_umd.cpp` - Enhanced version with UMD telemetry

### External Reference
- `nvtop/src/extract_gpuinfo_tenstorrent.c` - nvtop plugin for Tenstorrent (in nvtop repo)

## Key Concepts

### 1. nvtop Architecture

nvtop is a modular GPU monitoring tool with:
- **Plugin-based vendor support** (NVIDIA, AMD, Intel, Apple, etc.)
- **Common abstraction layer** for device info
- **Process discovery** via fdinfo
- **Interactive ncurses UI**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ncurses   â”‚  â† User Interface
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Abstraction â”‚  â† Vendor-agnostic layer
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Plugins   â”‚  â† NVIDIA | AMD | Intel | Tenstorrent
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Telemetry Sources

**sysfs (Current tt-smi):**
- Pros: Fast, no dependencies
- Cons: Limited data, local only

**UMD APIs (Enhanced tt-smi-umd):**
- Pros: Complete data, remote support
- Cons: Slower, device exclusivity

### 3. Process Discovery

**Current:** Scan `/proc/*/fd/` for `/dev/tenstorrent/*`
- Tells us which processes have devices open
- No per-process GPU usage yet

**Future:** Use fdinfo (requires tt-kmd changes)
- Per-process memory usage
- Per-process GPU utilization
- Match nvtop's capabilities

## Quick Start

### Build and Run tt-smi (current)
```bash
cd /home/ttuser/aperezvicente/tt-metal-apv/tt_metal/programming_examples/alexp_examples/memory_utilization_monitor

# Build
g++ -o tt_smi tt_smi.cpp -std=c++20 -lpthread -lstdc++fs

# Run (snapshot)
./tt_smi

# Run (watch mode)
./tt_smi -w
```

### Build and Run tt-smi-umd (enhanced)
```bash
# Set environment
export TT_METAL_HOME=/home/ttuser/aperezvicente/tt-metal-apv

# Build (requires UMD)
g++ -o tt_smi_umd tt_smi_umd.cpp \
    -std=c++20 \
    -I$TT_METAL_HOME/third_party/umd/device/api \
    -L$TT_METAL_HOME/build/lib \
    -lumd_device \
    -lpthread -lstdc++fs

# Run with UMD telemetry
./tt_smi_umd -w

# Compare with sysfs
./tt_smi_umd --sysfs -w
```

### Optional: Start Allocation Server for Memory Tracking
```bash
# Terminal 1: Start server
./allocation_server_poc

# Terminal 2: Run tt-smi
./tt_smi -w
# Now shows memory usage per device
```

## Understanding the Differences

### tt-smi vs tt-smi-umd

| Feature | tt-smi | tt-smi-umd |
|---------|--------|------------|
| Temperature | âš ï¸ sysfs (may be N/A) | âœ… UMD (always available) |
| Power | âŒ | âœ… TDP/TDC |
| Clocks | âŒ | âœ… AICLK/AXICLK/ARCCLK |
| Fan Speed | âŒ | âœ… RPM |
| Remote Devices | âŒ | âœ… |
| Speed | Fast (2ms) | Slower (100ms init) |
| Dependencies | None | Requires UMD |

### When to Use Each

**Use tt-smi when:**
- Quick development checks
- Device is busy with workload
- Minimal dependencies needed
- CI/CD health checks

**Use tt-smi-umd when:**
- Need complete telemetry data
- Monitoring remote devices
- Production monitoring
- Detailed power/clock analysis

## Architecture Comparison

### nvidia-smi / nvtop Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Tool     â”‚
â”‚ (nvidia-smi) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     NVML     â”‚  â† Vendor library
â”‚   Library    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Driver     â”‚  â† Kernel driver
â”‚  (nvidia.ko) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
    Hardware
```

### tt-smi Architecture (Current)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   tt-smi     â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
    â”‚      â”‚
    â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    â–¼
    â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              â”‚ Allocation   â”‚
    â”‚              â”‚   Server     â”‚
    â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                (memory only)
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    sysfs     â”‚  â† Limited telemetry
â”‚   (/sys)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   tt-kmd     â”‚  â† Kernel driver
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### tt-smi-umd Architecture (Enhanced)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  tt-smi-umd  â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
    â”‚      â”‚
    â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    â–¼
    â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              â”‚ Allocation   â”‚
    â”‚              â”‚   Server     â”‚
    â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                (memory)
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TT-UMD     â”‚  â† Complete telemetry
â”‚   Library    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Firmware    â”‚  â† Direct access
â”‚   (PCIe)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Implementation Roadmap

### âœ… Phase 1: Basic Monitoring (Done)
- [x] Device enumeration
- [x] Process discovery
- [x] Memory tracking via allocation server
- [x] sysfs telemetry (limited)

### ğŸ”„ Phase 2: Enhanced Telemetry (In Progress)
- [x] UMD telemetry integration
- [x] Complete temperature/power/clock data
- [ ] Device caching for performance
- [ ] Fallback to sysfs when device busy

### ğŸ“‹ Phase 3: Advanced Process Tracking (Planned)
- [ ] fdinfo support in tt-kmd
- [ ] Per-process GPU usage
- [ ] Per-process memory tracking
- [ ] Device-to-process mapping

### ğŸ“‹ Phase 4: Interactive UI (Planned)
- [ ] ncurses interface (like nvtop)
- [ ] Historical charts
- [ ] Process management (kill, sort, filter)
- [ ] Configuration system

### ğŸ“‹ Phase 5: Production Features (Future)
- [ ] Remote monitoring
- [ ] Web dashboard
- [ ] Prometheus exporter
- [ ] Alert system

## nvtop Plugin for Tenstorrent

We created a plugin for nvtop to support Tenstorrent devices:
- Located in: `nvtop/src/extract_gpuinfo_tenstorrent.c`
- Uses dynamic loading of UMD library
- Integrates with nvtop's UI and process tracking

To build nvtop with Tenstorrent support:
```bash
cd /home/ttuser/aperezvicente/nvtop
mkdir -p build && cd build
cmake .. \
    -DNVIDIA_SUPPORT=ON \
    -DAMDGPU_SUPPORT=ON \
    -DINTEL_SUPPORT=ON \
    -DTENSTORRENT_SUPPORT=ON
make
sudo make install
```

## Key Insights from nvtop

### 1. Plugin Architecture
- Each vendor is a self-contained plugin
- Registers itself via constructor attribute
- Dynamic library loading (no hard dependencies)

### 2. Validity Bitmasks
- Efficient way to track optional fields
- C-compatible (no std::optional)
- Fast validity checks

### 3. fdinfo for Process Tracking
- Kernel exposes GPU usage via /proc/<pid>/fdinfo/<fd>
- Standard across AMD, Intel, NVIDIA
- Tenstorrent needs tt-kmd support

### 4. Separation of Concerns
- Data extraction (vendor plugins)
- Data aggregation (core)
- Data presentation (UI)

## Common Issues and Solutions

### Issue: "N/A" for temperature
**Cause:** sysfs not available or not populated
**Solution:** Use tt-smi-umd with UMD telemetry

### Issue: Device busy error
**Cause:** Another process owns the device (exclusivity)
**Solution:**
1. Close other processes, or
2. Use sysfs fallback, or
3. Implement telemetry server

### Issue: No per-process memory
**Cause:** Processes not connected to allocation server
**Solution:** Instrument processes to report allocations

### Issue: Can't see which device a process uses
**Cause:** No fdinfo support in tt-kmd
**Solution:** Add fdinfo to kernel driver

## Contributing

### To add a new feature:
1. Check if nvtop has similar feature
2. Adapt to Tenstorrent specifics
3. Test on multiple device types
4. Update documentation

### To improve telemetry:
1. Check TT-UMD firmware provider API
2. Add new fields to TelemetryData struct
3. Update display logic
4. Test on real hardware

## References

### External
- [nvtop GitHub](https://github.com/Syllo/nvtop)
- [Linux DRM fdinfo](https://www.kernel.org/doc/html/latest/gpu/drm-usage-stats.html)
- [NVML Documentation](https://docs.nvidia.com/deploy/nvml-api/)

### Internal
- TT-UMD API docs (in tt-metal/third_party/umd)
- TT-Metal device APIs
- tt-kmd kernel driver

## Getting Help

1. Check the guide: `NVTOP_ARCHITECTURE_GUIDE.md`
2. Compare implementations: `IMPLEMENTATION_COMPARISON.md`
3. Review UMD usage: `TT_SMI_UMD_TELEMETRY_GUIDE.md`
4. Look at nvtop code for patterns

## Future Vision: tt-nvtop

A full interactive monitoring tool for Tenstorrent:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  tt-nvtop v1.0                    Mon Nov  3 14:30:00 2025  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Device 0: Blackhole                                        â”‚
â”‚    GPU  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 60% @ 1200 MHz  â”‚
â”‚    MEM  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 25% 3.0/12 GB   â”‚
â”‚    TEMP [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 68Â°C / 95Â°C     â”‚
â”‚    PWR  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 285W / 350W     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚           GPU Utilization History (60s)              â”‚ â”‚
â”‚  â”‚ 100%â”‚                     â•­â”€â•®                         â”‚ â”‚
â”‚  â”‚  75%â”‚         â•­â”€â•®    â•­â”€â”€â”€â•¯ â•°â”€â•®                      â”‚ â”‚
â”‚  â”‚  50%â”‚    â•­â”€â”€â”€â”€â•¯ â•°â”€â”€â”€â”€â•¯        â•°â”€â•®                   â”‚ â”‚
â”‚  â”‚  25%â”‚â•­â”€â”€â”€â•¯                      â•°â”€â”€â•®                â”‚ â”‚
â”‚  â”‚   0%â”‚â•¯                             â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Processes (4 running)                                      â”‚
â”‚  PID     User    Command         GPU%   MEM      TIME      â”‚
â”‚  12345   user1   model_train     45%    2.1GB    1:23:45   â”‚
â”‚  12346   user2   inference       15%    900MB    0:45:12   â”‚
â”‚  12347   user1   preprocess      5%     128MB    0:12:34   â”‚
â”‚  12348   user3   validation      3%     64MB     0:05:21   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  F1:Help F2:Setup F9:Kill F12:Save q:Quit
```

## Summary

**nvtop** is an excellent reference for building GPU monitoring tools. Its plugin architecture, robust error handling, and user-friendly interface make it a great template for Tenstorrent monitoring.

**Key takeaways:**
1. Use UMD APIs for complete telemetry (not just sysfs)
2. Add fdinfo to tt-kmd for per-process tracking
3. Build modular (separate data collection from UI)
4. Leverage existing tools (nvtop) as references

**Current state:**
- âœ… Basic monitoring works (tt-smi)
- âœ… Enhanced telemetry available (tt-smi-umd)
- âš ï¸ Per-process GPU usage needs kernel support
- ğŸ“‹ Interactive UI (tt-nvtop) is next big step

**Next steps:**
1. Test tt-smi-umd on your 4 Blackhole devices
2. Compare telemetry with sysfs version
3. Implement device caching for better performance
4. Start work on fdinfo in tt-kmd
