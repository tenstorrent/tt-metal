Search.setIndex({"docnames": ["index", "resources/contributing", "resources/support", "tools/index", "ttnn/about", "ttnn/adding_new_ttnn_operation", "ttnn/api", "ttnn/api/ttnn.Conv2dConfig", "ttnn/api/ttnn.Conv2dSliceConfig", "ttnn/api/ttnn.GetDefaultDevice", "ttnn/api/ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig", "ttnn/api/ttnn.MatmulMultiCoreReuseMultiCastBatchedDRAMShardedProgramConfig", "ttnn/api/ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig", "ttnn/api/ttnn.MatmulMultiCoreReuseMultiCastProgramConfig", "ttnn/api/ttnn.MatmulMultiCoreReuseProgramConfig", "ttnn/api/ttnn.SetDefaultDevice", "ttnn/api/ttnn.SoftmaxDefaultProgramConfig", "ttnn/api/ttnn.SoftmaxProgramConfig", "ttnn/api/ttnn.SoftmaxShardedMultiCoreProgramConfig", "ttnn/api/ttnn.abs", "ttnn/api/ttnn.abs_bw", "ttnn/api/ttnn.acos", "ttnn/api/ttnn.acos_bw", "ttnn/api/ttnn.acosh", "ttnn/api/ttnn.acosh_bw", "ttnn/api/ttnn.adaptive_avg_pool2d", "ttnn/api/ttnn.adaptive_max_pool2d", "ttnn/api/ttnn.add", "ttnn/api/ttnn.add_", "ttnn/api/ttnn.add_bw", "ttnn/api/ttnn.addalpha", "ttnn/api/ttnn.addalpha_bw", "ttnn/api/ttnn.addcdiv", "ttnn/api/ttnn.addcdiv_bw", "ttnn/api/ttnn.addcmul", "ttnn/api/ttnn.addcmul_bw", "ttnn/api/ttnn.addmm", "ttnn/api/ttnn.all_broadcast", "ttnn/api/ttnn.all_gather", "ttnn/api/ttnn.all_reduce", "ttnn/api/ttnn.all_to_all_combine", "ttnn/api/ttnn.all_to_all_dispatch", "ttnn/api/ttnn.alt_complex_rotate90", "ttnn/api/ttnn.angle", "ttnn/api/ttnn.angle_bw", "ttnn/api/ttnn.arange", "ttnn/api/ttnn.argmax", "ttnn/api/ttnn.as_tensor", "ttnn/api/ttnn.asin", "ttnn/api/ttnn.asin_bw", "ttnn/api/ttnn.asinh", "ttnn/api/ttnn.asinh_bw", "ttnn/api/ttnn.assign", "ttnn/api/ttnn.assign_bw", "ttnn/api/ttnn.atan", "ttnn/api/ttnn.atan2", "ttnn/api/ttnn.atan2_bw", "ttnn/api/ttnn.atan_bw", "ttnn/api/ttnn.atanh", "ttnn/api/ttnn.atanh_bw", "ttnn/api/ttnn.avg_pool2d", "ttnn/api/ttnn.batch_norm", "ttnn/api/ttnn.bcast", "ttnn/api/ttnn.bernoulli", "ttnn/api/ttnn.bias_gelu", "ttnn/api/ttnn.bias_gelu_", "ttnn/api/ttnn.bias_gelu_bw", "ttnn/api/ttnn.bitcast", "ttnn/api/ttnn.bitwise_and", "ttnn/api/ttnn.bitwise_left_shift", "ttnn/api/ttnn.bitwise_not", "ttnn/api/ttnn.bitwise_or", "ttnn/api/ttnn.bitwise_right_shift", "ttnn/api/ttnn.bitwise_xor", "ttnn/api/ttnn.broadcast", "ttnn/api/ttnn.cbrt", "ttnn/api/ttnn.ceil", "ttnn/api/ttnn.ceil_bw", "ttnn/api/ttnn.celu", "ttnn/api/ttnn.celu_bw", "ttnn/api/ttnn.chunk", "ttnn/api/ttnn.clamp", "ttnn/api/ttnn.clamp_bw", "ttnn/api/ttnn.clip", "ttnn/api/ttnn.clip_bw", "ttnn/api/ttnn.clone", "ttnn/api/ttnn.close_device", "ttnn/api/ttnn.complex_tensor", "ttnn/api/ttnn.concat", "ttnn/api/ttnn.concat_bw", "ttnn/api/ttnn.conj", "ttnn/api/ttnn.conj_bw", "ttnn/api/ttnn.conv1d", "ttnn/api/ttnn.conv2d", "ttnn/api/ttnn.conv_transpose2d", "ttnn/api/ttnn.copy", "ttnn/api/ttnn.copy_device_to_host_tensor", "ttnn/api/ttnn.copy_host_to_device_tensor", "ttnn/api/ttnn.cos", "ttnn/api/ttnn.cos_bw", "ttnn/api/ttnn.cosh", "ttnn/api/ttnn.cosh_bw", "ttnn/api/ttnn.create_sharded_memory_config", "ttnn/api/ttnn.cumprod", "ttnn/api/ttnn.cumsum", "ttnn/api/ttnn.deallocate", "ttnn/api/ttnn.deg2rad", "ttnn/api/ttnn.deg2rad_bw", "ttnn/api/ttnn.dequantize", "ttnn/api/ttnn.digamma", "ttnn/api/ttnn.digamma_bw", "ttnn/api/ttnn.div", "ttnn/api/ttnn.div_bw", "ttnn/api/ttnn.div_no_nan", "ttnn/api/ttnn.div_no_nan_bw", "ttnn/api/ttnn.divide", "ttnn/api/ttnn.divide_", "ttnn/api/ttnn.dram_prefetcher", "ttnn/api/ttnn.dump_tensor", "ttnn/api/ttnn.elu", "ttnn/api/ttnn.elu_bw", "ttnn/api/ttnn.ema", "ttnn/api/ttnn.embedding", "ttnn/api/ttnn.embedding_bw", "ttnn/api/ttnn.empty", "ttnn/api/ttnn.empty_like", "ttnn/api/ttnn.eq", "ttnn/api/ttnn.eq_", "ttnn/api/ttnn.eqz", "ttnn/api/ttnn.erf", "ttnn/api/ttnn.erf_bw", "ttnn/api/ttnn.erfc", "ttnn/api/ttnn.erfc_bw", "ttnn/api/ttnn.erfinv", "ttnn/api/ttnn.erfinv_bw", "ttnn/api/ttnn.exp", "ttnn/api/ttnn.exp2", "ttnn/api/ttnn.exp2_bw", "ttnn/api/ttnn.exp_bw", "ttnn/api/ttnn.expand", "ttnn/api/ttnn.experimental.conv3d", "ttnn/api/ttnn.experimental.dropout", "ttnn/api/ttnn.experimental.gelu_bw", "ttnn/api/ttnn.expm1", "ttnn/api/ttnn.expm1_bw", "ttnn/api/ttnn.fill", "ttnn/api/ttnn.fill_bw", "ttnn/api/ttnn.fill_cache", "ttnn/api/ttnn.fill_implicit_tile_padding", "ttnn/api/ttnn.fill_ones_rm", "ttnn/api/ttnn.fill_rm", "ttnn/api/ttnn.fill_zero_bw", "ttnn/api/ttnn.floor", "ttnn/api/ttnn.floor_bw", "ttnn/api/ttnn.floor_div", "ttnn/api/ttnn.fmod", "ttnn/api/ttnn.fmod_bw", "ttnn/api/ttnn.fold", "ttnn/api/ttnn.frac", "ttnn/api/ttnn.frac_bw", "ttnn/api/ttnn.from_buffer", "ttnn/api/ttnn.from_device", "ttnn/api/ttnn.from_torch", "ttnn/api/ttnn.full", "ttnn/api/ttnn.full_like", "ttnn/api/ttnn.gather", "ttnn/api/ttnn.gcd", "ttnn/api/ttnn.ge", "ttnn/api/ttnn.ge_", "ttnn/api/ttnn.geglu", "ttnn/api/ttnn.gelu", "ttnn/api/ttnn.gelu_bw", "ttnn/api/ttnn.generic_op", "ttnn/api/ttnn.get_device_tensors", "ttnn/api/ttnn.gez", "ttnn/api/ttnn.global_avg_pool2d", "ttnn/api/ttnn.glu", "ttnn/api/ttnn.grid_sample", "ttnn/api/ttnn.group_norm", "ttnn/api/ttnn.gt", "ttnn/api/ttnn.gt_", "ttnn/api/ttnn.gtz", "ttnn/api/ttnn.hardmish", "ttnn/api/ttnn.hardshrink", "ttnn/api/ttnn.hardshrink_bw", "ttnn/api/ttnn.hardsigmoid", "ttnn/api/ttnn.hardsigmoid_bw", "ttnn/api/ttnn.hardswish", "ttnn/api/ttnn.hardswish_bw", "ttnn/api/ttnn.hardtanh", "ttnn/api/ttnn.hardtanh_bw", "ttnn/api/ttnn.heaviside", "ttnn/api/ttnn.hypot", "ttnn/api/ttnn.hypot_bw", "ttnn/api/ttnn.i0", "ttnn/api/ttnn.i0_bw", "ttnn/api/ttnn.i1", "ttnn/api/ttnn.identity", "ttnn/api/ttnn.imag", "ttnn/api/ttnn.imag_bw", "ttnn/api/ttnn.index_fill", "ttnn/api/ttnn.indexed_fill", "ttnn/api/ttnn.interleaved_to_sharded", "ttnn/api/ttnn.interleaved_to_sharded_partial", "ttnn/api/ttnn.is_imag", "ttnn/api/ttnn.is_real", "ttnn/api/ttnn.isclose", "ttnn/api/ttnn.isfinite", "ttnn/api/ttnn.isinf", "ttnn/api/ttnn.isnan", "ttnn/api/ttnn.isneginf", "ttnn/api/ttnn.isposinf", "ttnn/api/ttnn.kv_cache.fill_cache_for_user_", "ttnn/api/ttnn.kv_cache.update_cache_for_token_", "ttnn/api/ttnn.l1_loss", "ttnn/api/ttnn.layer_norm", "ttnn/api/ttnn.layer_norm_post_all_gather", "ttnn/api/ttnn.layer_norm_pre_all_gather", "ttnn/api/ttnn.lcm", "ttnn/api/ttnn.ldexp", "ttnn/api/ttnn.ldexp_", "ttnn/api/ttnn.ldexp_bw", "ttnn/api/ttnn.le", "ttnn/api/ttnn.le_", "ttnn/api/ttnn.leaky_relu", "ttnn/api/ttnn.leaky_relu_bw", "ttnn/api/ttnn.lerp", "ttnn/api/ttnn.lerp_bw", "ttnn/api/ttnn.lez", "ttnn/api/ttnn.lgamma", "ttnn/api/ttnn.lgamma_bw", "ttnn/api/ttnn.linear", "ttnn/api/ttnn.load_tensor", "ttnn/api/ttnn.log", "ttnn/api/ttnn.log10", "ttnn/api/ttnn.log10_bw", "ttnn/api/ttnn.log1p", "ttnn/api/ttnn.log1p_bw", "ttnn/api/ttnn.log2", "ttnn/api/ttnn.log2_bw", "ttnn/api/ttnn.log_bw", "ttnn/api/ttnn.log_sigmoid", "ttnn/api/ttnn.log_sigmoid_bw", "ttnn/api/ttnn.logaddexp", "ttnn/api/ttnn.logaddexp2", "ttnn/api/ttnn.logaddexp2_", "ttnn/api/ttnn.logaddexp2_bw", "ttnn/api/ttnn.logaddexp_", "ttnn/api/ttnn.logaddexp_bw", "ttnn/api/ttnn.logical_and", "ttnn/api/ttnn.logical_and_", "ttnn/api/ttnn.logical_left_shift", "ttnn/api/ttnn.logical_not", "ttnn/api/ttnn.logical_not_", "ttnn/api/ttnn.logical_or", "ttnn/api/ttnn.logical_or_", "ttnn/api/ttnn.logical_right_shift", "ttnn/api/ttnn.logical_xor", "ttnn/api/ttnn.logical_xor_", "ttnn/api/ttnn.logit", "ttnn/api/ttnn.logit_bw", "ttnn/api/ttnn.logiteps_bw", "ttnn/api/ttnn.lt", "ttnn/api/ttnn.lt_", "ttnn/api/ttnn.ltz", "ttnn/api/ttnn.mac", "ttnn/api/ttnn.manage_device", "ttnn/api/ttnn.manual_seed", "ttnn/api/ttnn.matmul", "ttnn/api/ttnn.max", "ttnn/api/ttnn.max_bw", "ttnn/api/ttnn.max_pool2d", "ttnn/api/ttnn.maximum", "ttnn/api/ttnn.mean", "ttnn/api/ttnn.mesh_partition", "ttnn/api/ttnn.min", "ttnn/api/ttnn.min_bw", "ttnn/api/ttnn.minimum", "ttnn/api/ttnn.mish", "ttnn/api/ttnn.model_preprocessing.preprocess_model", "ttnn/api/ttnn.model_preprocessing.preprocess_model_parameters", "ttnn/api/ttnn.moe", "ttnn/api/ttnn.moe_expert_token_remap", "ttnn/api/ttnn.moe_routing_remap", "ttnn/api/ttnn.move", "ttnn/api/ttnn.mse_loss", "ttnn/api/ttnn.mul_bw", "ttnn/api/ttnn.multigammaln", "ttnn/api/ttnn.multigammaln_bw", "ttnn/api/ttnn.multiply", "ttnn/api/ttnn.multiply_", "ttnn/api/ttnn.ne", "ttnn/api/ttnn.ne_", "ttnn/api/ttnn.neg", "ttnn/api/ttnn.neg_bw", "ttnn/api/ttnn.nextafter", "ttnn/api/ttnn.nez", "ttnn/api/ttnn.nonzero", "ttnn/api/ttnn.normalize_global", "ttnn/api/ttnn.normalize_hw", "ttnn/api/ttnn.ones", "ttnn/api/ttnn.ones_like", "ttnn/api/ttnn.open_device", "ttnn/api/ttnn.outer", "ttnn/api/ttnn.pad", "ttnn/api/ttnn.pad_to_tile_shape", "ttnn/api/ttnn.permute", "ttnn/api/ttnn.point_to_point", "ttnn/api/ttnn.polar", "ttnn/api/ttnn.polar_bw", "ttnn/api/ttnn.polygamma", "ttnn/api/ttnn.polygamma_bw", "ttnn/api/ttnn.polyval", "ttnn/api/ttnn.pow", "ttnn/api/ttnn.pow_bw", "ttnn/api/ttnn.prelu", "ttnn/api/ttnn.prepare_conv_bias", "ttnn/api/ttnn.prepare_conv_transpose2d_bias", "ttnn/api/ttnn.prepare_conv_transpose2d_weights", "ttnn/api/ttnn.prepare_conv_weights", "ttnn/api/ttnn.prod", "ttnn/api/ttnn.prod_bw", "ttnn/api/ttnn.quantize", "ttnn/api/ttnn.rad2deg", "ttnn/api/ttnn.rad2deg_bw", "ttnn/api/ttnn.rand", "ttnn/api/ttnn.rdiv", "ttnn/api/ttnn.rdiv_bw", "ttnn/api/ttnn.real", "ttnn/api/ttnn.real_bw", "ttnn/api/ttnn.reallocate", "ttnn/api/ttnn.reciprocal", "ttnn/api/ttnn.reciprocal_bw", "ttnn/api/ttnn.reduce_scatter", "ttnn/api/ttnn.reduce_to_root", "ttnn/api/ttnn.register_post_operation_hook", "ttnn/api/ttnn.register_pre_operation_hook", "ttnn/api/ttnn.reglu", "ttnn/api/ttnn.relu", "ttnn/api/ttnn.relu6", "ttnn/api/ttnn.relu6_bw", "ttnn/api/ttnn.relu_bw", "ttnn/api/ttnn.relu_max", "ttnn/api/ttnn.relu_min", "ttnn/api/ttnn.remainder", "ttnn/api/ttnn.remainder_bw", "ttnn/api/ttnn.repeat", "ttnn/api/ttnn.repeat_bw", "ttnn/api/ttnn.repeat_interleave", "ttnn/api/ttnn.requantize", "ttnn/api/ttnn.reshape", "ttnn/api/ttnn.reshape_on_device", "ttnn/api/ttnn.reshard", "ttnn/api/ttnn.rms_norm", "ttnn/api/ttnn.rms_norm_post_all_gather", "ttnn/api/ttnn.rms_norm_pre_all_gather", "ttnn/api/ttnn.roll", "ttnn/api/ttnn.rotate", "ttnn/api/ttnn.round", "ttnn/api/ttnn.round_bw", "ttnn/api/ttnn.rpow", "ttnn/api/ttnn.rpow_bw", "ttnn/api/ttnn.rsqrt", "ttnn/api/ttnn.rsqrt_bw", "ttnn/api/ttnn.rsub", "ttnn/api/ttnn.rsub_", "ttnn/api/ttnn.rsub_bw", "ttnn/api/ttnn.sampling", "ttnn/api/ttnn.scale_causal_mask_hw_dims_softmax_in_place", "ttnn/api/ttnn.scale_mask_softmax", "ttnn/api/ttnn.scale_mask_softmax_in_place", "ttnn/api/ttnn.scatter", "ttnn/api/ttnn.scatter_add", "ttnn/api/ttnn.selu", "ttnn/api/ttnn.selu_bw", "ttnn/api/ttnn.set_printoptions", "ttnn/api/ttnn.sharded_to_interleaved", "ttnn/api/ttnn.sharded_to_interleaved_partial", "ttnn/api/ttnn.sigmoid", "ttnn/api/ttnn.sigmoid_accurate", "ttnn/api/ttnn.sigmoid_bw", "ttnn/api/ttnn.sign", "ttnn/api/ttnn.sign_bw", "ttnn/api/ttnn.signbit", "ttnn/api/ttnn.silu", "ttnn/api/ttnn.silu_bw", "ttnn/api/ttnn.sin", "ttnn/api/ttnn.sin_bw", "ttnn/api/ttnn.sinh", "ttnn/api/ttnn.sinh_bw", "ttnn/api/ttnn.slice", "ttnn/api/ttnn.softmax", "ttnn/api/ttnn.softmax_in_place", "ttnn/api/ttnn.softplus", "ttnn/api/ttnn.softplus_bw", "ttnn/api/ttnn.softshrink", "ttnn/api/ttnn.softshrink_bw", "ttnn/api/ttnn.softsign", "ttnn/api/ttnn.softsign_bw", "ttnn/api/ttnn.sort", "ttnn/api/ttnn.sparse_matmul", "ttnn/api/ttnn.split", "ttnn/api/ttnn.split_work_to_cores", "ttnn/api/ttnn.sqrt", "ttnn/api/ttnn.sqrt_bw", "ttnn/api/ttnn.square", "ttnn/api/ttnn.square_bw", "ttnn/api/ttnn.squared_difference", "ttnn/api/ttnn.squared_difference_", "ttnn/api/ttnn.squared_difference_bw", "ttnn/api/ttnn.squeeze", "ttnn/api/ttnn.stack", "ttnn/api/ttnn.std", "ttnn/api/ttnn.std_hw", "ttnn/api/ttnn.sub_bw", "ttnn/api/ttnn.subalpha", "ttnn/api/ttnn.subalpha_bw", "ttnn/api/ttnn.subtract", "ttnn/api/ttnn.subtract_", "ttnn/api/ttnn.sum", "ttnn/api/ttnn.swiglu", "ttnn/api/ttnn.swish", "ttnn/api/ttnn.synchronize_device", "ttnn/api/ttnn.tan", "ttnn/api/ttnn.tan_bw", "ttnn/api/ttnn.tanh", "ttnn/api/ttnn.tanh_bw", "ttnn/api/ttnn.tanhshrink", "ttnn/api/ttnn.tanhshrink_bw", "ttnn/api/ttnn.threshold", "ttnn/api/ttnn.threshold_bw", "ttnn/api/ttnn.tilize", "ttnn/api/ttnn.tilize_with_val_padding", "ttnn/api/ttnn.tilize_with_zero_padding", "ttnn/api/ttnn.to_device", "ttnn/api/ttnn.to_dtype", "ttnn/api/ttnn.to_layout", "ttnn/api/ttnn.to_memory_config", "ttnn/api/ttnn.to_torch", "ttnn/api/ttnn.topk", "ttnn/api/ttnn.transformer.attention_softmax", "ttnn/api/ttnn.transformer.attention_softmax_", "ttnn/api/ttnn.transformer.chunked_flash_mla_prefill", "ttnn/api/ttnn.transformer.chunked_scaled_dot_product_attention", "ttnn/api/ttnn.transformer.concatenate_heads", "ttnn/api/ttnn.transformer.flash_mla_prefill", "ttnn/api/ttnn.transformer.flash_multi_latent_attention_decode", "ttnn/api/ttnn.transformer.joint_scaled_dot_product_attention", "ttnn/api/ttnn.transformer.paged_flash_multi_latent_attention_decode", "ttnn/api/ttnn.transformer.paged_scaled_dot_product_attention_decode", "ttnn/api/ttnn.transformer.ring_distributed_scaled_dot_product_attention", "ttnn/api/ttnn.transformer.ring_joint_scaled_dot_product_attention", "ttnn/api/ttnn.transformer.scaled_dot_product_attention", "ttnn/api/ttnn.transformer.scaled_dot_product_attention_decode", "ttnn/api/ttnn.transformer.split_query_key_value_and_split_heads", "ttnn/api/ttnn.transformer.windowed_scaled_dot_product_attention", "ttnn/api/ttnn.transpose", "ttnn/api/ttnn.tril", "ttnn/api/ttnn.triu", "ttnn/api/ttnn.trunc", "ttnn/api/ttnn.trunc_bw", "ttnn/api/ttnn.typecast", "ttnn/api/ttnn.unary_chain", "ttnn/api/ttnn.uniform", "ttnn/api/ttnn.unsqueeze", "ttnn/api/ttnn.unsqueeze_to_4D", "ttnn/api/ttnn.untilize", "ttnn/api/ttnn.untilize_with_unpadding", "ttnn/api/ttnn.update_cache", "ttnn/api/ttnn.upsample", "ttnn/api/ttnn.var", "ttnn/api/ttnn.var_hw", "ttnn/api/ttnn.view", "ttnn/api/ttnn.where", "ttnn/api/ttnn.where_bw", "ttnn/api/ttnn.xlogy", "ttnn/api/ttnn.xlogy_bw", "ttnn/api/ttnn.zeros", "ttnn/api/ttnn.zeros_like", "ttnn/converting_torch_model_to_ttnn", "ttnn/demos", "ttnn/get_started", "ttnn/installing", "ttnn/onboarding", "ttnn/profiling_ttnn_operations", "ttnn/tensor", "ttnn/tutorials", "ttnn/tutorials/tutorials/ttnn_add_tensors", "ttnn/tutorials/tutorials/ttnn_basic_conv", "ttnn/tutorials/tutorials/ttnn_basic_matrix_multiplication", "ttnn/tutorials/tutorials/ttnn_basic_operations", "ttnn/tutorials/tutorials/ttnn_clip_zero_shot_classification", "ttnn/tutorials/tutorials/ttnn_intro", "ttnn/tutorials/tutorials/ttnn_mlp_inference_mnist", "ttnn/tutorials/tutorials/ttnn_multihead_attention", "ttnn/tutorials/tutorials/ttnn_simplecnn_inference", "ttnn/tutorials/tutorials/ttnn_tracer_model", "ttnn/tutorials/tutorials/ttnn_visualizer"], "filenames": ["index.rst", "resources/contributing.rst", "resources/support.rst", "tools/index.rst", "ttnn/about.rst", "ttnn/adding_new_ttnn_operation.rst", "ttnn/api.rst", "ttnn/api/ttnn.Conv2dConfig.rst", "ttnn/api/ttnn.Conv2dSliceConfig.rst", "ttnn/api/ttnn.GetDefaultDevice.rst", "ttnn/api/ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.rst", "ttnn/api/ttnn.MatmulMultiCoreReuseMultiCastBatchedDRAMShardedProgramConfig.rst", "ttnn/api/ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig.rst", "ttnn/api/ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.rst", "ttnn/api/ttnn.MatmulMultiCoreReuseProgramConfig.rst", "ttnn/api/ttnn.SetDefaultDevice.rst", "ttnn/api/ttnn.SoftmaxDefaultProgramConfig.rst", "ttnn/api/ttnn.SoftmaxProgramConfig.rst", "ttnn/api/ttnn.SoftmaxShardedMultiCoreProgramConfig.rst", "ttnn/api/ttnn.abs.rst", "ttnn/api/ttnn.abs_bw.rst", "ttnn/api/ttnn.acos.rst", "ttnn/api/ttnn.acos_bw.rst", "ttnn/api/ttnn.acosh.rst", "ttnn/api/ttnn.acosh_bw.rst", "ttnn/api/ttnn.adaptive_avg_pool2d.rst", "ttnn/api/ttnn.adaptive_max_pool2d.rst", "ttnn/api/ttnn.add.rst", "ttnn/api/ttnn.add_.rst", "ttnn/api/ttnn.add_bw.rst", "ttnn/api/ttnn.addalpha.rst", "ttnn/api/ttnn.addalpha_bw.rst", "ttnn/api/ttnn.addcdiv.rst", "ttnn/api/ttnn.addcdiv_bw.rst", "ttnn/api/ttnn.addcmul.rst", "ttnn/api/ttnn.addcmul_bw.rst", "ttnn/api/ttnn.addmm.rst", "ttnn/api/ttnn.all_broadcast.rst", "ttnn/api/ttnn.all_gather.rst", "ttnn/api/ttnn.all_reduce.rst", "ttnn/api/ttnn.all_to_all_combine.rst", "ttnn/api/ttnn.all_to_all_dispatch.rst", "ttnn/api/ttnn.alt_complex_rotate90.rst", "ttnn/api/ttnn.angle.rst", "ttnn/api/ttnn.angle_bw.rst", "ttnn/api/ttnn.arange.rst", "ttnn/api/ttnn.argmax.rst", "ttnn/api/ttnn.as_tensor.rst", "ttnn/api/ttnn.asin.rst", "ttnn/api/ttnn.asin_bw.rst", "ttnn/api/ttnn.asinh.rst", "ttnn/api/ttnn.asinh_bw.rst", "ttnn/api/ttnn.assign.rst", "ttnn/api/ttnn.assign_bw.rst", "ttnn/api/ttnn.atan.rst", "ttnn/api/ttnn.atan2.rst", "ttnn/api/ttnn.atan2_bw.rst", "ttnn/api/ttnn.atan_bw.rst", "ttnn/api/ttnn.atanh.rst", "ttnn/api/ttnn.atanh_bw.rst", "ttnn/api/ttnn.avg_pool2d.rst", "ttnn/api/ttnn.batch_norm.rst", "ttnn/api/ttnn.bcast.rst", "ttnn/api/ttnn.bernoulli.rst", "ttnn/api/ttnn.bias_gelu.rst", "ttnn/api/ttnn.bias_gelu_.rst", "ttnn/api/ttnn.bias_gelu_bw.rst", "ttnn/api/ttnn.bitcast.rst", "ttnn/api/ttnn.bitwise_and.rst", "ttnn/api/ttnn.bitwise_left_shift.rst", "ttnn/api/ttnn.bitwise_not.rst", "ttnn/api/ttnn.bitwise_or.rst", "ttnn/api/ttnn.bitwise_right_shift.rst", "ttnn/api/ttnn.bitwise_xor.rst", "ttnn/api/ttnn.broadcast.rst", "ttnn/api/ttnn.cbrt.rst", "ttnn/api/ttnn.ceil.rst", "ttnn/api/ttnn.ceil_bw.rst", "ttnn/api/ttnn.celu.rst", "ttnn/api/ttnn.celu_bw.rst", "ttnn/api/ttnn.chunk.rst", "ttnn/api/ttnn.clamp.rst", "ttnn/api/ttnn.clamp_bw.rst", "ttnn/api/ttnn.clip.rst", "ttnn/api/ttnn.clip_bw.rst", "ttnn/api/ttnn.clone.rst", "ttnn/api/ttnn.close_device.rst", "ttnn/api/ttnn.complex_tensor.rst", "ttnn/api/ttnn.concat.rst", "ttnn/api/ttnn.concat_bw.rst", "ttnn/api/ttnn.conj.rst", "ttnn/api/ttnn.conj_bw.rst", "ttnn/api/ttnn.conv1d.rst", "ttnn/api/ttnn.conv2d.rst", "ttnn/api/ttnn.conv_transpose2d.rst", "ttnn/api/ttnn.copy.rst", "ttnn/api/ttnn.copy_device_to_host_tensor.rst", "ttnn/api/ttnn.copy_host_to_device_tensor.rst", "ttnn/api/ttnn.cos.rst", "ttnn/api/ttnn.cos_bw.rst", "ttnn/api/ttnn.cosh.rst", "ttnn/api/ttnn.cosh_bw.rst", "ttnn/api/ttnn.create_sharded_memory_config.rst", "ttnn/api/ttnn.cumprod.rst", "ttnn/api/ttnn.cumsum.rst", "ttnn/api/ttnn.deallocate.rst", "ttnn/api/ttnn.deg2rad.rst", "ttnn/api/ttnn.deg2rad_bw.rst", "ttnn/api/ttnn.dequantize.rst", "ttnn/api/ttnn.digamma.rst", "ttnn/api/ttnn.digamma_bw.rst", "ttnn/api/ttnn.div.rst", "ttnn/api/ttnn.div_bw.rst", "ttnn/api/ttnn.div_no_nan.rst", "ttnn/api/ttnn.div_no_nan_bw.rst", "ttnn/api/ttnn.divide.rst", "ttnn/api/ttnn.divide_.rst", "ttnn/api/ttnn.dram_prefetcher.rst", "ttnn/api/ttnn.dump_tensor.rst", "ttnn/api/ttnn.elu.rst", "ttnn/api/ttnn.elu_bw.rst", "ttnn/api/ttnn.ema.rst", "ttnn/api/ttnn.embedding.rst", "ttnn/api/ttnn.embedding_bw.rst", "ttnn/api/ttnn.empty.rst", "ttnn/api/ttnn.empty_like.rst", "ttnn/api/ttnn.eq.rst", "ttnn/api/ttnn.eq_.rst", "ttnn/api/ttnn.eqz.rst", "ttnn/api/ttnn.erf.rst", "ttnn/api/ttnn.erf_bw.rst", "ttnn/api/ttnn.erfc.rst", "ttnn/api/ttnn.erfc_bw.rst", "ttnn/api/ttnn.erfinv.rst", "ttnn/api/ttnn.erfinv_bw.rst", "ttnn/api/ttnn.exp.rst", "ttnn/api/ttnn.exp2.rst", "ttnn/api/ttnn.exp2_bw.rst", "ttnn/api/ttnn.exp_bw.rst", "ttnn/api/ttnn.expand.rst", "ttnn/api/ttnn.experimental.conv3d.rst", "ttnn/api/ttnn.experimental.dropout.rst", "ttnn/api/ttnn.experimental.gelu_bw.rst", "ttnn/api/ttnn.expm1.rst", "ttnn/api/ttnn.expm1_bw.rst", "ttnn/api/ttnn.fill.rst", "ttnn/api/ttnn.fill_bw.rst", "ttnn/api/ttnn.fill_cache.rst", "ttnn/api/ttnn.fill_implicit_tile_padding.rst", "ttnn/api/ttnn.fill_ones_rm.rst", "ttnn/api/ttnn.fill_rm.rst", "ttnn/api/ttnn.fill_zero_bw.rst", "ttnn/api/ttnn.floor.rst", "ttnn/api/ttnn.floor_bw.rst", "ttnn/api/ttnn.floor_div.rst", "ttnn/api/ttnn.fmod.rst", "ttnn/api/ttnn.fmod_bw.rst", "ttnn/api/ttnn.fold.rst", "ttnn/api/ttnn.frac.rst", "ttnn/api/ttnn.frac_bw.rst", "ttnn/api/ttnn.from_buffer.rst", "ttnn/api/ttnn.from_device.rst", "ttnn/api/ttnn.from_torch.rst", "ttnn/api/ttnn.full.rst", "ttnn/api/ttnn.full_like.rst", "ttnn/api/ttnn.gather.rst", "ttnn/api/ttnn.gcd.rst", "ttnn/api/ttnn.ge.rst", "ttnn/api/ttnn.ge_.rst", "ttnn/api/ttnn.geglu.rst", "ttnn/api/ttnn.gelu.rst", "ttnn/api/ttnn.gelu_bw.rst", "ttnn/api/ttnn.generic_op.rst", "ttnn/api/ttnn.get_device_tensors.rst", "ttnn/api/ttnn.gez.rst", "ttnn/api/ttnn.global_avg_pool2d.rst", "ttnn/api/ttnn.glu.rst", "ttnn/api/ttnn.grid_sample.rst", "ttnn/api/ttnn.group_norm.rst", "ttnn/api/ttnn.gt.rst", "ttnn/api/ttnn.gt_.rst", "ttnn/api/ttnn.gtz.rst", "ttnn/api/ttnn.hardmish.rst", "ttnn/api/ttnn.hardshrink.rst", "ttnn/api/ttnn.hardshrink_bw.rst", "ttnn/api/ttnn.hardsigmoid.rst", "ttnn/api/ttnn.hardsigmoid_bw.rst", "ttnn/api/ttnn.hardswish.rst", "ttnn/api/ttnn.hardswish_bw.rst", "ttnn/api/ttnn.hardtanh.rst", "ttnn/api/ttnn.hardtanh_bw.rst", "ttnn/api/ttnn.heaviside.rst", "ttnn/api/ttnn.hypot.rst", "ttnn/api/ttnn.hypot_bw.rst", "ttnn/api/ttnn.i0.rst", "ttnn/api/ttnn.i0_bw.rst", "ttnn/api/ttnn.i1.rst", "ttnn/api/ttnn.identity.rst", "ttnn/api/ttnn.imag.rst", "ttnn/api/ttnn.imag_bw.rst", "ttnn/api/ttnn.index_fill.rst", "ttnn/api/ttnn.indexed_fill.rst", "ttnn/api/ttnn.interleaved_to_sharded.rst", "ttnn/api/ttnn.interleaved_to_sharded_partial.rst", "ttnn/api/ttnn.is_imag.rst", "ttnn/api/ttnn.is_real.rst", "ttnn/api/ttnn.isclose.rst", "ttnn/api/ttnn.isfinite.rst", "ttnn/api/ttnn.isinf.rst", "ttnn/api/ttnn.isnan.rst", "ttnn/api/ttnn.isneginf.rst", "ttnn/api/ttnn.isposinf.rst", "ttnn/api/ttnn.kv_cache.fill_cache_for_user_.rst", "ttnn/api/ttnn.kv_cache.update_cache_for_token_.rst", "ttnn/api/ttnn.l1_loss.rst", "ttnn/api/ttnn.layer_norm.rst", "ttnn/api/ttnn.layer_norm_post_all_gather.rst", "ttnn/api/ttnn.layer_norm_pre_all_gather.rst", "ttnn/api/ttnn.lcm.rst", "ttnn/api/ttnn.ldexp.rst", "ttnn/api/ttnn.ldexp_.rst", "ttnn/api/ttnn.ldexp_bw.rst", "ttnn/api/ttnn.le.rst", "ttnn/api/ttnn.le_.rst", "ttnn/api/ttnn.leaky_relu.rst", "ttnn/api/ttnn.leaky_relu_bw.rst", "ttnn/api/ttnn.lerp.rst", "ttnn/api/ttnn.lerp_bw.rst", "ttnn/api/ttnn.lez.rst", "ttnn/api/ttnn.lgamma.rst", "ttnn/api/ttnn.lgamma_bw.rst", "ttnn/api/ttnn.linear.rst", "ttnn/api/ttnn.load_tensor.rst", "ttnn/api/ttnn.log.rst", "ttnn/api/ttnn.log10.rst", "ttnn/api/ttnn.log10_bw.rst", "ttnn/api/ttnn.log1p.rst", "ttnn/api/ttnn.log1p_bw.rst", "ttnn/api/ttnn.log2.rst", "ttnn/api/ttnn.log2_bw.rst", "ttnn/api/ttnn.log_bw.rst", "ttnn/api/ttnn.log_sigmoid.rst", "ttnn/api/ttnn.log_sigmoid_bw.rst", "ttnn/api/ttnn.logaddexp.rst", "ttnn/api/ttnn.logaddexp2.rst", "ttnn/api/ttnn.logaddexp2_.rst", "ttnn/api/ttnn.logaddexp2_bw.rst", "ttnn/api/ttnn.logaddexp_.rst", "ttnn/api/ttnn.logaddexp_bw.rst", "ttnn/api/ttnn.logical_and.rst", "ttnn/api/ttnn.logical_and_.rst", "ttnn/api/ttnn.logical_left_shift.rst", "ttnn/api/ttnn.logical_not.rst", "ttnn/api/ttnn.logical_not_.rst", "ttnn/api/ttnn.logical_or.rst", "ttnn/api/ttnn.logical_or_.rst", "ttnn/api/ttnn.logical_right_shift.rst", "ttnn/api/ttnn.logical_xor.rst", "ttnn/api/ttnn.logical_xor_.rst", "ttnn/api/ttnn.logit.rst", "ttnn/api/ttnn.logit_bw.rst", "ttnn/api/ttnn.logiteps_bw.rst", "ttnn/api/ttnn.lt.rst", "ttnn/api/ttnn.lt_.rst", "ttnn/api/ttnn.ltz.rst", "ttnn/api/ttnn.mac.rst", "ttnn/api/ttnn.manage_device.rst", "ttnn/api/ttnn.manual_seed.rst", "ttnn/api/ttnn.matmul.rst", "ttnn/api/ttnn.max.rst", "ttnn/api/ttnn.max_bw.rst", "ttnn/api/ttnn.max_pool2d.rst", "ttnn/api/ttnn.maximum.rst", "ttnn/api/ttnn.mean.rst", "ttnn/api/ttnn.mesh_partition.rst", "ttnn/api/ttnn.min.rst", "ttnn/api/ttnn.min_bw.rst", "ttnn/api/ttnn.minimum.rst", "ttnn/api/ttnn.mish.rst", "ttnn/api/ttnn.model_preprocessing.preprocess_model.rst", "ttnn/api/ttnn.model_preprocessing.preprocess_model_parameters.rst", "ttnn/api/ttnn.moe.rst", "ttnn/api/ttnn.moe_expert_token_remap.rst", "ttnn/api/ttnn.moe_routing_remap.rst", "ttnn/api/ttnn.move.rst", "ttnn/api/ttnn.mse_loss.rst", "ttnn/api/ttnn.mul_bw.rst", "ttnn/api/ttnn.multigammaln.rst", "ttnn/api/ttnn.multigammaln_bw.rst", "ttnn/api/ttnn.multiply.rst", "ttnn/api/ttnn.multiply_.rst", "ttnn/api/ttnn.ne.rst", "ttnn/api/ttnn.ne_.rst", "ttnn/api/ttnn.neg.rst", "ttnn/api/ttnn.neg_bw.rst", "ttnn/api/ttnn.nextafter.rst", "ttnn/api/ttnn.nez.rst", "ttnn/api/ttnn.nonzero.rst", "ttnn/api/ttnn.normalize_global.rst", "ttnn/api/ttnn.normalize_hw.rst", "ttnn/api/ttnn.ones.rst", "ttnn/api/ttnn.ones_like.rst", "ttnn/api/ttnn.open_device.rst", "ttnn/api/ttnn.outer.rst", "ttnn/api/ttnn.pad.rst", "ttnn/api/ttnn.pad_to_tile_shape.rst", "ttnn/api/ttnn.permute.rst", "ttnn/api/ttnn.point_to_point.rst", "ttnn/api/ttnn.polar.rst", "ttnn/api/ttnn.polar_bw.rst", "ttnn/api/ttnn.polygamma.rst", "ttnn/api/ttnn.polygamma_bw.rst", "ttnn/api/ttnn.polyval.rst", "ttnn/api/ttnn.pow.rst", "ttnn/api/ttnn.pow_bw.rst", "ttnn/api/ttnn.prelu.rst", "ttnn/api/ttnn.prepare_conv_bias.rst", "ttnn/api/ttnn.prepare_conv_transpose2d_bias.rst", "ttnn/api/ttnn.prepare_conv_transpose2d_weights.rst", "ttnn/api/ttnn.prepare_conv_weights.rst", "ttnn/api/ttnn.prod.rst", "ttnn/api/ttnn.prod_bw.rst", "ttnn/api/ttnn.quantize.rst", "ttnn/api/ttnn.rad2deg.rst", "ttnn/api/ttnn.rad2deg_bw.rst", "ttnn/api/ttnn.rand.rst", "ttnn/api/ttnn.rdiv.rst", "ttnn/api/ttnn.rdiv_bw.rst", "ttnn/api/ttnn.real.rst", "ttnn/api/ttnn.real_bw.rst", "ttnn/api/ttnn.reallocate.rst", "ttnn/api/ttnn.reciprocal.rst", "ttnn/api/ttnn.reciprocal_bw.rst", "ttnn/api/ttnn.reduce_scatter.rst", "ttnn/api/ttnn.reduce_to_root.rst", "ttnn/api/ttnn.register_post_operation_hook.rst", "ttnn/api/ttnn.register_pre_operation_hook.rst", "ttnn/api/ttnn.reglu.rst", "ttnn/api/ttnn.relu.rst", "ttnn/api/ttnn.relu6.rst", "ttnn/api/ttnn.relu6_bw.rst", "ttnn/api/ttnn.relu_bw.rst", "ttnn/api/ttnn.relu_max.rst", "ttnn/api/ttnn.relu_min.rst", "ttnn/api/ttnn.remainder.rst", "ttnn/api/ttnn.remainder_bw.rst", "ttnn/api/ttnn.repeat.rst", "ttnn/api/ttnn.repeat_bw.rst", "ttnn/api/ttnn.repeat_interleave.rst", "ttnn/api/ttnn.requantize.rst", "ttnn/api/ttnn.reshape.rst", "ttnn/api/ttnn.reshape_on_device.rst", "ttnn/api/ttnn.reshard.rst", "ttnn/api/ttnn.rms_norm.rst", "ttnn/api/ttnn.rms_norm_post_all_gather.rst", "ttnn/api/ttnn.rms_norm_pre_all_gather.rst", "ttnn/api/ttnn.roll.rst", "ttnn/api/ttnn.rotate.rst", "ttnn/api/ttnn.round.rst", "ttnn/api/ttnn.round_bw.rst", "ttnn/api/ttnn.rpow.rst", "ttnn/api/ttnn.rpow_bw.rst", "ttnn/api/ttnn.rsqrt.rst", "ttnn/api/ttnn.rsqrt_bw.rst", "ttnn/api/ttnn.rsub.rst", "ttnn/api/ttnn.rsub_.rst", "ttnn/api/ttnn.rsub_bw.rst", "ttnn/api/ttnn.sampling.rst", "ttnn/api/ttnn.scale_causal_mask_hw_dims_softmax_in_place.rst", "ttnn/api/ttnn.scale_mask_softmax.rst", "ttnn/api/ttnn.scale_mask_softmax_in_place.rst", "ttnn/api/ttnn.scatter.rst", "ttnn/api/ttnn.scatter_add.rst", "ttnn/api/ttnn.selu.rst", "ttnn/api/ttnn.selu_bw.rst", "ttnn/api/ttnn.set_printoptions.rst", "ttnn/api/ttnn.sharded_to_interleaved.rst", "ttnn/api/ttnn.sharded_to_interleaved_partial.rst", "ttnn/api/ttnn.sigmoid.rst", "ttnn/api/ttnn.sigmoid_accurate.rst", "ttnn/api/ttnn.sigmoid_bw.rst", "ttnn/api/ttnn.sign.rst", "ttnn/api/ttnn.sign_bw.rst", "ttnn/api/ttnn.signbit.rst", "ttnn/api/ttnn.silu.rst", "ttnn/api/ttnn.silu_bw.rst", "ttnn/api/ttnn.sin.rst", "ttnn/api/ttnn.sin_bw.rst", "ttnn/api/ttnn.sinh.rst", "ttnn/api/ttnn.sinh_bw.rst", "ttnn/api/ttnn.slice.rst", "ttnn/api/ttnn.softmax.rst", "ttnn/api/ttnn.softmax_in_place.rst", "ttnn/api/ttnn.softplus.rst", "ttnn/api/ttnn.softplus_bw.rst", "ttnn/api/ttnn.softshrink.rst", "ttnn/api/ttnn.softshrink_bw.rst", "ttnn/api/ttnn.softsign.rst", "ttnn/api/ttnn.softsign_bw.rst", "ttnn/api/ttnn.sort.rst", "ttnn/api/ttnn.sparse_matmul.rst", "ttnn/api/ttnn.split.rst", "ttnn/api/ttnn.split_work_to_cores.rst", "ttnn/api/ttnn.sqrt.rst", "ttnn/api/ttnn.sqrt_bw.rst", "ttnn/api/ttnn.square.rst", "ttnn/api/ttnn.square_bw.rst", "ttnn/api/ttnn.squared_difference.rst", "ttnn/api/ttnn.squared_difference_.rst", "ttnn/api/ttnn.squared_difference_bw.rst", "ttnn/api/ttnn.squeeze.rst", "ttnn/api/ttnn.stack.rst", "ttnn/api/ttnn.std.rst", "ttnn/api/ttnn.std_hw.rst", "ttnn/api/ttnn.sub_bw.rst", "ttnn/api/ttnn.subalpha.rst", "ttnn/api/ttnn.subalpha_bw.rst", "ttnn/api/ttnn.subtract.rst", "ttnn/api/ttnn.subtract_.rst", "ttnn/api/ttnn.sum.rst", "ttnn/api/ttnn.swiglu.rst", "ttnn/api/ttnn.swish.rst", "ttnn/api/ttnn.synchronize_device.rst", "ttnn/api/ttnn.tan.rst", "ttnn/api/ttnn.tan_bw.rst", "ttnn/api/ttnn.tanh.rst", "ttnn/api/ttnn.tanh_bw.rst", "ttnn/api/ttnn.tanhshrink.rst", "ttnn/api/ttnn.tanhshrink_bw.rst", "ttnn/api/ttnn.threshold.rst", "ttnn/api/ttnn.threshold_bw.rst", "ttnn/api/ttnn.tilize.rst", "ttnn/api/ttnn.tilize_with_val_padding.rst", "ttnn/api/ttnn.tilize_with_zero_padding.rst", "ttnn/api/ttnn.to_device.rst", "ttnn/api/ttnn.to_dtype.rst", "ttnn/api/ttnn.to_layout.rst", "ttnn/api/ttnn.to_memory_config.rst", "ttnn/api/ttnn.to_torch.rst", "ttnn/api/ttnn.topk.rst", "ttnn/api/ttnn.transformer.attention_softmax.rst", "ttnn/api/ttnn.transformer.attention_softmax_.rst", "ttnn/api/ttnn.transformer.chunked_flash_mla_prefill.rst", "ttnn/api/ttnn.transformer.chunked_scaled_dot_product_attention.rst", "ttnn/api/ttnn.transformer.concatenate_heads.rst", "ttnn/api/ttnn.transformer.flash_mla_prefill.rst", "ttnn/api/ttnn.transformer.flash_multi_latent_attention_decode.rst", "ttnn/api/ttnn.transformer.joint_scaled_dot_product_attention.rst", "ttnn/api/ttnn.transformer.paged_flash_multi_latent_attention_decode.rst", "ttnn/api/ttnn.transformer.paged_scaled_dot_product_attention_decode.rst", "ttnn/api/ttnn.transformer.ring_distributed_scaled_dot_product_attention.rst", "ttnn/api/ttnn.transformer.ring_joint_scaled_dot_product_attention.rst", "ttnn/api/ttnn.transformer.scaled_dot_product_attention.rst", "ttnn/api/ttnn.transformer.scaled_dot_product_attention_decode.rst", "ttnn/api/ttnn.transformer.split_query_key_value_and_split_heads.rst", "ttnn/api/ttnn.transformer.windowed_scaled_dot_product_attention.rst", "ttnn/api/ttnn.transpose.rst", "ttnn/api/ttnn.tril.rst", "ttnn/api/ttnn.triu.rst", "ttnn/api/ttnn.trunc.rst", "ttnn/api/ttnn.trunc_bw.rst", "ttnn/api/ttnn.typecast.rst", "ttnn/api/ttnn.unary_chain.rst", "ttnn/api/ttnn.uniform.rst", "ttnn/api/ttnn.unsqueeze.rst", "ttnn/api/ttnn.unsqueeze_to_4D.rst", "ttnn/api/ttnn.untilize.rst", "ttnn/api/ttnn.untilize_with_unpadding.rst", "ttnn/api/ttnn.update_cache.rst", "ttnn/api/ttnn.upsample.rst", "ttnn/api/ttnn.var.rst", "ttnn/api/ttnn.var_hw.rst", "ttnn/api/ttnn.view.rst", "ttnn/api/ttnn.where.rst", "ttnn/api/ttnn.where_bw.rst", "ttnn/api/ttnn.xlogy.rst", "ttnn/api/ttnn.xlogy_bw.rst", "ttnn/api/ttnn.zeros.rst", "ttnn/api/ttnn.zeros_like.rst", "ttnn/converting_torch_model_to_ttnn.rst", "ttnn/demos.rst", "ttnn/get_started.rst", "ttnn/installing.md", "ttnn/onboarding.rst", "ttnn/profiling_ttnn_operations.rst", "ttnn/tensor.rst", "ttnn/tutorials.rst", "ttnn/tutorials/tutorials/ttnn_add_tensors.ipynb", "ttnn/tutorials/tutorials/ttnn_basic_conv.ipynb", "ttnn/tutorials/tutorials/ttnn_basic_matrix_multiplication.ipynb", "ttnn/tutorials/tutorials/ttnn_basic_operations.ipynb", "ttnn/tutorials/tutorials/ttnn_clip_zero_shot_classification.ipynb", "ttnn/tutorials/tutorials/ttnn_intro.ipynb", "ttnn/tutorials/tutorials/ttnn_mlp_inference_mnist.ipynb", "ttnn/tutorials/tutorials/ttnn_multihead_attention.ipynb", "ttnn/tutorials/tutorials/ttnn_simplecnn_inference.ipynb", "ttnn/tutorials/tutorials/ttnn_tracer_model.ipynb", "ttnn/tutorials/tutorials/ttnn_visualizer.md"], "titles": ["Welcome to TT-NN documentation!", "Contributing as a developer", "Support", "Tools", "What is TT-NN?", "Adding New TT-NN Operation", "APIs", "ttnn.Conv2dConfig", "ttnn.Conv2dSliceConfig", "ttnn.GetDefaultDevice", "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig", "ttnn.MatmulMultiCoreReuseMultiCastBatchedDRAMShardedProgramConfig", "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig", "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig", "ttnn.MatmulMultiCoreReuseProgramConfig", "ttnn.SetDefaultDevice", "ttnn.SoftmaxDefaultProgramConfig", "ttnn.SoftmaxProgramConfig", "ttnn.SoftmaxShardedMultiCoreProgramConfig", "ttnn.abs", "ttnn.abs_bw", "ttnn.acos", "ttnn.acos_bw", "ttnn.acosh", "ttnn.acosh_bw", "ttnn.adaptive_avg_pool2d", "ttnn.adaptive_max_pool2d", "ttnn.add", "ttnn.add_", "ttnn.add_bw", "ttnn.addalpha", "ttnn.addalpha_bw", "ttnn.addcdiv", "ttnn.addcdiv_bw", "ttnn.addcmul", "ttnn.addcmul_bw", "ttnn.addmm", "ttnn.all_broadcast", "ttnn.all_gather", "ttnn.all_reduce", "ttnn.all_to_all_combine", "ttnn.all_to_all_dispatch", "ttnn.alt_complex_rotate90", "ttnn.angle", "ttnn.angle_bw", "ttnn.arange", "ttnn.argmax", "ttnn.as_tensor", "ttnn.asin", "ttnn.asin_bw", "ttnn.asinh", "ttnn.asinh_bw", "ttnn.assign", "ttnn.assign_bw", "ttnn.atan", "ttnn.atan2", "ttnn.atan2_bw", "ttnn.atan_bw", "ttnn.atanh", "ttnn.atanh_bw", "ttnn.avg_pool2d", "ttnn.batch_norm", "ttnn.bcast", "ttnn.bernoulli", "ttnn.bias_gelu", "ttnn.bias_gelu_", "ttnn.bias_gelu_bw", "ttnn.bitcast", "ttnn.bitwise_and", "ttnn.bitwise_left_shift", "ttnn.bitwise_not", "ttnn.bitwise_or", "ttnn.bitwise_right_shift", "ttnn.bitwise_xor", "ttnn.broadcast", "ttnn.cbrt", "ttnn.ceil", "ttnn.ceil_bw", "ttnn.celu", "ttnn.celu_bw", "ttnn.chunk", "ttnn.clamp", "ttnn.clamp_bw", "ttnn.clip", "ttnn.clip_bw", "ttnn.clone", "ttnn.close_device", "ttnn.complex_tensor", "ttnn.concat", "ttnn.concat_bw", "ttnn.conj", "ttnn.conj_bw", "ttnn.conv1d", "ttnn.conv2d", "ttnn.conv_transpose2d", "ttnn.copy", "ttnn.copy_device_to_host_tensor", "ttnn.copy_host_to_device_tensor", "ttnn.cos", "ttnn.cos_bw", "ttnn.cosh", "ttnn.cosh_bw", "ttnn.create_sharded_memory_config", "ttnn.cumprod", "ttnn.cumsum", "ttnn.deallocate", "ttnn.deg2rad", "ttnn.deg2rad_bw", "ttnn.dequantize", "ttnn.digamma", "ttnn.digamma_bw", "ttnn.div", "ttnn.div_bw", "ttnn.div_no_nan", "ttnn.div_no_nan_bw", "ttnn.divide", "ttnn.divide_", "ttnn.dram_prefetcher", "ttnn.dump_tensor", "ttnn.elu", "ttnn.elu_bw", "ttnn.ema", "ttnn.embedding", "ttnn.embedding_bw", "ttnn.empty", "ttnn.empty_like", "ttnn.eq", "ttnn.eq_", "ttnn.eqz", "ttnn.erf", "ttnn.erf_bw", "ttnn.erfc", "ttnn.erfc_bw", "ttnn.erfinv", "ttnn.erfinv_bw", "ttnn.exp", "ttnn.exp2", "ttnn.exp2_bw", "ttnn.exp_bw", "ttnn.expand", "ttnn.experimental.conv3d", "ttnn.experimental.dropout", "ttnn.experimental.gelu_bw", "ttnn.expm1", "ttnn.expm1_bw", "ttnn.fill", "ttnn.fill_bw", "ttnn.fill_cache", "ttnn.fill_implicit_tile_padding", "ttnn.fill_ones_rm", "ttnn.fill_rm", "ttnn.fill_zero_bw", "ttnn.floor", "ttnn.floor_bw", "ttnn.floor_div", "ttnn.fmod", "ttnn.fmod_bw", "ttnn.fold", "ttnn.frac", "ttnn.frac_bw", "ttnn.from_buffer", "ttnn.from_device", "ttnn.from_torch", "ttnn.full", "ttnn.full_like", "ttnn.gather", "ttnn.gcd", "ttnn.ge", "ttnn.ge_", "ttnn.geglu", "ttnn.gelu", "ttnn.gelu_bw", "ttnn.generic_op", "ttnn.get_device_tensors", "ttnn.gez", "ttnn.global_avg_pool2d", "ttnn.glu", "ttnn.grid_sample", "ttnn.group_norm", "ttnn.gt", "ttnn.gt_", "ttnn.gtz", "ttnn.hardmish", "ttnn.hardshrink", "ttnn.hardshrink_bw", "ttnn.hardsigmoid", "ttnn.hardsigmoid_bw", "ttnn.hardswish", "ttnn.hardswish_bw", "ttnn.hardtanh", "ttnn.hardtanh_bw", "ttnn.heaviside", "ttnn.hypot", "ttnn.hypot_bw", "ttnn.i0", "ttnn.i0_bw", "ttnn.i1", "ttnn.identity", "ttnn.imag", "ttnn.imag_bw", "ttnn.index_fill", "ttnn.indexed_fill", "ttnn.interleaved_to_sharded", "ttnn.interleaved_to_sharded_partial", "ttnn.is_imag", "ttnn.is_real", "ttnn.isclose", "ttnn.isfinite", "ttnn.isinf", "ttnn.isnan", "ttnn.isneginf", "ttnn.isposinf", "ttnn.kv_cache.fill_cache_for_user_", "ttnn.kv_cache.update_cache_for_token_", "ttnn.l1_loss", "ttnn.layer_norm", "ttnn.layer_norm_post_all_gather", "ttnn.layer_norm_pre_all_gather", "ttnn.lcm", "ttnn.ldexp", "ttnn.ldexp_", "ttnn.ldexp_bw", "ttnn.le", "ttnn.le_", "ttnn.leaky_relu", "ttnn.leaky_relu_bw", "ttnn.lerp", "ttnn.lerp_bw", "ttnn.lez", "ttnn.lgamma", "ttnn.lgamma_bw", "ttnn.linear", "ttnn.load_tensor", "ttnn.log", "ttnn.log10", "ttnn.log10_bw", "ttnn.log1p", "ttnn.log1p_bw", "ttnn.log2", "ttnn.log2_bw", "ttnn.log_bw", "ttnn.log_sigmoid", "ttnn.log_sigmoid_bw", "ttnn.logaddexp", "ttnn.logaddexp2", "ttnn.logaddexp2_", "ttnn.logaddexp2_bw", "ttnn.logaddexp_", "ttnn.logaddexp_bw", "ttnn.logical_and", "ttnn.logical_and_", "ttnn.logical_left_shift", "ttnn.logical_not", "ttnn.logical_not_", "ttnn.logical_or", "ttnn.logical_or_", "ttnn.logical_right_shift", "ttnn.logical_xor", "ttnn.logical_xor_", "ttnn.logit", "ttnn.logit_bw", "ttnn.logiteps_bw", "ttnn.lt", "ttnn.lt_", "ttnn.ltz", "ttnn.mac", "ttnn.manage_device", "ttnn.manual_seed", "ttnn.matmul", "ttnn.max", "ttnn.max_bw", "ttnn.max_pool2d", "ttnn.maximum", "ttnn.mean", "ttnn.mesh_partition", "ttnn.min", "ttnn.min_bw", "ttnn.minimum", "ttnn.mish", "ttnn.model_preprocessing.preprocess_model", "ttnn.model_preprocessing.preprocess_model_parameters", "ttnn.moe", "ttnn.moe_expert_token_remap", "ttnn.moe_routing_remap", "ttnn.move", "ttnn.mse_loss", "ttnn.mul_bw", "ttnn.multigammaln", "ttnn.multigammaln_bw", "ttnn.multiply", "ttnn.multiply_", "ttnn.ne", "ttnn.ne_", "ttnn.neg", "ttnn.neg_bw", "ttnn.nextafter", "ttnn.nez", "ttnn.nonzero", "ttnn.normalize_global", "ttnn.normalize_hw", "ttnn.ones", "ttnn.ones_like", "ttnn.open_device", "ttnn.outer", "ttnn.pad", "ttnn.pad_to_tile_shape", "ttnn.permute", "ttnn.point_to_point", "ttnn.polar", "ttnn.polar_bw", "ttnn.polygamma", "ttnn.polygamma_bw", "ttnn.polyval", "ttnn.pow", "ttnn.pow_bw", "ttnn.prelu", "ttnn.prepare_conv_bias", "ttnn.prepare_conv_transpose2d_bias", "ttnn.prepare_conv_transpose2d_weights", "ttnn.prepare_conv_weights", "ttnn.prod", "ttnn.prod_bw", "ttnn.quantize", "ttnn.rad2deg", "ttnn.rad2deg_bw", "ttnn.rand", "ttnn.rdiv", "ttnn.rdiv_bw", "ttnn.real", "ttnn.real_bw", "ttnn.reallocate", "ttnn.reciprocal", "ttnn.reciprocal_bw", "ttnn.reduce_scatter", "ttnn.reduce_to_root", "ttnn.register_post_operation_hook", "ttnn.register_pre_operation_hook", "ttnn.reglu", "ttnn.relu", "ttnn.relu6", "ttnn.relu6_bw", "ttnn.relu_bw", "ttnn.relu_max", "ttnn.relu_min", "ttnn.remainder", "ttnn.remainder_bw", "ttnn.repeat", "ttnn.repeat_bw", "ttnn.repeat_interleave", "ttnn.requantize", "ttnn.reshape", "ttnn.reshape_on_device", "ttnn.reshard", "ttnn.rms_norm", "ttnn.rms_norm_post_all_gather", "ttnn.rms_norm_pre_all_gather", "ttnn.roll", "ttnn.rotate", "ttnn.round", "ttnn.round_bw", "ttnn.rpow", "ttnn.rpow_bw", "ttnn.rsqrt", "ttnn.rsqrt_bw", "ttnn.rsub", "ttnn.rsub_", "ttnn.rsub_bw", "ttnn.sampling", "ttnn.scale_causal_mask_hw_dims_softmax_in_place", "ttnn.scale_mask_softmax", "ttnn.scale_mask_softmax_in_place", "ttnn.scatter", "ttnn.scatter_add", "ttnn.selu", "ttnn.selu_bw", "ttnn.set_printoptions", "ttnn.sharded_to_interleaved", "ttnn.sharded_to_interleaved_partial", "ttnn.sigmoid", "ttnn.sigmoid_accurate", "ttnn.sigmoid_bw", "ttnn.sign", "ttnn.sign_bw", "ttnn.signbit", "ttnn.silu", "ttnn.silu_bw", "ttnn.sin", "ttnn.sin_bw", "ttnn.sinh", "ttnn.sinh_bw", "ttnn.slice", "ttnn.softmax", "ttnn.softmax_in_place", "ttnn.softplus", "ttnn.softplus_bw", "ttnn.softshrink", "ttnn.softshrink_bw", "ttnn.softsign", "ttnn.softsign_bw", "ttnn.sort", "ttnn.sparse_matmul", "ttnn.split", "ttnn.split_work_to_cores", "ttnn.sqrt", "ttnn.sqrt_bw", "ttnn.square", "ttnn.square_bw", "ttnn.squared_difference", "ttnn.squared_difference_", "ttnn.squared_difference_bw", "ttnn.squeeze", "ttnn.stack", "ttnn.std", "ttnn.std_hw", "ttnn.sub_bw", "ttnn.subalpha", "ttnn.subalpha_bw", "ttnn.subtract", "ttnn.subtract_", "ttnn.sum", "ttnn.swiglu", "ttnn.swish", "ttnn.synchronize_device", "ttnn.tan", "ttnn.tan_bw", "ttnn.tanh", "ttnn.tanh_bw", "ttnn.tanhshrink", "ttnn.tanhshrink_bw", "ttnn.threshold", "ttnn.threshold_bw", "ttnn.tilize", "ttnn.tilize_with_val_padding", "ttnn.tilize_with_zero_padding", "ttnn.to_device", "ttnn.to_dtype", "ttnn.to_layout", "ttnn.to_memory_config", "ttnn.to_torch", "ttnn.topk", "ttnn.transformer.attention_softmax", "ttnn.transformer.attention_softmax_", "ttnn.transformer.chunked_flash_mla_prefill", "ttnn.transformer.chunked_scaled_dot_product_attention", "ttnn.transformer.concatenate_heads", "ttnn.transformer.flash_mla_prefill", "ttnn.transformer.flash_multi_latent_attention_decode", "ttnn.transformer.joint_scaled_dot_product_attention", "ttnn.transformer.paged_flash_multi_latent_attention_decode", "ttnn.transformer.paged_scaled_dot_product_attention_decode", "ttnn.transformer.ring_distributed_scaled_dot_product_attention", "ttnn.transformer.ring_joint_scaled_dot_product_attention", "ttnn.transformer.scaled_dot_product_attention", "ttnn.transformer.scaled_dot_product_attention_decode", "ttnn.transformer.split_query_key_value_and_split_heads", "ttnn.transformer.windowed_scaled_dot_product_attention", "ttnn.transpose", "ttnn.tril", "ttnn.triu", "ttnn.trunc", "ttnn.trunc_bw", "ttnn.typecast", "ttnn.unary_chain", "ttnn.uniform", "ttnn.unsqueeze", "ttnn.unsqueeze_to_4D", "ttnn.untilize", "ttnn.untilize_with_unpadding", "ttnn.update_cache", "ttnn.upsample", "ttnn.var", "ttnn.var_hw", "ttnn.view", "ttnn.where", "ttnn.where_bw", "ttnn.xlogy", "ttnn.xlogy_bw", "ttnn.zeros", "ttnn.zeros_like", "Converting PyTorch Model to TT-NN", "Building and Uplifting Demos", "Getting Started", "Install", "Onboarding New Functionality", "Profiling TT-NN Operations", "Tensor", "Tutorials", "Add Tensors", "Basic Convolution", "Matrix Multiplication", "Basic Tensor Operations", "Building CLIP Model for Zero-Shot Image Classification with TT-NN", "TT-NN Introduction", "MLP Inference", "Multi-Head Attention", "Running a Simple CNN Inference on CIFAR-10", "TT-NN Tracer and BERT Model Visualization Tutorial", "TT-NN Visualizer"], "terms": {"what": [0, 41, 480, 484, 486], "i": [0, 3, 7, 9, 10, 11, 12, 13, 14, 15, 17, 18, 25, 26, 27, 28, 29, 30, 31, 32, 34, 36, 37, 38, 39, 40, 41, 46, 52, 55, 56, 60, 61, 62, 64, 65, 66, 68, 69, 70, 71, 72, 73, 81, 86, 88, 89, 93, 94, 95, 96, 102, 103, 111, 112, 113, 115, 116, 122, 123, 126, 127, 139, 145, 149, 150, 154, 155, 159, 160, 162, 165, 166, 167, 168, 169, 175, 176, 177, 178, 179, 180, 182, 192, 193, 195, 196, 200, 204, 205, 206, 207, 208, 209, 210, 211, 215, 216, 217, 218, 219, 220, 221, 222, 223, 227, 231, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 279, 280, 281, 284, 286, 288, 289, 290, 291, 292, 295, 302, 303, 304, 305, 308, 310, 312, 313, 315, 320, 321, 326, 331, 333, 334, 335, 336, 337, 342, 343, 344, 348, 349, 350, 353, 354, 355, 356, 357, 364, 365, 366, 367, 368, 369, 370, 390, 391, 392, 399, 400, 401, 402, 407, 408, 409, 410, 412, 413, 414, 415, 416, 417, 418, 419, 420, 422, 423, 424, 425, 427, 436, 438, 439, 440, 441, 442, 443, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 461, 468, 469, 470, 471, 472, 473, 475, 476, 479, 480, 481, 482, 483, 484, 485, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497], "get": [0, 7, 9, 43, 90, 173, 177, 198, 328, 378, 439, 444, 450, 479, 482, 486, 488, 489, 491, 496, 497], "start": [0, 45, 226, 390, 442, 443, 450, 479, 482, 484, 486, 491, 494, 496, 497], "1": [0, 7, 10, 13, 16, 18, 19, 20, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 49, 51, 53, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 99, 101, 103, 104, 107, 110, 111, 112, 113, 114, 115, 116, 119, 120, 121, 122, 123, 126, 127, 129, 130, 131, 132, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 149, 150, 151, 153, 154, 155, 156, 159, 160, 163, 165, 166, 167, 168, 169, 170, 171, 175, 176, 177, 178, 179, 180, 182, 183, 184, 186, 188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 200, 201, 202, 204, 205, 206, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 230, 233, 234, 235, 236, 237, 238, 239, 240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 281, 282, 283, 285, 286, 287, 288, 289, 290, 291, 292, 294, 295, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 320, 321, 322, 324, 325, 326, 327, 328, 329, 331, 332, 333, 334, 337, 340, 341, 342, 343, 344, 345, 346, 347, 349, 350, 351, 352, 353, 354, 355, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 369, 370, 371, 372, 373, 374, 378, 379, 380, 382, 385, 387, 389, 390, 391, 392, 393, 394, 395, 396, 398, 399, 400, 401, 402, 403, 404, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 439, 443, 445, 446, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 460, 463, 464, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 483, 485, 486, 487, 488, 489, 490, 491, 493, 494, 495, 497], "instal": [0, 3, 480, 484, 486, 497], "build": [0, 5, 455, 486, 488, 492, 497], "2": [0, 7, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 38, 39, 40, 41, 45, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 81, 82, 83, 84, 87, 89, 94, 96, 98, 99, 100, 101, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 120, 121, 122, 123, 124, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 161, 163, 164, 165, 166, 167, 168, 170, 171, 174, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 200, 202, 203, 206, 207, 208, 209, 210, 211, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 275, 276, 277, 278, 283, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 300, 301, 303, 304, 305, 306, 307, 308, 310, 311, 312, 313, 314, 315, 320, 322, 323, 324, 325, 326, 327, 330, 331, 332, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 372, 373, 374, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 421, 423, 424, 425, 426, 427, 428, 429, 430, 436, 437, 438, 450, 452, 454, 457, 458, 459, 460, 462, 464, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 485, 486, 487, 488, 489, 490, 491, 493, 494, 495], "explor": [0, 489, 496, 497], "our": [0, 3, 480, 482, 483, 485, 489, 490, 491, 492], "model": [0, 3, 4, 117, 279, 280, 431, 450, 480, 483, 484, 486, 487, 488, 492, 493, 494, 495], "demo": [0, 482, 484, 496, 497], "where": [0, 2, 7, 11, 36, 37, 38, 40, 45, 61, 62, 94, 102, 124, 125, 139, 160, 177, 178, 215, 216, 217, 231, 268, 274, 297, 308, 315, 333, 353, 354, 355, 357, 367, 368, 369, 371, 372, 400, 410, 413, 436, 442, 450, 451, 452, 455, 464, 471, 474, 480, 484, 485, 491, 492, 495, 497], "To": [0, 7, 267, 439, 483, 485, 486, 488, 489, 490, 491, 492, 497], "go": [0, 216, 217, 354, 355, 371, 372, 492], "from": [0, 2, 5, 7, 10, 37, 38, 39, 40, 45, 47, 52, 62, 63, 74, 86, 87, 88, 92, 93, 94, 95, 96, 97, 103, 104, 117, 123, 147, 148, 160, 161, 162, 165, 173, 177, 202, 203, 212, 213, 216, 217, 232, 282, 307, 308, 316, 317, 318, 319, 325, 349, 352, 354, 355, 364, 365, 366, 367, 368, 369, 371, 372, 376, 377, 391, 400, 417, 418, 443, 447, 450, 451, 457, 458, 463, 467, 468, 473, 479, 480, 482, 484, 485, 487, 488, 490, 491, 492, 493, 494, 495, 496, 497], "here": [0, 2, 216, 217, 354, 355, 482, 484, 487, 488, 489, 490, 492, 493, 497], "prerequisit": [0, 486], "set": [0, 7, 10, 15, 16, 27, 28, 30, 40, 55, 64, 65, 68, 69, 71, 72, 73, 88, 93, 94, 111, 113, 115, 116, 117, 126, 127, 149, 154, 155, 165, 166, 167, 168, 177, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 267, 268, 272, 277, 288, 289, 290, 291, 292, 295, 303, 312, 313, 315, 316, 317, 320, 344, 350, 364, 365, 367, 375, 400, 402, 407, 408, 415, 417, 418, 422, 431, 439, 444, 447, 451, 454, 457, 458, 475, 479, 484, 486, 489, 491, 492, 493, 495, 497], "up": [0, 4, 5, 27, 28, 30, 55, 64, 65, 68, 69, 71, 72, 73, 111, 113, 115, 116, 126, 127, 150, 154, 155, 166, 167, 168, 178, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 407, 408, 415, 417, 418, 475, 484, 486, 488, 489, 491, 492, 495], "hardwar": [0, 2, 3, 5, 172, 302, 479, 480, 481, 485, 487, 489, 491, 492, 493, 495, 496, 497], "softwar": [0, 488, 489, 490, 493, 494, 495], "depend": [0, 13, 177, 268, 316, 317, 318, 319, 400, 436, 473, 480, 484, 485, 486], "option": [0, 7, 10, 11, 12, 13, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 88, 89, 90, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 281, 282, 283, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 310, 311, 312, 313, 314, 315, 320, 321, 322, 323, 324, 325, 326, 327, 328, 330, 331, 332, 333, 334, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 402, 403, 404, 405, 406, 407, 408, 409, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 434, 436, 437, 438, 439, 440, 441, 442, 443, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 466, 467, 468, 469, 470, 471, 473, 474, 475, 476, 477, 478, 484, 488, 491, 492, 495, 497], "script": [0, 480, 484, 486, 487, 488, 489, 490, 493, 494, 495], "recommend": [0, 7, 221, 479, 486, 492, 497], "manual": [0, 5, 7, 267, 479, 486, 492], "metalium": [0, 5], "There": [0, 5, 268, 479, 485], "ar": [0, 7, 10, 11, 13, 14, 15, 27, 28, 29, 30, 36, 37, 38, 40, 41, 55, 60, 61, 62, 64, 65, 68, 69, 71, 72, 73, 74, 85, 88, 103, 104, 111, 112, 113, 115, 116, 126, 127, 128, 139, 150, 154, 155, 165, 166, 167, 168, 174, 177, 178, 179, 180, 181, 192, 204, 205, 206, 207, 208, 209, 210, 211, 215, 216, 217, 218, 219, 220, 222, 223, 228, 231, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 264, 267, 268, 271, 272, 277, 281, 289, 290, 291, 292, 295, 296, 297, 303, 306, 312, 313, 315, 335, 336, 344, 349, 353, 354, 355, 356, 357, 360, 364, 365, 367, 369, 371, 390, 400, 407, 408, 415, 417, 418, 422, 439, 443, 447, 450, 451, 454, 456, 457, 458, 461, 469, 475, 479, 480, 481, 484, 485, 486, 488, 489, 491, 492, 493, 494, 495, 496, 497], "four": [0, 349], "binari": [0, 27, 28, 30, 55, 62, 63, 64, 65, 68, 69, 71, 72, 73, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 407, 408, 415, 417, 418, 475, 492], "step": [0, 45, 177, 191, 216, 217, 354, 355, 390, 443, 480, 483, 491, 492, 497], "latest": [0, 484], "wheel": 0, "For": [0, 3, 5, 7, 20, 27, 28, 30, 33, 47, 55, 62, 64, 65, 66, 68, 69, 71, 72, 73, 93, 103, 104, 111, 112, 113, 115, 116, 121, 126, 127, 154, 155, 156, 162, 165, 166, 167, 168, 171, 178, 179, 180, 192, 193, 206, 216, 217, 218, 219, 220, 222, 223, 227, 235, 237, 239, 240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254, 255, 256, 257, 258, 260, 261, 262, 263, 268, 272, 277, 281, 289, 290, 291, 292, 295, 303, 312, 313, 314, 315, 321, 327, 332, 344, 345, 354, 355, 364, 365, 369, 370, 392, 394, 399, 404, 407, 408, 415, 417, 418, 439, 454, 455, 473, 475, 476, 479, 480, 483, 484, 485, 487, 491, 492, 493, 495, 496, 497], "user": [0, 5, 7, 36, 172, 197, 231, 267, 268, 279, 280, 302, 367, 400, 432, 480, 481, 483, 488, 489, 490, 493, 494, 495, 497], "onli": [0, 5, 7, 29, 31, 32, 34, 40, 41, 46, 52, 56, 66, 81, 82, 83, 84, 88, 89, 102, 108, 111, 112, 115, 123, 139, 150, 162, 167, 169, 176, 177, 178, 179, 193, 204, 205, 221, 222, 226, 227, 246, 248, 249, 262, 265, 267, 268, 269, 270, 273, 275, 276, 286, 304, 310, 318, 319, 320, 322, 331, 334, 337, 345, 349, 357, 366, 367, 368, 400, 409, 410, 412, 414, 416, 419, 420, 422, 425, 427, 431, 439, 446, 448, 449, 450, 452, 453, 454, 469, 470, 473, 476, 479, 480, 484, 485, 491, 492, 495, 496, 497], "environ": [0, 481, 486, 491, 496, 497], "docker": 0, "releas": [0, 105], "imag": [0, 7, 93, 94, 177, 204, 308, 480, 484, 485, 486, 488, 495], "sourc": [0, 4, 165, 212, 371, 372, 497], "clone": [0, 197, 454, 497], "repositori": [0, 1, 481, 486, 497], "librari": [0, 4, 486, 491, 495], "3": [0, 7, 19, 20, 22, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 47, 49, 51, 53, 55, 56, 57, 59, 60, 61, 63, 64, 66, 68, 69, 70, 71, 72, 73, 77, 78, 79, 81, 82, 83, 84, 88, 96, 99, 101, 103, 104, 105, 107, 109, 110, 111, 112, 113, 114, 115, 118, 119, 120, 122, 124, 126, 129, 130, 131, 132, 134, 135, 137, 138, 141, 142, 144, 145, 146, 149, 150, 151, 153, 154, 155, 156, 159, 160, 161, 164, 166, 167, 169, 170, 171, 175, 176, 177, 178, 179, 183, 184, 186, 188, 189, 190, 191, 192, 193, 195, 196, 197, 202, 203, 206, 218, 219, 221, 222, 224, 225, 226, 227, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 242, 243, 244, 246, 248, 249, 254, 256, 257, 259, 260, 261, 262, 265, 266, 269, 270, 271, 272, 273, 275, 276, 277, 283, 286, 287, 288, 289, 291, 294, 295, 300, 301, 304, 306, 308, 310, 311, 312, 313, 314, 320, 322, 324, 325, 326, 327, 330, 331, 332, 337, 340, 341, 342, 343, 344, 345, 346, 349, 358, 359, 360, 361, 362, 363, 364, 366, 368, 370, 372, 373, 374, 378, 379, 380, 382, 385, 387, 389, 393, 394, 395, 396, 398, 399, 400, 403, 404, 406, 407, 409, 412, 414, 415, 416, 417, 419, 420, 424, 425, 426, 427, 428, 429, 430, 436, 437, 438, 439, 454, 455, 458, 460, 461, 463, 469, 470, 473, 474, 475, 476, 477, 478, 485, 486, 487, 488, 491, 493, 494, 495], "virtual": [0, 443, 486, 489], "setup": [0, 13, 18, 61, 216, 217, 334, 353, 354, 355, 368, 369, 370, 480, 481, 486, 492, 497], "anaconda": 0, "packag": 0, "you": [0, 1, 2, 5, 177, 202, 393, 480, 481, 484, 485, 486, 497], "all": [0, 5, 7, 10, 13, 17, 36, 37, 38, 39, 40, 41, 46, 61, 74, 165, 172, 175, 215, 216, 217, 267, 268, 269, 273, 274, 275, 279, 280, 281, 283, 298, 304, 312, 320, 321, 333, 349, 350, 353, 354, 355, 369, 370, 391, 402, 410, 412, 419, 422, 439, 450, 451, 452, 470, 479, 480, 483, 484, 485, 487, 489, 491, 493, 494, 495, 496, 497], "verifi": [0, 483, 495], "your": [0, 480, 481, 484, 486, 497], "try": [0, 7, 480, 488, 491], "execut": [0, 3, 5, 7, 18, 165, 172, 267, 335, 336, 349, 367, 439, 450, 480, 484, 488, 491, 492, 493, 494, 495, 496, 497], "program": [0, 4, 5, 10, 11, 12, 13, 14, 16, 17, 18, 36, 74, 172, 216, 217, 231, 268, 354, 355, 368, 370, 392, 400, 440, 441, 443, 450, 451, 484, 486, 487, 488, 489, 490, 493, 494, 495], "exampl": [0, 7, 202, 283, 307, 334, 402, 480, 481, 483, 484, 485, 486, 492, 497], "interest": 0, "contribut": [0, 2, 481], "multi": [0, 3, 14, 18, 47, 267, 392, 450, 451, 469, 485, 486, 491, 493, 497], "card": [0, 480], "configur": [0, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 88, 89, 91, 92, 93, 94, 98, 99, 100, 101, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 119, 120, 121, 122, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 162, 163, 164, 165, 166, 167, 169, 170, 171, 172, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 204, 205, 206, 207, 208, 209, 210, 211, 214, 215, 216, 217, 218, 219, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 246, 248, 249, 251, 252, 253, 254, 256, 257, 259, 260, 261, 262, 264, 265, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 282, 283, 285, 286, 287, 288, 289, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 303, 304, 306, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 353, 354, 355, 357, 358, 359, 360, 361, 362, 363, 364, 366, 368, 369, 370, 371, 372, 373, 374, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 391, 392, 393, 394, 395, 396, 397, 398, 399, 403, 404, 405, 406, 407, 409, 412, 413, 414, 415, 416, 417, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 436, 437, 439, 440, 441, 442, 443, 445, 446, 448, 449, 450, 451, 452, 453, 454, 455, 457, 458, 459, 460, 461, 462, 463, 466, 467, 469, 470, 471, 473, 474, 475, 476, 477, 478, 480, 481, 486, 488, 491, 492, 495, 496, 497], "topologi": [0, 37, 38, 39, 40, 41, 74, 307, 333, 334, 450, 451], "machin": [0, 484, 497], "requir": [0, 5, 7, 10, 29, 31, 42, 53, 61, 62, 89, 95, 102, 112, 139, 148, 149, 150, 163, 177, 178, 279, 280, 284, 286, 300, 320, 366, 391, 414, 416, 435, 469, 474, 477, 480, 484, 488, 490, 491, 494, 495, 497], "overview": [0, 497], "why": [0, 489, 491], "It": [0, 3, 4, 5, 7, 14, 16, 17, 18, 40, 41, 92, 94, 172, 279, 280, 320, 333, 402, 438, 491, 492, 493, 495, 497], "matter": 0, "vm": 0, "introduct": 0, "ll": [0, 489, 496, 497], "learn": [0, 3, 485, 487, 489, 490, 491], "import": [0, 5, 10, 25, 26, 479, 480, 482, 484, 485, 486, 494], "devic": [0, 3, 4, 7, 9, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 98, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 204, 205, 206, 207, 208, 209, 210, 211, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 403, 404, 405, 406, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 443, 450, 451, 457, 458, 459, 460, 461, 462, 463, 466, 467, 468, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 482, 484, 485, 486, 494, 497], "initi": [0, 41, 63, 125, 267, 279, 280, 325, 479, 486, 487, 488, 490, 491, 493, 494, 497], "tensor": [0, 3, 4, 5, 7, 10, 11, 12, 13, 14, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 88, 89, 90, 91, 92, 93, 94, 95, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 270, 271, 272, 274, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 413, 414, 415, 416, 417, 418, 420, 421, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 464, 465, 466, 467, 468, 469, 471, 472, 473, 474, 475, 476, 477, 478, 479, 484, 486, 491, 493, 494, 495], "creation": [0, 18, 162, 485, 486, 488, 493, 496], "manag": [0, 3, 117, 266, 335, 336, 480, 482, 486, 495, 496], "creat": [0, 5, 7, 16, 18, 19, 20, 21, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 143, 144, 145, 146, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 204, 205, 206, 207, 208, 209, 210, 211, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 269, 270, 271, 272, 273, 275, 276, 277, 278, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 303, 304, 306, 308, 309, 310, 311, 312, 313, 314, 315, 320, 321, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 350, 354, 355, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 370, 371, 372, 373, 374, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 403, 404, 405, 406, 407, 408, 409, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 434, 435, 436, 437, 438, 439, 455, 457, 458, 459, 460, 461, 462, 463, 466, 467, 470, 471, 473, 474, 475, 476, 477, 478, 482, 483, 484, 485, 486, 487, 489, 490, 491, 493, 494, 495, 496, 497], "host": [0, 92, 145, 161, 177, 304, 390, 422, 435, 461, 482, 484, 485, 486, 488, 491, 496, 497], "move": [0, 5, 7, 121, 161, 434, 480, 491, 494, 496], "directli": [0, 5, 10, 11, 12, 13, 349, 370, 392, 486, 487, 488, 489, 490, 493, 494, 495], "pytorch": [0, 4, 92, 94, 177, 268, 281, 316, 317, 318, 319, 351, 401, 410, 439, 452, 454, 456, 464, 486, 488, 490, 491, 493, 495], "interoper": 0, "back": [0, 5, 38, 40, 94, 161, 232, 333, 349, 392, 444, 484, 488, 491, 493, 495, 496], "convert": [0, 4, 5, 7, 10, 27, 28, 30, 47, 55, 64, 65, 67, 68, 69, 71, 72, 73, 85, 106, 111, 113, 115, 116, 126, 127, 154, 155, 162, 166, 167, 168, 178, 179, 180, 192, 202, 203, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 279, 280, 289, 290, 291, 292, 295, 303, 308, 312, 313, 315, 316, 317, 318, 319, 323, 344, 349, 352, 364, 365, 367, 376, 377, 399, 407, 408, 415, 417, 418, 435, 437, 438, 475, 485, 488, 489, 490, 491, 493, 494, 495, 496], "4": [0, 7, 19, 20, 22, 24, 27, 29, 30, 31, 32, 33, 34, 35, 49, 51, 53, 55, 56, 57, 59, 60, 61, 64, 66, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 81, 82, 83, 84, 99, 101, 103, 104, 107, 109, 110, 111, 112, 113, 114, 115, 119, 120, 121, 122, 124, 125, 126, 129, 130, 131, 132, 134, 135, 137, 138, 141, 142, 144, 145, 146, 151, 153, 154, 155, 156, 159, 160, 162, 165, 166, 167, 170, 171, 177, 178, 179, 183, 184, 186, 188, 189, 190, 191, 192, 193, 195, 196, 197, 201, 206, 216, 217, 218, 219, 221, 222, 224, 225, 226, 227, 230, 233, 234, 235, 236, 237, 238, 239, 240, 242, 243, 244, 246, 248, 249, 251, 254, 257, 259, 260, 261, 262, 265, 267, 269, 270, 271, 272, 273, 275, 276, 277, 283, 286, 287, 288, 289, 291, 294, 295, 304, 310, 311, 312, 313, 314, 320, 322, 324, 326, 327, 331, 332, 334, 340, 341, 342, 343, 344, 345, 346, 349, 350, 351, 358, 359, 360, 361, 362, 363, 364, 366, 370, 372, 373, 374, 377, 378, 379, 380, 382, 385, 387, 389, 393, 394, 395, 396, 398, 399, 400, 403, 404, 406, 407, 409, 412, 414, 415, 416, 417, 419, 424, 425, 426, 427, 428, 429, 430, 436, 450, 457, 458, 460, 470, 472, 473, 474, 475, 476, 477, 478, 482, 485, 486, 488, 489, 490, 491, 493, 494, 495, 497], "understand": [0, 485, 491, 496, 497], "layout": [0, 5, 7, 10, 16, 18, 19, 20, 21, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 87, 89, 93, 96, 97, 98, 99, 100, 101, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 141, 142, 143, 144, 145, 146, 151, 152, 153, 154, 155, 156, 158, 159, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 200, 201, 202, 203, 206, 207, 208, 209, 210, 211, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 267, 268, 269, 270, 271, 272, 273, 275, 276, 277, 278, 281, 284, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 303, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 331, 332, 333, 337, 338, 339, 340, 341, 342, 343, 344, 345, 347, 348, 349, 352, 353, 354, 355, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 403, 404, 405, 406, 407, 408, 409, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 435, 436, 438, 439, 457, 458, 459, 460, 461, 462, 463, 466, 467, 470, 471, 473, 474, 475, 476, 477, 478, 479, 484, 486, 487, 488, 490, 491, 493, 494, 495, 496, 497], "type": [0, 4, 5, 7, 15, 17, 27, 36, 45, 46, 47, 52, 62, 63, 64, 66, 67, 85, 87, 92, 93, 94, 95, 96, 97, 103, 104, 112, 122, 123, 124, 125, 126, 147, 148, 149, 150, 160, 162, 163, 164, 166, 167, 171, 173, 175, 177, 178, 179, 200, 202, 203, 215, 216, 217, 218, 219, 222, 231, 243, 244, 249, 254, 257, 262, 268, 269, 272, 273, 275, 277, 281, 284, 291, 300, 301, 302, 305, 316, 317, 318, 319, 320, 325, 326, 352, 353, 354, 355, 364, 367, 368, 369, 370, 375, 376, 377, 391, 392, 399, 400, 401, 402, 407, 412, 417, 419, 422, 431, 432, 433, 435, 436, 437, 438, 439, 456, 461, 463, 466, 467, 468, 470, 475, 477, 478, 479, 484, 491, 493, 495, 496, 497], "tile": [0, 7, 10, 11, 12, 13, 14, 19, 20, 21, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 42, 44, 46, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 81, 82, 83, 84, 87, 89, 93, 96, 97, 98, 99, 100, 101, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 119, 120, 121, 124, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 141, 142, 143, 144, 145, 146, 148, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 162, 165, 166, 167, 168, 169, 170, 171, 174, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 200, 206, 207, 208, 209, 210, 211, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 268, 269, 270, 272, 273, 275, 276, 277, 278, 281, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 298, 299, 303, 305, 306, 309, 310, 311, 312, 313, 314, 315, 320, 321, 322, 323, 324, 326, 327, 331, 332, 333, 337, 338, 339, 340, 341, 342, 343, 344, 345, 347, 348, 349, 350, 353, 354, 355, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 373, 374, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 402, 403, 404, 405, 406, 407, 408, 409, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 435, 438, 439, 445, 446, 448, 449, 450, 452, 453, 456, 457, 458, 459, 460, 461, 462, 463, 466, 467, 470, 471, 472, 473, 474, 475, 476, 477, 478, 484, 485, 486, 487, 489, 491, 493, 495, 497], "default": [0, 7, 9, 10, 13, 15, 16, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 88, 89, 90, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 256, 257, 259, 260, 261, 262, 263, 264, 265, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 306, 308, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 330, 331, 332, 333, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 434, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 462, 463, 466, 467, 468, 469, 470, 471, 473, 474, 475, 476, 477, 478, 482, 484, 485, 489, 491, 495, 496], "behavior": [0, 7, 16, 117, 177, 485], "function": [0, 7, 9, 10, 11, 12, 13, 15, 20, 22, 24, 27, 28, 44, 47, 49, 51, 55, 57, 58, 59, 60, 64, 77, 78, 79, 82, 83, 84, 91, 99, 100, 101, 107, 109, 110, 114, 119, 120, 126, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 142, 144, 146, 151, 153, 159, 162, 166, 167, 169, 170, 171, 176, 177, 178, 179, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 194, 195, 196, 197, 199, 214, 218, 219, 222, 224, 225, 229, 230, 231, 235, 237, 239, 240, 242, 243, 244, 249, 253, 254, 257, 259, 260, 261, 262, 268, 272, 277, 278, 279, 280, 285, 287, 288, 291, 294, 298, 299, 305, 309, 310, 311, 314, 315, 320, 321, 324, 327, 329, 332, 337, 338, 339, 340, 341, 342, 343, 347, 359, 361, 363, 364, 373, 374, 378, 379, 380, 382, 384, 385, 387, 388, 389, 391, 392, 393, 394, 395, 396, 397, 398, 402, 404, 406, 407, 417, 418, 420, 421, 424, 426, 427, 428, 429, 430, 460, 475, 479, 480, 485, 486, 488, 490, 493, 495, 496], "differ": [0, 4, 7, 17, 27, 28, 30, 55, 64, 65, 68, 69, 71, 72, 73, 94, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 178, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 267, 268, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 378, 402, 407, 408, 409, 415, 417, 418, 425, 427, 435, 443, 461, 475, 482, 485, 489, 490, 491, 493, 495, 496], "preserv": [0, 399, 457, 458], "dure": [0, 5, 10, 11, 12, 13, 27, 28, 30, 55, 62, 64, 65, 68, 69, 71, 72, 73, 88, 111, 113, 115, 116, 126, 127, 154, 155, 162, 166, 167, 168, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 407, 408, 415, 417, 418, 438, 475, 485, 491, 497], "transfer": [0, 307, 490], "between": [0, 5, 7, 27, 28, 30, 45, 55, 61, 62, 64, 65, 68, 69, 71, 72, 73, 92, 93, 94, 111, 113, 115, 116, 121, 126, 127, 154, 155, 166, 167, 168, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 268, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 316, 317, 318, 319, 325, 339, 344, 364, 365, 367, 407, 408, 415, 417, 418, 437, 463, 475, 491, 495, 496, 497], "5": [0, 7, 27, 28, 30, 55, 61, 63, 64, 65, 68, 69, 71, 72, 73, 82, 84, 102, 103, 104, 109, 111, 113, 115, 116, 119, 122, 125, 126, 127, 154, 155, 160, 162, 166, 167, 168, 177, 179, 180, 182, 183, 184, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 259, 262, 263, 265, 272, 277, 283, 287, 289, 290, 291, 292, 295, 303, 312, 313, 315, 320, 327, 344, 364, 365, 371, 372, 395, 396, 399, 407, 408, 415, 417, 418, 455, 457, 458, 469, 473, 475, 482, 490, 491, 493, 494, 495], "data": [0, 4, 7, 10, 11, 12, 13, 14, 18, 27, 28, 30, 36, 37, 38, 39, 45, 46, 47, 52, 55, 60, 62, 63, 64, 65, 67, 68, 69, 71, 72, 73, 85, 87, 92, 93, 94, 95, 96, 97, 111, 113, 115, 116, 117, 122, 123, 124, 125, 126, 127, 148, 149, 150, 154, 155, 160, 162, 163, 164, 166, 167, 168, 172, 175, 177, 178, 179, 180, 192, 200, 202, 203, 206, 215, 216, 217, 218, 219, 220, 222, 223, 231, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 268, 269, 271, 272, 273, 275, 277, 281, 284, 289, 290, 291, 292, 295, 300, 301, 303, 307, 312, 313, 315, 316, 317, 318, 319, 320, 325, 326, 344, 353, 354, 355, 364, 365, 367, 368, 369, 370, 376, 377, 391, 392, 400, 407, 408, 412, 415, 417, 418, 419, 431, 432, 433, 435, 436, 437, 438, 439, 461, 463, 466, 467, 469, 470, 475, 477, 478, 479, 484, 486, 490, 491, 495, 496, 497], "precis": [0, 36, 177, 375, 379, 425, 427, 485, 491, 493], "support": [0, 1, 3, 4, 7, 19, 20, 21, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 42, 44, 46, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 81, 82, 83, 84, 87, 88, 89, 93, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 119, 120, 121, 124, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 151, 152, 153, 154, 155, 156, 158, 159, 165, 166, 167, 168, 169, 170, 171, 174, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 200, 206, 207, 208, 209, 210, 211, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 267, 269, 270, 271, 272, 273, 275, 276, 277, 278, 281, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 298, 299, 303, 304, 309, 310, 311, 312, 313, 314, 315, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 331, 332, 337, 338, 339, 340, 341, 342, 343, 344, 345, 347, 349, 353, 354, 355, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 391, 392, 393, 394, 395, 396, 397, 398, 399, 403, 404, 405, 406, 407, 408, 409, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 423, 424, 425, 426, 427, 428, 429, 430, 435, 439, 442, 446, 448, 449, 450, 453, 457, 458, 459, 460, 461, 462, 463, 469, 470, 471, 473, 474, 475, 476, 477, 478, 482, 485, 491, 495, 497], "perform": [0, 3, 7, 10, 11, 12, 13, 14, 18, 20, 22, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 43, 44, 49, 51, 52, 53, 55, 56, 57, 58, 59, 60, 62, 64, 65, 66, 68, 69, 71, 72, 73, 74, 77, 79, 82, 83, 84, 89, 91, 93, 99, 100, 101, 103, 104, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 119, 120, 122, 126, 127, 130, 132, 134, 137, 138, 139, 144, 146, 151, 153, 154, 155, 156, 159, 166, 167, 168, 169, 171, 175, 176, 177, 179, 180, 182, 183, 184, 186, 188, 189, 190, 192, 193, 195, 197, 198, 199, 201, 206, 216, 218, 219, 220, 221, 222, 223, 225, 226, 227, 229, 230, 231, 235, 237, 239, 240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 266, 268, 270, 271, 272, 276, 277, 281, 286, 287, 288, 289, 290, 291, 292, 294, 295, 298, 299, 303, 304, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 324, 326, 327, 328, 329, 332, 334, 337, 340, 341, 344, 345, 347, 349, 354, 356, 357, 359, 361, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 378, 380, 382, 385, 387, 388, 389, 390, 394, 395, 396, 398, 404, 406, 407, 408, 409, 414, 415, 416, 417, 418, 420, 424, 425, 426, 427, 428, 429, 430, 431, 439, 447, 450, 451, 460, 461, 473, 474, 475, 476, 479, 480, 482, 483, 486, 487, 488, 490, 491, 493, 494, 495, 496], "v": [0, 7, 150, 442, 443, 445, 446, 447, 448, 449, 451, 453, 455, 491, 495, 497], "accuraci": [0, 116, 480, 486, 495], "trade": 0, "off": [0, 2, 342, 358, 479, 485], "6": [0, 160, 177, 201, 265, 283, 287, 339, 358, 372, 375, 399, 457, 458, 473, 485, 488, 489, 490, 491, 493, 494, 495], "basic": [0, 14, 482, 486, 496], "oper": [0, 3, 4, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 93, 94, 96, 97, 98, 99, 100, 101, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 165, 166, 167, 168, 169, 170, 171, 172, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 204, 205, 206, 207, 208, 209, 210, 211, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 281, 282, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 303, 304, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 353, 354, 355, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 391, 392, 393, 394, 395, 396, 397, 398, 400, 403, 404, 405, 406, 407, 408, 409, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 435, 436, 438, 439, 440, 441, 442, 443, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 457, 458, 459, 460, 461, 462, 463, 466, 467, 469, 470, 471, 472, 473, 474, 475, 476, 480, 482, 483, 485, 486, 489, 491, 493, 494, 495], "test": [0, 5, 172, 379, 383, 479, 480, 483, 484, 486, 490, 491, 494, 497], "neural": [0, 4, 488, 491, 493, 495, 497], "network": [0, 4, 177, 491, 493, 495, 497], "7": [0, 26, 163, 265, 267, 283, 358, 372, 402, 457, 458, 473, 488, 489, 490, 491, 493, 494, 495], "just": [0, 5, 40, 41, 177, 178, 316, 317, 318, 319, 496], "In": [0, 5, 10, 18, 141, 178, 268, 350, 353, 367, 368, 370, 436, 441, 472, 479, 484, 485, 491, 493, 495, 497], "time": [0, 3, 103, 121, 182, 206, 279, 346, 443, 480, 484, 489, 491, 494, 497], "compil": [0, 443, 482, 489, 493, 494], "cach": [0, 4, 47, 86, 147, 212, 213, 279, 280, 377, 442, 443, 450, 468, 484, 485, 488, 489, 490, 491, 493, 494, 495, 496, 497], "first": [0, 10, 32, 34, 36, 62, 95, 162, 169, 176, 178, 196, 216, 231, 268, 297, 304, 337, 354, 367, 399, 400, 420, 443, 451, 456, 479, 482, 484, 489, 491, 493, 494, 495], "run": [0, 4, 5, 61, 74, 88, 93, 162, 216, 217, 279, 335, 336, 354, 371, 372, 425, 427, 439, 480, 481, 482, 483, 484, 486, 487, 489, 490, 493, 494, 496], "subsequ": [0, 10, 489, 491, 493, 494], "affect": [0, 10, 11, 12, 13, 14, 177, 368, 484], "direct": [0, 13, 491], "sram": 0, "l1": [0, 7, 18, 27, 28, 30, 36, 46, 55, 61, 63, 64, 65, 68, 69, 71, 72, 73, 87, 88, 94, 96, 97, 102, 103, 104, 111, 113, 115, 116, 117, 121, 126, 127, 154, 155, 165, 166, 167, 168, 178, 179, 180, 192, 200, 201, 206, 214, 215, 218, 219, 220, 222, 223, 231, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 268, 269, 272, 273, 275, 277, 281, 289, 290, 291, 292, 295, 302, 303, 312, 313, 315, 320, 344, 353, 364, 365, 367, 368, 370, 391, 392, 399, 400, 407, 408, 412, 415, 417, 418, 419, 434, 435, 437, 439, 461, 463, 470, 475, 485, 488, 491, 494, 495, 496, 497], "control": [0, 7, 10, 13, 14, 18, 38, 172, 177, 350, 482, 491], "advanc": [0, 10, 38, 350, 482, 491, 497], "shard": [0, 7, 10, 11, 12, 13, 18, 25, 26, 27, 28, 29, 30, 36, 38, 39, 40, 41, 46, 55, 60, 63, 64, 65, 68, 69, 71, 72, 73, 87, 88, 96, 97, 102, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 173, 178, 179, 180, 192, 202, 203, 206, 215, 216, 217, 218, 219, 220, 222, 223, 231, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 268, 269, 271, 272, 273, 275, 277, 281, 289, 290, 291, 292, 295, 303, 307, 312, 313, 315, 316, 317, 318, 319, 320, 344, 352, 353, 354, 355, 364, 365, 370, 376, 391, 392, 400, 407, 408, 412, 415, 417, 418, 419, 434, 435, 437, 439, 451, 454, 461, 463, 470, 475, 491, 497], "intermedi": [0, 14, 307, 333, 334, 369, 370], "result": [0, 14, 18, 27, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 45, 46, 47, 53, 55, 56, 60, 61, 64, 66, 68, 71, 73, 89, 103, 104, 111, 112, 113, 115, 121, 123, 150, 154, 155, 156, 164, 166, 177, 178, 192, 193, 197, 206, 214, 215, 216, 217, 218, 219, 221, 226, 227, 231, 243, 244, 246, 248, 249, 254, 257, 265, 268, 269, 270, 271, 272, 273, 275, 276, 277, 281, 285, 286, 289, 290, 295, 301, 303, 312, 313, 316, 317, 318, 319, 320, 325, 326, 331, 344, 345, 353, 354, 355, 360, 364, 366, 367, 368, 369, 370, 371, 372, 391, 400, 407, 409, 412, 414, 415, 416, 417, 419, 425, 427, 450, 457, 458, 470, 473, 474, 475, 476, 478, 484, 485, 486, 487, 488, 490, 491, 493, 495], "infer": [0, 3, 61, 93, 400, 450, 480, 484, 486, 496], "focu": [0, 491], "develop": [0, 4, 481, 482, 483, 484, 486, 487, 491, 497], "tool": [0, 4, 482, 484, 496, 497], "topic": 0, "8": [0, 18, 27, 28, 30, 37, 38, 39, 55, 64, 65, 68, 69, 71, 72, 73, 102, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 177, 178, 179, 180, 182, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 265, 272, 277, 283, 289, 290, 291, 292, 295, 303, 312, 313, 315, 333, 334, 344, 364, 365, 368, 370, 372, 400, 402, 407, 408, 415, 417, 418, 458, 473, 475, 484, 485, 489, 490, 491, 495, 496], "exercis": 0, "implement": [0, 7, 10, 13, 28, 38, 92, 116, 177, 178, 268, 289, 290, 304, 333, 349, 418, 446, 448, 449, 452, 453, 454, 469, 479, 483, 484, 486, 494, 496, 497], "scale": [0, 18, 30, 36, 61, 108, 141, 178, 215, 216, 217, 269, 273, 275, 322, 349, 353, 354, 355, 367, 368, 369, 370, 373, 393, 412, 415, 419, 442, 443, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455, 469, 470, 491, 495], "dot": [0, 93, 268, 442, 443, 446, 448, 449, 450, 452, 453, 455, 491], "product": [0, 36, 93, 103, 268, 303, 320, 321, 400, 442, 443, 446, 448, 449, 450, 452, 453, 455, 491], "attent": [0, 368, 369, 370, 391, 440, 441, 442, 443, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 486, 491, 496], "background": 0, "task": [0, 486, 491, 493, 496], "math": [0, 62, 102, 178, 484, 491], "fidel": [0, 484], "metal": [0, 2, 4, 74, 443, 481, 482, 486, 488, 490, 493, 494, 495, 497], "trace": [0, 3, 4, 302, 443, 484, 486], "shape": [0, 10, 13, 25, 26, 27, 28, 30, 36, 37, 38, 39, 41, 46, 47, 52, 55, 60, 61, 62, 63, 64, 65, 68, 69, 71, 72, 73, 87, 88, 94, 95, 96, 97, 102, 103, 104, 111, 113, 115, 116, 121, 124, 125, 126, 127, 139, 145, 149, 150, 154, 155, 157, 160, 161, 162, 163, 164, 165, 166, 167, 168, 175, 177, 178, 179, 180, 192, 201, 202, 206, 215, 217, 218, 219, 220, 222, 223, 231, 232, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 268, 271, 272, 277, 281, 284, 289, 290, 291, 292, 295, 297, 300, 301, 303, 304, 305, 306, 312, 313, 315, 320, 325, 330, 333, 344, 346, 347, 348, 349, 350, 351, 353, 355, 357, 364, 365, 367, 368, 370, 390, 392, 400, 407, 408, 410, 413, 415, 417, 418, 431, 432, 434, 435, 436, 437, 438, 439, 443, 444, 447, 450, 451, 452, 454, 461, 463, 464, 466, 467, 471, 472, 475, 477, 478, 479, 484, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497], "limit": [0, 20, 33, 46, 61, 63, 66, 87, 96, 97, 103, 104, 112, 121, 156, 171, 178, 193, 200, 215, 216, 217, 231, 235, 237, 239, 240, 242, 246, 248, 260, 261, 281, 314, 320, 321, 327, 332, 342, 343, 345, 353, 354, 355, 367, 368, 369, 370, 391, 392, 394, 404, 435, 439, 461, 463, 476, 480, 492, 496], "bfloat8_b": [0, 7, 18, 19, 20, 21, 23, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 42, 48, 50, 51, 53, 54, 55, 56, 57, 58, 64, 65, 66, 75, 76, 78, 81, 87, 89, 93, 96, 97, 98, 99, 100, 106, 107, 108, 109, 110, 112, 114, 119, 126, 127, 128, 129, 131, 133, 135, 136, 137, 138, 143, 144, 145, 152, 156, 158, 159, 162, 167, 168, 169, 170, 171, 174, 176, 178, 179, 180, 181, 182, 183, 184, 185, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 207, 208, 209, 210, 211, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 252, 253, 254, 255, 257, 258, 259, 260, 261, 262, 263, 264, 265, 268, 269, 270, 273, 275, 276, 278, 286, 291, 292, 293, 294, 295, 296, 312, 313, 314, 315, 321, 322, 323, 324, 325, 327, 331, 332, 337, 338, 339, 341, 342, 345, 349, 353, 354, 355, 358, 361, 362, 363, 364, 365, 366, 368, 369, 370, 373, 374, 379, 380, 381, 383, 384, 385, 386, 387, 388, 391, 392, 393, 394, 395, 396, 397, 400, 403, 404, 405, 406, 407, 408, 409, 412, 414, 415, 416, 417, 418, 419, 420, 421, 423, 424, 425, 427, 428, 435, 438, 457, 458, 459, 461, 462, 470, 473, 474, 476, 479, 484, 492, 494], "storag": [0, 10, 13, 14, 18, 492], "memori": [0, 3, 7, 10, 11, 12, 13, 14, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 117, 119, 120, 121, 122, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 162, 163, 164, 165, 166, 167, 169, 170, 171, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 214, 215, 216, 217, 218, 219, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 246, 248, 249, 251, 252, 253, 254, 256, 257, 259, 260, 261, 262, 264, 265, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 281, 282, 283, 284, 285, 286, 287, 288, 289, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 303, 304, 306, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 357, 358, 359, 360, 361, 362, 363, 364, 366, 367, 369, 370, 371, 372, 373, 374, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 401, 403, 404, 405, 406, 407, 409, 412, 413, 414, 415, 416, 417, 419, 420, 421, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 439, 440, 441, 442, 443, 444, 445, 446, 448, 449, 450, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 466, 467, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 482, 484, 486, 488, 489, 491, 492, 494, 495, 496], "config": [0, 7, 10, 11, 12, 13, 14, 16, 18, 37, 62, 74, 90, 121, 122, 123, 140, 202, 215, 268, 281, 284, 302, 308, 316, 317, 350, 351, 352, 368, 370, 390, 400, 401, 433, 439, 440, 441, 443, 444, 456, 479, 486, 488, 492, 495, 497], "api": [0, 452, 479, 481, 482, 483, 487, 488, 490, 492, 493, 494, 495, 497], "rank": [0, 40, 41, 61, 103, 104, 121, 124, 141, 142, 215, 216, 217, 304, 320, 353, 370, 390, 400, 438, 477, 478, 485], "to_rank": [0, 485], "open_devic": [0, 15, 25, 26, 86, 122, 161, 422, 434, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496], "close_devic": [0, 60, 271, 487, 488, 489, 490, 491, 493, 494, 495, 496], "manage_devic": 0, "synchronize_devic": [0, 492], "setdefaultdevic": 0, "getdefaultdevic": 0, "pad_to_tile_shap": 0, "create_sharded_memory_config": [0, 202, 352, 485, 492], "core": [0, 5, 7, 10, 11, 12, 13, 14, 18, 28, 36, 37, 38, 39, 40, 41, 65, 88, 102, 116, 117, 121, 127, 129, 131, 135, 152, 155, 165, 168, 170, 178, 180, 220, 223, 231, 233, 234, 236, 238, 245, 247, 263, 264, 267, 268, 269, 273, 275, 290, 292, 302, 333, 344, 350, 354, 355, 362, 365, 367, 371, 372, 390, 392, 400, 402, 403, 408, 412, 418, 419, 431, 439, 451, 459, 466, 470, 473, 479, 484, 485, 488, 489, 490, 492, 493, 494, 495, 496, 497], "as_tensor": 0, "copy_device_to_host_tensor": 0, "copy_host_to_device_tensor": 0, "dealloc": [0, 7, 25, 26, 60, 271, 330, 485, 492, 494], "dump_tensor": [0, 232], "from_devic": [0, 97, 435, 491, 492, 496], "from_torch": [0, 5, 19, 20, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 49, 51, 53, 55, 56, 57, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 81, 82, 83, 84, 89, 99, 101, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 119, 120, 122, 123, 126, 127, 129, 130, 131, 132, 134, 135, 137, 138, 141, 142, 144, 145, 146, 147, 151, 153, 154, 155, 156, 159, 166, 167, 168, 170, 171, 177, 178, 179, 180, 182, 183, 184, 186, 188, 189, 190, 191, 192, 193, 195, 196, 197, 206, 214, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 230, 233, 234, 235, 236, 237, 238, 239, 240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 267, 270, 271, 272, 276, 277, 285, 286, 287, 288, 289, 290, 291, 292, 294, 295, 297, 303, 307, 310, 311, 312, 313, 314, 315, 321, 322, 324, 326, 327, 331, 332, 334, 340, 341, 342, 343, 344, 345, 346, 347, 349, 350, 351, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 371, 372, 373, 374, 378, 379, 380, 382, 385, 387, 389, 393, 394, 395, 396, 398, 399, 400, 403, 404, 406, 407, 408, 409, 411, 413, 414, 415, 416, 417, 418, 424, 425, 426, 427, 428, 429, 430, 434, 457, 458, 460, 462, 468, 471, 472, 473, 474, 475, 476, 479, 485, 490, 491, 492, 493, 494, 495, 496], "get_device_tensor": 0, "load_tensor": 0, "realloc": [0, 25, 26, 60, 271, 350], "split_work_to_cor": [0, 5], "to_devic": [0, 122, 177, 178, 214, 271, 285, 400, 492, 494], "to_dtyp": [0, 491], "to_layout": [0, 162, 438, 485, 489, 491, 492, 494, 495], "to_memory_config": [0, 18, 368, 370, 485, 491, 492], "to_torch": [0, 5, 178, 479, 488, 490, 491, 492, 493, 494, 495, 496], "typecast": [0, 67, 349, 491, 492], "arang": [0, 350, 351, 367, 457, 458, 490, 491, 492], "bernoulli": 0, "complex_tensor": [0, 43, 44, 90, 91, 198, 199, 204, 205, 308, 309, 328, 329], "empti": [0, 5, 10, 41, 117, 267, 402], "empty_lik": 0, "from_buff": 0, "full": [0, 7, 63, 88, 256, 320, 367, 480, 484, 485, 486, 491, 492, 496, 497], "full_lik": 0, "index_fil": 0, "ones": [0, 301, 367, 463, 485, 490, 492, 496], "ones_lik": 0, "rand": [0, 5, 16, 18, 21, 23, 36, 37, 42, 43, 44, 46, 48, 50, 54, 58, 61, 75, 76, 85, 88, 89, 90, 91, 96, 97, 98, 100, 103, 104, 105, 106, 118, 121, 122, 125, 128, 133, 136, 143, 152, 158, 161, 164, 165, 169, 174, 175, 176, 178, 181, 185, 187, 194, 198, 199, 200, 201, 204, 205, 207, 208, 209, 210, 211, 215, 216, 217, 228, 229, 231, 232, 241, 252, 253, 264, 268, 269, 273, 275, 278, 281, 293, 296, 298, 299, 301, 303, 304, 306, 308, 309, 315, 320, 321, 323, 328, 329, 330, 337, 338, 339, 347, 348, 353, 354, 355, 367, 368, 369, 370, 381, 383, 384, 386, 388, 390, 391, 392, 397, 400, 405, 410, 412, 419, 420, 421, 423, 431, 432, 435, 436, 437, 438, 439, 459, 461, 464, 466, 467, 470, 478, 488, 489, 490, 492, 493, 495, 496], "uniform": [0, 325, 469, 485], "zero": [0, 60, 92, 93, 94, 108, 111, 115, 128, 139, 141, 149, 150, 151, 174, 177, 178, 181, 204, 205, 228, 256, 264, 266, 281, 283, 296, 297, 304, 307, 316, 317, 318, 319, 322, 326, 331, 334, 349, 400, 433, 457, 458, 478, 485, 486, 488, 490, 492, 496], "zeros_lik": [0, 74], "matrix": [0, 4, 7, 10, 13, 36, 122, 268, 400, 485, 486, 487, 490, 491, 492, 497], "multipl": [0, 4, 5, 7, 13, 14, 36, 80, 85, 105, 150, 163, 169, 176, 178, 216, 218, 268, 279, 281, 286, 289, 290, 300, 316, 317, 318, 319, 337, 354, 367, 371, 420, 433, 436, 439, 442, 443, 450, 477, 482, 484, 486, 490, 491, 492, 497], "matmul": [0, 7, 10, 11, 12, 13, 14, 36, 231, 400, 489, 490, 491, 492, 494, 497], "linear": [0, 37, 74, 169, 176, 182, 307, 334, 337, 393, 420, 451, 479, 491, 492, 493, 494, 495], "addmm": 0, "sparse_matmul": 0, "matmulmulticorereuseprogramconfig": [0, 268], "compute_with_storage_grid_s": [0, 5, 10, 13, 14, 18, 368, 369, 370, 400, 492], "from_json": [0, 10, 11, 12, 13, 14], "in0_block_w": [0, 10, 11, 12, 13, 14, 400], "out_subblock_h": [0, 10, 13, 14, 400], "out_subblock_w": [0, 10, 13, 14, 400], "per_core_m": [0, 10, 11, 12, 13, 14, 400], "per_core_n": [0, 10, 11, 12, 13, 14, 400], "to_json": [0, 10, 11, 12, 13, 14], "matmulmulticorereusemulticastprogramconfig": [0, 268], "fuse_batch": [0, 10, 13, 400], "fused_activ": [0, 10, 11, 12, 13, 231, 268, 400], "out_block_h": [0, 10, 13, 400], "out_block_w": [0, 10, 13, 400], "transpose_mcast": [0, 13], "matmulmulticorereusemulticast1dprogramconfig": [0, 268, 400], "gather_in0": [0, 10], "hop_cor": [0, 10], "mcast_in0": [0, 10, 268, 400], "num_global_cb_receiv": [0, 10], "untilize_out": [0, 10], "matmulmulticorereusemulticastdramshardedprogramconfig": [0, 268], "matmulmulticorereusemulticastbatcheddramshardedprogramconfig": [0, 268], "pointwis": 0, "unari": [0, 5, 327, 462, 492], "ab": [0, 20, 492], "aco": [0, 22], "acosh": [0, 24], "alt_complex_rotate90": 0, "angl": [0, 44, 308, 357], "asin": [0, 49], "asinh": [0, 51], "atan": [0, 57], "atanh": [0, 59], "bitcast": 0, "bitwise_left_shift": 0, "bitwise_not": 0, "bitwise_right_shift": 0, "cbrt": 0, "ceil": [0, 60, 77, 178, 271], "celu": [0, 79], "clamp": [0, 82, 259, 339], "clip": [0, 84, 486], "conj": [0, 91], "co": [0, 492], "cosh": [0, 101], "deg2rad": [0, 107], "digamma": [0, 110], "eqz": 0, "erf": [0, 130], "erfc": [0, 132], "erfinv": [0, 134], "exp": [0, 144, 243, 247, 248, 451, 462, 491, 492, 496], "exp2": [0, 137, 244, 245, 246], "experiment": [0, 7, 25, 26, 368, 480], "dropout": 0, "elu": [0, 120], "expm1": [0, 144], "fill": [0, 146, 147, 148, 149, 150, 151, 163, 164, 200, 201, 256, 300, 301, 325, 346, 357, 390, 439, 477, 478, 484, 487, 491, 492], "floor": [0, 60, 111, 112, 115, 153, 154, 326, 327], "frac": [0, 32, 55, 61, 111, 115, 159, 178, 182, 215, 216, 217, 353, 354, 355, 391, 392], "geglu": 0, "gelu": [0, 7, 13, 64, 65, 66, 142, 169, 171, 479, 491], "gez": 0, "glu": 0, "gtz": 0, "hardmish": 0, "hardshrink": [0, 184, 225], "hardsigmoid": [0, 186], "hardswish": [0, 188], "hardtanh": [0, 190], "heavisid": 0, "i0": [0, 195], "i1": 0, "ident": [0, 37, 38, 39, 41, 178, 215, 370], "is_imag": 0, "is_real": 0, "isfinit": 0, "isinf": 0, "isnan": 0, "isneginf": 0, "isposinf": 0, "leaky_relu": [0, 225], "lez": 0, "lgamma": [0, 230], "log": [0, 229, 230, 236, 237, 241, 242, 243, 244, 245, 246, 247, 248, 287, 288, 451, 475, 476, 484, 487, 489, 492, 493, 495, 496, 497], "log10": [0, 235], "log1p": [0, 237], "log2": [0, 239, 244], "log_sigmoid": 0, "logical_left_shift": 0, "logical_not": [0, 253], "logical_not_": 0, "logical_right_shift": 0, "logit": [0, 260, 261, 491, 493, 495, 496], "ltz": 0, "mish": [0, 182], "multigammaln": 0, "neg": [0, 210, 225, 294, 314], "nez": 0, "normalize_glob": 0, "normalize_hw": 0, "polar": [0, 309], "polygamma": [0, 311], "prelu": 0, "rad2deg": [0, 324], "rdiv": [0, 327], "real": [0, 3, 36, 205, 308, 329, 480], "reciproc": [0, 178, 332, 362, 363, 485], "reglu": 0, "relu": [0, 7, 13, 224, 225, 337, 341, 342, 343, 393, 462, 492, 493, 495], "relu6": [0, 340], "relu_max": 0, "relu_min": 0, "remaind": [0, 155, 345], "round": [0, 11, 12, 53, 111, 112, 115, 289, 290, 326, 327, 359, 485], "rsqrt": 0, "selu": [0, 374], "sigmoid": [0, 7, 185, 186, 241, 242, 379, 380], "sigmoid_accur": 0, "sign": [0, 382, 383], "signbit": 0, "silu": [0, 7, 385, 420], "sin": [0, 387, 492], "sinh": [0, 389], "softplu": [0, 394], "softshrink": [0, 396], "softsign": [0, 398], "sqrt": [0, 61, 178, 192, 215, 216, 217, 353, 354, 355, 362, 413, 491, 492], "squar": [0, 7, 285, 353, 354, 355, 362, 363, 403, 404, 406, 407, 408, 409, 440, 441, 485, 491, 492], "std_hw": 0, "swiglu": 0, "swish": [0, 187, 188, 384, 420], "tan": [0, 424], "tanh": [0, 7, 66, 142, 171, 189, 190, 426, 427, 428], "tanhshrink": [0, 428], "threshold": [0, 367, 393, 394, 430], "tril": 0, "triu": [0, 491, 492], "trunc": [0, 111, 112, 115, 326, 327], "unary_chain": 0, "var_hw": 0, "add": [0, 28, 29, 30, 32, 34, 62, 243, 244, 245, 246, 247, 248, 304, 369, 370, 372, 440, 441, 464, 480, 482, 483, 486, 490, 491, 492, 494], "add_": 0, "addalpha": [0, 31], "atan2": [0, 56], "bias_gelu": [0, 65, 66], "bias_gelu_": 0, "bitwise_and": [0, 69, 72], "bitwise_or": 0, "bitwise_xor": 0, "div": 0, "div_no_nan": [0, 114], "divid": [0, 7, 13, 14, 32, 80, 111, 112, 116, 178, 402, 440, 441, 491, 492, 497], "divide_": 0, "eq": 0, "eq_": 0, "floor_div": 0, "fmod": [0, 156], "gcd": 0, "ge": 0, "ge_": 0, "gt": 0, "gt_": 0, "hypot": [0, 193], "isclos": 0, "lcm": 0, "ldexp": [0, 220, 221], "ldexp_": 0, "le": 0, "le_": 0, "logaddexp": [0, 247, 248], "logaddexp2": [0, 245, 246], "logaddexp2_": 0, "logaddexp_": 0, "logical_and": 0, "logical_and_": 0, "logical_or": 0, "logical_or_": 0, "logical_xor": 0, "logical_xor_": 0, "lt": 0, "lt_": 0, "maximum": [0, 7, 46, 81, 82, 83, 84, 190, 270, 271, 442, 492], "minimum": [0, 5, 81, 82, 83, 84, 190, 276, 485], "multipli": [0, 7, 30, 32, 34, 36, 231, 251, 268, 286, 290, 368, 369, 370, 400, 415, 469, 485, 486, 492], "multiply_": 0, "ne": 0, "ne_": 0, "nextaft": 0, "outer": 0, "polyv": 0, "pow": [0, 492], "rpow": [0, 361], "rsub": 0, "rsub_": 0, "squared_differ": [0, 408, 409], "squared_difference_": 0, "subalpha": [0, 416], "subtract": [0, 364, 365, 366, 414, 415, 418, 454, 492], "subtract_": 0, "xlogi": [0, 476], "ternari": 0, "addcdiv": [0, 33], "addcmul": [0, 35], "lerp": [0, 227], "mac": 0, "quantiz": [0, 108, 349], "dequant": [0, 349], "requant": 0, "loss": [0, 214, 285], "l1_loss": 0, "mse_loss": 0, "reduct": [0, 4, 7, 39, 46, 214, 282, 285, 320, 334, 371], "argmax": [0, 491, 493, 495], "cumprod": 0, "cumsum": 0, "ema": 0, "manual_se": [0, 479, 488, 492, 494, 495], "max": [0, 26, 81, 82, 83, 84, 182, 189, 190, 271, 334, 342, 443, 491, 495], "mean": [0, 7, 14, 36, 61, 178, 214, 215, 216, 217, 285, 353, 354, 355, 471, 485, 491, 492, 494], "min": [0, 81, 82, 83, 84, 182, 189, 190, 342, 343], "moe": [0, 282, 283], "prod": [0, 102, 321], "sampl": [0, 20, 22, 24, 44, 49, 51, 57, 59, 77, 79, 82, 84, 91, 99, 101, 107, 110, 114, 120, 130, 132, 134, 137, 138, 144, 146, 151, 153, 159, 171, 177, 184, 186, 188, 190, 195, 199, 225, 230, 235, 237, 239, 240, 242, 260, 261, 288, 294, 309, 311, 314, 321, 324, 327, 329, 332, 340, 341, 347, 359, 361, 363, 374, 380, 382, 385, 387, 389, 394, 396, 398, 404, 406, 424, 426, 428, 430, 460, 486, 493, 497], "std": [0, 5, 269, 273, 275, 375, 419, 470, 495], "sum": [0, 39, 104, 281, 334, 371, 451, 491, 492], "topk": [0, 281, 282], "var": [0, 413, 471], "movement": [0, 117, 172, 492, 497], "assign": [0, 53, 267, 450], "bcast": 0, "chunk": [0, 7, 10, 14, 282, 401, 442, 443, 445, 446, 447, 448, 449, 450, 451, 452, 453, 489], "concat": [0, 41, 89, 483, 491, 492], "copi": [0, 5, 52, 85, 96, 97, 139, 161, 197, 434, 497], "expand": [0, 27, 28, 30, 55, 64, 65, 68, 69, 71, 72, 73, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 177, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 348, 349, 357, 364, 365, 407, 408, 415, 417, 418, 475, 485, 490], "fill_implicit_tile_pad": 0, "fill_ones_rm": 0, "fill_rm": [0, 149], "fold": [0, 7], "gather": [0, 38, 41, 216, 217, 354, 355, 450, 451], "indexed_fil": 0, "interleaved_to_shard": 0, "interleaved_to_sharded_parti": 0, "moe_expert_token_remap": 0, "moe_routing_remap": 0, "nonzero": 0, "pad": [0, 7, 38, 60, 85, 92, 93, 94, 122, 148, 150, 162, 178, 215, 216, 217, 271, 281, 305, 306, 316, 317, 318, 319, 350, 353, 354, 355, 367, 370, 390, 432, 433, 436, 439, 447, 451, 456, 467, 468, 472, 485, 488, 491, 492, 495, 496], "permut": [0, 25, 26, 60, 178, 271, 454, 488, 491, 492, 494, 495], "repeat": [0, 4, 347, 348, 367, 372, 490], "repeat_interleav": 0, "reshap": [0, 7, 25, 26, 60, 149, 150, 177, 178, 271, 349, 351, 367, 454, 457, 458, 488, 491, 492, 493, 494, 495, 496], "reshape_on_devic": 0, "reshard": [0, 7], "roll": 0, "scatter": [0, 333, 372], "scatter_add": 0, "sharded_to_interleav": [0, 202, 203, 377], "sharded_to_interleaved_parti": 0, "slice": [0, 7, 93, 94, 203, 377, 447, 451, 492], "sort": [0, 439, 491, 492], "split": [0, 7, 80, 88, 169, 176, 178, 337, 402, 420, 442, 447, 451, 454, 485, 491], "squeez": [0, 320, 438], "stack": [0, 482, 492], "tiliz": [0, 27, 28, 30, 47, 55, 61, 64, 65, 68, 69, 71, 72, 73, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 178, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 407, 408, 415, 417, 418, 432, 466, 467, 475, 489, 494], "tilize_with_val_pad": 0, "tilize_with_zero_pad": [0, 178], "transpos": [0, 13, 94, 231, 268, 306, 454, 485, 491, 492, 493, 495], "unsqueez": [0, 491, 492], "unsqueeze_to_4d": 0, "until": [0, 10, 27, 28, 30, 55, 64, 65, 68, 69, 71, 72, 73, 96, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 407, 408, 415, 417, 418, 438, 467, 475], "untilize_with_unpad": 0, "view": [0, 3, 177, 178, 350, 400, 484, 493, 495, 497], "normal": [0, 61, 177, 178, 197, 215, 216, 217, 298, 299, 353, 354, 355, 462, 491, 492, 493, 495], "batch_norm": 0, "group_norm": 0, "layer_norm": [0, 216, 217, 491, 492], "layer_norm_post_all_gath": [0, 217], "layer_norm_pre_all_gath": [0, 216], "rms_norm": [0, 354, 355], "rms_norm_post_all_gath": [0, 355], "rms_norm_pre_all_gath": [0, 354], "scale_causal_mask_hw_dims_softmax_in_plac": 0, "scale_mask_softmax": 0, "scale_mask_softmax_in_plac": [0, 18, 368], "softmax": [0, 16, 17, 18, 281, 367, 368, 369, 370, 392, 440, 441, 491, 492, 494], "softmax_in_plac": [0, 16], "softmaxdefaultprogramconfig": [0, 368, 370, 392, 440, 441], "softmaxprogramconfig": [0, 368, 370, 392, 440, 441], "softmaxshardedmulticoreprogramconfig": [0, 368, 370], "block_h": [0, 18, 368, 370], "block_w": [0, 18, 368, 370], "subblock_w": [0, 18, 368, 370], "transform": [0, 4, 7, 177, 231, 308, 357, 368, 479, 486, 492, 493, 494, 495, 496], "attention_softmax": 0, "attention_softmax_": [0, 494], "chunked_flash_mla_prefil": 0, "chunked_scaled_dot_product_attent": 0, "concatenate_head": [0, 494], "flash_mla_prefil": 0, "flash_multi_latent_attention_decod": 0, "joint_scaled_dot_product_attent": 0, "paged_flash_multi_latent_attention_decod": 0, "paged_scaled_dot_product_attention_decod": 0, "ring_distributed_scaled_dot_product_attent": 0, "ring_joint_scaled_dot_product_attent": 0, "scaled_dot_product_attent": [0, 492], "scaled_dot_product_attention_decod": 0, "split_query_key_value_and_split_head": [0, 494], "windowed_scaled_dot_product_attent": 0, "ccl": [0, 4, 282, 451, 492], "all_broadcast": 0, "all_gath": [0, 216, 217, 274, 354, 355, 492], "all_reduc": 0, "all_to_all_combin": 0, "all_to_all_dispatch": 0, "broadcast": [0, 10, 13, 27, 28, 29, 30, 37, 38, 55, 62, 64, 65, 68, 69, 71, 72, 73, 111, 112, 113, 115, 116, 126, 127, 154, 155, 165, 166, 167, 168, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 268, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 333, 344, 348, 349, 364, 365, 368, 369, 370, 407, 408, 414, 415, 417, 418, 445, 452, 475, 486, 488, 491, 493, 495], "mesh_partit": 0, "point_to_point": 0, "reduce_scatt": 0, "reduce_to_root": 0, "embed": [0, 123, 491, 492, 496], "convolut": [0, 4, 7, 60, 88, 92, 93, 94, 140, 271, 316, 317, 318, 319, 486, 491], "conv1d": [0, 7], "conv2d": [0, 7, 94, 316, 317, 318, 319, 488, 491, 495], "conv_transpose2d": [0, 7], "conv3d": 0, "prepare_conv_bia": 0, "prepare_conv_transpose2d_bia": 0, "prepare_conv_transpose2d_weight": 0, "prepare_conv_weight": [0, 491], "conv2dconfig": [0, 92, 93, 94, 316, 317, 318, 319, 488, 495], "act_block_h_overrid": [0, 7, 495], "act_block_w_div": [0, 7, 495], "activ": [0, 7, 10, 11, 12, 13, 27, 28, 64, 78, 119, 126, 166, 167, 170, 179, 182, 185, 187, 189, 190, 218, 219, 222, 224, 231, 243, 244, 249, 254, 257, 262, 268, 272, 277, 278, 291, 315, 338, 339, 364, 373, 378, 379, 384, 393, 397, 407, 417, 418, 421, 475, 479, 482, 486, 491, 493, 494, 495, 497], "config_tensors_in_dram": [0, 7], "core_grid": [0, 7, 36, 102, 121, 178, 202, 231, 268, 352, 400, 402, 479, 489, 492, 494, 495], "deallocate_activ": [0, 7, 60, 271, 495], "enable_act_double_buff": [0, 7, 495], "enable_activation_reus": [0, 7], "enable_kernel_stride_fold": [0, 7, 495], "enable_weights_double_buff": [0, 7, 495], "force_split_read": [0, 7], "full_inner_dim": [0, 7], "output_layout": [0, 7, 60, 178, 271, 495], "override_output_sharding_config": [0, 7], "override_sharding_config": [0, 7, 495], "reallocate_halo_output": [0, 7, 60, 271, 495], "reshard_if_not_optim": [0, 7, 495], "shard_layout": [0, 7, 495], "transpose_shard": [0, 7, 495], "weights_dtyp": [0, 7, 316, 317, 488, 495], "conv2dsliceconfig": [0, 93, 94], "pool": [0, 7, 25, 26, 60, 175, 271, 486], "adaptive_avg_pool2d": 0, "adaptive_max_pool2d": 0, "avg_pool2d": 0, "global_avg_pool2d": 0, "max_pool2d": [0, 495], "prefetch": 0, "dram_prefetch": 0, "vision": [0, 455, 486], "grid_sampl": 0, "upsampl": 0, "rotat": [0, 42], "gener": [0, 7, 13, 63, 122, 141, 150, 177, 178, 267, 268, 308, 325, 367, 368, 450, 463, 469, 480, 484, 486, 488, 489, 490, 492, 493, 494, 495], "generic_op": 0, "kv": [0, 442, 443, 450], "kv_cach": 0, "fill_cache_for_user_": 0, "update_cache_for_token_": 0, "fill_cach": 0, "update_cach": [0, 147], "backward": [0, 20, 22, 24, 29, 31, 33, 35, 44, 49, 51, 53, 56, 57, 59, 66, 77, 79, 82, 84, 89, 91, 99, 101, 107, 110, 112, 114, 120, 123, 130, 132, 134, 137, 138, 142, 144, 146, 151, 153, 156, 159, 171, 184, 186, 188, 190, 193, 195, 199, 221, 225, 227, 230, 235, 237, 239, 240, 242, 246, 248, 260, 261, 270, 276, 286, 288, 294, 309, 311, 314, 321, 324, 327, 329, 332, 340, 341, 345, 347, 359, 361, 363, 366, 374, 380, 382, 385, 387, 389, 394, 396, 398, 404, 406, 409, 414, 416, 424, 426, 428, 430, 460, 474, 476], "abs_bw": 0, "acos_bw": 0, "acosh_bw": 0, "add_bw": 0, "addalpha_bw": 0, "addcdiv_bw": 0, "addcmul_bw": 0, "angle_bw": 0, "asin_bw": 0, "asinh_bw": 0, "assign_bw": 0, "atan2_bw": 0, "atan_bw": 0, "atanh_bw": 0, "bias_gelu_bw": 0, "ceil_bw": 0, "celu_bw": 0, "clamp_bw": 0, "clip_bw": 0, "concat_bw": 0, "conj_bw": 0, "cos_bw": 0, "cosh_bw": 0, "deg2rad_bw": 0, "digamma_bw": 0, "div_bw": 0, "div_no_nan_bw": 0, "elu_bw": 0, "embedding_bw": 0, "erf_bw": 0, "erfc_bw": 0, "erfinv_bw": 0, "exp2_bw": 0, "exp_bw": 0, "gelu_bw": 0, "expm1_bw": 0, "fill_bw": 0, "fill_zero_bw": 0, "floor_bw": 0, "fmod_bw": 0, "frac_bw": 0, "hardshrink_bw": 0, "hardsigmoid_bw": 0, "hardswish_bw": 0, "hardtanh_bw": 0, "hypot_bw": 0, "i0_bw": 0, "imag_bw": 0, "ldexp_bw": 0, "leaky_relu_bw": 0, "lerp_bw": 0, "lgamma_bw": 0, "log10_bw": 0, "log1p_bw": 0, "log2_bw": 0, "log_bw": 0, "log_sigmoid_bw": 0, "logaddexp2_bw": 0, "logaddexp_bw": 0, "logit_bw": 0, "logiteps_bw": 0, "max_bw": 0, "min_bw": 0, "mul_bw": 0, "multigammaln_bw": 0, "neg_bw": 0, "polar_bw": 0, "polygamma_bw": 0, "pow_bw": 0, "prod_bw": 0, "rad2deg_bw": 0, "rdiv_bw": 0, "real_bw": 0, "reciprocal_bw": 0, "relu6_bw": 0, "relu_bw": 0, "remainder_bw": 0, "repeat_bw": 0, "round_bw": 0, "rpow_bw": 0, "rsqrt_bw": 0, "rsub_bw": 0, "selu_bw": 0, "sigmoid_bw": 0, "sign_bw": 0, "silu_bw": 0, "sin_bw": 0, "sinh_bw": 0, "softplus_bw": 0, "softshrink_bw": 0, "softsign_bw": 0, "sqrt_bw": 0, "square_bw": 0, "squared_difference_bw": 0, "sub_bw": 0, "subalpha_bw": 0, "tan_bw": 0, "tanh_bw": 0, "tanhshrink_bw": 0, "threshold_bw": 0, "trunc_bw": 0, "where_bw": 0, "xlogy_bw": 0, "convers": [0, 67, 107, 324, 486, 489, 492, 493, 495, 496], "model_preprocess": [0, 479, 494, 496], "preprocess_model": 0, "preprocess_model_paramet": [0, 479, 496], "report": [0, 3, 93, 268, 480, 486, 492, 493], "set_printopt": 0, "hook": [0, 335, 336], "register_pre_operation_hook": 0, "register_post_operation_hook": 0, "tutori": [0, 3, 481, 487, 488, 489, 490, 491, 492, 493, 494, 495, 497], "open": [0, 4, 161, 266, 302, 434, 486, 491, 492, 494, 496, 497], "tenstorr": [0, 2, 3, 5, 7, 74, 93, 443, 479, 480, 481, 482, 486, 489, 491, 492, 493, 495, 496, 497], "addit": [0, 11, 12, 27, 28, 29, 30, 165, 268, 370, 392, 399, 479, 482, 486, 490, 492, 495], "close": [0, 86, 206, 266, 482, 486, 491, 494], "output": [0, 5, 7, 10, 11, 12, 13, 14, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 88, 89, 90, 91, 92, 93, 94, 98, 99, 100, 101, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 267, 268, 269, 270, 271, 272, 273, 275, 276, 277, 278, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 301, 303, 304, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 326, 327, 328, 329, 331, 332, 333, 334, 335, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 403, 404, 405, 406, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 436, 437, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 478, 479, 480, 482, 484, 485, 486, 491, 492, 496, 497], "base": [0, 5, 7, 10, 11, 12, 13, 14, 16, 17, 18, 40, 41, 61, 88, 102, 121, 137, 164, 165, 178, 234, 235, 238, 239, 268, 301, 305, 325, 367, 400, 442, 478, 485, 486, 491, 492, 494, 496], "arithmet": [0, 486, 492], "simul": [0, 486, 488, 489, 493, 494, 495, 497], "row": [0, 7, 10, 27, 28, 30, 38, 40, 41, 55, 64, 65, 68, 69, 71, 72, 73, 77, 111, 113, 115, 116, 117, 126, 127, 140, 146, 150, 151, 153, 154, 155, 159, 166, 167, 168, 178, 179, 180, 192, 202, 203, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 268, 272, 277, 283, 289, 290, 291, 292, 295, 297, 303, 312, 313, 315, 333, 344, 359, 364, 365, 382, 400, 402, 407, 408, 415, 417, 418, 460, 475, 484, 485, 486, 489, 491, 492, 497], "vector": [0, 5, 37, 268, 312, 334, 378, 486, 491, 492, 493], "expans": [0, 139, 349, 486], "b": [0, 5, 27, 28, 30, 40, 41, 55, 64, 65, 68, 69, 71, 72, 73, 111, 113, 115, 116, 121, 126, 127, 154, 155, 166, 167, 168, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 268, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 400, 407, 408, 415, 417, 418, 442, 443, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455, 475, 482, 486, 488, 492, 495], "random": [0, 21, 23, 48, 50, 54, 58, 60, 63, 75, 76, 98, 100, 106, 128, 133, 136, 143, 152, 158, 169, 174, 175, 176, 178, 181, 185, 187, 194, 207, 208, 209, 210, 211, 228, 229, 241, 252, 253, 264, 267, 271, 278, 293, 296, 298, 299, 323, 325, 337, 338, 339, 367, 381, 383, 384, 386, 388, 397, 405, 420, 421, 423, 459, 462, 463, 486, 488, 492, 493, 495, 496], "valu": [0, 7, 10, 13, 14, 19, 20, 21, 23, 30, 31, 32, 33, 34, 35, 40, 41, 42, 45, 46, 48, 50, 54, 58, 60, 61, 62, 63, 67, 69, 70, 72, 75, 76, 78, 79, 81, 82, 83, 84, 85, 88, 90, 98, 100, 103, 104, 106, 109, 114, 115, 119, 120, 124, 125, 128, 129, 131, 133, 135, 136, 143, 145, 147, 148, 149, 150, 152, 158, 160, 162, 163, 164, 165, 169, 170, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 187, 189, 190, 191, 194, 196, 197, 200, 201, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 216, 224, 225, 228, 229, 233, 234, 236, 238, 241, 251, 252, 253, 259, 261, 264, 267, 269, 271, 272, 273, 275, 277, 278, 283, 287, 288, 293, 295, 296, 298, 299, 300, 301, 302, 304, 306, 310, 313, 314, 323, 325, 326, 327, 331, 334, 337, 338, 339, 342, 343, 349, 350, 354, 357, 358, 360, 361, 362, 367, 371, 372, 373, 375, 378, 379, 381, 383, 384, 386, 388, 390, 393, 394, 395, 396, 397, 399, 400, 403, 405, 412, 415, 416, 419, 420, 421, 423, 425, 427, 429, 430, 432, 433, 439, 447, 451, 452, 454, 455, 456, 457, 458, 459, 462, 463, 468, 470, 473, 477, 478, 480, 485, 486, 487, 490, 491, 492, 493, 494, 495, 496, 497], "inspect": [0, 486, 497], "us": [0, 4, 5, 7, 9, 10, 13, 14, 15, 16, 17, 25, 26, 27, 28, 30, 32, 34, 36, 37, 38, 39, 40, 41, 55, 60, 64, 65, 66, 68, 69, 71, 72, 73, 74, 81, 83, 87, 88, 92, 94, 96, 97, 102, 111, 112, 113, 115, 116, 117, 122, 123, 125, 126, 127, 129, 131, 135, 139, 141, 142, 150, 154, 155, 160, 162, 164, 165, 166, 167, 168, 170, 171, 172, 177, 178, 179, 180, 192, 193, 196, 197, 200, 202, 206, 216, 217, 218, 219, 220, 222, 223, 231, 233, 234, 236, 238, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 268, 269, 271, 272, 273, 275, 277, 279, 280, 289, 290, 291, 292, 295, 301, 302, 303, 304, 312, 313, 315, 316, 317, 318, 319, 320, 326, 327, 333, 335, 336, 344, 347, 349, 350, 354, 355, 357, 362, 364, 365, 367, 368, 369, 370, 375, 378, 379, 391, 392, 393, 399, 400, 402, 403, 407, 408, 412, 415, 417, 418, 419, 431, 432, 433, 437, 438, 439, 442, 443, 446, 448, 449, 450, 451, 453, 454, 455, 461, 466, 467, 468, 470, 475, 478, 479, 480, 481, 482, 483, 485, 486, 488, 490, 491, 492, 493, 494, 495, 497], "more": [0, 1, 3, 4, 5, 7, 10, 13, 14, 20, 33, 61, 66, 93, 112, 156, 171, 178, 193, 215, 216, 217, 235, 237, 239, 240, 242, 246, 248, 260, 261, 314, 321, 327, 331, 332, 345, 353, 354, 355, 392, 394, 402, 404, 457, 458, 476, 481, 482, 484, 485, 486, 487, 488, 490, 491, 492, 494, 495, 497], "mlp": [0, 486, 491], "load": [0, 3, 4, 7, 10, 13, 232, 450, 484, 486, 491, 496, 497], "mnist": [0, 486], "pretrain": [0, 486, 495], "weight": [0, 7, 61, 92, 93, 94, 122, 123, 140, 177, 226, 231, 281, 283, 315, 316, 317, 318, 319, 355, 479, 486, 488, 492, 494, 496], "track": [0, 41, 486, 495, 496], "loop": [0, 486], "flatten": [0, 62, 486, 488, 491, 492, 495], "head": [0, 26, 440, 441, 442, 444, 445, 452, 454, 486, 491, 496], "write": [0, 1, 5, 36, 102, 231, 268, 400, 479, 485, 486, 491], "seed": [0, 141, 267, 325, 367, 486], "forward": [0, 479, 486, 491, 496], "method": [0, 5, 171, 357, 425, 427, 486, 492], "input": [0, 5, 7, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 88, 89, 90, 91, 92, 93, 94, 95, 98, 99, 100, 101, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 119, 120, 121, 122, 123, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 162, 164, 165, 166, 167, 168, 169, 170, 171, 172, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 268, 270, 271, 272, 274, 276, 277, 278, 281, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 301, 303, 304, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 326, 327, 328, 329, 330, 331, 332, 333, 334, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 371, 372, 373, 374, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 403, 404, 405, 406, 407, 408, 409, 410, 413, 414, 415, 416, 417, 418, 420, 421, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 436, 437, 438, 439, 440, 441, 442, 444, 445, 446, 448, 449, 450, 452, 453, 454, 456, 457, 458, 459, 460, 461, 462, 464, 466, 467, 468, 469, 471, 472, 474, 475, 476, 478, 480, 482, 484, 485, 486, 487, 489, 491, 492, 493, 494, 495, 496, 497], "paramet": [0, 4, 7, 10, 11, 12, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 282, 283, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 298, 299, 300, 301, 303, 305, 306, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 403, 404, 405, 406, 407, 408, 409, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 457, 458, 459, 460, 461, 462, 463, 466, 467, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 482, 486, 491, 492, 494, 495, 496], "simpl": [0, 3, 482, 486, 487, 488, 490, 493], "cnn": [0, 93, 486, 488], "cifar": [0, 486], "10": [0, 36, 45, 97, 122, 175, 195, 196, 200, 231, 234, 235, 265, 268, 310, 348, 367, 371, 429, 434, 435, 455, 457, 458, 461, 473, 482, 486, 488, 489, 491, 492, 493, 497], "dataset": [0, 485, 486, 493, 497], "defin": [0, 5, 10, 36, 47, 60, 82, 84, 149, 150, 172, 201, 216, 217, 231, 271, 314, 320, 321, 327, 347, 354, 355, 361, 391, 400, 430, 455, 483, 485, 486, 491, 492, 496], "stage": [0, 486, 497], "shot": [0, 486], "classif": [0, 486, 493, 495], "doe": [0, 14, 85, 162, 178, 267, 320, 410, 438, 443, 480, 484, 486, 492], "util": [0, 4, 11, 12, 117, 178, 485, 486, 489, 493, 495, 497], "architectur": [0, 93, 486, 496], "process": [0, 5, 10, 11, 12, 13, 14, 18, 88, 103, 104, 178, 216, 217, 316, 317, 354, 355, 368, 369, 370, 442, 443, 484, 486, 492, 493, 494, 495], "pipelin": [0, 5, 117, 480, 484, 486, 492, 493], "complet": [0, 27, 28, 30, 55, 64, 65, 68, 69, 71, 72, 73, 96, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 407, 408, 415, 417, 418, 422, 475, 486, 487, 488, 489, 490, 492, 493, 494, 495, 496, 497], "compon": [0, 482, 486], "kei": [0, 4, 443, 447, 451, 454, 455, 479, 485, 486, 492, 494, 495], "preprocess": [0, 5, 7, 47, 92, 93, 94, 177, 279, 280, 316, 317, 318, 319, 485, 486, 495, 496], "download": [0, 482, 486, 492, 493, 495], "token": [0, 40, 41, 122, 147, 213, 282, 446, 447, 448, 449, 451, 452, 453, 455, 468, 486, 496], "text": [0, 61, 111, 115, 178, 206, 215, 216, 217, 295, 303, 353, 354, 355, 391, 392, 480, 486], "visual": [0, 3, 4, 486, 491, 492], "profil": [0, 3, 197, 375, 486, 492], "analysi": [0, 484, 486, 492], "upload": [0, 484, 486], "tab": [0, 484, 486], "buffer": [0, 3, 5, 7, 105, 117, 160, 172, 268, 283, 302, 333, 353, 392, 451, 484, 485, 486], "graph": [0, 3, 4, 279, 486, 492, 496], "recap": [0, 486], "tracer": [0, 486], "bert": [0, 479, 486], "layer": [0, 117, 215, 216, 217, 353, 354, 355, 484, 486, 488, 491, 493, 495], "written": [0, 5, 36, 147, 212, 213, 231, 268, 400, 468, 486, 494], "onboard": 0, "new": [0, 38, 52, 125, 139, 164, 301, 346, 350, 351, 411, 443, 464, 472, 478, 480, 482, 484, 492], "rewrit": 0, "switch": [0, 304, 393], "optim": [0, 7, 10, 11, 12, 13, 18, 117, 121, 177, 368, 391, 425, 427, 450, 469, 485, 489, 491, 492, 494, 495, 496, 497], "ad": [0, 36, 92, 93, 94, 231, 316, 317, 318, 319, 372, 464, 483, 491], "faq": 0, "need": [0, 1, 2, 10, 13, 36, 117, 178, 200, 231, 268, 400, 431, 439, 480, 484, 485, 489, 491, 492, 495, 497], "c": [0, 4, 25, 26, 27, 28, 30, 55, 60, 61, 64, 65, 68, 69, 71, 72, 73, 92, 93, 94, 111, 113, 115, 116, 121, 126, 127, 140, 149, 150, 154, 155, 157, 166, 167, 168, 177, 178, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 271, 272, 277, 281, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 357, 364, 365, 367, 378, 407, 408, 413, 415, 417, 418, 439, 469, 471, 475, 482, 484, 485, 488, 489, 491, 492, 495], "python": [0, 4, 160, 443, 482, 483, 484, 485, 486, 487, 490, 491, 492, 497], "bind": [0, 488, 490, 492, 493, 494, 495], "golden": [0, 479, 497], "usag": [0, 7, 10, 13, 14, 172, 216, 217, 354, 355, 480, 482, 492, 497], "doc": [0, 497], "perf": [0, 7, 480, 497], "header": 0, "profile_thi": 0, "descript": [0, 62, 95, 148, 149, 150, 284, 483, 491], "uplift": 0, "bug": 0, "featur": [0, 4, 7, 10, 368, 482, 483, 491, 492, 497], "propos": [0, 483], "request": [0, 436, 483, 491, 496], "troubleshoot": [0, 480], "debug": [0, 4, 5, 172, 483, 492, 495, 496, 497], "tip": 0, "commun": [0, 10, 13, 451, 482, 492], "index": [0, 40, 41, 147, 165, 201, 203, 212, 213, 367, 371, 372, 377, 399, 443, 446, 448, 449, 453, 468, 484, 491, 495], "modul": [0, 5, 94, 279, 280, 479, 493, 495, 496, 497], "search": [0, 497], "page": [0, 442, 443, 450, 482, 489, 497], "If": [1, 2, 5, 7, 10, 11, 12, 13, 27, 28, 30, 36, 38, 39, 40, 41, 46, 55, 60, 62, 63, 64, 65, 68, 69, 71, 72, 73, 85, 88, 92, 93, 94, 102, 103, 104, 111, 113, 115, 116, 117, 121, 126, 127, 154, 155, 162, 165, 166, 167, 168, 178, 179, 180, 192, 206, 215, 216, 218, 219, 220, 222, 223, 231, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 268, 269, 272, 273, 275, 277, 279, 280, 284, 289, 290, 291, 292, 295, 302, 303, 304, 312, 313, 315, 320, 321, 344, 353, 354, 355, 356, 357, 364, 365, 367, 369, 390, 391, 399, 400, 407, 408, 410, 412, 415, 417, 418, 419, 422, 438, 439, 446, 448, 449, 450, 452, 453, 454, 461, 470, 475, 481, 482, 483, 484, 485, 491, 492, 495, 496, 497], "would": [1, 177, 178, 216, 217, 354, 355, 392, 483, 484, 485, 491], "like": [1, 7, 36, 139, 151, 177, 393, 455, 479, 484, 485, 487, 488, 491, 495], "thi": [1, 5, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 25, 26, 36, 37, 40, 41, 46, 60, 61, 63, 85, 87, 88, 92, 93, 94, 96, 97, 117, 123, 139, 142, 145, 175, 177, 178, 196, 197, 200, 215, 216, 217, 221, 267, 271, 274, 281, 305, 316, 317, 318, 319, 320, 333, 342, 343, 349, 350, 354, 355, 367, 368, 369, 370, 392, 393, 400, 402, 435, 439, 442, 443, 447, 450, 451, 452, 455, 461, 463, 472, 479, 480, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497], "project": [1, 2, 4, 481, 491, 497], "pleas": [1, 2, 268, 480, 481, 483], "review": [1, 481, 483], "standard": [1, 2, 5, 16, 177, 343, 349, 392, 413, 455, 480, 481, 482, 491, 492, 495, 496], "gain": 1, "access": [1, 2, 4, 10, 11, 12, 13, 14, 450, 482, 491], "read": [1, 102, 443, 481, 485], "section": [1, 2, 5, 480, 485, 486, 491, 497], "detail": [1, 3, 20, 33, 61, 66, 112, 156, 171, 178, 193, 215, 216, 217, 235, 237, 239, 240, 242, 246, 248, 260, 261, 314, 321, 327, 332, 345, 353, 354, 355, 394, 404, 476, 481, 488, 495, 497], "contact": 1, "u": [1, 483, 491, 493], "have": [2, 5, 13, 14, 27, 28, 30, 38, 41, 46, 52, 55, 61, 62, 63, 64, 65, 67, 68, 69, 71, 72, 73, 87, 96, 97, 103, 104, 105, 111, 113, 115, 116, 121, 123, 126, 127, 139, 148, 154, 155, 165, 166, 167, 168, 178, 179, 180, 192, 206, 215, 216, 217, 218, 219, 220, 222, 223, 226, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 268, 269, 272, 273, 275, 277, 283, 289, 290, 291, 292, 295, 303, 312, 313, 315, 316, 317, 320, 326, 344, 351, 353, 354, 355, 364, 365, 368, 369, 370, 391, 392, 400, 407, 408, 410, 412, 415, 417, 418, 419, 431, 432, 433, 439, 461, 466, 467, 468, 470, 475, 480, 482, 484, 485, 491, 495, 497], "formal": 2, "permiss": 2, "cloud": 2, "issu": [2, 7, 268, 393, 443, 480, 483, 484, 488], "file": [2, 3, 5, 47, 118, 232, 480, 482, 484, 487, 488, 490, 493, 495, 497], "an": [2, 3, 4, 5, 27, 28, 30, 41, 55, 60, 64, 65, 68, 69, 71, 72, 73, 88, 92, 93, 94, 111, 113, 115, 116, 121, 122, 124, 126, 127, 139, 140, 150, 151, 154, 155, 165, 166, 167, 168, 175, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 266, 267, 268, 272, 277, 289, 290, 291, 292, 295, 303, 308, 312, 313, 315, 320, 325, 344, 349, 354, 355, 356, 357, 364, 365, 368, 402, 407, 408, 410, 415, 417, 418, 434, 438, 447, 451, 455, 475, 480, 481, 482, 483, 484, 485, 486, 488, 491, 494, 495, 497], "github": [2, 74, 443, 480, 481, 482, 497], "can": [2, 4, 5, 7, 10, 11, 12, 13, 14, 18, 27, 28, 30, 45, 55, 64, 65, 66, 68, 69, 71, 72, 73, 82, 83, 84, 92, 93, 94, 111, 112, 113, 115, 116, 125, 126, 127, 139, 154, 155, 164, 166, 167, 168, 171, 178, 179, 180, 192, 202, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 268, 272, 277, 289, 290, 291, 292, 295, 301, 303, 304, 312, 313, 315, 316, 317, 318, 319, 326, 327, 335, 336, 344, 356, 364, 365, 375, 393, 407, 408, 415, 417, 418, 422, 452, 461, 475, 478, 479, 482, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497], "check": [2, 3, 5, 126, 128, 167, 174, 179, 181, 204, 205, 206, 207, 208, 209, 210, 211, 222, 228, 262, 264, 268, 291, 296, 480, 482, 492, 493, 494, 495], "out": [2, 3, 5, 7, 36, 40, 41, 103, 104, 121, 141, 165, 177, 281, 343, 390, 399, 439, 457, 458, 487, 488, 492, 494, 495], "relev": [2, 480], "ever": 2, "help": [2, 178, 483, 486, 491, 496, 497], "we": [2, 7, 38, 39, 40, 41, 162, 178, 268, 274, 333, 368, 410, 438, 464, 480, 483, 485, 486, 487, 488, 489, 490, 491, 492, 493, 495, 496, 497], "offici": [2, 497], "discord": 2, "channel": [2, 7, 25, 26, 60, 61, 92, 93, 94, 149, 150, 175, 177, 178, 271, 316, 317, 318, 319, 349, 357, 413, 469, 471, 484, 488, 491, 495], "repres": [2, 4, 202, 367, 450, 484, 485, 491, 495], "both": [2, 7, 10, 11, 12, 13, 14, 29, 62, 67, 87, 92, 93, 94, 95, 103, 104, 177, 178, 268, 271, 313, 316, 317, 318, 319, 392, 447, 451, 469, 479, 480, 484, 485, 491, 492, 497], "join": 2, "discuss": [2, 480], "board": 2, "member": 2, "bounc": 2, "idea": [2, 480], "each": [2, 5, 10, 11, 12, 13, 14, 30, 37, 38, 40, 41, 60, 61, 76, 88, 150, 152, 165, 166, 175, 177, 178, 192, 218, 219, 243, 244, 271, 274, 283, 304, 312, 346, 348, 357, 360, 364, 367, 381, 383, 390, 400, 405, 407, 413, 415, 443, 450, 452, 471, 473, 475, 482, 484, 485, 486, 491, 492, 493, 495, 497], "other": [2, 5, 7, 10, 27, 28, 30, 37, 55, 64, 65, 68, 69, 71, 72, 73, 74, 111, 113, 115, 116, 126, 127, 154, 155, 165, 166, 167, 168, 172, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 268, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 407, 408, 415, 417, 418, 475, 479, 480, 481, 485, 495, 497], "refer": [2, 3, 4, 20, 33, 36, 66, 93, 105, 112, 125, 145, 156, 171, 172, 178, 193, 214, 216, 217, 235, 237, 239, 240, 242, 246, 248, 260, 261, 268, 285, 288, 314, 321, 327, 332, 345, 354, 355, 394, 404, 472, 476, 482, 483, 485, 489, 492, 495, 497], "code": [2, 5, 281, 335, 336, 351, 401, 410, 439, 450, 454, 456, 464, 481, 482, 483, 484, 485, 488, 490, 491, 492, 493, 494, 495], "conduct": 2, "when": [2, 5, 7, 9, 10, 13, 15, 38, 40, 41, 46, 60, 61, 88, 92, 93, 94, 102, 112, 116, 177, 178, 193, 217, 231, 249, 266, 268, 271, 272, 274, 279, 289, 290, 306, 313, 320, 327, 333, 349, 354, 367, 371, 379, 400, 425, 427, 436, 439, 443, 450, 456, 468, 480, 483, 485, 488, 491, 492, 494, 496], "interact": [2, 3, 497], "tt": [3, 15, 45, 47, 62, 63, 74, 87, 96, 97, 105, 118, 124, 125, 157, 160, 161, 162, 163, 164, 200, 232, 284, 300, 301, 325, 330, 431, 432, 433, 434, 435, 436, 437, 438, 443, 451, 461, 463, 466, 467, 477, 478, 481, 486, 487, 489, 490, 493, 495], "smi": [3, 482], "The": [3, 4, 5, 7, 9, 13, 15, 18, 25, 26, 27, 28, 30, 36, 37, 38, 39, 40, 41, 42, 45, 46, 47, 55, 60, 61, 63, 64, 65, 68, 69, 71, 72, 73, 74, 78, 80, 86, 87, 92, 93, 94, 96, 97, 105, 111, 113, 115, 116, 117, 118, 119, 121, 122, 123, 124, 125, 126, 127, 139, 142, 145, 147, 150, 154, 155, 160, 162, 163, 164, 165, 166, 167, 168, 172, 173, 175, 177, 178, 179, 180, 191, 192, 200, 206, 216, 218, 219, 220, 222, 223, 224, 231, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 259, 262, 263, 266, 267, 268, 269, 271, 272, 273, 274, 275, 277, 281, 288, 289, 290, 291, 292, 295, 300, 301, 302, 303, 305, 312, 313, 315, 316, 317, 318, 319, 320, 325, 333, 335, 336, 342, 343, 344, 346, 349, 351, 354, 356, 357, 364, 365, 367, 368, 369, 370, 391, 392, 399, 400, 402, 407, 408, 412, 413, 415, 417, 418, 419, 422, 434, 438, 439, 442, 443, 445, 446, 447, 448, 449, 450, 451, 452, 453, 461, 463, 468, 469, 470, 471, 472, 473, 475, 477, 478, 479, 480, 481, 482, 483, 484, 485, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497], "system": [3, 117, 169, 176, 326, 337, 342, 343, 420, 482, 484, 492, 497], "interfac": [3, 17, 172, 493, 495], "command": [3, 162, 302, 422, 438, 450, 484], "line": 3, "provid": [3, 11, 12, 13, 16, 17, 18, 27, 28, 30, 36, 46, 55, 61, 64, 65, 68, 69, 71, 72, 73, 74, 88, 93, 94, 111, 113, 115, 116, 121, 126, 127, 154, 155, 165, 166, 167, 168, 172, 177, 178, 179, 180, 192, 202, 206, 215, 216, 218, 219, 220, 222, 223, 231, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 267, 268, 269, 272, 273, 275, 277, 279, 280, 289, 290, 291, 292, 295, 303, 312, 313, 315, 320, 321, 344, 349, 353, 354, 355, 364, 365, 368, 369, 370, 391, 400, 407, 408, 410, 412, 415, 417, 418, 419, 422, 432, 439, 443, 450, 452, 463, 470, 475, 480, 482, 483, 484, 485, 486, 491, 492, 493, 494, 495, 497], "wai": [3, 4, 279, 280, 401, 479, 482, 486], "collect": [3, 38, 39, 216, 217, 305, 354, 355, 422, 480, 484, 486, 492, 497], "telemetri": 3, "displai": [3, 491, 492], "firmwar": [3, 482, 489], "inform": [3, 93, 165, 331, 402, 482, 491, 492, 495, 497], "nn": [3, 45, 47, 63, 74, 87, 96, 97, 105, 118, 124, 125, 160, 161, 162, 163, 164, 177, 178, 200, 232, 279, 280, 300, 301, 325, 330, 434, 435, 436, 437, 438, 461, 463, 477, 478, 481, 486, 487, 489, 490, 493, 495], "analyz": [3, 484, 497], "insight": [3, 493, 495, 497], "through": [3, 491, 492, 493, 495, 497], "plot": [3, 497], "flow": [3, 172, 484, 491, 496, 497], "diagram": [3, 491], "instanc": [3, 497], "via": [3, 7, 447, 451, 482, 493, 495, 497], "ssh": [3, 497], "guid": [3, 74, 481, 482, 486, 488, 492, 497], "vllm": 3, "A": [3, 4, 5, 7, 27, 28, 30, 37, 40, 41, 45, 55, 64, 65, 68, 69, 71, 72, 73, 93, 111, 113, 115, 116, 117, 126, 127, 148, 154, 155, 160, 162, 163, 164, 165, 166, 167, 168, 179, 180, 192, 200, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 267, 268, 269, 272, 273, 275, 277, 289, 290, 291, 292, 295, 300, 301, 303, 312, 313, 315, 325, 344, 356, 364, 365, 393, 399, 400, 402, 407, 408, 412, 415, 417, 418, 419, 446, 448, 449, 453, 470, 475, 477, 478, 480, 482, 483, 485, 489, 491, 497], "high": [3, 149, 150, 325, 482, 485, 487, 490, 492, 497], "throughput": [3, 492], "effici": [3, 7, 10, 11, 12, 13, 14, 182, 369, 370, 392, 447, 451, 452, 482, 489, 490, 491, 492, 493], "serv": 3, "engin": [3, 196, 485, 492], "larg": [3, 7, 13, 370, 393, 479, 485, 491, 492, 496], "languag": [3, 491], "llm": 3, "traci": [3, 484, 492, 497], "design": [3, 4, 18, 368, 486, 487, 497], "deep": [3, 491, 492], "instruct": [3, 480, 481, 482, 484, 486], "built": [4, 482, 495], "feel": [4, 497], "familiar": 4, "experienc": 4, "includ": [4, 5, 7, 60, 61, 85, 172, 402, 443, 480, 485, 490, 491, 492, 493, 495, 496, 497], "than": [4, 7, 27, 28, 30, 55, 64, 65, 68, 69, 71, 72, 73, 76, 88, 109, 111, 113, 115, 116, 126, 127, 139, 148, 149, 150, 152, 154, 155, 166, 167, 168, 174, 177, 178, 179, 180, 181, 192, 200, 206, 218, 219, 220, 222, 223, 228, 229, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 264, 272, 277, 288, 289, 290, 291, 292, 295, 303, 312, 313, 315, 320, 344, 364, 365, 367, 368, 392, 407, 408, 415, 417, 418, 442, 463, 468, 475, 484, 491, 494], "200": 4, "fuse": [4, 7, 10, 11, 12, 13, 349, 369, 370, 479, 492, 494], "etc": [4, 5, 7, 485, 491, 495], "enabl": [4, 7, 18, 93, 375, 379, 425, 427, 439, 482, 483, 488, 490, 491, 492, 493, 494, 495, 497], "distribut": [4, 10, 11, 12, 13, 14, 36, 63, 102, 177, 216, 217, 231, 268, 283, 325, 354, 355, 367, 400, 402, 450, 451, 463, 485, 491, 492, 497], "abil": [4, 497], "regist": [4, 5, 335, 336], "custom": [4, 165, 172, 267, 335, 336, 357, 450, 482, 488, 491, 493, 494, 495, 496, 497], "nativ": [4, 27, 28, 30, 55, 64, 65, 68, 69, 71, 72, 73, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 407, 408, 415, 417, 418, 475, 485, 490, 491], "mesh": [4, 37, 38, 39, 40, 41, 74, 162, 274, 283, 333, 438, 451, 488, 490, 492, 493, 494, 495], "comput": [4, 5, 7, 10, 11, 12, 13, 14, 18, 19, 21, 23, 25, 29, 30, 31, 36, 48, 50, 54, 55, 56, 58, 60, 61, 63, 64, 66, 75, 76, 85, 88, 89, 92, 93, 94, 98, 100, 103, 104, 109, 113, 121, 123, 129, 131, 133, 135, 136, 140, 143, 152, 154, 155, 158, 165, 166, 172, 175, 178, 182, 192, 193, 194, 196, 206, 214, 215, 216, 217, 218, 219, 221, 226, 229, 231, 233, 234, 236, 238, 241, 243, 244, 246, 248, 249, 250, 252, 254, 255, 257, 258, 259, 265, 268, 269, 270, 272, 273, 275, 276, 277, 278, 285, 286, 287, 293, 295, 303, 310, 312, 316, 317, 318, 319, 320, 326, 331, 344, 349, 353, 354, 355, 360, 362, 364, 366, 368, 369, 370, 386, 388, 391, 392, 400, 403, 405, 407, 409, 412, 413, 414, 415, 416, 419, 423, 425, 440, 441, 446, 447, 448, 449, 450, 451, 453, 454, 459, 463, 469, 470, 471, 475, 476, 484, 489, 490, 491, 492, 493, 495, 496, 497], "significantli": [4, 7, 10, 18, 489, 492, 494], "speed": [4, 489, 492], "comparison": [4, 127, 167, 168, 179, 180, 206, 222, 223, 262, 263, 292, 483, 491, 492], "mode": [4, 53, 61, 66, 112, 115, 116, 129, 131, 135, 170, 171, 172, 177, 214, 233, 234, 236, 238, 271, 285, 289, 290, 327, 362, 375, 378, 379, 400, 403, 425, 427, 469, 488, 489, 490, 492, 493, 494, 495, 496, 497], "long": [4, 442, 443, 483], "sequenc": [4, 5, 40, 41, 305, 369, 370, 422, 442, 443, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455, 491, 492, 496], "against": [4, 165, 480, 497], "known": [4, 94], "document": [5, 178, 216, 217, 354, 355, 480, 482, 483, 484, 492, 497], "meant": [5, 7], "contributor": 5, "Not": [5, 60, 209, 292, 296, 325, 479], "mai": [5, 13, 14, 105, 112, 117, 193, 221, 268, 283, 327, 331, 350, 370, 392, 401, 425, 427, 434, 479, 485, 488, 490, 491, 492, 493, 494, 495, 496], "grayskul": [5, 47, 479, 496], "wormhol": [5, 47, 117, 479, 482, 484, 492], "take": [5, 7, 444, 480, 481, 485, 494, 495], "one": [5, 7, 10, 37, 40, 41, 82, 83, 84, 165, 177, 178, 307, 320, 352, 354, 356, 443, 447, 450, 451, 481, 485, 486, 490], "produc": [5, 25, 26, 93, 177, 267, 331, 425, 427, 480, 489, 492, 493], "call": [5, 7, 10, 20, 22, 24, 44, 49, 51, 57, 59, 77, 79, 82, 84, 91, 99, 101, 107, 110, 114, 120, 130, 132, 134, 137, 138, 144, 146, 151, 153, 159, 162, 171, 184, 186, 188, 190, 195, 199, 225, 230, 235, 237, 239, 240, 242, 260, 261, 288, 294, 309, 311, 314, 321, 324, 327, 329, 332, 335, 336, 340, 341, 347, 359, 361, 363, 374, 380, 382, 385, 387, 389, 394, 396, 398, 404, 406, 424, 426, 428, 430, 438, 443, 460, 483, 484, 485, 491, 492, 494, 497], "struct": [5, 491], "satisfi": [5, 439], "deviceoperationconcept": 5, "specifi": [5, 7, 10, 11, 12, 13, 14, 16, 36, 37, 38, 39, 40, 41, 45, 60, 80, 85, 88, 103, 104, 118, 121, 124, 125, 147, 148, 160, 163, 164, 165, 172, 200, 231, 267, 268, 269, 273, 274, 275, 279, 280, 283, 284, 300, 301, 304, 305, 306, 320, 325, 333, 346, 349, 350, 356, 357, 358, 371, 372, 391, 399, 400, 402, 410, 412, 419, 422, 432, 434, 445, 446, 448, 449, 452, 453, 457, 458, 463, 464, 468, 470, 477, 478, 479, 482, 484, 485, 489, 491, 492, 494, 496], "how": [5, 7, 10, 11, 12, 13, 14, 177, 178, 400, 402, 480, 484, 485, 487, 489, 490, 491, 492, 493, 494, 495, 496, 497], "simpli": [5, 436, 497], "invok": 5, "ttnn": [5, 482, 483, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 497], "register_oper": 5, "exist": [5, 148, 302, 482, 484, 492, 493, 495], "bind_registered_oper": 5, "auto": [5, 375, 491], "attach": [5, 279, 280], "attach_golden_funct": 5, "let": [5, 62, 485, 487, 488, 489, 490, 492, 493, 494, 495], "": [5, 7, 18, 40, 41, 85, 92, 93, 139, 172, 178, 215, 268, 269, 273, 275, 279, 280, 334, 353, 355, 356, 369, 371, 372, 412, 419, 442, 443, 445, 446, 448, 449, 450, 452, 453, 455, 461, 470, 473, 480, 482, 485, 489, 490, 491, 492, 493, 495, 496, 497], "order": [5, 102, 194, 196, 268, 311, 366, 399, 439, 450, 484, 485, 491, 494, 497], "follow": [5, 7, 36, 46, 63, 87, 93, 94, 96, 97, 117, 150, 200, 216, 217, 231, 268, 269, 273, 275, 281, 320, 350, 354, 355, 367, 368, 369, 370, 391, 392, 400, 412, 419, 435, 439, 461, 463, 470, 472, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 497], "directori": [5, 482, 486, 491, 496, 497], "structur": [5, 7, 479, 492, 493, 497], "shown": [5, 485, 487, 488], "below": [5, 268, 400, 457, 458, 480, 484, 485, 487, 488, 492], "cpp": [5, 488, 489, 490, 493, 494, 495, 497], "categori": [5, 491, 497], "operation_nam": 5, "_device_oper": 5, "hpp": 5, "program_factory_0": 5, "_program_factori": 5, "mani": [5, 10, 11, 12, 13, 14, 479, 489, 492], "factori": 5, "But": 5, "concret": [5, 178, 268], "found": [5, 367, 479, 484, 493, 495, 497], "example_device_oper": 5, "spdx": 5, "filecopyrighttext": 5, "2023": 5, "inc": 5, "licens": 5, "identifi": [5, 367, 451, 497], "apach": 5, "0": [5, 7, 15, 18, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 38, 40, 41, 45, 60, 61, 63, 68, 69, 71, 72, 73, 74, 79, 81, 82, 83, 84, 86, 87, 89, 93, 94, 103, 104, 108, 109, 113, 114, 120, 121, 122, 123, 128, 141, 148, 149, 150, 160, 161, 162, 163, 174, 177, 178, 181, 182, 183, 184, 189, 190, 196, 200, 201, 202, 206, 225, 226, 228, 229, 251, 252, 256, 261, 264, 266, 267, 269, 271, 273, 275, 281, 283, 296, 300, 301, 302, 304, 306, 307, 312, 314, 315, 320, 321, 322, 325, 327, 334, 339, 342, 343, 347, 348, 349, 350, 352, 357, 358, 361, 367, 368, 369, 370, 371, 372, 390, 393, 394, 395, 396, 400, 401, 402, 405, 412, 415, 417, 418, 419, 422, 429, 430, 432, 434, 438, 450, 454, 455, 456, 457, 458, 463, 468, 470, 472, 473, 474, 477, 478, 479, 482, 485, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497], "pragma": 5, "onc": [5, 316, 317, 318, 319, 443, 447, 451, 489, 491, 492, 493, 497], "variant": [5, 17, 32, 34, 178, 268, 442], "device_oper": 5, "decor": [5, 483], "namespac": 5, "exampledeviceoper": 5, "attribut": [5, 479, 484, 485], "store": [5, 7, 12, 36, 61, 165, 334, 461, 472, 484, 485, 492, 494], "variabl": [5, 443, 482, 491, 496, 497], "aren": [5, 9], "t": [5, 9, 40, 41, 121, 177, 178, 197, 268, 279, 280, 367, 447, 451, 455, 484, 485, 491, 492, 494, 496], "operation_attributes_t": 5, "bool": [5, 7, 25, 26, 28, 29, 31, 40, 46, 53, 60, 61, 89, 92, 93, 94, 96, 102, 103, 104, 105, 111, 112, 115, 116, 117, 129, 131, 135, 165, 170, 177, 178, 206, 214, 231, 233, 234, 236, 238, 268, 269, 271, 273, 275, 279, 280, 282, 285, 286, 289, 290, 304, 318, 319, 320, 350, 354, 355, 357, 362, 366, 368, 369, 370, 375, 378, 379, 391, 392, 399, 400, 402, 403, 412, 414, 416, 418, 419, 431, 432, 439, 440, 441, 445, 446, 448, 449, 452, 453, 454, 466, 467, 470, 474, 495], "int": [5, 18, 25, 26, 36, 37, 38, 39, 40, 41, 45, 46, 60, 74, 88, 89, 92, 93, 94, 102, 103, 104, 117, 122, 124, 145, 147, 149, 150, 157, 162, 164, 165, 169, 176, 178, 200, 201, 202, 203, 212, 213, 231, 266, 268, 271, 274, 282, 302, 304, 305, 310, 313, 316, 317, 318, 319, 320, 321, 325, 333, 337, 347, 358, 367, 371, 372, 375, 377, 378, 390, 391, 399, 400, 401, 402, 420, 422, 438, 440, 441, 442, 443, 446, 448, 449, 450, 451, 452, 453, 454, 457, 458, 468, 469, 485, 488, 495], "some_other_attribut": 5, "argument": [5, 7, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 88, 89, 90, 91, 92, 93, 94, 95, 98, 99, 100, 101, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 119, 120, 121, 122, 123, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 162, 165, 166, 167, 168, 169, 170, 171, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 204, 205, 206, 207, 208, 209, 210, 211, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 256, 257, 259, 260, 261, 262, 263, 264, 265, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 298, 299, 302, 303, 306, 308, 309, 310, 311, 312, 313, 314, 315, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 331, 332, 333, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 353, 354, 355, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 403, 404, 405, 406, 407, 408, 409, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 457, 458, 459, 460, 461, 462, 463, 466, 467, 469, 470, 471, 473, 474, 475, 476, 485], "pass": [5, 7, 10, 92, 142, 150, 335, 336, 443, 451, 454, 472, 479, 483, 485, 488, 491, 493, 495, 496], "don": [5, 268, 491, 492, 496], "thei": [5, 37, 178, 268, 480, 485, 489, 491, 494], "tensor_args_t": 5, "const": [5, 375], "input_tensor": [5, 18, 19, 20, 21, 22, 23, 24, 25, 26, 37, 38, 39, 40, 41, 42, 43, 44, 46, 48, 49, 50, 51, 53, 54, 57, 58, 59, 60, 61, 65, 66, 67, 70, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 88, 90, 91, 92, 94, 98, 99, 100, 101, 106, 107, 108, 109, 110, 112, 114, 116, 119, 120, 122, 123, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 151, 152, 153, 158, 159, 168, 169, 170, 171, 174, 175, 176, 177, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 194, 195, 196, 197, 198, 199, 202, 203, 204, 205, 207, 208, 209, 210, 211, 212, 220, 223, 224, 225, 228, 229, 230, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 245, 247, 252, 253, 259, 260, 261, 263, 264, 269, 271, 273, 274, 275, 278, 281, 287, 288, 290, 292, 293, 294, 296, 297, 298, 299, 304, 306, 307, 308, 309, 310, 311, 312, 313, 314, 321, 322, 323, 324, 326, 327, 328, 329, 331, 332, 333, 337, 338, 339, 340, 341, 342, 343, 346, 347, 348, 349, 350, 351, 352, 356, 357, 358, 359, 360, 361, 362, 363, 365, 367, 368, 369, 370, 373, 374, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 401, 403, 404, 405, 406, 408, 410, 411, 412, 413, 419, 420, 421, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 440, 441, 444, 454, 456, 457, 458, 459, 460, 462, 464, 466, 467, 469, 470, 471, 472, 488, 491, 495, 496], "howev": [5, 304, 492], "show": [5, 268, 485, 489, 492, 494, 496, 497], "els": [5, 38, 74, 94, 111, 115, 274, 333, 492, 493, 495, 496], "done": [5, 7, 177, 480, 482, 484, 491], "pre": [5, 25, 26, 60, 97, 117, 172, 178, 216, 217, 271, 336, 354, 355, 482, 491, 494, 495], "alloc": [5, 40, 41, 45, 97, 124, 125, 160, 163, 164, 172, 178, 300, 301, 325, 370, 392, 477, 478, 485, 488, 495, 496, 497], "io_tensor": [5, 172], "optional_output_tensor": [5, 36, 231, 268, 283, 367, 400], "vector_of_tensor": 5, "tupl": [5, 41, 92, 93, 94, 102, 271, 282, 304, 316, 317, 318, 319, 357, 399, 402, 454, 488, 495, 496], "tuple_of_tensor": 5, "vector_of_optional_tensor": 5, "some_crazy_tuple_of_tensor": 5, "return": [5, 9, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 162, 163, 164, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 281, 282, 283, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 357, 358, 359, 360, 361, 362, 363, 364, 365, 367, 368, 369, 370, 371, 372, 373, 374, 375, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 412, 413, 415, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 466, 467, 469, 470, 471, 472, 474, 475, 476, 477, 478, 479, 488, 490, 491, 492, 494, 495, 496], "spec": [5, 27, 28, 30, 55, 64, 65, 68, 69, 71, 72, 73, 96, 97, 102, 111, 113, 115, 116, 126, 127, 154, 155, 162, 166, 167, 168, 179, 180, 192, 206, 215, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 352, 364, 365, 407, 408, 415, 417, 418, 475], "singl": [5, 177, 202, 267, 269, 273, 275, 349, 401, 412, 419, 446, 448, 449, 452, 453, 470, 479, 480, 483, 484, 485], "tensorspec": [5, 162], "spec_return_value_t": 5, "tensor_return_value_t": 5, "note": [5, 7, 10, 13, 91, 165, 177, 178, 199, 268, 300, 301, 329, 350, 450, 469, 472, 482, 484, 485, 491, 492, 494, 497], "should": [5, 7, 10, 13, 25, 26, 36, 60, 94, 96, 117, 121, 139, 165, 169, 176, 177, 178, 197, 206, 216, 217, 226, 231, 268, 271, 297, 304, 313, 326, 334, 337, 354, 355, 371, 372, 420, 439, 450, 480, 483, 484, 487, 491, 492, 495, 497], "same": [5, 7, 27, 28, 30, 32, 34, 37, 39, 40, 41, 55, 61, 62, 63, 64, 65, 67, 68, 69, 71, 72, 73, 87, 92, 94, 96, 97, 103, 104, 111, 113, 115, 116, 121, 123, 125, 126, 127, 139, 145, 149, 154, 164, 165, 166, 167, 168, 172, 177, 179, 180, 192, 197, 202, 203, 206, 215, 216, 218, 219, 220, 222, 223, 226, 231, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 265, 268, 269, 272, 273, 275, 277, 279, 280, 284, 289, 290, 291, 292, 295, 297, 301, 303, 312, 313, 316, 317, 318, 319, 320, 333, 349, 351, 354, 356, 357, 364, 365, 368, 369, 370, 371, 376, 377, 391, 392, 407, 408, 412, 415, 417, 418, 419, 436, 439, 452, 461, 470, 472, 475, 478, 484, 485, 490, 492, 493, 494, 497], "pattern": [5, 7, 10, 11, 12, 13, 67, 88, 279, 368, 455, 483, 493, 495], "e": [5, 7, 13, 40, 41, 46, 135, 143, 178, 215, 267, 268, 269, 273, 275, 281, 320, 367, 368, 371, 391, 392, 400, 412, 419, 470, 471, 482, 484, 485, 491, 492, 493, 495, 497], "singlecor": 5, "share": [5, 10, 11, 12, 13, 14, 485, 491], "override_runtime_argu": 5, "shared_variables_t": 5, "tt_metal": [5, 451, 482, 497], "kernelhandl": 5, "unary_reader_kernel_id": 5, "unary_writer_kernel_id": 5, "cached_program_t": 5, "cachedprogram": 5, "static": 5, "operation_attribut": 5, "tensor_arg": 5, "tensor_return_valu": 5, "void": 5, "cached_program": 5, "multicor": [5, 304, 367, 431, 432, 433, 439, 466, 467], "size_t": 5, "num_cor": [5, 7, 216, 402], "num_cores_i": 5, "program_factory_t": 5, "mandatori": 5, "select": [5, 7, 16, 38, 40, 41, 61, 121, 178, 283, 333, 350, 367, 457, 458, 473, 483, 493], "arg": [5, 150, 297, 307, 316, 317, 318, 319, 334, 335, 336, 402, 491, 492, 495, 496], "select_program_factori": 5, "valid": [5, 7, 41, 62, 74, 95, 148, 149, 150, 178, 196, 268, 279, 280, 281, 284, 439, 479, 480, 484, 485, 489], "usual": [5, 496], "validate_on_program_cache_miss": 5, "reus": [5, 7, 14, 316, 317, 318, 319, 370, 443, 491, 497], "less": [5, 152, 200, 222, 223, 228, 262, 263, 264, 320, 367, 402, 431, 463, 468, 484], "validate_on_program_cache_hit": 5, "compute_output_spec": 5, "create_output_tensor": 5, "prim": 5, "tensor_op": 5, "some_condition_based_on_operation_attributes_and_or_tensor_arg": 5, "true": [5, 7, 10, 13, 20, 22, 24, 25, 26, 29, 31, 33, 35, 46, 49, 51, 53, 56, 57, 59, 60, 61, 66, 77, 79, 82, 84, 89, 92, 93, 94, 96, 99, 101, 102, 105, 107, 110, 111, 112, 114, 115, 116, 117, 120, 123, 129, 130, 131, 132, 134, 135, 137, 138, 142, 144, 146, 151, 153, 156, 159, 165, 170, 171, 177, 178, 184, 186, 188, 190, 193, 195, 221, 225, 227, 230, 233, 234, 235, 236, 237, 238, 239, 240, 242, 246, 248, 260, 261, 268, 269, 270, 271, 273, 275, 276, 286, 288, 289, 290, 294, 311, 314, 320, 321, 324, 327, 332, 340, 341, 345, 347, 350, 357, 359, 361, 362, 363, 366, 369, 374, 375, 378, 380, 382, 385, 387, 389, 391, 392, 394, 396, 398, 399, 400, 403, 404, 406, 409, 412, 414, 416, 419, 424, 425, 426, 427, 428, 430, 431, 432, 439, 445, 446, 448, 449, 452, 453, 454, 460, 466, 467, 470, 474, 476, 479, 485, 491, 492, 493, 495, 497], "logical_shap": 5, "tensorlayout": 5, "dtype": [5, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 141, 142, 143, 144, 145, 146, 147, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 204, 205, 206, 207, 208, 209, 210, 211, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 267, 268, 269, 270, 271, 272, 273, 275, 276, 277, 278, 281, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 303, 304, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 334, 337, 338, 339, 340, 341, 342, 343, 344, 345, 347, 348, 349, 350, 351, 353, 354, 355, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 443, 457, 458, 459, 460, 461, 462, 463, 464, 466, 467, 468, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 485, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496], "pageconfig": 5, "memoryconfig": [5, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 88, 89, 90, 91, 92, 93, 98, 99, 100, 101, 102, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 119, 120, 121, 122, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 162, 163, 164, 165, 166, 167, 169, 170, 171, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 204, 205, 206, 207, 208, 209, 210, 211, 214, 215, 216, 217, 218, 219, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 246, 248, 249, 251, 252, 253, 254, 256, 257, 259, 260, 261, 262, 264, 265, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 282, 283, 284, 285, 286, 287, 288, 289, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 303, 304, 306, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 352, 353, 354, 355, 357, 358, 359, 360, 361, 362, 363, 364, 366, 368, 369, 370, 371, 372, 373, 374, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 391, 393, 394, 395, 396, 397, 398, 399, 400, 403, 404, 405, 406, 407, 409, 412, 413, 414, 415, 416, 417, 419, 420, 421, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 434, 436, 437, 439, 440, 441, 442, 443, 445, 446, 448, 449, 450, 452, 453, 454, 455, 457, 458, 459, 460, 461, 462, 463, 466, 467, 469, 470, 471, 473, 474, 475, 476, 477, 478], "output_spec": 5, "create_device_tensor": 5, "operationtyp": 5, "42": [5, 108, 267, 492, 493], "launch": [5, 497], "single_core_program_factori": 5, "work_split": 5, "tensor_accessor_arg": 5, "output_tensor": [5, 19, 21, 23, 27, 30, 32, 34, 38, 40, 41, 42, 46, 48, 50, 54, 62, 64, 67, 68, 69, 70, 71, 72, 73, 75, 76, 78, 81, 88, 98, 106, 111, 115, 122, 123, 126, 128, 129, 131, 133, 135, 136, 138, 141, 143, 145, 146, 152, 158, 163, 164, 166, 167, 170, 171, 174, 178, 179, 181, 182, 185, 187, 191, 194, 196, 197, 201, 207, 208, 209, 210, 211, 214, 218, 219, 222, 224, 228, 233, 234, 236, 238, 241, 243, 244, 249, 251, 252, 254, 256, 257, 262, 264, 272, 277, 278, 281, 285, 289, 291, 293, 294, 296, 301, 307, 313, 314, 323, 326, 331, 333, 334, 338, 339, 342, 343, 351, 358, 360, 362, 363, 364, 378, 379, 381, 383, 384, 385, 386, 391, 393, 397, 401, 403, 404, 405, 407, 410, 415, 417, 421, 423, 425, 426, 427, 437, 439, 456, 459, 461, 462, 464, 473, 474, 475, 478, 492], "src_buffer": 5, "dst_buffer": 5, "dataformat": 5, "cb_data_format": 5, "datatype_to_dataformat_convert": 5, "uint32_t": [5, 141, 267, 442, 445, 450], "single_tile_s": 5, "tile_s": [5, 178], "cb_data_format_output": 5, "single_tile_size_output": 5, "num_til": 5, "physical_volum": 5, "constant": [5, 61, 178, 215, 216, 217, 353, 354, 355, 485], "tile_hw": 5, "corecoord": [5, 18, 202, 352, 368, 370, 400, 402, 451], "y": [5, 13, 14, 18, 30, 52, 55, 62, 95, 177, 178, 192, 219, 243, 244, 284, 295, 308, 351, 357, 368, 369, 370, 402, 407, 415, 475, 476, 484, 485, 489, 492, 494], "all_cor": [5, 402], "core_group_1": [5, 402], "core_group_2": [5, 402], "num_tiles_per_core_group_1": 5, "num_tiles_per_core_group_2": 5, "src0_cb_index": 5, "cbindex": 5, "c_0": 5, "num_input_til": 5, "circularbufferconfig": 5, "cb_src0_config": 5, "set_page_s": 5, "createcircularbuff": 5, "output_cb_index": 5, "c_2": 5, "num_output_til": 5, "cb_output_config": 5, "reader_compile_time_arg": 5, "tensoraccessorarg": 5, "append_to": 5, "writer_compile_time_arg": 5, "createkernel": 5, "eltwis": [5, 155, 315, 344, 487], "kernel": [5, 7, 25, 26, 36, 60, 61, 63, 85, 92, 93, 94, 121, 140, 142, 172, 178, 216, 217, 231, 268, 269, 271, 273, 275, 316, 317, 318, 319, 349, 354, 355, 368, 369, 370, 391, 392, 400, 412, 419, 443, 450, 452, 463, 469, 470, 482, 484, 488, 489, 491, 492, 495, 497], "dataflow": 5, "reader_unary_interleaved_start_id": 5, "readerdatamovementconfig": 5, "writer_unary_interleaved_start_id": 5, "writerdatamovementconfig": 5, "compute_kernel_args_group_1": 5, "per_core_block_cnt": 5, "per_core_block_s": 5, "math_approx_mod": [5, 492], "fals": [5, 7, 10, 25, 26, 40, 46, 60, 61, 92, 93, 94, 102, 103, 104, 111, 115, 116, 129, 131, 135, 165, 170, 177, 178, 206, 231, 233, 234, 236, 238, 249, 268, 269, 271, 273, 275, 289, 290, 320, 357, 362, 368, 369, 370, 375, 378, 379, 399, 400, 402, 403, 412, 419, 425, 427, 431, 440, 441, 462, 470, 479, 491, 492, 493, 494, 495, 496, 497], "eltwise_sfpu": 5, "computeconfig": 5, "math_fidel": [5, 492], "mathfidel": [5, 492], "hifi4": [5, 484, 492], "compile_arg": 5, "rang": [5, 27, 28, 38, 45, 62, 68, 69, 70, 71, 72, 73, 74, 95, 100, 148, 149, 150, 165, 166, 177, 182, 195, 196, 218, 221, 236, 251, 252, 256, 267, 272, 278, 284, 287, 310, 325, 350, 358, 367, 388, 402, 417, 418, 423, 424, 439, 485, 489, 491, 492], "compute_kernel_args_group_2": 5, "num_tiles_written": 5, "num_tiles_per_cor": 5, "contain": [5, 7, 37, 40, 41, 45, 46, 60, 103, 104, 117, 121, 122, 165, 172, 177, 178, 200, 271, 283, 307, 325, 331, 367, 371, 372, 399, 400, 402, 473, 482, 485, 490, 491, 495, 497], "tt_assert": 5, "setruntimearg": 5, "address": [5, 117, 489, 497], "shared_vari": 5, "runtime_arg": 5, "getruntimearg": 5, "multi_core_program_factori": 5, "idevic": [5, 316, 317], "composit": [5, 38, 333, 349, 492], "primit": 5, "compositeexampleoper": 5, "abl": [5, 178], "composite_exampl": 5, "after": [5, 7, 18, 60, 96, 97, 140, 161, 271, 295, 304, 335, 347, 375, 392, 434, 435, 436, 437, 461, 483, 484, 485, 488, 491, 492, 493, 495, 497], "op": [5, 7, 9, 10, 25, 26, 27, 28, 30, 46, 55, 60, 64, 65, 68, 69, 71, 72, 73, 92, 93, 94, 102, 111, 113, 115, 116, 123, 126, 127, 154, 155, 166, 167, 168, 178, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 271, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 367, 407, 408, 415, 417, 418, 422, 439, 445, 446, 447, 448, 449, 451, 452, 453, 462, 475, 483, 484, 488, 497], "another_copi": 5, "constexpr": 5, "_nanobind": 5, "example_nanobind": 5, "2025": [5, 487, 488, 489, 490, 493, 494, 495, 497], "ai": [5, 482, 488, 490, 491, 492, 493, 494, 495], "ulc": 5, "nanobind": [5, 9, 15, 173, 302, 305, 375, 402, 422], "nanobind_fwd": 5, "nb": 5, "bind_example_oper": 5, "module_": 5, "mod": 5, "examples_nanobind": 5, "py_modul": 5, "final": [5, 36, 216, 268, 354, 367, 451, 479, 480, 483, 491, 493, 495, 497], "wherev": 5, "want": [5, 410, 464, 482, 489, 496], "compar": [5, 126, 167, 178, 179, 222, 262, 268, 291, 367, 489, 491, 493, 495], "its": [5, 7, 85, 92, 93, 94, 105, 118, 125, 232, 268, 272, 304, 354, 390, 436, 437, 479, 480, 483, 484, 485, 491, 492, 493, 495, 496], "equival": [5, 27, 28, 30, 41, 55, 64, 65, 68, 69, 71, 72, 73, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 179, 180, 192, 206, 216, 217, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 281, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 351, 354, 355, 364, 365, 401, 407, 408, 410, 415, 417, 418, 439, 454, 456, 464, 475, 485, 486, 489, 492], "torch": [5, 19, 20, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 44, 47, 49, 51, 53, 55, 56, 57, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 81, 82, 83, 84, 89, 90, 91, 99, 101, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 119, 120, 122, 123, 126, 127, 129, 130, 131, 132, 134, 135, 137, 138, 139, 141, 142, 144, 145, 146, 147, 149, 150, 151, 153, 154, 155, 156, 159, 162, 166, 167, 168, 170, 171, 177, 178, 179, 180, 182, 183, 184, 186, 188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 201, 204, 205, 206, 214, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 230, 233, 234, 235, 236, 237, 238, 239, 240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 267, 270, 271, 272, 276, 277, 279, 280, 281, 285, 286, 287, 288, 289, 290, 291, 292, 294, 295, 297, 303, 307, 308, 309, 310, 311, 312, 313, 314, 315, 321, 322, 324, 326, 327, 328, 329, 331, 332, 334, 340, 341, 342, 343, 344, 345, 346, 347, 349, 350, 351, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 371, 372, 373, 374, 378, 379, 380, 382, 385, 387, 389, 393, 394, 395, 396, 398, 399, 400, 401, 403, 404, 406, 407, 408, 409, 410, 411, 413, 414, 415, 416, 417, 418, 424, 425, 426, 427, 428, 429, 430, 434, 438, 439, 454, 456, 457, 458, 460, 462, 464, 468, 471, 472, 473, 474, 475, 476, 479, 485, 488, 490, 491, 492, 493, 494, 495, 496], "signatur": 5, "keep": [5, 46, 269, 273, 275, 320, 412, 419, 457, 458, 470, 479, 485, 491, 492], "mind": [5, 494], "And": [5, 479, 485, 489], "ignor": [5, 36, 367, 410, 491], "kwarg": [5, 316, 317, 318, 319, 335, 336], "def": [5, 479, 488, 490, 491, 492, 494, 495, 496], "golden_funct": 5, "befor": [5, 7, 47, 103, 104, 304, 316, 317, 318, 319, 336, 369, 392, 393, 451, 480, 485, 491, 492], "automat": [5, 7, 16, 25, 26, 38, 40, 41, 93, 266, 333, 436, 450, 469, 480, 482, 484, 485, 489, 492], "some": [5, 297, 400, 422, 482, 492, 496], "case": [5, 16, 111, 115, 160, 162, 178, 206, 268, 295, 315, 353, 400, 436, 439, 479, 480, 485, 492, 495, 497], "postprocess": 5, "pack": [5, 177, 466, 467, 496], "preprocess_golden_function_input": 5, "ttnn_input_tensor": 5, "postprocess_golden_function_output": 5, "torch_output_tensor": [5, 178], "name": [5, 47, 118, 232, 279, 280, 452, 479, 483, 484, 491, 492, 495, 497], "becaus": [5, 485, 489, 492, 494], "wa": [5, 40, 41, 178, 472, 480, 484, 485, 491, 496], "good": [5, 480, 489, 491, 492], "practic": [5, 491, 492, 496], "demonstr": [5, 177, 480, 484, 487, 489, 490, 491, 492, 493, 495, 496], "simplest": [5, 482], "approach": [5, 393, 479, 483, 492], "make": [5, 279, 280, 392, 393, 439, 454, 479, 485, 497], "difficult": 5, "date": 5, "prevent": [5, 7, 482, 491], "snippet": [5, 493], "being": [5, 177, 320, 335, 336, 402, 484, 485], "better": [5, 14, 116, 177, 289, 290, 368, 378, 491, 492, 496, 497], "place": [5, 18, 28, 61, 65, 95, 116, 127, 147, 168, 180, 212, 213, 220, 223, 245, 247, 253, 263, 290, 292, 356, 358, 365, 368, 370, 392, 408, 418, 434, 441, 463, 468, 485], "examples_map": 5, "py": [5, 172, 479, 480, 484, 486, 487, 488, 489, 490, 493, 494, 495, 497], "map": [5, 40, 41, 47, 177, 267, 282, 283, 350, 442, 443, 469, 491], "appear": [5, 279, 280, 497], "function_to_examples_mapping_dict": 5, "dictionari": [5, 479, 491, 495], "test_exampl": 5, "test_example_exampl": 5, "appropri": [5, 16, 27, 28, 30, 55, 64, 65, 68, 69, 71, 72, 73, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 178, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 268, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 407, 408, 415, 417, 418, 475, 480, 482, 491, 495, 496], "sure": [5, 439], "top": [5, 281, 367, 439], "test_data_movement_exampl": 5, "data_mov": 5, "test_core_exampl": 5, "pytest": [5, 479, 480, 484, 497], "bfloat16": [5, 7, 16, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 93, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 141, 142, 143, 144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 161, 162, 163, 164, 165, 167, 168, 169, 170, 171, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 204, 205, 206, 207, 208, 209, 210, 211, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 252, 253, 254, 255, 257, 258, 259, 260, 261, 262, 263, 264, 265, 268, 269, 270, 271, 272, 273, 275, 276, 277, 278, 281, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 303, 304, 306, 308, 309, 310, 311, 312, 313, 314, 315, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 337, 338, 339, 340, 341, 342, 343, 344, 345, 347, 348, 349, 350, 351, 353, 354, 355, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 457, 458, 459, 460, 461, 462, 463, 464, 466, 467, 468, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 484, 485, 488, 489, 490, 491, 492, 493, 494, 495, 496], "row_major_layout": [5, 46, 47, 60, 85, 87, 97, 122, 160, 162, 163, 178, 200, 201, 267, 271, 300, 325, 354, 355, 367, 371, 372, 431, 432, 436, 477, 485, 488, 489, 491, 492, 494, 495], "ensur": [5, 11, 12, 85, 200, 399, 422, 480, 482, 485, 488, 491, 492, 495, 496, 497], "ci": [5, 480], "class": [7, 10, 11, 12, 13, 14, 16, 17, 18, 479, 480, 484, 485, 491, 492, 493], "object": [7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 36, 102, 117, 173, 302, 305, 375, 402, 422, 479, 483, 485, 491], "specif": [7, 18, 46, 69, 70, 72, 78, 85, 93, 109, 119, 129, 131, 135, 145, 148, 165, 170, 172, 182, 183, 189, 191, 196, 197, 224, 233, 234, 236, 238, 251, 259, 267, 268, 287, 302, 310, 321, 326, 331, 342, 343, 358, 362, 367, 368, 373, 378, 379, 393, 395, 399, 403, 425, 427, 429, 446, 448, 449, 453, 455, 480, 485, 486, 491, 492, 495, 496, 497], "flag": [7, 177, 482], "properti": [7, 10, 11, 12, 13, 14, 18, 485, 491, 492], "size": [7, 10, 13, 14, 18, 25, 26, 27, 28, 30, 40, 41, 45, 55, 60, 64, 65, 67, 68, 69, 71, 72, 73, 92, 93, 94, 103, 104, 111, 113, 115, 116, 121, 122, 123, 126, 127, 139, 154, 155, 165, 166, 167, 168, 178, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 268, 269, 271, 272, 273, 275, 277, 282, 289, 290, 291, 292, 295, 302, 303, 312, 313, 315, 316, 317, 318, 319, 320, 344, 351, 364, 365, 370, 390, 401, 402, 407, 408, 410, 412, 415, 417, 418, 419, 443, 445, 446, 448, 449, 450, 452, 453, 454, 455, 464, 469, 470, 475, 485, 487, 489, 491, 496], "block": [7, 10, 11, 12, 13, 14, 18, 27, 28, 30, 55, 60, 63, 64, 65, 68, 69, 71, 72, 73, 87, 92, 93, 94, 96, 97, 102, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 178, 179, 180, 192, 202, 203, 206, 215, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 266, 268, 271, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 316, 317, 318, 319, 331, 344, 352, 353, 364, 365, 370, 407, 408, 415, 417, 418, 435, 443, 450, 455, 461, 463, 475, 485, 491, 492], "height": [7, 10, 13, 14, 18, 25, 26, 27, 28, 30, 55, 60, 63, 64, 65, 68, 69, 71, 72, 73, 85, 87, 88, 92, 93, 94, 96, 97, 102, 111, 113, 115, 116, 126, 127, 148, 149, 150, 154, 155, 163, 166, 167, 168, 175, 178, 179, 180, 192, 202, 203, 206, 215, 216, 217, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 268, 269, 271, 272, 273, 275, 277, 289, 290, 291, 292, 295, 299, 300, 303, 312, 313, 315, 316, 317, 318, 319, 344, 353, 354, 355, 357, 364, 365, 368, 391, 407, 408, 412, 413, 415, 417, 418, 419, 433, 435, 436, 461, 463, 470, 471, 475, 477, 484, 485, 488, 491, 492, 495], "avail": [7, 14, 469, 486, 489, 491, 492, 497], "among": [7, 40, 41], "also": [7, 27, 28, 30, 36, 55, 64, 65, 68, 69, 71, 72, 73, 92, 93, 94, 103, 104, 111, 113, 115, 116, 121, 126, 127, 154, 155, 166, 167, 168, 179, 180, 192, 206, 216, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 288, 289, 290, 291, 292, 295, 303, 312, 313, 315, 320, 344, 353, 354, 364, 365, 368, 407, 408, 415, 417, 418, 475, 480, 484, 485, 486, 489, 491, 492, 496, 497], "further": [7, 489, 492], "subdivid": 7, "within": [7, 10, 13, 14, 45, 60, 69, 72, 206, 251, 256, 271, 333, 367, 480, 485, 496, 497], "possibl": [7, 268, 438, 443, 479], "which": [7, 10, 13, 36, 38, 40, 41, 52, 60, 62, 67, 69, 72, 80, 93, 102, 103, 104, 117, 160, 163, 164, 165, 178, 200, 231, 251, 256, 267, 268, 269, 271, 273, 275, 279, 280, 283, 300, 301, 325, 326, 333, 356, 367, 371, 372, 390, 391, 399, 400, 411, 412, 419, 445, 446, 448, 449, 451, 452, 453, 470, 477, 478, 479, 484, 485, 487, 488, 489, 491, 495, 496, 497], "equal": [7, 76, 95, 126, 127, 128, 152, 167, 168, 174, 206, 215, 216, 217, 222, 223, 228, 291, 292, 296, 354, 355, 367, 369, 370, 399, 491], "output_matrix_height_per_cor": 7, "lead": [7, 117], "temporari": 7, "circular": [7, 117, 356, 392, 497], "oom": 7, "must": [7, 11, 12, 13, 14, 18, 27, 28, 30, 42, 46, 52, 55, 61, 62, 63, 64, 65, 67, 68, 69, 71, 72, 73, 87, 92, 95, 96, 97, 103, 104, 111, 113, 115, 116, 121, 123, 126, 127, 139, 140, 154, 155, 157, 165, 166, 167, 168, 172, 177, 178, 179, 180, 192, 200, 206, 215, 216, 217, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 267, 268, 269, 272, 273, 275, 277, 281, 288, 289, 290, 291, 292, 295, 303, 312, 313, 314, 315, 316, 317, 320, 326, 344, 350, 351, 353, 354, 355, 356, 364, 365, 367, 368, 369, 370, 371, 372, 376, 390, 391, 392, 399, 400, 407, 408, 412, 415, 417, 418, 419, 431, 432, 433, 434, 435, 439, 442, 443, 447, 450, 451, 461, 463, 466, 467, 470, 472, 473, 475, 482, 485, 489, 491, 497], "32": [7, 14, 16, 18, 27, 28, 30, 36, 37, 38, 39, 42, 43, 44, 46, 55, 64, 65, 67, 68, 69, 71, 72, 73, 74, 85, 88, 89, 90, 91, 97, 111, 113, 115, 116, 126, 127, 140, 150, 154, 155, 163, 166, 167, 168, 169, 175, 176, 177, 178, 179, 180, 192, 198, 199, 200, 201, 202, 204, 205, 206, 215, 216, 217, 218, 219, 220, 222, 223, 231, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 268, 272, 277, 281, 283, 289, 290, 291, 292, 295, 297, 298, 299, 300, 303, 306, 308, 309, 312, 313, 315, 321, 328, 329, 333, 334, 337, 344, 347, 353, 354, 355, 357, 364, 365, 367, 370, 390, 391, 392, 400, 407, 408, 415, 417, 418, 420, 431, 432, 434, 435, 436, 439, 461, 462, 466, 467, 468, 475, 477, 485, 487, 489, 490, 491, 492, 493, 495, 496], "evenli": [7, 13, 14, 45, 178, 401, 402], "reduc": [7, 10, 14, 39, 40, 46, 62, 178, 269, 273, 275, 282, 320, 333, 334, 371, 391, 412, 419, 439, 450, 470, 492], "width": [7, 10, 11, 12, 13, 14, 18, 25, 26, 27, 28, 30, 55, 60, 63, 64, 65, 68, 69, 71, 72, 73, 85, 87, 92, 93, 94, 96, 97, 102, 111, 113, 115, 116, 126, 127, 148, 149, 150, 154, 155, 163, 166, 167, 168, 175, 177, 178, 179, 180, 192, 202, 203, 206, 215, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 268, 269, 271, 272, 273, 275, 277, 289, 290, 291, 292, 295, 299, 300, 303, 312, 313, 315, 316, 317, 318, 319, 344, 353, 357, 364, 365, 368, 369, 370, 407, 408, 412, 413, 415, 417, 418, 419, 433, 435, 436, 439, 444, 461, 463, 470, 471, 475, 477, 484, 488, 491, 492, 495], "greater": [7, 76, 88, 109, 148, 149, 150, 167, 168, 174, 179, 180, 181, 229, 288], "n150": [7, 492], "thats": 7, "64": [7, 16, 25, 26, 36, 42, 46, 88, 97, 102, 121, 149, 150, 169, 176, 178, 215, 231, 268, 281, 306, 337, 353, 357, 367, 390, 391, 400, 413, 420, 431, 432, 434, 435, 439, 466, 467, 471, 485, 491, 492, 493, 494, 495, 496], "2048": [7, 492, 495], "divisor": [7, 13, 14, 60, 166, 327], "halv": 7, "appli": [7, 10, 11, 12, 13, 18, 19, 21, 23, 25, 26, 27, 28, 42, 48, 50, 52, 54, 60, 61, 64, 69, 70, 72, 75, 76, 78, 81, 85, 92, 93, 94, 98, 103, 104, 106, 119, 121, 126, 128, 129, 131, 133, 135, 136, 140, 141, 142, 143, 145, 152, 158, 166, 167, 169, 170, 174, 175, 176, 178, 179, 181, 182, 183, 185, 187, 189, 191, 194, 196, 197, 207, 208, 209, 210, 211, 215, 216, 217, 218, 219, 222, 224, 228, 231, 233, 234, 236, 238, 241, 243, 244, 249, 252, 253, 254, 257, 262, 264, 268, 269, 271, 272, 273, 275, 277, 278, 291, 293, 296, 304, 315, 316, 317, 318, 319, 320, 323, 331, 337, 338, 339, 342, 343, 353, 354, 355, 358, 360, 362, 364, 367, 368, 369, 370, 371, 373, 378, 379, 381, 383, 384, 386, 391, 392, 393, 395, 397, 403, 405, 407, 412, 417, 418, 419, 420, 421, 423, 425, 427, 429, 439, 459, 462, 470, 475, 480, 491, 492, 493, 495, 496], "none": [7, 15, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 256, 257, 259, 260, 261, 262, 263, 264, 265, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 303, 304, 306, 308, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 330, 331, 332, 333, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 473, 474, 475, 476, 477, 478, 491, 492, 496], "unarywithparam": [7, 231, 268, 462, 495], "unaryoptyp": [7, 462, 495], "boolean": [7, 204, 205, 400, 425, 427, 439], "determin": [7, 10, 11, 12, 13, 14, 38, 40, 41, 62, 94, 178, 268, 279, 280, 333, 400, 485, 495], "them": [7, 283, 454, 482, 484, 487, 491], "dram": [7, 11, 12, 27, 28, 30, 46, 55, 61, 62, 63, 64, 65, 68, 69, 71, 72, 73, 87, 88, 93, 94, 96, 97, 103, 104, 111, 113, 115, 116, 117, 121, 126, 127, 154, 155, 165, 166, 167, 168, 178, 179, 180, 192, 200, 206, 215, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 268, 269, 272, 273, 275, 277, 281, 289, 290, 291, 292, 295, 303, 312, 313, 315, 320, 344, 350, 353, 364, 365, 367, 391, 399, 400, 407, 408, 412, 415, 417, 418, 419, 434, 435, 437, 439, 461, 463, 470, 475, 485, 491, 492, 497], "l1_small": 7, "2d": [7, 10, 13, 14, 92, 93, 94, 175, 268, 354, 355, 469, 485, 488, 491, 495], "instead": [7, 60, 88, 113, 178, 197, 202, 231, 268, 320, 343, 455, 494], "risc": [7, 484], "grid": [7, 10, 13, 14, 18, 28, 36, 38, 65, 88, 116, 121, 127, 129, 131, 135, 152, 155, 168, 170, 177, 178, 180, 202, 203, 220, 223, 231, 233, 234, 236, 238, 245, 247, 263, 264, 268, 269, 273, 275, 290, 292, 344, 350, 354, 355, 362, 365, 390, 400, 402, 403, 408, 412, 418, 419, 445, 446, 448, 449, 451, 452, 453, 459, 466, 470, 473, 479, 485, 495], "indic": [7, 41, 46, 122, 123, 139, 165, 177, 200, 271, 297, 367, 371, 372, 390, 399, 400, 439, 442, 443, 467, 491, 496, 497], "whether": [7, 10, 13, 25, 26, 40, 46, 60, 96, 103, 104, 105, 177, 231, 268, 269, 271, 273, 275, 279, 280, 318, 319, 368, 369, 370, 391, 392, 400, 402, 412, 419, 431, 432, 433, 439, 446, 448, 449, 453, 454, 466, 467, 470, 495], "conv": [7, 488, 495], "halo": [7, 60, 271], "micro": 7, "anoth": [7, 125, 164, 301, 307, 352, 478, 482, 485], "ha": [7, 36, 37, 69, 72, 94, 177, 251, 256, 268, 274, 279, 280, 318, 319, 422, 454, 468, 479, 480, 484, 485, 486, 491, 496, 497], "effect": 7, "doubl": 7, "allow": [7, 10, 13, 165, 172, 267, 268, 320, 442, 480, 483, 492, 493], "stall": 7, "reader": [7, 117, 279, 469], "improv": [7, 10, 13, 182, 393, 480, 492, 497], "increas": [7, 13, 497], "consecut": [7, 45, 485], "boost": 7, "image2column": 7, "so": [7, 150, 178, 479, 491], "bound": [7, 81, 83, 177, 259, 325, 463], "adjust": [7, 85, 393, 497], "stride": [7, 25, 26, 60, 92, 93, 94, 139, 157, 271, 316, 317, 318, 319, 488, 491, 495], "condit": [7, 350, 472, 473, 480], "forc": [7, 105, 375], "disabl": [7, 279, 280, 375, 488, 489, 490, 493, 494, 495, 496, 497], "nhwc": [7, 357, 488, 495], "format": [7, 25, 26, 27, 28, 30, 40, 41, 42, 47, 55, 60, 64, 65, 68, 69, 71, 72, 73, 92, 93, 94, 111, 113, 115, 116, 117, 126, 127, 140, 154, 155, 162, 166, 167, 168, 177, 178, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 271, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 316, 317, 318, 319, 331, 344, 357, 364, 365, 399, 407, 408, 415, 417, 418, 475, 483, 485, 488, 491, 492, 493, 495, 496], "n": [7, 10, 11, 12, 13, 14, 25, 26, 36, 60, 61, 88, 92, 93, 94, 103, 104, 140, 149, 150, 157, 177, 178, 269, 271, 273, 275, 281, 297, 311, 312, 353, 354, 355, 357, 367, 400, 412, 413, 419, 439, 447, 451, 469, 470, 471, 482, 484, 489, 490, 491, 492, 495], "h": [7, 25, 26, 40, 41, 60, 61, 62, 92, 93, 94, 140, 149, 150, 157, 177, 178, 271, 281, 353, 357, 367, 413, 439, 469, 471, 485, 488, 491, 492, 495], "w": [7, 25, 26, 52, 60, 61, 62, 92, 93, 94, 95, 140, 149, 150, 157, 177, 178, 215, 216, 217, 271, 281, 284, 351, 353, 357, 367, 413, 439, 469, 471, 484, 488, 491, 495], "ic": 7, "oc": 7, "pad_h": [7, 60, 271], "pad_w": [7, 60, 271], "becom": [7, 436, 497], "met": [7, 350, 472], "dimens": [7, 10, 11, 12, 13, 14, 18, 27, 28, 30, 38, 40, 41, 42, 46, 55, 62, 64, 65, 68, 69, 71, 72, 73, 80, 85, 88, 89, 103, 104, 108, 111, 113, 115, 116, 121, 126, 127, 139, 149, 150, 154, 155, 157, 163, 165, 166, 167, 168, 169, 175, 176, 177, 178, 179, 180, 192, 200, 201, 206, 215, 216, 217, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 268, 269, 272, 273, 274, 275, 277, 281, 289, 290, 291, 292, 295, 298, 299, 300, 303, 304, 306, 312, 313, 315, 320, 321, 322, 333, 337, 344, 346, 348, 349, 350, 351, 355, 356, 357, 364, 365, 367, 368, 369, 370, 371, 372, 390, 391, 392, 399, 402, 407, 408, 410, 411, 412, 413, 415, 417, 418, 419, 420, 436, 438, 439, 442, 444, 445, 446, 447, 448, 449, 451, 452, 453, 454, 464, 470, 471, 472, 475, 477, 485, 488, 489, 490, 491, 492, 493, 496], "kernel_s": [7, 60, 92, 93, 94, 271, 316, 317, 318, 319, 488, 491, 495], "least": [7, 218, 268, 484], "No": [7, 62, 114, 284, 358, 488, 491, 493], "dilat": [7, 92, 93, 94, 271, 316, 317, 318, 319, 491, 495], "divis": [7, 13, 18, 111, 112, 113, 114, 115, 116, 154, 155, 169, 176, 256, 326, 327, 337, 344, 401, 402, 420, 450, 451, 485, 492], "respect": [7, 61, 94, 123, 178, 215, 216, 217, 268, 485, 487], "except": [7, 165, 268, 281, 369, 370, 491, 494], "OR": [7, 71, 254, 255, 350, 472], "particularli": [7, 196, 455, 492], "benefici": [7, 13], "unalign": 7, "g": [7, 13, 40, 94, 178, 267, 268, 371, 482, 484, 485, 491, 493, 495, 497], "small": [7, 61, 178, 215, 216, 217, 302, 353, 354, 355, 488, 491, 496], "count": [7, 149, 150, 178, 215, 484], "rgb": [7, 491], "2x2": [7, 485], "7x7": 7, "12": [7, 18, 89, 178, 215, 216, 265, 304, 353, 354, 368, 370, 457, 458, 473, 479, 482, 486, 491, 492, 494], "1x1": [7, 495], "align": [7, 11, 12, 14, 177, 215, 268, 369, 370, 450, 484], "4x4": [7, 457, 458, 485], "implicitli": [7, 268], "writer": [7, 469], "carri": [7, 343], "bottleneck": [7, 497], "overrid": [7, 497], "heurist": 7, "By": [7, 393, 483, 485, 489, 491, 492, 496], "inner": [7, 10, 11, 12, 13, 14], "dim": [7, 16, 38, 39, 46, 62, 74, 80, 88, 89, 102, 103, 104, 148, 165, 169, 176, 200, 201, 215, 216, 269, 273, 274, 275, 281, 306, 307, 320, 321, 333, 334, 337, 347, 348, 351, 353, 354, 356, 368, 371, 372, 390, 391, 399, 401, 410, 411, 412, 419, 420, 439, 451, 452, 454, 456, 464, 468, 470, 485, 491, 492, 493, 494, 495], "kernel_h": [7, 60, 271], "constraint": [7, 268, 367, 368, 370, 439, 485], "space": [7, 45, 92, 93, 94, 316, 317, 318, 319, 485, 491], "either": [7, 27, 28, 30, 40, 41, 55, 62, 64, 65, 68, 69, 71, 72, 73, 93, 102, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 178, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 267, 268, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 407, 408, 415, 417, 418, 436, 475, 482, 485, 496, 497], "row_major": [7, 18, 19, 20, 21, 23, 27, 28, 29, 30, 31, 42, 44, 45, 46, 48, 50, 53, 54, 55, 56, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 78, 79, 81, 82, 83, 84, 87, 89, 93, 96, 97, 98, 102, 106, 111, 112, 113, 114, 115, 116, 119, 120, 124, 125, 126, 127, 128, 129, 131, 133, 135, 136, 143, 145, 152, 154, 155, 156, 157, 158, 160, 162, 166, 167, 168, 169, 170, 171, 174, 176, 178, 179, 180, 181, 182, 183, 184, 185, 187, 189, 190, 191, 192, 193, 194, 196, 197, 200, 202, 203, 206, 207, 208, 209, 210, 211, 215, 216, 218, 219, 220, 221, 222, 223, 224, 225, 228, 233, 234, 236, 238, 241, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 254, 255, 256, 257, 258, 259, 261, 262, 263, 264, 267, 269, 270, 272, 273, 275, 276, 277, 278, 286, 289, 290, 291, 292, 293, 294, 295, 296, 298, 299, 303, 309, 310, 311, 312, 313, 314, 315, 320, 321, 323, 326, 327, 331, 332, 337, 338, 339, 342, 343, 344, 345, 347, 353, 354, 358, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 373, 378, 379, 381, 383, 384, 385, 386, 393, 394, 395, 396, 397, 400, 403, 405, 407, 408, 409, 412, 414, 415, 416, 417, 418, 419, 420, 421, 423, 425, 427, 429, 430, 431, 432, 433, 435, 457, 458, 459, 461, 462, 466, 467, 470, 475, 476, 477, 478, 484, 485, 489, 492], "expect": [7, 10, 25, 26, 40, 41, 60, 94, 140, 150, 178, 216, 271, 454, 480, 483, 488, 491, 493, 496, 497], "next": [7, 178, 295, 482, 485, 495], "impact": [7, 10, 13, 14, 18, 480, 492], "part": [7, 87, 158, 159, 169, 176, 198, 199, 204, 205, 216, 328, 329, 337, 354, 400, 420, 447, 451, 457, 458, 459, 480, 484, 494], "current": [7, 46, 60, 85, 88, 102, 165, 177, 178, 279, 280, 318, 319, 367, 371, 434, 439, 446, 448, 449, 452, 453, 485, 489, 491], "block_shard": [7, 25, 26, 60, 271], "without": [7, 67, 113, 114, 125, 268, 390, 491, 492], "addition": [7, 450], "nhw": [7, 25, 26, 60, 271], "number": [7, 10, 11, 12, 13, 14, 25, 26, 27, 28, 29, 36, 37, 38, 39, 40, 41, 60, 63, 64, 65, 66, 74, 80, 81, 88, 92, 93, 94, 108, 111, 112, 113, 115, 117, 126, 127, 149, 150, 154, 155, 156, 165, 167, 168, 178, 179, 180, 200, 203, 209, 219, 220, 222, 223, 227, 243, 244, 245, 247, 249, 254, 257, 262, 263, 265, 267, 268, 269, 271, 272, 273, 274, 275, 277, 281, 283, 286, 289, 291, 292, 297, 302, 304, 306, 314, 315, 316, 317, 318, 319, 322, 325, 333, 344, 345, 346, 348, 349, 350, 356, 364, 365, 367, 375, 377, 400, 402, 407, 408, 412, 414, 417, 418, 419, 432, 439, 440, 441, 450, 451, 470, 473, 475, 483, 484, 485, 488, 489, 491, 493, 494, 495, 497], "match": [7, 27, 28, 30, 41, 52, 55, 64, 65, 68, 69, 71, 72, 73, 85, 103, 104, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 178, 179, 180, 192, 200, 206, 215, 216, 217, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 268, 272, 277, 279, 280, 281, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 347, 349, 350, 353, 354, 355, 364, 365, 367, 407, 408, 415, 417, 418, 461, 475, 485, 487, 489, 490, 491, 492, 493, 494, 496], "fragment": [7, 350], "ideal": [7, 60, 271, 439, 485], "face": [7, 485, 491, 496], "alreadi": [7, 40, 94, 279, 280, 302, 392, 491, 495], "anywai": 7, "previou": [7, 36, 123, 487, 491, 493, 497], "due": [7, 139, 331, 484, 485, 491, 492], "tensormemorylayout": [7, 18, 25, 26, 60, 201, 202, 203, 271, 368, 370], "own": [7, 485], "height_shard": [7, 18, 202, 203, 368, 370], "width_shard": 7, "orient": [7, 102, 202, 203, 268, 368, 485], "major": [7, 10, 27, 28, 30, 38, 40, 41, 55, 64, 65, 68, 69, 71, 72, 73, 77, 111, 113, 115, 116, 117, 126, 127, 140, 146, 150, 151, 153, 154, 155, 159, 166, 167, 168, 178, 179, 180, 192, 202, 203, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 268, 272, 277, 289, 290, 291, 292, 295, 297, 303, 312, 313, 315, 333, 344, 359, 364, 365, 382, 400, 407, 408, 415, 417, 418, 460, 475, 485, 489, 491, 492, 497], "column": [7, 41, 178, 216, 283, 484, 485, 489, 490, 492], "bia": [7, 61, 64, 65, 66, 92, 93, 94, 140, 316, 317, 318, 319, 479, 488, 491, 493, 494, 495], "respons": [7, 491, 495], "prepar": [7, 178, 480, 488, 491, 492, 495], "unspecifi": [7, 38, 274, 333], "float32": [7, 19, 21, 23, 27, 28, 30, 32, 34, 36, 42, 44, 46, 48, 50, 54, 55, 58, 61, 63, 64, 65, 67, 75, 76, 78, 81, 87, 93, 96, 97, 98, 100, 103, 104, 106, 116, 124, 125, 126, 127, 128, 133, 135, 136, 143, 145, 152, 155, 158, 165, 167, 168, 177, 179, 180, 183, 185, 187, 189, 191, 192, 194, 197, 200, 207, 208, 209, 210, 211, 215, 217, 219, 220, 222, 223, 224, 226, 229, 231, 233, 234, 236, 238, 241, 243, 244, 245, 247, 249, 250, 254, 255, 257, 258, 259, 262, 263, 265, 268, 269, 272, 273, 275, 277, 278, 287, 289, 290, 291, 292, 293, 295, 296, 298, 299, 303, 309, 312, 313, 315, 323, 325, 326, 338, 339, 343, 344, 353, 354, 355, 358, 362, 364, 365, 368, 369, 370, 371, 372, 379, 381, 383, 384, 386, 388, 391, 392, 393, 395, 397, 400, 403, 407, 408, 412, 415, 417, 418, 419, 421, 423, 425, 427, 435, 439, 457, 458, 459, 461, 462, 463, 470, 473, 477, 478, 479, 484, 485, 487, 490, 492, 493], "alia": 8, "op2dsliceconfig": 8, "nb_func": [9, 15, 173, 302, 305, 375, 402, 422], "_ttnn": [9, 15, 18, 96, 97, 122, 173, 302, 330, 402, 422, 465], "multi_devic": [9, 15, 422], "meshdevic": [9, 15, 47, 74, 86, 92, 93, 94, 124, 125, 160, 162, 163, 164, 232, 266, 267, 300, 301, 325, 422, 434, 438, 451, 477, 478, 495], "plan": [9, 15, 305], "deprec": [9, 15, 305], "futur": [9, 15, 267, 305, 491], "1d": [10, 92, 178, 200, 268, 297, 315, 320, 349, 493], "multicast": [10, 11, 12, 13, 268], "veri": [10, 12, 370, 392, 393, 484], "narrow": [10, 12], "interleav": [10, 13, 27, 28, 30, 40, 41, 46, 55, 61, 62, 63, 64, 65, 68, 69, 71, 72, 73, 87, 88, 96, 97, 103, 104, 111, 113, 115, 116, 121, 126, 127, 140, 154, 155, 165, 166, 167, 168, 178, 179, 180, 192, 200, 201, 202, 203, 206, 215, 217, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 268, 269, 272, 273, 275, 277, 281, 289, 290, 291, 292, 295, 303, 312, 313, 315, 320, 344, 348, 353, 355, 364, 365, 367, 368, 371, 372, 376, 391, 399, 400, 407, 408, 412, 415, 417, 418, 419, 434, 435, 437, 439, 454, 461, 463, 470, 475, 485, 492], "capabl": [10, 13, 14, 18, 491], "along": [10, 11, 12, 13, 14, 27, 28, 30, 37, 38, 39, 40, 41, 55, 64, 65, 68, 69, 71, 72, 73, 80, 88, 103, 104, 111, 113, 115, 116, 121, 126, 127, 154, 155, 157, 165, 166, 167, 168, 178, 179, 180, 192, 200, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 269, 272, 273, 274, 275, 277, 283, 289, 290, 291, 292, 295, 299, 303, 312, 313, 315, 321, 333, 344, 346, 349, 356, 364, 365, 369, 370, 371, 372, 390, 391, 392, 399, 407, 408, 411, 412, 415, 417, 418, 419, 439, 444, 451, 456, 470, 475, 485, 492, 495], "while": [10, 178, 182, 457, 458, 483, 491, 492, 493], "batch": [10, 11, 13, 18, 25, 26, 40, 41, 60, 61, 92, 93, 94, 149, 150, 177, 201, 268, 271, 282, 316, 317, 318, 319, 368, 369, 370, 413, 446, 448, 449, 452, 453, 468, 471, 484, 488, 491, 492, 495, 496], "incorpor": [10, 480], "second": [10, 13, 32, 34, 36, 53, 62, 95, 162, 169, 176, 178, 216, 231, 268, 304, 315, 337, 350, 354, 399, 400, 420, 456, 472, 484, 491, 492, 494, 495], "elimin": [10, 492], "separ": [10, 13, 349, 469, 492], "overal": [10, 367, 493, 495, 497], "scenario": [10, 11, 12, 13, 268, 485], "intern": [10, 27, 28, 30, 55, 64, 65, 68, 69, 71, 72, 73, 94, 111, 113, 115, 116, 117, 126, 127, 154, 155, 166, 167, 168, 178, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 370, 407, 408, 415, 417, 418, 447, 450, 451, 455, 475, 481], "left": [10, 55, 69, 111, 115, 166, 178, 218, 251, 483], "k": [10, 11, 12, 13, 14, 40, 41, 177, 281, 310, 391, 392, 400, 439, 443, 445, 446, 447, 448, 449, 451, 452, 453, 455, 489, 491, 492, 495], "granular": [10, 11, 12, 13, 14, 18], "wide": [10, 11, 12, 13, 14, 178, 392], "input_tensor_a": [10, 11, 12, 13, 22, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 49, 51, 52, 53, 55, 56, 57, 59, 60, 62, 64, 65, 66, 68, 69, 71, 72, 73, 77, 79, 89, 99, 101, 107, 110, 111, 112, 113, 115, 116, 120, 126, 127, 130, 132, 134, 137, 144, 151, 153, 154, 155, 156, 159, 166, 167, 168, 179, 180, 184, 186, 188, 192, 193, 195, 201, 206, 218, 219, 220, 221, 222, 223, 225, 227, 230, 235, 237, 239, 240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254, 255, 256, 257, 258, 260, 261, 262, 263, 265, 270, 271, 272, 276, 277, 286, 288, 289, 290, 291, 292, 295, 303, 312, 314, 315, 324, 327, 332, 340, 341, 344, 345, 359, 364, 365, 366, 374, 380, 382, 387, 389, 396, 398, 406, 407, 408, 409, 414, 415, 416, 417, 418, 424, 428, 460, 474, 475, 476], "input_tensor_b": [10, 11, 12, 13, 27, 28, 29, 30, 31, 32, 33, 34, 35, 52, 53, 55, 56, 62, 64, 65, 66, 68, 69, 71, 72, 73, 89, 111, 112, 113, 115, 116, 126, 127, 154, 155, 156, 166, 167, 168, 179, 180, 192, 193, 201, 206, 218, 219, 220, 221, 222, 223, 227, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 265, 270, 272, 276, 277, 286, 289, 290, 291, 292, 295, 303, 315, 344, 345, 364, 365, 366, 407, 408, 409, 414, 415, 416, 417, 418, 474, 475, 476], "across": [10, 11, 12, 13, 14, 37, 38, 39, 40, 41, 74, 117, 175, 216, 217, 274, 298, 333, 334, 354, 355, 402, 413, 450, 451, 452, 471, 484, 485, 490, 491, 492], "bandwidth": [10, 11, 12, 492], "certain": [10, 13], "m": [10, 11, 12, 13, 14, 36, 334, 400, 482, 489, 492, 497], "subblock": [10, 13, 14], "schedul": [10, 13, 480], "particip": 10, "workload": [10, 481, 492], "balanc": [10, 13, 450, 491], "crucial": 10, "achiev": [10, 480, 483], "specialis": 11, "signific": [11, 12, 489], "benefit": [11, 12, 483, 492], "avoid": [11, 12, 13, 279, 370, 392, 393, 484, 485, 488, 490, 491, 492, 496, 497], "trip": [11, 12], "chosen": [11, 12, 13, 14, 268, 484], "strategi": [11, 12, 102, 202, 268, 352, 447, 451, 484, 485, 492, 495], "conflict": [11, 12], "compat": [11, 12, 18, 178, 495], "special": [12, 349, 368, 480, 485, 491], "x": [13, 14, 18, 30, 52, 55, 61, 62, 64, 94, 95, 135, 136, 143, 144, 177, 178, 192, 215, 216, 217, 219, 236, 237, 243, 244, 268, 284, 295, 308, 331, 351, 353, 354, 355, 357, 362, 368, 369, 370, 402, 407, 413, 415, 442, 443, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455, 471, 475, 476, 482, 484, 485, 488, 489, 491, 492, 494], "smaller": [13, 14, 401, 485, 490], "overhead": [13, 490, 492], "explicitli": [13, 16, 27, 28, 30, 55, 64, 65, 68, 69, 71, 72, 73, 105, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 407, 408, 415, 417, 418, 475, 491, 492], "reusabl": 14, "togeth": [14, 400, 487, 488, 489, 490, 493, 494, 495], "suggest": 14, "decreas": 14, "larger": [14, 139, 177, 490, 496], "fewer": [14, 457, 458], "work": [14, 268, 390, 402, 479, 480, 482, 492], "total_m": 14, "total_n": 14, "device_id": [15, 25, 26, 86, 122, 161, 266, 302, 422, 434, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496], "suitabl": [16, 316, 317, 318, 319], "most": [16, 268, 485, 490, 492, 494], "characterist": [16, 182, 331, 497], "tile_layout": [16, 18, 19, 20, 21, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 42, 43, 44, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 89, 90, 91, 98, 99, 100, 101, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 119, 120, 121, 122, 123, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 141, 142, 143, 144, 145, 146, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 162, 163, 164, 166, 167, 168, 169, 170, 171, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 204, 205, 206, 207, 208, 209, 210, 211, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 268, 270, 272, 276, 277, 278, 281, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 298, 299, 300, 301, 308, 309, 310, 311, 312, 313, 314, 315, 321, 322, 323, 324, 325, 326, 327, 328, 329, 331, 332, 337, 338, 339, 340, 341, 342, 343, 344, 345, 347, 349, 353, 354, 355, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 373, 374, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 391, 392, 393, 394, 395, 396, 397, 398, 400, 403, 404, 405, 406, 407, 408, 409, 413, 414, 415, 416, 417, 418, 420, 421, 423, 424, 425, 426, 427, 428, 429, 430, 436, 457, 458, 459, 460, 462, 463, 471, 473, 474, 475, 476, 477, 478, 479, 485, 487, 489, 490, 491, 492, 493, 494, 495], "program_config": [16, 18, 36, 215, 216, 217, 231, 268, 353, 354, 355, 368, 370, 392, 400, 440, 441, 442, 443, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455], "common": [17, 166, 218, 479, 480, 491, 492, 495], "customiz": 18, "fine": [18, 483, 492, 496], "grain": [18, 492], "over": [18, 46, 61, 92, 93, 94, 140, 175, 178, 215, 216, 217, 269, 273, 275, 321, 353, 355, 391, 412, 413, 419, 445, 446, 447, 448, 449, 451, 452, 453, 470, 471, 482, 492, 497], "sub": [18, 28, 38, 62, 65, 88, 116, 127, 129, 131, 135, 152, 155, 168, 170, 180, 220, 223, 233, 234, 236, 238, 245, 247, 263, 264, 290, 292, 344, 350, 362, 365, 390, 403, 408, 415, 418, 422, 451, 459, 466, 473, 485, 491], "vertic": [18, 357], "horizont": [18, 357], "modifi": [18, 95, 194, 196, 370, 375, 392, 393], "proper": [18, 55, 177, 450, 482], "mask": [18, 178, 281, 400, 440, 441, 446, 447, 448, 449, 450, 451, 453, 455, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497], "input_shap": [18, 25, 26, 38, 60, 123, 177, 202, 271, 274, 320, 333, 352, 368, 369, 370], "attention_mask_t": [18, 368, 369, 370], "tt_output": [18, 25, 26, 60, 271, 369, 370], "logger": [18, 19, 20, 21, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 96, 97, 98, 99, 100, 101, 103, 104, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 143, 144, 145, 146, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 161, 162, 163, 164, 166, 167, 168, 169, 170, 171, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 204, 205, 206, 207, 208, 209, 210, 211, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 268, 269, 270, 271, 272, 273, 275, 276, 277, 278, 281, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 303, 304, 306, 308, 309, 310, 311, 312, 313, 314, 315, 320, 321, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 350, 353, 354, 355, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 403, 404, 405, 406, 407, 408, 409, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 434, 435, 436, 437, 438, 439, 457, 458, 459, 460, 461, 462, 463, 466, 467, 470, 471, 473, 474, 475, 476, 477, 478, 487, 488, 490, 491, 493, 494, 495], "info": [18, 19, 20, 21, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 96, 97, 98, 99, 100, 101, 103, 104, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 143, 144, 145, 146, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 161, 162, 163, 164, 166, 167, 168, 169, 170, 171, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 204, 205, 206, 207, 208, 209, 210, 211, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 268, 269, 270, 271, 272, 273, 275, 276, 277, 278, 281, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 303, 304, 306, 308, 309, 310, 311, 312, 313, 314, 315, 320, 321, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 350, 353, 354, 355, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 403, 404, 405, 406, 407, 408, 409, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 434, 435, 436, 437, 438, 439, 457, 458, 459, 460, 461, 462, 463, 466, 467, 470, 471, 473, 474, 475, 476, 477, 478, 487, 488, 489, 490, 491, 493, 494, 495, 497], "f": [18, 19, 20, 21, 22, 23, 24, 27, 29, 30, 31, 32, 33, 34, 35, 36, 42, 43, 44, 46, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 89, 90, 91, 98, 99, 100, 101, 103, 104, 106, 107, 109, 110, 111, 112, 113, 114, 115, 119, 120, 121, 122, 123, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 143, 144, 145, 146, 151, 152, 153, 154, 155, 156, 158, 159, 166, 167, 169, 170, 171, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 204, 205, 206, 207, 208, 209, 210, 211, 214, 215, 216, 217, 218, 219, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 246, 248, 249, 251, 252, 253, 254, 256, 257, 259, 260, 261, 262, 264, 265, 268, 269, 270, 271, 272, 273, 275, 276, 277, 278, 281, 285, 286, 287, 288, 289, 291, 293, 294, 295, 296, 298, 299, 303, 308, 309, 310, 311, 312, 313, 314, 315, 320, 321, 323, 324, 326, 327, 328, 329, 331, 332, 337, 338, 339, 340, 341, 342, 343, 344, 345, 347, 353, 354, 355, 358, 359, 360, 361, 362, 363, 364, 366, 367, 368, 369, 370, 371, 372, 373, 374, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 402, 403, 404, 405, 406, 407, 409, 412, 413, 414, 415, 416, 417, 419, 420, 421, 423, 424, 425, 426, 427, 428, 429, 430, 439, 457, 458, 459, 460, 462, 470, 471, 473, 474, 475, 476, 488, 489, 490, 491, 492, 493, 494, 495], "compute_grid_s": [18, 368, 369, 370], "fuse_head": [18, 369, 370], "num_cores_r": [18, 368, 369, 370], "384": [18, 368, 369, 370, 479, 494, 496], "768": [18, 368, 369, 370, 491, 494], "grid_coord": [18, 368, 370], "shard_grid": [18, 368, 370], "corerangeset": [18, 28, 38, 65, 88, 102, 116, 127, 129, 131, 135, 152, 155, 165, 168, 170, 180, 202, 220, 223, 233, 234, 236, 238, 245, 247, 263, 264, 269, 273, 275, 290, 292, 344, 350, 352, 362, 365, 367, 368, 370, 371, 372, 390, 402, 403, 408, 412, 418, 419, 431, 439, 459, 466, 470, 473], "corerang": [18, 202, 352, 368, 370, 402], "shard_shap": [18, 202, 368, 370], "shard_spec": [18, 368, 370], "shardspec": [18, 368, 370], "shardorient": [18, 102, 202, 203, 368, 370], "sharded_mem_config": [18, 178, 368, 370], "buffertyp": [18, 201, 368, 370], "input_shard": [18, 368, 370], "24": [18, 368, 370, 495, 496], "self": [18, 479, 485, 491, 496], "complextensor": [19, 20, 29, 43, 44, 87, 90, 91, 111, 112, 115, 198, 199, 204, 205, 286, 308, 309, 328, 329, 331, 332, 414], "memory_config": [19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 88, 89, 90, 91, 92, 93, 98, 99, 100, 101, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 119, 120, 121, 122, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 162, 163, 164, 165, 166, 167, 169, 170, 171, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 204, 205, 206, 207, 208, 209, 210, 211, 214, 215, 216, 217, 218, 219, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 246, 248, 249, 251, 252, 253, 254, 256, 257, 259, 260, 261, 262, 264, 265, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 281, 282, 283, 285, 286, 287, 288, 289, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 303, 304, 306, 308, 309, 310, 311, 312, 313, 314, 315, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 357, 358, 359, 360, 361, 362, 363, 364, 366, 369, 371, 372, 373, 374, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 393, 394, 395, 396, 397, 398, 399, 400, 401, 403, 404, 405, 406, 407, 409, 412, 413, 414, 415, 416, 417, 419, 420, 421, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 436, 437, 439, 440, 441, 442, 443, 444, 445, 446, 448, 449, 450, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 466, 467, 469, 470, 471, 473, 474, 475, 476, 477, 478, 479, 489, 490, 491, 492, 494], "element": [19, 21, 23, 27, 30, 32, 34, 42, 46, 48, 50, 54, 70, 75, 76, 78, 81, 88, 92, 93, 94, 95, 98, 103, 104, 106, 115, 121, 126, 128, 129, 131, 133, 135, 136, 141, 143, 145, 152, 154, 158, 165, 166, 167, 169, 170, 172, 174, 175, 176, 177, 178, 179, 181, 182, 185, 187, 191, 192, 194, 196, 206, 207, 208, 209, 210, 211, 218, 219, 222, 224, 228, 233, 234, 236, 238, 241, 243, 244, 249, 252, 254, 257, 262, 264, 272, 277, 278, 281, 284, 289, 291, 293, 296, 297, 304, 312, 313, 316, 317, 318, 319, 320, 323, 326, 331, 337, 338, 339, 342, 343, 348, 356, 358, 360, 362, 364, 378, 379, 381, 383, 384, 386, 393, 397, 399, 403, 405, 407, 415, 417, 420, 421, 423, 425, 427, 439, 457, 458, 459, 461, 462, 467, 473, 475, 485, 490, 492, 496], "wise": [19, 21, 23, 27, 32, 34, 42, 48, 50, 54, 70, 75, 76, 78, 81, 98, 106, 115, 126, 128, 129, 131, 133, 135, 136, 141, 143, 145, 152, 154, 158, 167, 169, 170, 174, 176, 179, 181, 182, 185, 187, 191, 194, 196, 206, 207, 208, 209, 210, 211, 222, 224, 228, 233, 234, 236, 238, 241, 249, 252, 254, 257, 262, 264, 272, 277, 278, 289, 291, 293, 296, 313, 323, 326, 331, 337, 338, 339, 342, 343, 358, 360, 362, 378, 379, 381, 383, 384, 386, 393, 397, 402, 403, 405, 417, 420, 421, 423, 425, 427, 459, 462, 490, 492, 496], "mathrm": [19, 20, 21, 22, 23, 24, 27, 28, 30, 32, 34, 42, 48, 49, 50, 51, 54, 55, 57, 58, 59, 64, 65, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 98, 99, 100, 101, 103, 104, 106, 107, 109, 110, 111, 113, 114, 115, 116, 121, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 141, 143, 144, 145, 151, 152, 153, 154, 155, 158, 159, 166, 167, 168, 169, 170, 174, 175, 176, 179, 180, 181, 182, 183, 185, 186, 187, 188, 191, 192, 194, 195, 196, 197, 206, 207, 208, 209, 210, 211, 218, 219, 220, 222, 223, 224, 226, 228, 229, 230, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 260, 262, 263, 264, 278, 287, 288, 289, 290, 291, 292, 293, 295, 296, 298, 299, 303, 311, 312, 313, 315, 323, 324, 331, 332, 337, 338, 339, 340, 341, 342, 343, 344, 358, 359, 360, 361, 362, 363, 364, 365, 374, 378, 379, 380, 381, 382, 383, 384, 386, 387, 388, 389, 393, 395, 397, 398, 403, 405, 406, 407, 408, 413, 415, 417, 418, 420, 421, 423, 424, 425, 427, 428, 429, 430, 457, 458, 459, 460, 462, 471, 475], "_tensor": [19, 20, 21, 22, 23, 24, 27, 28, 30, 32, 34, 42, 48, 49, 50, 51, 54, 55, 57, 58, 59, 64, 65, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 98, 99, 100, 101, 106, 107, 109, 110, 111, 113, 114, 115, 116, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 141, 143, 144, 145, 151, 152, 153, 154, 155, 158, 159, 166, 167, 168, 169, 170, 174, 175, 176, 179, 180, 181, 182, 183, 185, 186, 187, 188, 191, 192, 194, 195, 196, 197, 206, 207, 208, 209, 210, 211, 218, 219, 220, 222, 223, 224, 226, 228, 229, 230, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 260, 262, 263, 264, 278, 287, 288, 289, 290, 291, 292, 293, 295, 296, 298, 299, 303, 311, 312, 313, 315, 323, 324, 331, 332, 337, 338, 339, 340, 341, 342, 343, 344, 358, 359, 360, 361, 362, 363, 364, 365, 374, 378, 379, 380, 381, 382, 383, 384, 386, 387, 388, 389, 393, 395, 397, 398, 403, 405, 406, 407, 408, 413, 415, 417, 418, 420, 421, 423, 424, 425, 427, 428, 429, 430, 457, 458, 459, 460, 462, 471, 475], "_i": [19, 20, 21, 22, 23, 24, 27, 28, 32, 34, 48, 49, 50, 51, 54, 55, 57, 58, 59, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 98, 99, 100, 101, 103, 104, 106, 107, 109, 110, 111, 114, 115, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 141, 143, 144, 145, 151, 152, 153, 158, 159, 166, 167, 169, 170, 174, 175, 176, 179, 181, 182, 183, 185, 186, 187, 188, 191, 192, 194, 195, 196, 197, 207, 208, 209, 210, 211, 218, 222, 224, 228, 229, 230, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 260, 262, 264, 278, 287, 288, 289, 291, 293, 295, 296, 298, 299, 311, 312, 313, 323, 324, 331, 332, 337, 338, 339, 340, 341, 342, 343, 358, 359, 360, 361, 362, 363, 364, 374, 378, 379, 380, 381, 382, 383, 384, 386, 387, 388, 389, 393, 395, 397, 398, 403, 405, 406, 413, 417, 418, 420, 421, 423, 424, 425, 427, 428, 429, 430, 457, 458, 459, 460, 462, 471, 475], "verb": [19, 20, 21, 22, 23, 24, 48, 49, 50, 51, 54, 57, 58, 59, 64, 65, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 98, 99, 100, 101, 106, 107, 109, 110, 113, 114, 116, 130, 132, 133, 134, 136, 137, 141, 143, 144, 145, 151, 152, 153, 154, 155, 158, 159, 166, 169, 176, 183, 185, 186, 187, 188, 191, 194, 195, 197, 207, 208, 209, 210, 211, 218, 219, 220, 224, 226, 229, 230, 235, 237, 239, 240, 241, 242, 243, 244, 245, 247, 251, 253, 256, 260, 278, 287, 288, 290, 293, 298, 299, 311, 315, 323, 324, 331, 332, 337, 338, 339, 340, 341, 342, 343, 344, 358, 359, 360, 361, 363, 374, 378, 379, 380, 381, 382, 383, 384, 386, 387, 388, 389, 395, 397, 398, 405, 406, 407, 408, 413, 420, 421, 423, 424, 428, 429, 430, 457, 458, 459, 460, 462, 471], "keyword": [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 88, 89, 90, 91, 92, 93, 94, 98, 99, 100, 101, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 119, 120, 121, 122, 123, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 162, 165, 166, 167, 168, 169, 170, 171, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 256, 257, 259, 260, 261, 262, 263, 264, 265, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 281, 282, 283, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 302, 303, 304, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 403, 404, 405, 406, 407, 408, 409, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 466, 467, 468, 469, 470, 471, 473, 474, 475, 476], "prealloc": [19, 21, 23, 27, 29, 30, 31, 32, 34, 38, 42, 46, 48, 50, 53, 54, 61, 62, 64, 67, 68, 69, 70, 71, 72, 73, 75, 76, 78, 81, 88, 89, 98, 103, 104, 106, 111, 112, 115, 121, 122, 123, 126, 128, 129, 131, 133, 135, 136, 138, 141, 142, 143, 145, 146, 152, 158, 163, 164, 165, 166, 167, 170, 171, 174, 179, 181, 182, 185, 187, 191, 194, 196, 197, 207, 208, 209, 210, 211, 214, 218, 219, 222, 224, 228, 233, 234, 236, 238, 241, 243, 244, 249, 251, 252, 254, 256, 257, 262, 264, 272, 277, 278, 281, 282, 285, 286, 289, 291, 293, 294, 296, 301, 313, 314, 323, 326, 331, 333, 338, 339, 342, 343, 358, 360, 362, 363, 364, 366, 367, 378, 379, 381, 383, 384, 385, 386, 393, 397, 399, 403, 404, 405, 407, 414, 415, 416, 417, 421, 423, 425, 426, 427, 439, 459, 461, 462, 473, 474, 475, 478], "absolut": [19, 20, 206, 214, 431, 442, 443, 450], "grad_tensor": [20, 22, 24, 29, 31, 33, 35, 44, 49, 51, 53, 56, 57, 59, 66, 77, 79, 82, 84, 89, 91, 99, 101, 107, 110, 112, 114, 120, 123, 130, 132, 134, 137, 138, 142, 144, 146, 151, 153, 156, 159, 171, 184, 186, 188, 190, 193, 195, 199, 221, 225, 227, 230, 235, 237, 239, 240, 242, 246, 248, 260, 261, 270, 276, 286, 288, 294, 309, 311, 314, 321, 324, 327, 329, 332, 340, 341, 345, 347, 359, 361, 363, 366, 374, 380, 382, 385, 387, 389, 394, 396, 398, 404, 406, 409, 414, 416, 424, 426, 428, 430, 460, 474, 476], "list": [20, 22, 24, 25, 26, 27, 28, 29, 31, 33, 35, 36, 37, 44, 49, 51, 53, 56, 57, 59, 60, 64, 66, 77, 79, 82, 84, 88, 89, 91, 99, 101, 102, 107, 110, 112, 114, 117, 120, 122, 124, 126, 130, 132, 134, 137, 138, 144, 146, 151, 153, 156, 159, 160, 166, 167, 171, 172, 173, 179, 184, 186, 188, 190, 193, 195, 199, 202, 218, 219, 221, 222, 225, 227, 230, 231, 235, 237, 239, 240, 242, 243, 244, 246, 248, 249, 254, 257, 260, 261, 262, 268, 270, 271, 272, 276, 277, 286, 288, 291, 294, 297, 304, 305, 306, 309, 311, 314, 315, 320, 321, 324, 325, 327, 329, 332, 340, 341, 345, 347, 356, 359, 361, 363, 364, 366, 374, 380, 382, 385, 387, 389, 390, 394, 396, 398, 399, 400, 401, 404, 406, 407, 409, 410, 411, 414, 416, 417, 418, 422, 424, 426, 428, 430, 439, 446, 448, 449, 451, 453, 460, 462, 466, 467, 474, 475, 476, 481, 487, 491, 492, 497], "given": [20, 22, 24, 29, 31, 33, 35, 41, 44, 46, 49, 51, 53, 56, 57, 59, 66, 77, 79, 82, 84, 89, 91, 94, 99, 101, 103, 104, 107, 110, 112, 114, 120, 121, 125, 130, 132, 134, 137, 138, 144, 146, 151, 153, 156, 159, 171, 178, 184, 186, 188, 190, 193, 195, 199, 200, 221, 225, 227, 230, 235, 237, 239, 240, 242, 246, 248, 260, 261, 270, 276, 279, 280, 286, 288, 294, 302, 305, 309, 311, 314, 321, 324, 325, 327, 329, 332, 340, 341, 345, 347, 348, 359, 361, 363, 366, 367, 371, 372, 374, 380, 382, 385, 387, 389, 394, 396, 398, 404, 406, 409, 414, 416, 424, 426, 428, 430, 439, 446, 448, 449, 453, 457, 458, 460, 461, 469, 474, 476, 479, 490, 491, 495], "grad": [20, 22, 24, 49, 51, 57, 59, 77, 99, 101, 107, 110, 114, 130, 132, 134, 137, 144, 151, 153, 159, 186, 188, 195, 230, 235, 237, 239, 240, 242, 260, 288, 311, 324, 332, 340, 341, 347, 359, 361, 363, 374, 380, 382, 387, 389, 398, 406, 424, 428, 430, 460], "gradient": [20, 22, 24, 29, 31, 33, 35, 44, 49, 51, 53, 56, 57, 59, 66, 77, 79, 82, 84, 89, 91, 94, 99, 101, 107, 110, 112, 114, 120, 123, 130, 132, 134, 137, 138, 142, 144, 146, 151, 153, 156, 159, 165, 171, 184, 186, 188, 190, 193, 195, 199, 221, 225, 227, 230, 235, 237, 239, 240, 242, 246, 248, 260, 261, 270, 276, 286, 288, 294, 309, 311, 314, 321, 324, 327, 329, 332, 340, 341, 345, 347, 359, 361, 363, 366, 374, 380, 382, 385, 387, 389, 394, 396, 398, 404, 406, 409, 414, 416, 424, 426, 428, 430, 460, 474, 476, 491], "about": [20, 33, 66, 156, 171, 235, 237, 239, 240, 242, 246, 248, 260, 261, 314, 321, 331, 332, 345, 394, 402, 404, 476, 485, 487, 489, 490, 492, 494, 497], "requires_grad": [20, 22, 24, 29, 31, 33, 35, 49, 51, 53, 56, 57, 59, 66, 77, 79, 82, 84, 89, 99, 101, 107, 110, 112, 114, 120, 123, 130, 132, 134, 137, 138, 142, 144, 146, 151, 153, 156, 159, 171, 184, 186, 188, 190, 193, 195, 221, 225, 227, 230, 235, 237, 239, 240, 242, 246, 248, 260, 261, 270, 276, 286, 288, 294, 311, 314, 321, 324, 327, 332, 340, 341, 345, 347, 359, 361, 363, 366, 374, 380, 382, 385, 387, 389, 394, 396, 398, 404, 406, 409, 414, 416, 424, 426, 428, 430, 460, 474, 476], "arccosin": [21, 22, 23, 24], "invers": [22, 24, 40, 49, 51, 57, 59, 133, 134, 274], "cosin": [22, 24, 98, 99, 100, 101, 491], "hyperbol": [23, 24, 50, 51, 58, 59, 100, 101, 388, 389, 425, 426], "batch_siz": [25, 26, 60, 92, 93, 94, 123, 175, 271, 316, 317, 318, 319, 353, 444, 454, 479, 488, 491, 493, 494, 495, 496], "input_h": [25, 26, 60, 271, 495], "input_w": [25, 26, 60, 271, 495], "output_s": [25, 26], "applied_shard_schem": [25, 26, 60, 271], "compute_kernel_config": [25, 36, 60, 61, 63, 85, 121, 140, 178, 215, 216, 217, 231, 268, 269, 273, 275, 353, 354, 355, 368, 369, 370, 391, 392, 400, 412, 419, 442, 443, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455, 463, 468, 469, 470, 492], "devicecomputekernelconfig": [25, 36, 60, 61, 63, 92, 93, 94, 140, 178, 215, 216, 217, 231, 268, 316, 317, 318, 319, 353, 354, 355, 368, 369, 370, 391, 392, 400, 442, 443, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455, 463, 468, 469], "deallocate_input": [25, 26, 60, 271], "reallocate_output": [25, 26], "queue_id": [25, 26, 450], "adapt": [25, 26, 175], "averag": [25, 60, 121, 141, 175, 484], "unlik": [25, 26, 67, 139, 491], "regular": [25, 26, 44, 199, 329, 496], "calcul": [25, 26, 36, 55, 60, 102, 177, 192, 295, 413, 471, 484], "desir": [25, 26, 103, 104, 125, 139, 148, 149, 150, 162, 177, 435, 436, 437, 438, 461, 492], "target": [25, 26, 85, 320, 480, 495], "scheme": [25, 26, 60, 202, 203, 271, 316, 317, 318, 319, 349], "non": [25, 26, 38, 40, 41, 60, 111, 115, 268, 271, 283, 297, 314, 326, 331, 360, 400, 447, 450, 451, 491], "queue": [25, 26, 96, 97, 162, 302, 422, 438, 450], "id": [25, 26, 37, 38, 39, 40, 41, 96, 97, 162, 266, 267, 302, 333, 422, 438, 450, 484, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497], "l1_small_siz": [25, 26, 60, 271, 302, 488, 491, 495, 496], "8192": [25, 26, 60, 271, 439, 488, 491, 495, 496], "nchw_shape": [25, 26, 60, 271], "256": [25, 26, 37, 38, 39, 60, 74, 256, 271, 333, 357, 410, 450, 464, 495, 496], "in_n": [25, 26, 60, 271], "in_c": [25, 26, 60, 271], "in_h": [25, 26, 60, 271], "in_w": [25, 26, 60, 271], "randn": [25, 26, 38, 39, 47, 60, 74, 123, 162, 177, 271, 357, 371, 411, 413, 434, 462, 471, 492, 494], "input_perm": [25, 26, 60, 271], "input_reshap": [25, 26, 60, 271], "tt_input": [25, 26, 60, 200, 271], "global": [25, 41, 117, 175, 282, 298, 349, 451, 484, 491, 497], "classifi": [26, 491], "datatyp": [27, 36, 45, 47, 60, 61, 63, 64, 67, 85, 92, 93, 94, 96, 97, 103, 104, 122, 123, 124, 125, 126, 160, 162, 163, 164, 166, 167, 175, 178, 179, 202, 203, 215, 216, 217, 218, 219, 222, 231, 243, 244, 249, 254, 257, 262, 268, 271, 272, 277, 289, 290, 291, 300, 301, 316, 317, 318, 319, 325, 353, 354, 355, 364, 376, 377, 392, 400, 407, 417, 435, 436, 437, 461, 475, 477, 478, 487, 489, 490, 495, 497], "str": [27, 28, 47, 53, 64, 112, 118, 126, 142, 166, 167, 177, 179, 218, 219, 222, 231, 232, 243, 244, 249, 254, 257, 262, 268, 272, 277, 279, 280, 291, 357, 364, 375, 407, 417, 418, 447, 451, 469, 475, 491, 495, 496], "_a": [27, 28, 30, 32, 34, 55, 64, 65, 68, 69, 71, 72, 73, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 289, 290, 291, 292, 295, 303, 315, 344, 364, 365, 407, 408, 415, 417, 418, 475], "_b": [27, 28, 30, 32, 34, 55, 64, 65, 68, 69, 71, 72, 73, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 289, 290, 291, 292, 295, 303, 315, 344, 364, 365, 407, 408, 415, 417, 418, 475], "elementwis": [27, 28, 30, 55, 62, 64, 65, 68, 69, 71, 72, 73, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 407, 408, 415, 417, 418, 475], "independ": [27, 28, 30, 38, 55, 64, 65, 68, 69, 71, 72, 73, 88, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 407, 408, 415, 417, 418, 475], "operand": [27, 28, 30, 55, 64, 65, 68, 69, 71, 72, 73, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 407, 408, 415, 417, 418, 475], "dnchw": [27, 28, 30, 55, 64, 65, 68, 69, 71, 72, 73, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 407, 408, 415, 417, 418, 475], "ani": [27, 28, 30, 41, 55, 64, 65, 68, 69, 71, 72, 73, 111, 113, 115, 116, 126, 127, 148, 149, 150, 154, 155, 160, 166, 167, 168, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 289, 290, 291, 292, 295, 303, 304, 312, 313, 315, 320, 344, 364, 365, 407, 408, 415, 417, 418, 475, 480, 485, 491, 492, 497], "duplic": [27, 28, 30, 40, 41, 55, 64, 65, 68, 69, 71, 72, 73, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 407, 408, 415, 417, 418, 475], "16": [27, 28, 30, 55, 64, 65, 67, 68, 69, 71, 72, 73, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 177, 178, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 289, 290, 291, 292, 295, 303, 304, 307, 312, 313, 315, 344, 364, 365, 390, 400, 407, 408, 415, 417, 418, 457, 458, 475, 485, 490, 492, 494, 495], "higher": [27, 28, 30, 55, 64, 65, 68, 69, 71, 72, 73, 111, 113, 115, 116, 126, 127, 139, 154, 155, 166, 167, 168, 177, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 393, 407, 408, 415, 417, 418, 475, 484, 485, 492], "attempt": [27, 28, 30, 55, 64, 65, 68, 69, 71, 72, 73, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 407, 408, 415, 417, 418, 475], "best": [27, 28, 30, 55, 64, 65, 68, 69, 71, 72, 73, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 407, 408, 415, 417, 418, 475, 479, 491, 492], "decis": [27, 28, 30, 55, 64, 65, 68, 69, 71, 72, 73, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 407, 408, 415, 417, 418, 475], "consider": [27, 28, 30, 55, 64, 65, 68, 69, 71, 72, 73, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 407, 408, 415, 417, 418, 475, 492], "prefer": [27, 28, 30, 55, 64, 65, 68, 69, 71, 72, 73, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 179, 180, 192, 206, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 251, 254, 255, 256, 257, 258, 262, 263, 272, 277, 289, 290, 291, 292, 295, 303, 312, 313, 315, 344, 364, 365, 407, 408, 415, 417, 418, 475, 482, 487, 491, 497], "int32": [27, 28, 32, 34, 46, 67, 68, 69, 70, 71, 72, 73, 81, 87, 96, 97, 103, 108, 116, 126, 128, 145, 155, 165, 166, 167, 174, 179, 181, 200, 218, 222, 228, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 262, 264, 272, 277, 289, 290, 291, 296, 344, 349, 364, 365, 367, 371, 372, 383, 405, 407, 408, 417, 418, 435, 439, 443, 461, 473, 496], "uint32": [27, 28, 46, 67, 68, 69, 71, 72, 73, 87, 96, 97, 103, 122, 123, 126, 128, 145, 165, 197, 200, 201, 250, 251, 252, 253, 254, 255, 256, 257, 258, 267, 289, 291, 296, 364, 365, 367, 399, 405, 407, 408, 417, 418, 433, 435, 439, 461, 484, 485, 491, 492, 496], "4294967295": [27, 28, 252, 417, 418], "uint16": [27, 28, 46, 67, 68, 69, 71, 72, 73, 87, 96, 97, 116, 126, 128, 145, 163, 165, 197, 249, 250, 252, 254, 255, 257, 258, 289, 290, 291, 296, 300, 364, 365, 399, 405, 407, 408, 417, 418, 435, 439, 461, 477, 485, 492], "65535": [27, 28, 68, 71, 73, 252, 417, 418], "two": [27, 28, 30, 53, 55, 64, 65, 68, 71, 73, 85, 88, 111, 113, 115, 116, 126, 127, 154, 155, 166, 167, 168, 169, 176, 179, 180, 192, 206, 216, 217, 218, 219, 220, 222, 223, 243, 244, 245, 247, 249, 250, 254, 255, 257, 258, 262, 263, 268, 272, 277, 281, 289, 290, 291, 292, 295, 303, 313, 337, 344, 350, 354, 355, 364, 365, 369, 370, 399, 400, 407, 408, 415, 417, 418, 420, 436, 443, 447, 450, 451, 454, 472, 473, 475, 479, 482, 485, 486, 487, 491, 492, 493, 494, 495, 497], "tensor1": [27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 53, 55, 56, 64, 65, 66, 68, 69, 71, 72, 73, 88, 89, 111, 112, 113, 115, 116, 126, 127, 147, 154, 155, 156, 166, 167, 168, 179, 180, 192, 193, 206, 218, 219, 220, 221, 222, 223, 226, 227, 243, 244, 245, 246, 247, 248, 249, 250, 254, 255, 257, 258, 262, 263, 265, 268, 270, 272, 276, 277, 286, 289, 290, 291, 292, 295, 303, 313, 315, 344, 345, 364, 365, 366, 400, 407, 408, 409, 414, 415, 416, 417, 418, 468, 473, 474, 475, 476], "tensor2": [27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 53, 55, 56, 64, 65, 66, 68, 69, 71, 72, 73, 88, 89, 111, 112, 113, 115, 116, 126, 127, 147, 154, 155, 156, 166, 167, 168, 179, 180, 192, 193, 206, 218, 219, 220, 221, 222, 223, 226, 227, 243, 244, 245, 246, 247, 248, 249, 250, 254, 255, 257, 258, 262, 263, 265, 268, 270, 272, 276, 277, 286, 289, 290, 291, 292, 295, 303, 313, 344, 345, 364, 365, 366, 400, 407, 408, 409, 414, 415, 416, 417, 418, 468, 473, 474, 475, 476], "input_tensor_a_activ": [28, 418], "input_tensor_b_activ": [28, 418], "use_legaci": [28, 249, 418], "sub_core_grid": [28, 38, 65, 88, 116, 127, 129, 131, 135, 152, 155, 165, 168, 170, 180, 220, 223, 233, 234, 236, 238, 245, 247, 263, 264, 267, 269, 273, 275, 290, 292, 344, 350, 362, 365, 367, 371, 372, 390, 403, 408, 412, 418, 419, 431, 439, 459, 466, 470, 473], "input_a": [28, 32, 34, 52, 62, 65, 95, 116, 127, 168, 180, 201, 220, 223, 245, 247, 263, 269, 273, 275, 290, 292, 365, 408, 412, 418, 419, 470], "input_b": [28, 32, 34, 52, 62, 65, 95, 116, 127, 168, 180, 201, 220, 223, 245, 247, 263, 290, 292, 365, 408, 418], "legaci": [28, 418, 443], "inplac": [28, 65, 116, 127, 168, 178, 180, 220, 223, 245, 247, 250, 253, 255, 258, 263, 290, 292, 365, 408, 418], "are_required_output": [29, 31, 53, 89, 112, 286, 366, 414, 416, 474], "input_grad": [29, 31, 53, 89, 112, 142, 286, 366, 414, 416], "other_grad": [29, 31, 53, 89, 112, 286, 366, 414, 416], "scalar": [29, 32, 34, 81, 83, 111, 112, 114, 115, 156, 163, 164, 227, 249, 267, 269, 272, 273, 275, 286, 311, 315, 320, 326, 327, 345, 349, 412, 414, 419, 443, 470], "bfloat4_b": [29, 31, 36, 42, 56, 66, 87, 89, 96, 97, 108, 112, 162, 193, 221, 226, 227, 231, 246, 248, 265, 268, 270, 276, 286, 322, 325, 349, 366, 400, 409, 414, 416, 425, 427, 435, 438, 461, 473, 476, 492], "alpha": [30, 31, 33, 35, 36, 78, 79, 112, 119, 120, 121, 373, 415, 416], "float": [30, 31, 32, 33, 34, 35, 36, 61, 78, 79, 82, 83, 84, 114, 119, 120, 121, 141, 145, 148, 150, 155, 162, 163, 164, 177, 178, 183, 184, 189, 190, 191, 200, 206, 215, 216, 224, 225, 226, 259, 261, 269, 273, 275, 295, 304, 306, 311, 312, 313, 315, 325, 326, 327, 342, 343, 349, 353, 354, 357, 360, 361, 368, 369, 370, 373, 375, 393, 394, 395, 396, 412, 415, 416, 419, 429, 430, 442, 443, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455, 456, 469, 470, 485, 491, 492], "input_c": [32, 34], "input_tensor_c": [32, 33, 34, 35, 227, 265, 474], "_c": [32, 34], "third": [32, 34], "ttt": [32, 34], "three": [32, 33, 34, 35, 216, 226, 227, 265, 354, 474, 493, 496], "tensor3": [32, 33, 34, 35, 226, 227, 265, 473, 474], "beta": [36, 61, 217, 355, 393, 394], "matmulprogramconfig": [36, 231, 268], "coregrid": [36, 102, 121, 178, 202, 203, 231, 268, 400, 489, 492, 494], "output_til": [36, 231, 268, 400], "global_cb": [36, 117], "globalcircularbuff": [36, 117], "sub_device_id": [36, 422], "subdeviceid": [36, 37, 38, 39, 40, 41, 333, 422, 451], "p": [36, 268, 485, 491, 497], "content": [36, 118, 232, 491], "overwritten": 36, "factor": [36, 121, 177, 269, 273, 275, 368, 369, 370, 412, 419, 447, 450, 451, 455, 469, 470, 491, 492], "look": [36, 178, 268, 281, 439, 480, 481, 484, 485, 491, 497], "dram_memory_config": [36, 43, 44, 45, 85, 90, 91, 124, 125, 160, 178, 198, 199, 204, 205, 231, 268, 308, 309, 325, 328, 329, 376, 400, 434, 485, 490, 491], "highest": [36, 367, 491, 492, 493, 495], "tbd": 36, "128": [36, 121, 177, 231, 256, 268, 334, 357, 488, 490, 492, 493, 494, 495], "cluster_axi": [37, 38, 39, 40, 41, 74, 274, 283, 333, 451], "subdevice_id": [37, 38, 39, 40, 41, 333, 451], "num_link": [37, 38, 39, 40, 41, 74, 333, 451, 492], "correspond": [37, 40, 41, 74, 117, 122, 123, 173, 272, 277, 442, 446, 448, 449, 453, 473, 497], "cluster": [37, 38, 39, 40, 41, 74, 274, 283, 333, 488, 489, 490, 493, 494, 495, 497], "axi": [37, 38, 39, 40, 41, 74, 103, 104, 108, 274, 283, 322, 333, 348, 349, 451, 490], "otherwis": [37, 39, 40, 41, 60, 206, 320, 422, 439, 473, 494], "subdevic": [37, 38, 39, 40, 41, 333], "worker": [37, 38, 39, 40, 41, 333, 488, 490, 493, 494, 495, 496], "link": [37, 38, 39, 40, 41, 74, 333, 451], "fabric": [37, 38, 39, 41, 307, 333, 334], "mesh_devic": [37, 38, 39, 74, 307, 334, 451, 488, 490, 492, 493, 494, 495], "open_mesh_devic": [37, 38, 39, 74, 492], "meshshap": [37, 38, 39, 74, 492], "mesh_mapp": [37, 38, 39, 47, 74, 162, 307, 334, 492], "replicatetensortomesh": 37, "chunks_per_sync": [38, 333], "num_workers_per_link": [38, 333], "num_buffers_per_channel": [38, 333], "concaten": [38, 88, 89, 444, 447, 451, 454, 491], "fall": [38, 94, 221, 333, 349, 392], "hyperparamet": [38, 333], "output_shap": [38, 139, 274, 320, 333], "num_devic": [38, 74, 274, 333, 451], "total": [38, 40, 41, 274, 283, 333, 402, 484, 493, 495], "full_tensor": [38, 39], "ttnn_tensor": [38, 39, 74, 96, 105, 118, 162, 232, 330, 333, 434, 436, 437, 438], "input_dtyp": [38, 39, 74, 316, 317, 318, 319, 491], "mem_config": [38, 39, 74], "shardtensor2dmesh": [38, 39], "mesh_shap": [38, 39], "print": [38, 39, 177, 266, 302, 333, 357, 375, 402, 484, 485, 488, 489, 492, 495], "expert_metadata_tensor": [40, 41, 282], "expert_mapping_tensor": [40, 41, 282], "local_reduc": 40, "output_shard_dim": 40, "combin": [40, 217, 268, 355, 367, 487, 491, 493], "expert": [40, 41, 281, 282, 283], "metadata": [40, 41, 282, 495], "dispatch": [40, 41, 302, 443, 484, 492, 496], "origin": [40, 139, 269, 273, 275, 279, 280, 305, 320, 399, 412, 419, 447, 451, 470, 480, 483, 491, 492, 494], "length": [40, 41, 92, 315, 356, 442, 443, 445, 446, 448, 449, 450, 451, 452, 453, 455, 491, 496], "hidden": [40, 41, 491, 496], "per": [40, 41, 172, 177, 216, 349, 356, 402, 443, 452, 485, 489, 492, 497], "d": [40, 41, 140, 482, 492], "insid": [40, 485], "tabl": [40, 400, 442, 443, 450, 497], "hot": [40, 41], "encod": [40, 41, 491], "locat": [40, 41, 117, 177, 281, 284, 304, 367, 371, 439, 481, 484, 486, 491, 492, 496, 497], "local": [40, 41, 282, 283, 479, 480, 485, 486, 488, 489, 490, 493, 494, 495, 497], "prior": 40, "cross": [40, 41, 92, 94, 316, 317, 318, 319], "though": [40, 41, 268], "assert": [40, 41, 435, 461, 494], "spars": [40, 41, 165, 400], "popul": [40, 41, 212, 279, 484], "placehold": [40, 41, 492], "output_memory_config": [40, 352], "expert_indices_tensor": 41, "output_concat_dim": 41, "send": [41, 307], "did": 41, "fulli": [41, 483, 484, 493, 495], "replic": [41, 283, 452, 469], "care": 41, "guarante": 41, "init": 41, "unless": [41, 160], "garbag": 41, "metadata_tensor": 41, "_": [42, 121, 479, 484, 492, 494], "2i": 42, "last": [42, 46, 85, 121, 148, 163, 169, 172, 176, 177, 178, 215, 216, 217, 281, 300, 337, 350, 353, 354, 355, 357, 367, 369, 370, 391, 392, 399, 420, 436, 439, 452, 454, 472, 477, 484, 485, 491, 492], "even": [42, 105, 266, 268, 289, 290, 492], "altern": [42, 52, 88, 268, 490, 492, 493], "complex": [42, 43, 44, 87, 90, 91, 198, 199, 204, 205, 308, 309, 328, 329, 488, 495], "90": [42, 357, 392, 495], "degre": [42, 106, 107, 323, 324, 357], "real_part": [43, 90, 198, 204, 205, 328], "imag_part": [43, 90, 198, 204, 205, 328], "phase": 43, "input_r": [44, 91, 199, 309, 329], "input_imag": [44, 91, 199, 309, 329], "grad_data": [44, 123, 199, 329], "end": [45, 103, 104, 111, 115, 206, 226, 295, 390, 467, 480, 484, 490, 491, 492, 494, 496, 497], "inclus": [45, 325, 485], "exclus": [45, 304, 320, 325], "9": [45, 81, 83, 100, 122, 265, 367, 388, 457, 473, 491, 493, 494], "keepdim": [46, 269, 273, 275, 320, 412, 419, 470, 491], "tensor_input": [46, 103, 104, 121, 269, 273, 275, 412, 419, 439, 470], "yield": [46, 269, 273, 275, 412, 419, 470, 495], "output_onedim": 46, "onedim": 46, "output_alldim": 46, "alldim": 46, "cache_file_nam": 47, "pathlib": [47, 118, 232, 496], "path": [47, 118, 232, 349, 469, 482, 484, 493, 495, 496, 497], "callabl": [47, 279, 280], "cpptensortomesh": 47, "serial": 47, "tensortomesh": [47, 162], "truncat": [47, 459, 460], "mantissa": 47, "bit": [47, 67, 251, 256, 383, 492], "bfp": [47, 485], "rais": [47, 438, 491], "runtim": [47, 400, 443, 482, 496, 497], "error": [47, 129, 130, 131, 132, 133, 134, 214, 266, 268, 285, 357, 438, 480, 488, 490, 492, 493, 494, 495], "rte": 47, "bfp8": 47, "bfp4": 47, "arcsin": [48, 49, 50, 51], "sine": [49, 51, 386, 387, 388, 389], "z": [52, 95, 284, 351, 484], "tensor_a": 52, "rounding_mod": [53, 111, 112, 115, 326, 327], "other_tensor": [53, 112], "arctang": [54, 55, 57, 58, 59], "arctan": 55, "right": [55, 72, 111, 115, 166, 178, 218, 256], "quadrant": 55, "handl": [55, 172, 177, 443, 447, 451, 491, 492, 496], "tangent": [57, 59, 423, 424, 425, 426], "ceil_mod": [60, 271, 495], "count_include_pad": 60, "divisor_overrid": 60, "window": [60, 271, 446, 448, 449, 452, 453, 455, 488, 491, 495], "convolv": [60, 92, 93, 94, 271, 316, 317, 318, 319], "formula": [60, 79, 120, 184, 225, 226, 261, 394, 396], "avg": 60, "region": [60, 93, 149, 150, 302, 491], "createdevic": [60, 271], "kernel_w": [60, 271], "stride_h": [60, 157, 271], "stride_w": [60, 157, 271], "40": [60, 271], "ep": [61, 259, 261], "1e": [61, 178, 206, 215, 216, 353, 354, 492], "05": [61, 490], "momentum": 61, "running_mean": 61, "running_var": 61, "train": [61, 491, 492, 493, 495, 497], "norm": [61, 178, 215, 216, 217, 353, 354, 355, 491], "see": [61, 178, 215, 216, 217, 353, 354, 355, 400, 480, 481, 483, 484, 487, 488, 490, 492, 493, 494, 495, 497], "spatial": [61, 175, 177, 357, 469, 488], "gamma": [61, 109, 217, 229, 230, 287, 288, 355], "epsilon": [61, 178, 215, 216, 217, 259, 261, 353, 354, 355, 492], "mu": [61, 178, 215, 216, 217, 471], "sigma": [61, 178, 215, 216, 217, 413], "cdot": [61, 103, 104, 178, 215, 216, 217, 353, 354, 355, 475, 490], "varianc": [61, 178, 215, 216, 217, 354, 413, 471, 485], "learnabl": [61, 178, 215, 216, 217, 491], "shift": [61, 69, 72, 178, 215, 216, 217, 251, 256, 353, 354, 355, 356], "updat": [61, 213, 443, 461, 463, 468, 480, 483, 493, 495], "evalu": [61, 312], "These": [61, 268, 480, 484, 486, 490, 491, 492, 493, 497], "math_op": 62, "w0": 62, "z0": 62, "y0": 62, "x0": 62, "w1": [62, 492, 493], "z1": 62, "y1": 62, "x1": [62, 492], "bcastopdim": 62, "hw": [62, 150, 299, 368, 413, 471], "hold": [62, 443], "ye": [62, 95, 148, 149, 150, 284], "aggreg": [62, 484], "bcastopmath": 62, "mul": [62, 490, 494], "acceler": [62, 157, 284, 431, 432, 433, 466, 467, 482, 489, 491, 492, 493], "bank": 62, "param": [62, 80, 139, 147, 161, 202, 203, 281, 304, 351, 352, 376, 377, 401, 410, 411, 433, 434, 456, 464, 468], "draw": [63, 178], "probabl": [63, 141, 367, 491], "accord": [63, 87, 96, 97, 200, 268, 306, 346, 367, 371, 372, 435, 461, 463], "nd": [63, 87, 96, 97, 117, 148, 269, 273, 275, 412, 419, 435, 461, 463, 470], "50": [63, 492], "chanc": 63, "fill_valu": [63, 145, 148, 163, 164, 367, 485, 487, 490, 491], "approxim": [66, 111, 115, 116, 129, 131, 135, 142, 170, 171, 182, 233, 234, 236, 238, 289, 290, 362, 378, 379, 403, 425, 427], "string": [66, 111, 115, 171, 279, 280, 326, 327, 375, 491], "reinterpret": 67, "pair": [67, 166, 177, 218], "16457": 67, "16429": 67, "32641": 67, "31744": 67, "integ": [68, 69, 70, 71, 72, 73, 76, 103, 152, 166, 218, 251, 256, 267, 283, 304, 313, 325, 356, 410, 443, 446, 448, 449, 453, 459, 469, 492, 496], "bitwis": [68, 69, 70, 71, 72, 73], "AND": [68, 249, 250], "shift_bit": [69, 72, 251, 256], "31": [69, 72, 149, 150, 200, 251, 256, 267, 487, 490], "2147483647": [70, 166], "NOT": [70, 252, 253], "xor": [73, 257, 258], "sender_coord": [74, 307], "meshcoordin": [74, 307, 334], "ring": [74, 450, 451], "sender": 74, "coordin": [74, 177, 307, 309, 334, 357, 450, 469, 485], "meshtensor": 74, "http": [74, 443, 481, 482, 491, 497], "com": [74, 443, 481, 482, 491], "blob": 74, "main": [74, 457, 458, 483, 487, 488, 490, 491, 493, 494, 495, 497], "tech_report": 74, "programming_mesh_of_devic": 74, "programming_mesh_of_devices_with_tt": 74, "md": [74, 480, 482], "sender_tensor": 74, "device_tensor": [74, 96, 97, 492], "device_idx": 74, "sender_coord_tupl": 74, "append": [74, 484], "mesh_tensor_torch": 74, "cat": [74, 454, 491, 494], "mesh_mapper_config": 74, "meshmapperconfig": 74, "placementrepl": 74, "placementshard": 74, "create_mesh_mapp": 74, "cube": 75, "root": [75, 334, 353, 354, 355, 362, 363, 403, 404, 440, 441, 492, 493, 495, 497], "smallest": [76, 439], "version": [81, 94, 178, 279, 280, 320, 368, 379, 431, 446, 448, 449, 453, 482, 484, 486, 488, 489, 490, 492, 493, 494, 495, 496, 497], "min_tensor": [81, 83], "max_tensor": [81, 83], "min_val": [82, 84, 189], "max_val": [82, 84, 189], "computekernelconfig": [85, 121, 269, 273, 275, 412, 419, 470], "alter": 85, "l1_memory_config": [85, 434, 437, 479, 485, 489, 492, 494], "unpad": [85, 436, 467], "necessari": [85, 480, 486, 491, 492, 493], "remov": [86, 410, 438, 467], "success": [86, 488, 497], "imaginari": [87, 198, 199, 204], "uint8": [87, 96, 97, 197, 435, 461], "constructor": 87, "group": [88, 92, 93, 94, 177, 178, 283, 316, 317, 318, 319, 402, 485, 488, 491, 495], "partit": [88, 274, 283], "recombin": 88, "residu": [88, 217, 355, 491, 496], "concatenated_tensor": 88, "14": [89, 164, 457, 488, 490, 491, 493, 494, 495], "30": [89, 357, 371, 487, 490, 491, 495], "conjug": [90, 91], "grad_real": [91, 309], "grad_imag": [91, 309], "weight_tensor": [92, 93, 94, 140, 318, 319, 488, 491, 495], "in_channel": [92, 93, 94, 316, 317, 318, 319, 488, 491, 495], "out_channel": [92, 93, 94, 316, 317, 318, 319, 488, 491, 495], "input_length": 92, "bias_tensor": [92, 94, 140, 488, 495], "conv_config": [92, 93, 94, 316, 317, 318, 319, 488, 495], "compute_config": [92, 93, 94, 316, 317, 318, 319], "return_output_dim": [92, 93, 94, 491], "return_weights_and_bia": [92, 93, 94, 491], "signal": [92, 93, 140, 175], "compos": [92, 93, 94, 140, 175, 438, 491, 493, 495], "sever": [92, 93, 94, 140, 175, 494, 495], "plane": [92, 93, 94, 140, 175], "kernel_height": [92, 93, 318, 319], "kernel_width": [92, 93, 318, 319], "correl": [92, 94, 316, 317, 318, 319, 480, 497], "side": [92, 93, 94, 139, 177, 304, 316, 317, 318, 319, 484], "pad_length": 92, "pad_left": [92, 93, 94, 316, 317, 318, 319], "pad_right": [92, 93, 94, 316, 317, 318, 319], "connect": [92, 93, 94, 316, 317, 318, 319, 491, 493, 495, 496, 497], "bias": [92, 94, 215, 216, 217, 479, 491, 493, 494, 495], "input_height": [93, 94, 316, 317, 318, 319, 488, 491, 495], "input_width": [93, 94, 316, 317, 318, 319, 488, 491, 495], "slice_config": 93, "travers": [93, 102], "4d": [93, 178, 281, 367, 391, 413, 439, 471, 491], "overlap": [93, 178, 491], "tech": 93, "typic": [93, 121, 175, 177, 368, 450, 485, 489, 491, 495], "pad_height": [93, 94, 316, 317, 318, 319], "pad_width": [93, 94, 316, 317, 318, 319], "pad_top": [93, 94, 316, 317, 318, 319], "pad_bottom": [93, 94, 316, 317, 318, 319], "mirror_kernel": 94, "dram_slice_config": 94, "seen": [94, 102, 489, 491], "fraction": [94, 158, 159, 469], "deconvolut": 94, "o": [94, 482, 484, 491, 493, 494, 495, 496], "k_h": 94, "k_w": 94, "equat": 94, "h_out": [94, 491], "h_in": [94, 177], "output_pad": 94, "w_out": [94, 491], "w_in": [94, 177], "mirror": [94, 269, 273, 275, 412, 419, 470], "been": [94, 279, 280, 422, 480, 491, 497], "host_tensor": [96, 97, 492], "cq_id": [96, 97, 162, 422, 438, 492], "queueid": [96, 97, 422], "allocate_tensor_on_host": 96, "device_tensor_copi": 97, "allocate_tensor_on_devic": 97, "shardstrategi": [102, 202, 352, 492], "use_height_and_width_as_shard_shap": 102, "320": 102, "reverse_ord": [103, 104], "cumul": [103, 104, 367, 455], "_1": [103, 104], "_2": [103, 104, 491], "cast": [103, 104, 461], "accumul": [103, 104, 492], "begin": [103, 104, 111, 115, 206, 295, 484, 490, 492], "tensor_output": [103, 104, 121, 269, 273, 275, 281, 412, 419, 470], "With": [103, 104, 121, 486, 492], "preallocated_output": [103, 104, 121], "resourc": [105, 491, 492, 497], "whose": [105, 125, 356], "radian": [106, 107, 308, 323, 324], "zero_point": [108, 322], "de": 108, "point": [108, 155, 226, 295, 307, 322, 325, 349, 357, 375, 393, 469, 484, 485], "127": 108, "43": 108, "001173": [108, 322, 349], "213": [108, 322, 349], "logarithm": [109, 233, 234, 235, 238, 239, 240, 241, 288, 492], "deriv": 109, "fast_and_approximate_mod": [111, 115, 116, 129, 131, 135, 170, 233, 234, 236, 238, 289, 290, 362, 378, 379, 403, 425, 427], "_mode": [111, 115], "fast": [111, 115, 116, 129, 131, 135, 170, 233, 234, 236, 238, 289, 290, 362, 378, 379, 403, 492, 496], "accur": [111, 115, 379], "pcc": [112, 193, 221, 327, 480, 483, 492, 497], "degrad": [112, 193, 221, 327, 488, 490, 493, 494, 495], "nan": [113, 114, 206, 209], "denomin": [114, 326], "fpu": [116, 289, 290], "sfpu": [116, 197, 289, 290], "sinc": [116, 197, 290, 451, 491], "tensor_addr": 117, "num_lay": [117, 491], "enable_performance_mod": 117, "asynchron": 117, "fetch": 117, "neighbour": 117, "push": 117, "consum": [117, 392], "t1_l1": 117, "t2_l1": 117, "t1_l2": 117, "t2_l2": 117, "t1_l3": 117, "t2_l3": 117, "cb": [117, 178, 484], "downstream": 117, "4u": 117, "todo": [117, 492], "file_nam": [118, 232], "dump": [118, 232, 279, 497], "save": [118, 392, 492], "tensorbin": [118, 232], "exponenti": [121, 135, 137, 138, 313, 379, 492, 496], "_t": 121, "_0": 121, "smooth": [121, 357], "99": [121, 492, 495, 496], "padding_idx": 122, "embeddings_typ": 122, "embeddingstyp": 122, "retriev": [122, 491], "word": 122, "lookup": [122, 491], "output_gradient_tensor": 123, "extract": [123, 165, 381, 491, 495], "vocabulari": [123, 491, 496], "seq_len": [123, 491, 492, 496], "embedding_dim": 123, "num_embed": 123, "1024": [123, 256, 489, 492, 495], "4096": [123, 492], "3200": 123, "input_index": 123, "randint": [123, 201, 371, 492, 496], "weights_shap": 123, "weights_ttnn": 123, "grad_shap": 123, "uniniti": [124, 125], "bfloat_8": 124, "reference_tensor": [125, 164, 301, 478], "_tensor_i": [128, 174, 181, 228, 252, 264, 296], "complementari": [131, 132], "singleton": 139, "cost": [139, 350, 472], "lack": [139, 491], "conv3dconfig": 140, "3d": 140, "kd": 140, "kh": [140, 488], "kw": [140, 488], "c_in": [140, 491], "c_out": [140, 491], "rng": 141, "total_elem": 141, "124": 141, "prob": [141, 491], "algorithm": [142, 178, 379, 469], "batch_idx": 147, "cache_tensor": [147, 377], "implicit": 148, "tile_height": [148, 215, 216, 217, 354, 355, 450], "tile_width": [148, 215, 216, 353, 354, 369, 370], "tt_lib": [148, 149, 150, 284, 434], "inf": [148, 182, 278, 287, 491, 492], "hone": [149, 150], "wone": [149, 150], "val_hi": [149, 150], "val_lo": [149, 150], "96": [149, 150, 495], "33": [149, 150], "xt": [149, 150], "tolist": [149, 150], "Ones": 149, "nchw": [150, 495], "rest": 150, "hfill": 150, "wfill": 150, "hi": 150, "lo": 150, "low": [150, 325, 431, 492], "largest": [152, 367, 439, 485], "modulo": 155, "bfloat4": 160, "bfloat8": [160, 439], "ttnn_tensor_on_devic": 161, "ttnn_tensor_on_host": 161, "pad_valu": [162, 306, 350, 390, 432, 456], "itself": [162, 491], "twice": [162, 484], "purpos": [162, 480, 483, 485, 491, 496, 497], "now": [162, 216, 217, 354, 355, 399, 438, 485, 487, 491, 492, 494], "mapper": 162, "torch_tensor": [162, 438, 490, 492], "templat": [164, 301, 478, 484, 492], "exce": [165, 367], "do": [165, 367, 402, 480], "sparse_grad": 165, "ttnn_input": 165, "ttnn_index": 165, "greatest": 166, "2147483648": 166, "gate": [169, 176, 337, 420], "unit": [169, 176, 337, 402, 420, 480, 482, 491], "program_descriptor": 172, "programdescriptor": 172, "meshprogramdescriptor": 172, "flexibl": [172, 177, 443, 492, 495, 497], "construct": 172, "descriptor": 172, "semaphor": [172, 451], "spmd": 172, "explicit": [172, 349, 455, 492], "unit_test": 172, "test_generic_op": 172, "multidevic": 173, "entir": [175, 422, 431, 491, 492, 497], "_avg": 175, "_pool": 175, "padding_mod": 177, "align_corn": 177, "use_precomputed_grid": 177, "batch_output_channel": 177, "bilinear": [177, 357, 469], "interpol": [177, 226, 357, 491], "arbitrari": [177, 357, 491], "commonli": [177, 196, 369, 370], "warp": 177, "h_grid": 177, "w_grid": 177, "leftmost": 177, "rightmost": 177, "topmost": 177, "bottommost": 177, "precomput": 177, "pixel": [177, 357, 491, 495], "prepare_grid_sample_grid": 177, "corner": 177, "extend": [177, 268], "doesn": [177, 279, 280, 491], "disappear": 177, "theta": [177, 308], "affine_grid": 177, "grid_tensor": 177, "output_default": 177, "workflow": [177, 480, 482, 497], "natur": [177, 233, 240, 491, 492], "natural_grid": 177, "batched_grid": 177, "batched_grid_tensor": 177, "output_w_extend": 177, "output_c_extend": 177, "grid_float32": 177, "output_float32": 177, "grid_float32_host": 177, "prepared_grid": 177, "output_dtyp": [177, 202, 203, 316, 317, 318, 319, 376, 377], "output_precomput": 177, "num_group": 178, "num_out_block": 178, "negative_mask": 178, "use_welford": 178, "groupnorm": 178, "tradition": 178, "slightli": 178, "form": [178, 308, 469], "determine_expected_group_norm_sharded_config_and_grid_s": 178, "create_group_norm_input_mask": 178, "create_group_norm_weight_bias_rm": 178, "properli": [178, 484, 488, 496], "fact": 178, "rm": [178, 353, 354, 355, 482], "welford": 178, "fp32": [178, 492], "upon": [178, 480], "rather": 178, "torch_input_tensor": 178, "torch_weight": 178, "torch_bia": 178, "grid_siz": [178, 402], "num_channel": 178, "input_nhw": 178, "is_height_shard": 178, "is_row_major": 178, "correct": [178, 269, 273, 275, 412, 419, 470, 492, 493, 495], "block_wt": 178, "As": [178, 487, 489, 494], "input_mask_tensor": 178, "everi": [178, 320, 480, 484, 490, 494, 497], "half": 178, "32x32": [178, 485, 490, 491], "num_cores_across_channel": 178, "explain": [178, 484], "suppli": 178, "data_typ": [178, 494], "isn": [178, 485], "Then": [178, 454, 479, 482, 497], "tiles_per_core_tot": 178, "num_cores_x": [178, 479, 494], "gamma_t": 178, "beta_t": 178, "480": 178, "input_tensor_row_major": 178, "input_tensor_til": 178, "use_multicor": [178, 304, 431, 432, 433, 466, 467], "width_per_group": 178, "max_tiles_group_can_span": 178, "values_per_chunk": 178, "values_per_chunk_per_til": 178, "15": [178, 455, 457, 458, 488, 490, 492, 495], "gamma_beta": 178, "20": [182, 278, 348, 371, 393, 394, 455, 495, 497], "hard": [182, 183, 184, 185, 186, 187, 188, 189, 190, 393], "piecewis": 182, "offer": [182, 497], "maintain": [182, 442, 483, 492], "similar": [182, 455, 484, 491, 492, 497], "lambd": [183, 184, 395, 396], "shrinkag": [183, 395], "shrink": [183, 184, 395, 396, 427, 428], "lambda": [184, 396, 479, 491, 496], "hypotenus": [192, 193], "zeroth": 194, "bessel": [194, 195, 196, 269, 273, 275, 412, 419, 470], "i_1": 196, "kind": 196, "physic": [196, 443, 485, 489], "problem": [196, 268], "cylindr": 196, "symmetri": 196, "shouldn": 197, "lower": [197, 304, 325, 343, 457, 463, 492], "float16": 197, "unchang": 197, "tt_index": 200, "batch_id": 201, "replac": [201, 429, 492], "denot": 201, "input_a_shap": 201, "input_b_shap": 201, "batch_id_ttnn": 201, "shard_schem": [202, 203], "ttl": [202, 203, 484], "shard_orient": 202, "col": [202, 203, 485], "sharded_memory_config": 202, "kwtype": [202, 203, 281, 350, 376, 377], "orientt": 202, "sharded_tensor": [202, 203, 352, 492], "sharded_memory_config_dict": [202, 352], "dict": [202, 279, 280, 352, 491, 495], "shard_memory_config": [202, 352], "input_sharded_memory_config_arg": [202, 352], "partial": [203, 377], "num_slic": [203, 377], "slice_index": [203, 377], "shard_orientt": 203, "pure": [204, 205, 492], "rtol": 206, "05f": 206, "atol": 206, "08f": 206, "equal_nan": 206, "leq": 206, "rel": 206, "toler": 206, "treat": [206, 268, 368], "finit": 207, "infinit": 208, "infin": [210, 211, 485], "posit": [211, 357, 360, 442, 446, 448, 449, 450, 452, 453, 491, 496], "batch_index": 212, "update_index": [213, 468], "batch_offset": [213, 468], "input_refer": [214, 285], "input_predict": [214, 285], "predict": [214, 285, 493, 495, 496], "programconfig": [215, 216, 217, 353, 354, 355], "unshard": [215, 217, 353, 355, 370, 485], "cannot": [215, 216, 217, 353, 354, 355, 402, 491], "stick": 215, "conjunct": [216, 217, 354, 355], "layernorm": [216, 491, 492], "statist": [216, 354, 491], "On": [216, 217, 354, 355, 482, 497], "post": [216, 217, 335, 354, 355, 484, 491, 492], "sum_": [217, 312, 353, 354, 355, 391, 392], "Its": [217, 355], "stat": [217, 355], "32768": 218, "32767": 218, "80": [221, 492, 495], "outsid": [221, 357], "negative_slop": [224, 225], "slope": [224, 225], "leaki": [224, 225], "01": [225, 497], "transpose_a": [231, 268], "transpose_b": [231, 268, 491], "behaviour": [231, 268], "loaded_tensor": 232, "1e7": 236, "logic": [249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 268, 279, 280, 451], "shift_amt": [251, 256], "vacat": 256, "512": [256, 489, 492], "land": [257, 258], "lnot": [257, 258], "lor": [257, 258], "logitep": 261, "11": [265, 457, 458, 473, 482, 491], "context": [266, 335, 336, 491, 496], "exit": 266, "occur": 266, "user_id": 267, "pseudo": 267, "prng": 267, "potenti": [267, 492], "associ": [267, 372, 422, 491, 497], "constrain": [267, 367], "meaning": 267, "chang": [267, 350, 431, 432, 433, 436, 437, 466, 467, 472, 485, 492], "seed_tensor": 267, "user_id_tensor": 267, "dimension": [268, 288, 485], "although": 268, "variou": [268, 490, 492], "abov": [268, 457, 458, 482, 485, 491, 492], "criteria": 268, "messag": [268, 484, 495, 497], "unexpect": 268, "obviou": 268, "relat": 268, "swap": 268, "j": [268, 391, 392], "patch": [268, 491], "leverag": [268, 492], "those": [268, 484], "n_size": 268, "m_size": 268, "k_size": 268, "carefulli": 268, "fix": [268, 443, 488, 491, 497], "describ": [268, 400, 480], "subcor": [269, 273, 275, 412, 419, 470], "return_indic": 271, "dilation_h": 271, "dilation_w": 271, "tt_input_dev": 271, "16777216": 272, "find": [272, 277, 295, 297, 482, 491, 497], "th": [274, 281], "tt_input_tensors_list": 274, "output_mem_config": 274, "initialize_model": [279, 280, 479, 496], "model_nam": [279, 280, 479, 491, 496], "convert_to_ttnn": [279, 280, 479], "custom_preprocessor": [279, 280, 479, 496], "parameterdict": [279, 280], "prefix": [279, 280, 443, 450, 491], "run_model": [279, 492], "reader_patterns_cach": 279, "git": [279, 280, 482], "hash": [279, 280, 484], "invalid": [279, 280, 281, 400], "preprocessor": [279, 280], "put": [279, 280, 479, 488, 489, 490, 492, 493, 494, 495], "submodul": [279, 280, 482, 493, 495], "ttnn_module_arg": 279, "tmp": [279, 497], "model_graph": 279, "svg": 279, "recomput": [279, 350], "val": 281, "ind": 281, "expert_mask_tensor": 281, "topk_mask_tensor": 281, "everyth": [281, 488, 489, 490, 493, 494, 495, 497], "power": [281, 313, 314, 360, 361, 462, 492, 496, 497], "exactli": [281, 367], "expert_mask": 281, "tope_mask": 281, "topk_tensor": 282, "output_mapping_tensor": 282, "output_reduced_tensor": 282, "reduction_s": 282, "remap": [282, 283, 482], "score": [282, 454, 491, 492], "seq": 282, "select_experts_k": 282, "experts_per_devic": 282, "routing_weights_tensor": 283, "non_zero_weight_s": 283, "expert_parallel_s": 283, "rout": [283, 469, 482], "uniformli": [283, 325, 463], "total_expert": 283, "arg0": 284, "arg1": 284, "mse": 285, "multivari": [287, 288], "mvlgamma": 288, "5f": 288, "nearest": [289, 290, 357, 433, 439, 469], "rne": [289, 290], "inequ": [291, 292], "negat": 294, "_float": 295, "neq": 295, "represent": [295, 393, 485, 491, 496, 497], "toward": 295, "well": [297, 482, 483, 491], "input_tensor_ttnn": [297, 399], "nonzero_indic": 297, "trace_region_s": 302, "dispatch_core_config": [302, 496], "dispatchcoreconfig": [302, 496], "worker_l1_s": 302, "default_l1_small_s": 302, "default_trace_region_s": 302, "num_command_queu": 302, "allocat": 302, "0x7fbac5bfc1b0": 302, "otim": 303, "mutual": [304, 320], "output_tensor_shap": [304, 432], "input_tensor_start": 304, "union": [304, 401], "padded_tensor": 304, "abc": [305, 422], "unpadded_shap": 305, "padded_shap": 305, "tthe": 306, "broken": [306, 456], "permuted_tensor": 306, "receiv": [307, 484], "receiver_coord": 307, "intermediate_tensor": [307, 334], "input_tensor_torch": 307, "arbitari": 307, "shardtensortomesh": [307, 334, 492], "sent_tensor": 307, "cartesian": 308, "r": [308, 482, 497], "magnitud": [308, 485, 494], "14159": 308, "decim": [310, 358, 375], "coeff": 312, "coeffici": [312, 480], "polynomi": 312, "expon": [313, 314, 360, 361, 485], "arrai": [315, 490, 491], "25": [315, 455, 495], "invoc": [316, 317, 318, 319], "exact": [316, 317, 318, 319, 485, 491], "input_memory_config": [316, 317, 318, 319, 491], "input_layout": [316, 317, 318, 319, 491], "convtranspose2d": [317, 318], "conv_tranpose2d": 318, "weights_format": [318, 319, 491], "iohw": 318, "has_bia": [318, 319, 491], "term": [318, 319, 483], "oihw": [319, 491], "overload": [320, 402, 492], "nich": 320, "nc": 320, "definit": 320, "intend": [320, 483], "output_all_dim": 320, "particular": [321, 479], "taken": [321, 473, 491, 492], "all_dims_output": 321, "upper": [325, 342, 458, 463, 485, 491], "reproduc": [325, 367, 488], "consid": [326, 367], "numer": [326, 368, 369, 370, 391, 392, 393, 481, 485, 489, 491, 492], "revers": [326, 327, 360, 361, 364, 365, 366, 491], "ttnn_tensor_realloc": 330, "inaccur": [331, 485], "fp": 331, "intermediate_memory_config": 333, "break": [333, 493, 495], "apart": 333, "output_tensor_l": 334, "sdpa": [334, 443, 455, 492], "tree": 334, "input_tensor_l": 334, "l": [334, 447, 451], "input_tensor_": 334, "state": [334, 442, 488, 491], "input_tensor_m": 334, "root_coord": 334, "output_tensor_": 334, "output_tensor_m": 334, "input_tensor_torch_l": 334, "input_tensor_torch_": 334, "input_tensor_torch_m": 334, "scale_fp32": 334, "upper_limit": 342, "cap": 342, "lower_limit": 343, "modulu": 344, "whb0": 345, "repetition_vector": 346, "smallvector": 346, "repetit": [346, 348], "input_tensor_tt": [346, 350], "repeated_tensor": [346, 348], "2x": [347, 492], "he": 348, "fit": 348, "in_scal": 349, "in_zero_point": 349, "out_scal": 349, "out_zero_point": 349, "re": [349, 479, 481, 489, 491, 492, 497], "mix": 349, "mathemat": [349, 492, 496], "q": [349, 443, 445, 446, 447, 448, 449, 451, 452, 453, 455, 491, 492], "s_in": 349, "s_out": 349, "z_out": 349, "z_in": 349, "decompos": [349, 485], "002727": 349, "73": [349, 497], "recreate_mapping_tensor": 350, "allevi": 350, "slow": [350, 492], "reshaped_tensor": 350, "rmsnorm": 354, "use_2d_core_grid": [354, 355], "center": [357, 452, 491], "interpolation_mod": 357, "around": [357, 484], "area": 357, "counter": [357, 493, 495], "clockwis": 357, "sharp": [357, 491], "faster": [357, 425, 427, 484, 492, 494, 496], "slower": [357, 491, 492], "dram_interleav": 357, "45": [357, 423, 424, 455], "output_cw": 357, "smoother": 357, "output_smooth": 357, "white": 357, "output_custom": 357, "subract": 366, "nucleu": 367, "under": [367, 480, 483, 484, 497], "remain": [367, 485, 492], "regardless": [367, 491], "mass": 367, "filter": [367, 497], "corpu": 367, "multinomi": 367, "randomli": 367, "final_index": 367, "assist": 367, "temperatur": [367, 491], "attr": 367, "sequenti": [367, 493], "indices_1d": 367, "indices_reshap": 367, "k_tensor": 367, "p_tensor": 367, "temp_tensor": 367, "numeric_st": [368, 369, 370, 391, 392, 491, 494], "causal": [368, 369, 370, 440, 441, 442, 443, 445, 447, 450, 451, 452, 491, 492], "d_k": [368, 492], "stabl": [368, 369, 370, 391, 392, 399, 487, 491], "hw_dims_onli": 368, "input_til": 368, "tt_output_shard": 368, "is_causal_mask": [369, 370], "mechan": [369, 370, 455], "inherit": [369, 391], "restrict": [370, 431, 455, 482], "src": [371, 372], "scatterreductiontyp": 371, "onto": [371, 372], "amax": 371, "amin": 371, "input_torch": [371, 372], "index_torch": [371, 372], "int64": [371, 372], "source_torch": [371, 372], "input_ttnn": [371, 372], "index_ttnn": [371, 372], "source_ttnn": [371, 372], "0507": 373, "67326": 373, "sci_mod": 375, "scientif": 375, "notat": 375, "detect": 375, "digit": [375, 493], "short": 375, "interleaved_tensor": [376, 377], "vector_mod": 378, "rc": [378, 482], "slice_start": 390, "slice_end": 390, "slice_step": 390, "input_tensor_shap": 390, "unmodifi": 390, "undefin": 390, "sliced_tensor": 390, "x_i": [391, 392, 491], "x_j": [391, 392], "steep": 393, "steeper": 393, "stabil": [393, 483, 491], "soft": [395, 396], "ascend": 399, "descend": 399, "sorted_tensor": 399, "sorted_tensor_desc": 399, "indices_desc": 399, "input_tensor_2d": 399, "input_tensor_2d_ttnn": 399, "sorted_tensor_dim": 399, "indices_dim": 399, "nnz": 400, "is_input_a_spars": 400, "is_input_b_spars": 400, "skip": [400, 446, 448, 449, 453, 496], "interpret": 400, "sparsity_bitmask": 400, "randperm": 400, "numel": 400, "num_split": 401, "split_siz": 401, "dim2": [401, 456], "units_to_divid": 402, "row_wis": 402, "iter": [402, 443, 492, 493, 494], "involv": [402, 496], "units_per_core_group_1": 402, "units_per_core_group_2": 402, "100": [402, 491, 492, 493, 495, 496], "8x8": 402, "units_1": 402, "units_2": 402, "core_rangeset": 402, "255": 405, "deviat": 413, "synchron": [422, 451, 497], "wait": [422, 484], "ran": [422, 484, 494], "chip": [422, 488, 489, 490, 491, 493, 494, 495, 497], "set_sub_device_stall_group": 422, "assum": [422, 468, 469], "queu": 422, "minor": [425, 427], "approx": [425, 427], "use_low_perf": 431, "IF": 431, "IN": 431, "tilized_tensor": [431, 432, 466, 467], "tensor_on_host": 434, "organ": [436, 480, 485, 491], "ttnn_tensor_layout_chang": 436, "ttnn_tensor_memory_config_chang": 437, "torch_rank": 438, "mesh_compos": 438, "cppmeshtotensor": 438, "Will": 438, "reach": 438, "indices_tensor": 439, "output_value_tensor": 439, "output_index_tensor": 439, "fundament": [439, 492], "manipul": [439, 492, 493], "restor": [439, 450], "afterward": 439, "65536": 439, "head_siz": [440, 441, 444, 454, 491, 494], "attention_mask": [440, 441, 491, 494, 496], "causal_mask": [440, 441, 492], "input_tensor_q": [442, 443, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455], "input_tensor_k": [442, 443, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455], "page_table_tensor": [442, 443], "chunk_start_idx": [442, 443, 450], "head_dim_v": [442, 445], "sdpaprogramconfig": [442, 443, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455], "longer": [442, 489], "nqh": [442, 443, 445, 450, 452, 455], "dh": [442, 443, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455], "nkv": [442, 443, 445, 446, 448, 449, 450, 452, 453, 455], "num_pag": [442, 443, 450], "q_chunk_siz": [442, 443, 450], "input_tensor_v": [443, 446, 447, 448, 449, 450, 451, 452, 453, 455], "chunk_start_idx_tensor": 443, "convent": 443, "offset": [443, 451], "later": 443, "captur": [443, 496], "replai": [443, 492], "recompil": 443, "One": [443, 492], "chunk_": 443, "max_block": 443, "block_": 443, "k_chunk_siz": 443, "workaround": 443, "35225": 443, "omit": [443, 447, 451], "num_head": [444, 454, 491, 492, 494], "sequence_s": [444, 454, 479, 491, 494, 496], "attn_mask": [445, 446, 448, 449, 452, 453, 491], "is_caus": [445, 446, 448, 449, 452, 453, 492], "mla": 445, "accept": [445, 446, 448, 449, 452, 453, 455, 480, 483, 495], "parallel": [445, 446, 448, 449, 452, 453, 484, 490, 491, 492, 497], "impli": 445, "cur_po": [446, 448, 449, 453], "cur_pos_tensor": [446, 448, 449, 453], "sliding_window_s": [446, 448, 449, 452, 453], "decod": [446, 448, 449, 453], "flash": [446, 448, 449, 453, 482], "mqa": [446, 448, 449, 453], "sdpamulticoreprogramconfig": [446, 448, 449, 453], "nh": [446, 447, 448, 449, 451, 453], "slide": [446, 448, 449, 452, 453, 488, 491, 495], "pnh": [446, 448, 449, 453], "joint_tensor_q": [447, 451], "joint_tensor_k": [447, 451], "joint_tensor_v": [447, 451], "joint_strategi": [447, 451], "jointattent": 447, "queri": [447, 450, 451, 452, 454, 455, 491, 492, 494], "rear": [447, 451], "joint": [447, 451], "qk": [447, 451, 455], "ring_siz": 450, "ring_id": 450, "page_t": 450, "subset": [450, 485], "redund": 450, "caus": [450, 485], "earli": [450, 480], "late": 450, "contigu": [450, 454, 485, 495], "level": [450, 482, 484, 487, 490, 497], "reshuffl": 450, "max_num_block": 450, "block_siz": 450, "local_": 450, "persistent_output_buffer_k": 451, "persistent_output_buffer_v": 451, "logical_n": 451, "multi_device_global_semaphor": 451, "globalsemaphor": 451, "ccl_core_grid_offset": 451, "ringjointattent": 451, "persist": 451, "attention_sink": 452, "mimick": 452, "flashattent": 452, "attend": [452, 491], "sink": 452, "kv_input_tensor": 454, "num_kv_head": 454, "transpose_kei": 454, "hidden_s": [454, 479, 491, 494, 496], "readi": [454, 480, 492], "q1": 454, "k1": 454, "v1": 454, "qn": 454, "kn": 454, "vn": 454, "num": 454, "cu_window_seqlen": 455, "diagon": [455, 457, 458, 491, 492], "qwen2": 455, "vl": 455, "boundari": 455, "window_count": 455, "dim1": 456, "triangular": [457, 458, 491], "17": [457, 458, 495], "output_diag_0": [457, 458], "13": [457, 488, 490, 491, 493, 494, 495], "output_diag_1": [457, 458], "output_diag_neg1": [457, 458], "ops_chain": 462, "chain": 462, "drawn": 463, "continu": [463, 480, 482, 483], "use_pack_until": [466, 467], "untilized_tensor": [466, 467], "output_tensor_end": 467, "update_idx": 468, "scale_factor": 469, "fastest": 469, "new_shap": 472, "true_valu": 473, "false_valu": 473, "entri": 473, "basi": 479, "rewritten": 479, "modeling_bert": [479, 496], "bertintermedi": 479, "__init__": [479, 491], "super": 479, "dens": 479, "intermediate_s": 479, "hidden_st": [479, 491, 494, 496], "tdd": 479, "torch_bert": 479, "utility_funct": 479, "torch_random": 479, "utils_for_test": 479, "assert_with_pcc": 479, "mark": [479, 480], "parametr": 479, "phiyodr": [479, 496], "finetun": [479, 496], "squad2": [479, 496], "test_bert_intermedi": 479, "bertconfig": [479, 496], "from_pretrain": [479, 491, 496], "eval": [479, 496], "torch_hidden_st": [479, 494], "torch_output": [479, 494], "bert_intermedi": 479, "9999": [479, 497], "turn": 479, "ttnn_bert": [479, 496], "999": [479, 493], "someth": 479, "ttnn_optimized_bert": [479, 496], "isinst": [479, 491], "preprocess_linear_weight": [479, 494], "preprocess_linear_bia": [479, 494], "ff1_weight": 479, "ff1_bia": 479, "integr": [479, 480, 492], "incredibli": 480, "excit": 480, "exploratori": 480, "folder": [480, 484], "freedom": 480, "showcas": [480, 489], "few": [480, 485, 495], "question": [480, 496], "answer": [480, 496], "highlight": [480, 485], "successfulli": [480, 487, 492], "migrat": 480, "readm": [480, 482], "credit": 480, "author": 480, "might": [480, 489], "encount": 480, "adequ": 480, "pearson": 480, "metric": 480, "meet": 480, "commit": 480, "ongo": 480, "complianc": 480, "catch": 480, "regress": [480, 491], "varieti": 480, "measur": [480, 491, 492], "run_device_perf_model": 480, "run_perform": 480, "sh": [480, 482, 484, 497], "annot": 480, "models_device_performance_bare_met": 480, "clear": [480, 483, 488, 490, 493, 494, 495], "autom": 480, "extern": [480, 483, 485], "servic": 480, "impl": 480, "yaml": [480, 497], "models_performance_bare_met": 480, "run_demos_single_card_n150_test": 480, "run_demos_single_card_n300_test": 480, "run_t3000_demo_test": 480, "test_ttnn_functional_resnet50": 480, "resnet50testinfra": 480, "friendli": 481, "ml": [481, 482], "choic": 481, "jupyt": 481, "notebook": 481, "comprehens": [482, 492, 497], "deploy": 482, "asset": [482, 491], "tag": 482, "quick": [482, 495], "curl": 482, "fssl": 482, "v2": 482, "chmod": 482, "warn": [482, 484, 488, 490, 493, 494, 495, 496], "galaxi": 482, "6u": 482, "blackhol": [482, 492], "driver": [482, 488, 489, 490, 493, 494, 495, 497], "kmd": [482, 488, 489, 490, 493, 494, 495, 497], "ubuntu": 482, "22": [482, 488, 490, 493, 494, 495], "04": [482, 490], "fw_pack": 482, "19": [482, 495], "fwbundl": 482, "v19": 482, "v3": 482, "38": [482, 485, 487, 494], "fw": [482, 484, 488, 489, 490, 493, 494, 495], "visit": [482, 497], "immedi": 482, "closer": [482, 492], "conveni": 482, "who": [482, 485], "linux": 482, "distro": 482, "glibc": 482, "34": [482, 488, 490, 493, 494, 495, 497], "newer": [482, 496], "pip": [482, 497], "cpu": [482, 484, 488, 490, 491, 492, 493, 494, 495], "governor": 482, "export": [482, 497], "pythonpath": 482, "pwd": 482, "python_env": [482, 497], "dev": 482, "txt": [482, 497], "sudo": [482, 484], "apt": 482, "cpufrequtil": 482, "cpupow": 482, "frequenc": 482, "registri": 482, "pull": [482, 483, 488], "ghcr": 482, "io": [482, 484, 491], "amd64": 482, "bash": 482, "recurs": 482, "install_depend": 482, "build_met": [482, 484, 497], "cmake": 482, "mkdir": 482, "cd": [482, 484], "ninja": 482, "dcmake_build_typ": 482, "relwithdebuginfo": 482, "dcmake_cxx_compil": 482, "envirion": 482, "python_env_dir": 482, "path_to_your_env_directori": 482, "create_venv": 482, "bin": 482, "driven": [482, 483], "recip": 482, "conda": 482, "forg": 482, "python3": [482, 486, 487, 488, 489, 490, 493, 494, 495], "run_op_on_devic": 482, "eth": [482, 496], "loudbox": 482, "quietbox": 482, "iommu": [482, 488, 489, 490, 493, 494, 495, 497], "isol": 482, "passthrough": 482, "translat": 482, "viommu": 482, "hypervisor": 482, "secur": 482, "dma": 482, "pcie": [482, 489], "guest": 482, "corrupt": 482, "reliabl": [482, 483], "intel_iommu": 482, "amd_iommu": 482, "provis": 482, "intel": 482, "vt": 482, "amd": 482, "vi": [482, 497], "simultan": [483, 497], "tune": [483, 492, 496], "themselv": [483, 485, 491], "goal": 483, "ask": 483, "popular": 483, "kent": 483, "beck": 483, "submit": 483, "label": [483, 485, 493, 495], "fallback": 483, "branch": 483, "brief": 483, "4730": 483, "rst": 483, "referenc": 483, "sweep": 483, "codeown": 483, "pr": 483, "reflect": 483, "merg": [483, 491], "comment": 483, "bert_tini": 484, "tt_metal_hom": [484, 487, 488, 489, 490, 493, 494, 495, 497], "test_demo": 484, "finish": 484, "csv": [484, 497], "consol": 484, "give": [484, 491, 492, 497], "shorter": 484, "cli": 484, "reset": 484, "tt_smi": 484, "tensix_reset": 484, "tensix": [484, 489, 490], "skew": 484, "timer": 484, "reboot": 484, "wh": 484, "much": [484, 489], "1000": [484, 488, 490, 492, 493, 494, 495], "fixtur": 484, "readdeviceprofil": 484, "drop": 484, "120": [484, 495], "eighth": 484, "mention": 484, "come": 484, "python_fallback": 484, "tt_dnn_cpu": 484, "tt_dnn_devic": 484, "field": 484, "lofi": [484, 492], "hifi2": [484, 492], "hifi3": [484, 492], "clock": 484, "stamp": 484, "durat": [484, 494], "nanosecond": 484, "end_t": 484, "start_t": 484, "cycl": 484, "earliest": 484, "core_frequ": 484, "marker": 484, "brisc": 484, "ncrisc": 484, "trisc0": 484, "trisc1": 484, "trisc2": 484, "front": 484, "spent": [484, 494], "cb_wait_front": 484, "reserv": 484, "cb_reserve_back": 484, "datamov": 484, "input_0_memori": 484, "channels_last": 484, "dev_0_dram": 484, "dec_0_l1": 484, "noc": 484, "timelin": 484, "npe": 484, "subdirectori": 484, "npe_viz": 484, "traffic": 484, "congest": 484, "item": [484, 491, 492, 493, 495], "timestamp": [484, 497], "ops_perf_results_2025_06_25_14_04_34": 484, "2025_06_25_14_04_34": 484, "actual": [485, 492, 493, 495], "still": 485, "transit": 485, "illustr": [485, 491], "16x16": 485, "li": 485, "fashion": 485, "face0": 485, "face1": 485, "face2": 485, "face3": 485, "pictur": 485, "reason": [485, 492], "matric": [485, 489], "transpose_til": 485, "torch_t": 485, "byte": [485, 491], "That": [485, 489], "sizeof": 485, "introduc": [485, 492], "observ": [485, 495], "flush": 485, "instabl": 485, "extrem": 485, "domin": 485, "lose": 485, "7014118346046923e": 485, "frequent": 485, "occurr": 485, "deal": 485, "critic": 485, "applic": 485, "homogen": 485, "unsuit": 485, "inher": 485, "owned_host_storag": 485, "borrowed_host_storag": 485, "borrow": 485, "numpi": [485, 490, 493], "device_storag": 485, "abstract": 485, "awai": 485, "compress": 485, "128x128": 485, "know": [485, 491], "smoothli": 486, "lightweight": 486, "minim": [486, 492], "standalon": 486, "basic_python": [486, 487, 488, 489, 490, 493, 494, 495], "llama": 487, "mistral": 487, "diffus": 487, "ttnn_add_tensor": [487, 489], "loguru": [487, 488, 490, 491, 493, 494, 495], "tt_tensor1": 487, "tt_tensor2": 487, "tt_result": 487, "06": [487, 494], "23": [487, 489], "09": [487, 488, 489, 497], "36": 487, "58": 487, "211": 487, "__main__": [487, 488, 490, 493, 494, 495], "29": [487, 490], "00000": 487, "37": 487, "00": [487, 493, 495], "524": 487, "525": 487, "ttnn_basic_conv": 488, "8kb": [488, 496], "enough": [488, 495], "32kb": 488, "bchw": 488, "permuted_input": 488, "flat": 488, "reshaped_input": 488, "out_torch": 488, "07": [488, 490, 493, 494, 495], "02": [488, 497], "649": 488, "silicondriv": [488, 490, 493, 494, 495, 497], "pci": [488, 490, 493, 494, 495, 497], "pci_devic": [488, 489, 490, 493, 494, 495, 497], "198": [488, 490, 493, 494, 495], "651": 488, "658": 488, "tt_cluster": [488, 489, 490, 493, 494, 495, 497], "190": [488, 490, 493, 494, 495], "659": 488, "666": 488, "667": 488, "673": 488, "harvest": [488, 489, 490, 493, 494, 495, 497], "0x100": [488, 490, 493, 494, 495], "noc0": [488, 489, 490, 493, 494, 495, 497], "0x0": [488, 489, 490, 493, 494, 495, 497], "282": [488, 490, 493, 494, 495], "772": 488, "817": 488, "remot": [488, 489, 490, 493, 494, 495, 497], "147": [488, 490, 493, 494, 495], "828": 488, "ethernet": [488, 489, 490, 493, 494, 495], "1039": [488, 490, 493, 494, 495], "915": 488, "clk": [488, 490, 493, 494, 495], "mhz": [488, 490, 493, 494, 495], "metal_context": [488, 490, 493, 494, 495], "487": 488, "428": [488, 490, 493, 494, 495], "489": 488, "unabl": [488, 490, 493, 494, 495], "thread": [488, 490, 493, 494, 495, 497], "hardware_command_queu": [488, 490, 493, 494, 495], "74": [488, 490, 493, 494, 495], "921": 488, "reprocess": 488, "563": 488, "922": 488, "582": 488, "390": 488, "78": 488, "488": [488, 490, 493, 494, 495], "391": 488, "468": [488, 490, 493, 494, 495], "783": [488, 490, 493, 494, 495], "392": 488, "conver": 489, "effeci": 489, "could": 489, "enjoi": 489, "massiv": 489, "ttnn_basic_matrix_multipl": 489, "03": [489, 493], "21": [489, 495], "386": 489, "209": 489, "umd": 489, "0x20": 489, "394": 489, "751": 489, "252": 489, "18": 489, "232": 489, "174": 489, "177": 489, "752": 489, "1085": 489, "765": 489, "pin": 489, "hugepag": 489, "0x7f5480000000": 489, "0x40000000": 489, "0x4c0000000": 489, "536": 489, "258": 489, "0000": [489, 490], "260": 489, "266": 489, "272": 489, "46": 489, "028": 489, "426": 489, "ttnn_basic_oper": 490, "np": [490, 493], "host_rand": 490, "helper": [490, 491], "to_tt_til": 490, "tt_t1": 490, "tt_t2": 490, "tt_t3": 490, "tt_t4": 490, "t5": 490, "tt_t5": 490, "add_result": 490, "mul_result": 490, "matmul_result": 490, "bmatrix": 490, "rightarrow": 490, "broadcast_vector": 490, "broadcast_tt": 490, "broadcast_add_result": 490, "850": 490, "852": [490, 493], "859": 490, "860": 490, "866": 490, "867": 490, "873": 490, "970": 490, "015": 490, "025": 490, "111": 490, "678": 490, "680": 490, "537": 490, "564": 490, "47": 490, "08": [490, 497], "072": 490, "49": [490, 491], "82812": 490, "04688": 490, "32812": 490, "00781": 490, "39844": 490, "03906": 490, "14844": 490, "24219": 490, "65625": 490, "31250": 490, "21094": 490, "21875": 490, "33594": 490, "37500": 490, "62500": 490, "670": 490, "52": 490, "12500": 490, "23438": 490, "96875": 490, "02600": 490, "97656": 490, "18164": 490, "87891": 490, "44531": 490, "59375": 490, "48438": 490, "50781": 490, "35938": 490, "229": 490, "55": [490, 494], "50000": 490, "25000": 490, "56250": 490, "43750": 490, "57": 490, "231": 490, "59": 490, "233": 490, "63": 490, "8242": 490, "0469": 490, "2500": 490, "3750": 490, "3945": 490, "0391": 490, "5625": 490, "1250": 490, "2188": 490, "8750": 490, "4375": 490, "7500": 490, "6250": 490, "7422": 490, "1484": 490, "9531": 490, "5000": 490, "6562": 490, "3281": 490, "0938": 490, "2158": 490, "3359": 490, "8438": 490, "234": 490, "contrast": 491, "foundat": 491, "multimod": 491, "openai": 491, "concept": [491, 492], "supervis": 491, "tradit": [491, 492], "bridg": 491, "gap": 491, "textual": 491, "consist": 491, "vit": 491, "never": 491, "prompt": 491, "dog": 491, "resiz": 491, "patch32": 491, "hug": [491, 496], "pil": 491, "torchvis": [491, 493, 495], "cliptoken": 491, "clipmodel": 491, "bytesio": 491, "safetensor": 491, "centercrop": 491, "totensor": [491, 493, 495], "interpolationmod": 491, "simplifi": 491, "asid": 491, "portion": 491, "fail": 491, "open_ttnn": 491, "close_ttnn": 491, "clean": 491, "get_devic": 491, "vice": 491, "versa": 491, "state_dict": 491, "convert_model_to_ttnn": 491, "ttnn_state_dict": 491, "elif": 491, "maxim": [491, 492], "modal": 491, "perceptron": [491, 493], "deeper": 491, "multiheadattent": 491, "head_dim": [491, 492], "q_proj_weight": 491, "q_proj": 491, "q_proj_bia": 491, "k_proj_weight": 491, "k_proj": 491, "k_proj_bia": 491, "v_proj_weight": 491, "v_proj": 491, "v_proj_bia": 491, "out_proj_weight": 491, "out_proj": 491, "out_proj_bia": 491, "bring": 491, "satur": 491, "overflow": 491, "attn_weight": [491, 492], "attn_output": 491, "dense_out": 491, "multilayerperceptron": 491, "mlp_c_fc_weight": 491, "fc1": [491, 495], "mlp_c_fc_bia": 491, "mlp_c_proj_weight": 491, "fc2": [491, 495], "mlp_c_proj_bia": 491, "residualattentionblock": 491, "self_attn": 491, "layer_norm_1_weight": 491, "layer_norm1": 491, "layer_norm_1_bia": 491, "layer_norm_2_weight": 491, "layer_norm2": 491, "layer_norm_2_bia": 491, "multihead": 491, "d_model": 491, "n_head": 491, "need_weight": 491, "x_post_layer_norm": 491, "text_model": 491, "vision_model": 491, "len": 491, "prepend": 491, "visiontransform": 491, "num_vision_lay": 491, "output_dim": 491, "conv2_state_dict_nam": 491, "patch_embed": 491, "vision_width": 491, "patch_siz": 491, "vision_head": 491, "class_embed": 491, "positional_embed": 491, "position_embed": 491, "proj": 491, "visual_project": 491, "conv1_weight": 491, "ln_pre_weight": 491, "pre_layrnorm": 491, "ln_pre_bia": 491, "ln_post_weight": 491, "post_layernorm": 491, "ln_post_bia": 491, "rearrang": [491, 495], "224x224": 491, "224": [491, 495], "depthwis": 491, "unflatten": 491, "num_patch": 491, "embed_dim": 491, "cl": 491, "capac": [491, 492], "emb": 491, "patch_1": 491, "patch_2": 491, "patch_49": 491, "instanti": 491, "encode_text": 491, "encode_imag": 491, "token_embed": 491, "text_project": 491, "context_length": 491, "vocab_s": [491, 496], "transformer_width": 491, "final_layer_norm": 491, "transformer_head": 491, "ln_final_weight": 491, "ln_final_bia": 491, "logit_scal": 491, "hardcod": 491, "num_text_lay": 491, "build_attention_mask": 491, "cheat": 491, "essenti": [491, 493], "autoregress": [491, 492], "triangl": 491, "exclud": 491, "thu": 491, "eot": 491, "yet": 491, "torch_token": 491, "torch_x": 491, "eot_indic": 491, "torch_selected_featur": 491, "logits_per_imag": 491, "batch_size_imag": 491, "batch_size_text": 491, "logits_per_text": 491, "text_featur": 491, "batch_text": 491, "image_featur": 491, "batch_imag": 491, "l2": 491, "norm_image_featur": 491, "moreh": 491, "norm_text_featur": 491, "color": 491, "bicub": 491, "crop": 491, "imagenet": 491, "preprocess_imag": 491, "model_resolut": 491, "_convert_image_to_rgb": 491, "transform_fn": 491, "48145466": 491, "4578275": 491, "40821073": 491, "26862954": 491, "26130258": 491, "27577711": 491, "url": 491, "download_imag": 491, "timeout": 491, "raise_for_statu": 491, "bad": 491, "statu": [491, 493], "requestexcept": 491, "ttnn_tutorials_models_clip_path": [491, 496], "download_model": 491, "clip_model_loc": 491, "cache_dir": [491, 496], "getenv": [491, 496], "download_token": 491, "tokenizer_nam": 491, "clip_tokenizer_loc": 491, "408": 491, "eo": 491, "77": 491, "image_url": 491, "media": 491, "githubusercont": 491, "ref": 491, "clip_tutori": 491, "png": 491, "preferred_dtyp": 491, "tt_imag": 491, "max_length": 491, "return_tensor": 491, "pt": [491, 493, 495], "tokenized_input": 491, "tokens_pretrained_host": 491, "input_id": [491, 492, 496], "num_prompt": 491, "tokens_pretrain": 491, "time_start": 491, "time_end": 491, "3f": [491, 492], "probs_torch": 491, "enumer": [491, 493, 495], "4f": [491, 492], "2f": [491, 492, 493, 495], "welcom": 492, "framework": 492, "seamless": 492, "eas": 492, "prototyp": 492, "hybrid": 492, "world": 492, "device_tensor_2": 492, "strength": 492, "easili": 492, "__version__": 492, "host_ttnn_from_torch": 492, "ntt": [492, 493, 495], "device_ttnn_from_torch": 492, "often": 492, "syntax": 492, "host_tensor_alt": 492, "seamlessli": 492, "torch_tensor_result": 492, "ntensor": 492, "ndevic": 492, "primari": 492, "transpar": 492, "worri": 492, "vari": 492, "rand_tensor": 492, "ultra": 492, "compact": 492, "4x": 492, "8x": 492, "lowest": 492, "qualiti": 492, "x_bf16": 492, "x_float32": 492, "x_uint16": 492, "x_bf8_b": 492, "x_bf4_b": 492, "ntip": 492, "result_add": 492, "result_mul": 492, "result_sub": 492, "result_div": 492, "nall": 492, "trigonometr": 492, "sin_x": 492, "cos_x": 492, "exp_x": 492, "log_x": 492, "sqrt_x": 492, "pow_x": 492, "emb_weight": 492, "w2": [492, 493], "jit": 492, "trigger": 492, "first_tim": 492, "cached_tim": 492, "speedup": 492, "nspeedup": 492, "1f": 492, "ti": 492, "1337": 492, "placement": [492, 496, 497], "hierarchi": 492, "108": 492, "mb": 492, "n300": 492, "192": [492, 497], "p100a": 492, "180": 492, "p150a": 492, "210": 492, "dram_tensor": 492, "sram_tensor": 492, "warmup": 492, "free": [492, 497], "technic": 492, "interleaved_l1_tim": 492, "sharded_config": 492, "width_sharded_tim": 492, "height_sharded_tim": 492, "x2": 492, "x3": 492, "differenti": 492, "autograd": 492, "knowledg": 492, "composite_sdpa": 492, "q_scale": 492, "hint": [492, 497], "k_t": 492, "attn_scor": 492, "masked_scor": 492, "num_iter": 492, "warmup_iter": 492, "q_torch": 492, "k_torch": 492, "v_torch": 492, "q_tt": 492, "k_tt": 492, "v_tt": 492, "causal_mask_tt": 492, "output_composit": 492, "output_composite_torch": 492, "output_optim": 492, "output_optimized_torch": 492, "output_torch": 492, "pcc_composit": 492, "corrcoef": 492, "pcc_optim": 492, "rmse_composit": 492, "rmse_optim": 492, "6f": 492, "rmse": 492, "warm": 492, "perf_count": 492, "composite_tim": 492, "optimized_tim": 492, "medium": 492, "tt_a": 492, "tt_b": 492, "bf16": 492, "lsb": 492, "nois": 492, "use_fp32_acc": 492, "wormholecomputekernelconfig": 492, "fp32_dest_acc_en": 492, "packer_l1_acc": 492, "result_tt": 492, "elaps": 492, "mean_err": 492, "8f": 492, "record": [492, 495], "tid": 492, "begin_trace_captur": 492, "end_trace_captur": 492, "execute_trac": 492, "1x2": 492, "mesh_tensor": 492, "ttnn_mlp_inference_mnist": 493, "disk": [493, 495, 497], "throughout": [493, 495, 497], "backend": 493, "outcom": 493, "28x28": 493, "grayscal": 493, "dataload": [493, 495], "testset": [493, 495], "testload": [493, 495], "shuffl": [493, 495], "train_and_export_mlp": 493, "poor": [493, 495], "mlp_mnist_weight": 493, "b1": 493, "b2": 493, "w3": [493, 495], "b3": [493, 495], "correctli": 493, "28": 493, "five": [493, 495], "raw": 493, "_layout": 493, "image_tt": 493, "1x128": 493, "w1_final": 493, "b1_final": 493, "out1": 493, "w2_final": 493, "b2_final": 493, "out2": 493, "w3_final": 493, "b3_final": 493, "out3": 493, "predicted_label": [493, 495], "41": 493, "990": 493, "992": 493, "998": 493, "006": 493, "007": 493, "013": 493, "110": 493, "172": 493, "182": [493, 495], "268": 493, "886": 493, "888": 493, "44": 493, "48": 493, "677": 493, "87": [493, 495], "682": 493, "686": 493, "690": 493, "695": 493, "89": [493, 495], "696": 493, "697": 493, "six": 494, "similarli": 494, "Be": 494, "multi_head_attent": 494, "query_weight": 494, "query_bia": 494, "key_weight": 494, "key_bia": 494, "value_weight": 494, "value_bia": 494, "output_weight": 494, "output_bia": 494, "fallback_reshap": 494, "get_fallback_funct": 494, "attention_scor": 494, "attention_prob": 494, "context_lay": 494, "self_output": 494, "torch_attention_mask": [494, 496], "torch_query_weight": 494, "torch_query_bia": 494, "torch_key_weight": 494, "torch_key_bia": 494, "torch_value_weight": 494, "torch_value_bia": 494, "torch_output_weight": 494, "torch_output_bia": 494, "fly": 494, "fortun": 494, "ahead": 494, "optimized_multi_head_attent": 494, "fused_qkv_weight": 494, "fused_qkv_bia": 494, "self_output_weight": 494, "self_output_bia": 494, "fused_qkv_output": 494, "context_layer_after_concatenate_head": 494, "qkv": 494, "torch_qkv_weight": 494, "torch_qkv_bia": 494, "qkv_weight": 494, "qkv_bia": 494, "optimized_output": 494, "torch_optimized_output": 494, "allclos": 494, "ttnn_multihead_attent": 494, "769": 494, "776": [494, 497], "777": 494, "784": 494, "790": 494, "887": 494, "931": 494, "942": 494, "39": 494, "027": [494, 495], "603": 494, "605": 494, "51": [494, 497], "001": 494, "132": [494, 495], "265338897705078": 494, "056": 494, "151": [494, 495], "05480194091796875": 494, "363": 494, "259": 494, "2866740226745605": 494, "366": 494, "274": 494, "002416849136352539": 494, "417": 494, "418": 494, "460": 494, "scratchpad": 495, "kb": 495, "cifar10": 495, "train_and_export_cnn": 495, "simple_cnn_cifar10_weight": 495, "conv1": 495, "conv2": 495, "conv_pool_stag": 495, "encapsul": 495, "undergo": 495, "sizegur": 495, "again": 495, "modular": 495, "input_nhwc": 495, "conv_outchannel": 495, "weight_str": 495, "bias_str": 495, "log_first_sampl": 495, "conv_kernel_s": 495, "conv_strid": 495, "conv_pad": 495, "conv1_out": 495, "max_pool2d_kernel_s": 495, "max_pool2d_strid": 495, "max_pool2d_pad": 495, "max_pool2d_dil": 495, "max_pool2d_out": 495, "simplecnn": 495, "obtain": 495, "ttnn_imag": 495, "ttnn_image_permu": 495, "log_thi": 495, "conv1_pool": 495, "conv2_pool": 495, "fc": 495, "out_flat": 495, "w4": 495, "b4": 495, "w3_tt": 495, "b3_tt": 495, "x_tt": 495, "w4_tt": 495, "b4_tt": 495, "ttnn_simplecnn_infer": 495, "041": 495, "043": 495, "050": 495, "051": 495, "057": 495, "058": 495, "064": 495, "161": 495, "235": 495, "321": 495, "889": 495, "891": 495, "734": 495, "471": 495, "075": 495, "86": 495, "88": 495, "076": 495, "91": 495, "92": 495, "93": 495, "94": 495, "95": 495, "97": 495, "98": 495, "nullopt": 495, "enable_split_read": 495, "enable_subblock_pad": 495, "101": 495, "960": 495, "129": 495, "130": 495, "131": 495, "961": 495, "133": 495, "134": 495, "135": 495, "136": 495, "137": 495, "138": 495, "139": 495, "026": 495, "157": [495, 497], "158": 495, "121": 495, "669": 495, "238": 495, "166": 495, "181": 495, "240": 495, "183": 495, "ttnn_config_overrid": [496, 497], "enable_fast_runtime_mod": [496, 497], "suppress": 496, "cleaner": 496, "set_verbosity_error": 496, "scope": 496, "download_google_bert_model_and_config": 496, "bertselfoutput": 496, "model_loc": 496, "ttnn_tutorials_models_tracer_path": 496, "config_google_bert": 496, "json": [496, 497], "download_ttnn_bert_config": 496, "config_loc": 496, "config_ttnn_bert": 496, "download_ttnn_bert_model": 496, "bertforquestionansw": 496, "tini": 496, "googl": 496, "bert_uncased_l": 496, "4_h": 496, "256_a": 496, "dummi": 496, "dispatch_core_typ": 496, "dispatchcoretyp": 496, "arch_nam": 496, "num_hidden_lay": 496, "token_type_id": 496, "segment": 496, "qa": 496, "torch_token_type_id": 496, "position_id": 496, "sequence_length": 496, "torch_position_id": 496, "ttnn_bert_input": 496, "preprocess_input": 496, "span": 496, "bert_for_question_answ": 496, "intuit": 497, "depth": 497, "searchabl": 497, "peak": 497, "hierarch": 497, "server": 497, "opportun": 497, "watch": 497, "walkthrough": 497, "video": 497, "offlin": 497, "localhost": 497, "8000": 497, "browser": 497, "chrome": 497, "greet": 497, "homepag": 497, "yolov4": 497, "320x320": 497, "coco": 497, "predefin": 497, "wrap": 497, "ttnn_config_path": 497, "inlin": 497, "past": 497, "enable_log": 497, "report_nam": 497, "ttnn_visualizer_tutori": 497, "enable_graph_report": 497, "enable_detailed_buffer_report": 497, "enable_detailed_tensor_report": 497, "enable_comparison_mod": 497, "test_ttnn_yolov4": 497, "test_yolov4": 497, "pretrained_weight_tru": 497, "At": 497, "664": 497, "665": 497, "83": 497, "cache_path": 497, "model_cache_path": 497, "tmp_dir": 497, "enable_model_cach": 497, "throw_exception_on_fallback": 497, "comparison_mode_should_raise_except": 497, "comparison_mode_pcc": 497, "root_report_path": 497, "4042956046390500517": 497, "754": 497, "197": 497, "758": 497, "761": 497, "764": 497, "0x80": 497, "295": 497, "836": 497, "navig": 497, "db": 497, "sqlite": 497, "cluster_descriptor": 497, "physical_chip_mesh_coordinate_mapping_1_of_1": 497, "physical_chip_mesh_coordinate_mapping_x_of_i": 497, "termin": 497, "session": 497, "unset": 497, "regener": 497, "Or": 497, "731": 497, "process_ops_log": 497, "generate_report": 497, "905": 497, "2025_08_01_10_51_02": 497, "ops_perf_results_2025_08_01_10_51_02": 497, "diredtori": 497, "ops_perf_results_": 497, "device_profile_log": 497, "bottom": 497, "click": 497, "breakdown": 497, "relationship": 497, "easi": 497, "candid": 497, "chart": 497, "lifetim": 497, "estim": 497, "headroom": 497, "pinpoint": 497, "ineffici": 497, "node": 497, "edg": 497, "zoom": 497, "pan": 497, "subnetwork": 497, "flop": 497, "underutil": 497, "toggl": 497, "suboptim": 497, "summar": 497, "deepli": 497}, "objects": {"ttnn": [[7, 0, 1, "", "Conv2dConfig"], [8, 2, 1, "", "Conv2dSliceConfig"], [9, 3, 1, "", "GetDefaultDevice"], [10, 0, 1, "", "MatmulMultiCoreReuseMultiCast1DProgramConfig"], [11, 0, 1, "", "MatmulMultiCoreReuseMultiCastBatchedDRAMShardedProgramConfig"], [12, 0, 1, "", "MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig"], [13, 0, 1, "", "MatmulMultiCoreReuseMultiCastProgramConfig"], [14, 0, 1, "", "MatmulMultiCoreReuseProgramConfig"], [15, 3, 1, "", "SetDefaultDevice"], [485, 0, 1, "", "Shape"], [16, 0, 1, "", "SoftmaxDefaultProgramConfig"], [17, 0, 1, "", "SoftmaxProgramConfig"], [18, 0, 1, "", "SoftmaxShardedMultiCoreProgramConfig"], [19, 4, 1, "", "abs"], [20, 4, 1, "", "abs_bw"], [21, 4, 1, "", "acos"], [22, 4, 1, "", "acos_bw"], [23, 4, 1, "", "acosh"], [24, 4, 1, "", "acosh_bw"], [25, 4, 1, "", "adaptive_avg_pool2d"], [26, 4, 1, "", "adaptive_max_pool2d"], [27, 4, 1, "", "add"], [28, 4, 1, "", "add_"], [29, 4, 1, "", "add_bw"], [30, 4, 1, "", "addalpha"], [31, 4, 1, "", "addalpha_bw"], [32, 4, 1, "", "addcdiv"], [33, 4, 1, "", "addcdiv_bw"], [34, 4, 1, "", "addcmul"], [35, 4, 1, "", "addcmul_bw"], [36, 4, 1, "", "addmm"], [37, 4, 1, "", "all_broadcast"], [38, 4, 1, "", "all_gather"], [39, 4, 1, "", "all_reduce"], [40, 4, 1, "", "all_to_all_combine"], [41, 4, 1, "", "all_to_all_dispatch"], [42, 4, 1, "", "alt_complex_rotate90"], [43, 4, 1, "", "angle"], [44, 4, 1, "", "angle_bw"], [45, 4, 1, "", "arange"], [46, 4, 1, "", "argmax"], [47, 4, 1, "", "as_tensor"], [48, 4, 1, "", "asin"], [49, 4, 1, "", "asin_bw"], [50, 4, 1, "", "asinh"], [51, 4, 1, "", "asinh_bw"], [52, 4, 1, "", "assign"], [53, 4, 1, "", "assign_bw"], [54, 4, 1, "", "atan"], [55, 4, 1, "", "atan2"], [56, 4, 1, "", "atan2_bw"], [57, 4, 1, "", "atan_bw"], [58, 4, 1, "", "atanh"], [59, 4, 1, "", "atanh_bw"], [60, 4, 1, "", "avg_pool2d"], [61, 4, 1, "", "batch_norm"], [62, 4, 1, "", "bcast"], [63, 4, 1, "", "bernoulli"], [64, 4, 1, "", "bias_gelu"], [65, 4, 1, "", "bias_gelu_"], [66, 4, 1, "", "bias_gelu_bw"], [67, 4, 1, "", "bitcast"], [68, 4, 1, "", "bitwise_and"], [69, 4, 1, "", "bitwise_left_shift"], [70, 4, 1, "", "bitwise_not"], [71, 4, 1, "", "bitwise_or"], [72, 4, 1, "", "bitwise_right_shift"], [73, 4, 1, "", "bitwise_xor"], [74, 4, 1, "", "broadcast"], [75, 4, 1, "", "cbrt"], [76, 4, 1, "", "ceil"], [77, 4, 1, "", "ceil_bw"], [78, 4, 1, "", "celu"], [79, 4, 1, "", "celu_bw"], [80, 4, 1, "", "chunk"], [81, 4, 1, "", "clamp"], [82, 4, 1, "", "clamp_bw"], [83, 4, 1, "", "clip"], [84, 4, 1, "", "clip_bw"], [85, 4, 1, "", "clone"], [86, 4, 1, "", "close_device"], [87, 4, 1, "", "complex_tensor"], [88, 4, 1, "", "concat"], [89, 4, 1, "", "concat_bw"], [90, 4, 1, "", "conj"], [91, 4, 1, "", "conj_bw"], [92, 4, 1, "", "conv1d"], [93, 4, 1, "", "conv2d"], [94, 4, 1, "", "conv_transpose2d"], [95, 4, 1, "", "copy"], [96, 4, 1, "", "copy_device_to_host_tensor"], [97, 4, 1, "", "copy_host_to_device_tensor"], [98, 4, 1, "", "cos"], [99, 4, 1, "", "cos_bw"], [100, 4, 1, "", "cosh"], [101, 4, 1, "", "cosh_bw"], [102, 4, 1, "", "create_sharded_memory_config"], [103, 4, 1, "", "cumprod"], [104, 4, 1, "", "cumsum"], [105, 4, 1, "", "deallocate"], [106, 4, 1, "", "deg2rad"], [107, 4, 1, "", "deg2rad_bw"], [108, 4, 1, "", "dequantize"], [109, 4, 1, "", "digamma"], [110, 4, 1, "", "digamma_bw"], [111, 4, 1, "", "div"], [112, 4, 1, "", "div_bw"], [113, 4, 1, "", "div_no_nan"], [114, 4, 1, "", "div_no_nan_bw"], [115, 4, 1, "", "divide"], [116, 4, 1, "", "divide_"], [117, 4, 1, "", "dram_prefetcher"], [118, 4, 1, "", "dump_tensor"], [119, 4, 1, "", "elu"], [120, 4, 1, "", "elu_bw"], [121, 4, 1, "", "ema"], [122, 4, 1, "", "embedding"], [123, 4, 1, "", "embedding_bw"], [124, 4, 1, "", "empty"], [125, 4, 1, "", "empty_like"], [126, 4, 1, "", "eq"], [127, 4, 1, "", "eq_"], [128, 4, 1, "", "eqz"], [129, 4, 1, "", "erf"], [130, 4, 1, "", "erf_bw"], [131, 4, 1, "", "erfc"], [132, 4, 1, "", "erfc_bw"], [133, 4, 1, "", "erfinv"], [134, 4, 1, "", "erfinv_bw"], [135, 4, 1, "", "exp"], [136, 4, 1, "", "exp2"], [137, 4, 1, "", "exp2_bw"], [138, 4, 1, "", "exp_bw"], [139, 4, 1, "", "expand"], [143, 4, 1, "", "expm1"], [144, 4, 1, "", "expm1_bw"], [145, 4, 1, "", "fill"], [146, 4, 1, "", "fill_bw"], [147, 4, 1, "", "fill_cache"], [148, 4, 1, "", "fill_implicit_tile_padding"], [149, 4, 1, "", "fill_ones_rm"], [150, 4, 1, "", "fill_rm"], [151, 4, 1, "", "fill_zero_bw"], [152, 4, 1, "", "floor"], [153, 4, 1, "", "floor_bw"], [154, 4, 1, "", "floor_div"], [155, 4, 1, "", "fmod"], [156, 4, 1, "", "fmod_bw"], [157, 4, 1, "", "fold"], [158, 4, 1, "", "frac"], [159, 4, 1, "", "frac_bw"], [160, 4, 1, "", "from_buffer"], [161, 4, 1, "", "from_device"], [162, 4, 1, "", "from_torch"], [163, 4, 1, "", "full"], [164, 4, 1, "", "full_like"], [165, 4, 1, "", "gather"], [166, 4, 1, "", "gcd"], [167, 4, 1, "", "ge"], [168, 4, 1, "", "ge_"], [169, 4, 1, "", "geglu"], [170, 4, 1, "", "gelu"], [171, 4, 1, "", "gelu_bw"], [172, 4, 1, "", "generic_op"], [173, 3, 1, "", "get_device_tensors"], [174, 4, 1, "", "gez"], [175, 4, 1, "", "global_avg_pool2d"], [176, 4, 1, "", "glu"], [177, 4, 1, "", "grid_sample"], [178, 4, 1, "", "group_norm"], [179, 4, 1, "", "gt"], [180, 4, 1, "", "gt_"], [181, 4, 1, "", "gtz"], [182, 4, 1, "", "hardmish"], [183, 4, 1, "", "hardshrink"], [184, 4, 1, "", "hardshrink_bw"], [185, 4, 1, "", "hardsigmoid"], [186, 4, 1, "", "hardsigmoid_bw"], [187, 4, 1, "", "hardswish"], [188, 4, 1, "", "hardswish_bw"], [189, 4, 1, "", "hardtanh"], [190, 4, 1, "", "hardtanh_bw"], [191, 4, 1, "", "heaviside"], [192, 4, 1, "", "hypot"], [193, 4, 1, "", "hypot_bw"], [194, 4, 1, "", "i0"], [195, 4, 1, "", "i0_bw"], [196, 4, 1, "", "i1"], [197, 4, 1, "", "identity"], [198, 4, 1, "", "imag"], [199, 4, 1, "", "imag_bw"], [200, 4, 1, "", "index_fill"], [201, 4, 1, "", "indexed_fill"], [202, 4, 1, "", "interleaved_to_sharded"], [203, 4, 1, "", "interleaved_to_sharded_partial"], [204, 4, 1, "", "is_imag"], [205, 4, 1, "", "is_real"], [206, 4, 1, "", "isclose"], [207, 4, 1, "", "isfinite"], [208, 4, 1, "", "isinf"], [209, 4, 1, "", "isnan"], [210, 4, 1, "", "isneginf"], [211, 4, 1, "", "isposinf"], [214, 4, 1, "", "l1_loss"], [215, 4, 1, "", "layer_norm"], [216, 4, 1, "", "layer_norm_post_all_gather"], [217, 4, 1, "", "layer_norm_pre_all_gather"], [218, 4, 1, "", "lcm"], [219, 4, 1, "", "ldexp"], [220, 4, 1, "", "ldexp_"], [221, 4, 1, "", "ldexp_bw"], [222, 4, 1, "", "le"], [223, 4, 1, "", "le_"], [224, 4, 1, "", "leaky_relu"], [225, 4, 1, "", "leaky_relu_bw"], [226, 4, 1, "", "lerp"], [227, 4, 1, "", "lerp_bw"], [228, 4, 1, "", "lez"], [229, 4, 1, "", "lgamma"], [230, 4, 1, "", "lgamma_bw"], [231, 4, 1, "", "linear"], [232, 4, 1, "", "load_tensor"], [233, 4, 1, "", "log"], [234, 4, 1, "", "log10"], [235, 4, 1, "", "log10_bw"], [236, 4, 1, "", "log1p"], [237, 4, 1, "", "log1p_bw"], [238, 4, 1, "", "log2"], [239, 4, 1, "", "log2_bw"], [240, 4, 1, "", "log_bw"], [241, 4, 1, "", "log_sigmoid"], [242, 4, 1, "", "log_sigmoid_bw"], [243, 4, 1, "", "logaddexp"], [244, 4, 1, "", "logaddexp2"], [245, 4, 1, "", "logaddexp2_"], [246, 4, 1, "", "logaddexp2_bw"], [247, 4, 1, "", "logaddexp_"], [248, 4, 1, "", "logaddexp_bw"], [249, 4, 1, "", "logical_and"], [250, 4, 1, "", "logical_and_"], [251, 4, 1, "", "logical_left_shift"], [252, 4, 1, "", "logical_not"], [253, 4, 1, "", "logical_not_"], [254, 4, 1, "", "logical_or"], [255, 4, 1, "", "logical_or_"], [256, 4, 1, "", "logical_right_shift"], [257, 4, 1, "", "logical_xor"], [258, 4, 1, "", "logical_xor_"], [259, 4, 1, "", "logit"], [260, 4, 1, "", "logit_bw"], [261, 4, 1, "", "logiteps_bw"], [262, 4, 1, "", "lt"], [263, 4, 1, "", "lt_"], [264, 4, 1, "", "ltz"], [265, 4, 1, "", "mac"], [266, 4, 1, "", "manage_device"], [267, 4, 1, "", "manual_seed"], [268, 4, 1, "", "matmul"], [269, 4, 1, "", "max"], [270, 4, 1, "", "max_bw"], [271, 4, 1, "", "max_pool2d"], [272, 4, 1, "", "maximum"], [273, 4, 1, "", "mean"], [274, 4, 1, "", "mesh_partition"], [275, 4, 1, "", "min"], [276, 4, 1, "", "min_bw"], [277, 4, 1, "", "minimum"], [278, 4, 1, "", "mish"], [281, 4, 1, "", "moe"], [282, 4, 1, "", "moe_expert_token_remap"], [283, 4, 1, "", "moe_routing_remap"], [284, 4, 1, "", "move"], [285, 4, 1, "", "mse_loss"], [286, 4, 1, "", "mul_bw"], [287, 4, 1, "", "multigammaln"], [288, 4, 1, "", "multigammaln_bw"], [289, 4, 1, "", "multiply"], [290, 4, 1, "", "multiply_"], [291, 4, 1, "", "ne"], [292, 4, 1, "", "ne_"], [293, 4, 1, "", "neg"], [294, 4, 1, "", "neg_bw"], [295, 4, 1, "", "nextafter"], [296, 4, 1, "", "nez"], [297, 4, 1, "", "nonzero"], [298, 4, 1, "", "normalize_global"], [299, 4, 1, "", "normalize_hw"], [300, 4, 1, "", "ones"], [301, 4, 1, "", "ones_like"], [302, 3, 1, "", "open_device"], [303, 4, 1, "", "outer"], [304, 4, 1, "", "pad"], [305, 3, 1, "", "pad_to_tile_shape"], [306, 4, 1, "", "permute"], [307, 4, 1, "", "point_to_point"], [308, 4, 1, "", "polar"], [309, 4, 1, "", "polar_bw"], [310, 4, 1, "", "polygamma"], [311, 4, 1, "", "polygamma_bw"], [312, 4, 1, "", "polyval"], [313, 4, 1, "", "pow"], [314, 4, 1, "", "pow_bw"], [315, 4, 1, "", "prelu"], [316, 4, 1, "", "prepare_conv_bias"], [317, 4, 1, "", "prepare_conv_transpose2d_bias"], [318, 4, 1, "", "prepare_conv_transpose2d_weights"], [319, 4, 1, "", "prepare_conv_weights"], [320, 4, 1, "", "prod"], [321, 4, 1, "", "prod_bw"], [322, 4, 1, "", "quantize"], [323, 4, 1, "", "rad2deg"], [324, 4, 1, "", "rad2deg_bw"], [325, 4, 1, "", "rand"], [326, 4, 1, "", "rdiv"], [327, 4, 1, "", "rdiv_bw"], [328, 4, 1, "", "real"], [329, 4, 1, "", "real_bw"], [330, 4, 1, "", "reallocate"], [331, 4, 1, "", "reciprocal"], [332, 4, 1, "", "reciprocal_bw"], [333, 4, 1, "", "reduce_scatter"], [334, 4, 1, "", "reduce_to_root"], [335, 4, 1, "", "register_post_operation_hook"], [336, 4, 1, "", "register_pre_operation_hook"], [337, 4, 1, "", "reglu"], [338, 4, 1, "", "relu"], [339, 4, 1, "", "relu6"], [340, 4, 1, "", "relu6_bw"], [341, 4, 1, "", "relu_bw"], [342, 4, 1, "", "relu_max"], [343, 4, 1, "", "relu_min"], [344, 4, 1, "", "remainder"], [345, 4, 1, "", "remainder_bw"], [346, 4, 1, "", "repeat"], [347, 4, 1, "", "repeat_bw"], [348, 4, 1, "", "repeat_interleave"], [349, 4, 1, "", "requantize"], [350, 4, 1, "", "reshape"], [351, 4, 1, "", "reshape_on_device"], [352, 4, 1, "", "reshard"], [353, 4, 1, "", "rms_norm"], [354, 4, 1, "", "rms_norm_post_all_gather"], [355, 4, 1, "", "rms_norm_pre_all_gather"], [356, 4, 1, "", "roll"], [357, 4, 1, "", "rotate"], [358, 4, 1, "", "round"], [359, 4, 1, "", "round_bw"], [360, 4, 1, "", "rpow"], [361, 4, 1, "", "rpow_bw"], [362, 4, 1, "", "rsqrt"], [363, 4, 1, "", "rsqrt_bw"], [364, 4, 1, "", "rsub"], [365, 4, 1, "", "rsub_"], [366, 4, 1, "", "rsub_bw"], [367, 4, 1, "", "sampling"], [368, 4, 1, "", "scale_causal_mask_hw_dims_softmax_in_place"], [369, 4, 1, "", "scale_mask_softmax"], [370, 4, 1, "", "scale_mask_softmax_in_place"], [371, 4, 1, "", "scatter"], [372, 4, 1, "", "scatter_add"], [373, 4, 1, "", "selu"], [374, 4, 1, "", "selu_bw"], [375, 3, 1, "", "set_printoptions"], [376, 4, 1, "", "sharded_to_interleaved"], [377, 4, 1, "", "sharded_to_interleaved_partial"], [378, 4, 1, "", "sigmoid"], [379, 4, 1, "", "sigmoid_accurate"], [380, 4, 1, "", "sigmoid_bw"], [381, 4, 1, "", "sign"], [382, 4, 1, "", "sign_bw"], [383, 4, 1, "", "signbit"], [384, 4, 1, "", "silu"], [385, 4, 1, "", "silu_bw"], [386, 4, 1, "", "sin"], [387, 4, 1, "", "sin_bw"], [388, 4, 1, "", "sinh"], [389, 4, 1, "", "sinh_bw"], [390, 4, 1, "", "slice"], [391, 4, 1, "", "softmax"], [392, 4, 1, "", "softmax_in_place"], [393, 4, 1, "", "softplus"], [394, 4, 1, "", "softplus_bw"], [395, 4, 1, "", "softshrink"], [396, 4, 1, "", "softshrink_bw"], [397, 4, 1, "", "softsign"], [398, 4, 1, "", "softsign_bw"], [399, 4, 1, "", "sort"], [400, 4, 1, "", "sparse_matmul"], [401, 4, 1, "", "split"], [402, 3, 1, "", "split_work_to_cores"], [403, 4, 1, "", "sqrt"], [404, 4, 1, "", "sqrt_bw"], [405, 4, 1, "", "square"], [406, 4, 1, "", "square_bw"], [407, 4, 1, "", "squared_difference"], [408, 4, 1, "", "squared_difference_"], [409, 4, 1, "", "squared_difference_bw"], [410, 4, 1, "", "squeeze"], [411, 4, 1, "", "stack"], [412, 4, 1, "", "std"], [413, 4, 1, "", "std_hw"], [414, 4, 1, "", "sub_bw"], [415, 4, 1, "", "subalpha"], [416, 4, 1, "", "subalpha_bw"], [417, 4, 1, "", "subtract"], [418, 4, 1, "", "subtract_"], [419, 4, 1, "", "sum"], [420, 4, 1, "", "swiglu"], [421, 4, 1, "", "swish"], [422, 3, 1, "", "synchronize_device"], [423, 4, 1, "", "tan"], [424, 4, 1, "", "tan_bw"], [425, 4, 1, "", "tanh"], [426, 4, 1, "", "tanh_bw"], [427, 4, 1, "", "tanhshrink"], [428, 4, 1, "", "tanhshrink_bw"], [429, 4, 1, "", "threshold"], [430, 4, 1, "", "threshold_bw"], [431, 4, 1, "", "tilize"], [432, 4, 1, "", "tilize_with_val_padding"], [433, 4, 1, "", "tilize_with_zero_padding"], [434, 4, 1, "", "to_device"], [435, 4, 1, "", "to_dtype"], [436, 4, 1, "", "to_layout"], [437, 4, 1, "", "to_memory_config"], [438, 4, 1, "", "to_torch"], [439, 4, 1, "", "topk"], [456, 4, 1, "", "transpose"], [457, 4, 1, "", "tril"], [458, 4, 1, "", "triu"], [459, 4, 1, "", "trunc"], [460, 4, 1, "", "trunc_bw"], [461, 4, 1, "", "typecast"], [462, 4, 1, "", "unary_chain"], [463, 4, 1, "", "uniform"], [464, 4, 1, "", "unsqueeze"], [465, 4, 1, "", "unsqueeze_to_4D"], [466, 4, 1, "", "untilize"], [467, 4, 1, "", "untilize_with_unpadding"], [468, 4, 1, "", "update_cache"], [469, 4, 1, "", "upsample"], [470, 4, 1, "", "var"], [471, 4, 1, "", "var_hw"], [472, 4, 1, "", "view"], [473, 4, 1, "", "where"], [474, 4, 1, "", "where_bw"], [475, 4, 1, "", "xlogy"], [476, 4, 1, "", "xlogy_bw"], [477, 4, 1, "", "zeros"], [478, 4, 1, "", "zeros_like"]], "ttnn.Conv2dConfig": [[7, 1, 1, "", "act_block_h_override"], [7, 1, 1, "", "act_block_w_div"], [7, 1, 1, "", "activation"], [7, 1, 1, "", "config_tensors_in_dram"], [7, 1, 1, "", "core_grid"], [7, 1, 1, "", "deallocate_activation"], [7, 1, 1, "", "enable_act_double_buffer"], [7, 1, 1, "", "enable_activation_reuse"], [7, 1, 1, "", "enable_kernel_stride_folding"], [7, 1, 1, "", "enable_weights_double_buffer"], [7, 1, 1, "", "force_split_reader"], [7, 1, 1, "", "full_inner_dim"], [7, 1, 1, "", "output_layout"], [7, 1, 1, "", "override_output_sharding_config"], [7, 1, 1, "", "override_sharding_config"], [7, 1, 1, "", "reallocate_halo_output"], [7, 1, 1, "", "reshard_if_not_optimal"], [7, 1, 1, "", "shard_layout"], [7, 1, 1, "", "transpose_shards"], [7, 1, 1, "", "weights_dtype"]], "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig": [[10, 1, 1, "", "compute_with_storage_grid_size"], [10, 2, 1, "", "from_json"], [10, 1, 1, "", "fuse_batch"], [10, 1, 1, "", "fused_activation"], [10, 1, 1, "", "gather_in0"], [10, 1, 1, "", "hop_cores"], [10, 1, 1, "", "in0_block_w"], [10, 1, 1, "", "mcast_in0"], [10, 1, 1, "", "num_global_cb_receivers"], [10, 1, 1, "", "out_block_h"], [10, 1, 1, "", "out_block_w"], [10, 1, 1, "", "out_subblock_h"], [10, 1, 1, "", "out_subblock_w"], [10, 1, 1, "", "per_core_M"], [10, 1, 1, "", "per_core_N"], [10, 2, 1, "", "to_json"], [10, 1, 1, "", "untilize_out"]], "ttnn.MatmulMultiCoreReuseMultiCastBatchedDRAMShardedProgramConfig": [[11, 2, 1, "", "from_json"], [11, 1, 1, "", "fused_activation"], [11, 1, 1, "", "in0_block_w"], [11, 1, 1, "", "per_core_M"], [11, 1, 1, "", "per_core_N"], [11, 2, 1, "", "to_json"]], "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig": [[12, 2, 1, "", "from_json"], [12, 1, 1, "", "fused_activation"], [12, 1, 1, "", "in0_block_w"], [12, 1, 1, "", "per_core_M"], [12, 1, 1, "", "per_core_N"], [12, 2, 1, "", "to_json"]], "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig": [[13, 1, 1, "", "compute_with_storage_grid_size"], [13, 2, 1, "", "from_json"], [13, 1, 1, "", "fuse_batch"], [13, 1, 1, "", "fused_activation"], [13, 1, 1, "", "in0_block_w"], [13, 1, 1, "", "out_block_h"], [13, 1, 1, "", "out_block_w"], [13, 1, 1, "", "out_subblock_h"], [13, 1, 1, "", "out_subblock_w"], [13, 1, 1, "", "per_core_M"], [13, 1, 1, "", "per_core_N"], [13, 2, 1, "", "to_json"], [13, 1, 1, "", "transpose_mcast"]], "ttnn.MatmulMultiCoreReuseProgramConfig": [[14, 1, 1, "", "compute_with_storage_grid_size"], [14, 2, 1, "", "from_json"], [14, 1, 1, "", "in0_block_w"], [14, 1, 1, "", "out_subblock_h"], [14, 1, 1, "", "out_subblock_w"], [14, 1, 1, "", "per_core_M"], [14, 1, 1, "", "per_core_N"], [14, 2, 1, "", "to_json"]], "ttnn.Shape": [[485, 1, 1, "", "rank"], [485, 2, 1, "", "to_rank"]], "ttnn.SoftmaxShardedMultiCoreProgramConfig": [[18, 1, 1, "", "block_h"], [18, 1, 1, "", "block_w"], [18, 1, 1, "", "compute_with_storage_grid_size"], [18, 1, 1, "", "subblock_w"]], "ttnn.experimental": [[140, 4, 1, "", "conv3d"], [141, 4, 1, "", "dropout"], [142, 4, 1, "", "gelu_bw"]], "ttnn.kv_cache": [[212, 4, 1, "", "fill_cache_for_user_"], [213, 4, 1, "", "update_cache_for_token_"]], "ttnn.model_preprocessing": [[279, 4, 1, "", "preprocess_model"], [280, 4, 1, "", "preprocess_model_parameters"]], "ttnn.transformer": [[440, 4, 1, "", "attention_softmax"], [441, 4, 1, "", "attention_softmax_"], [442, 4, 1, "", "chunked_flash_mla_prefill"], [443, 4, 1, "", "chunked_scaled_dot_product_attention"], [444, 4, 1, "", "concatenate_heads"], [445, 4, 1, "", "flash_mla_prefill"], [446, 4, 1, "", "flash_multi_latent_attention_decode"], [447, 4, 1, "", "joint_scaled_dot_product_attention"], [448, 4, 1, "", "paged_flash_multi_latent_attention_decode"], [449, 4, 1, "", "paged_scaled_dot_product_attention_decode"], [450, 4, 1, "", "ring_distributed_scaled_dot_product_attention"], [451, 4, 1, "", "ring_joint_scaled_dot_product_attention"], [452, 4, 1, "", "scaled_dot_product_attention"], [453, 4, 1, "", "scaled_dot_product_attention_decode"], [454, 4, 1, "", "split_query_key_value_and_split_heads"], [455, 4, 1, "", "windowed_scaled_dot_product_attention"]]}, "objtypes": {"0": "py:class", "1": "py:property", "2": "py:attribute", "3": "py:data", "4": "py:function"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "property", "Python property"], "2": ["py", "attribute", "Python attribute"], "3": ["py", "data", "Python data"], "4": ["py", "function", "Python function"]}, "titleterms": {"welcom": 0, "tt": [0, 4, 5, 479, 482, 484, 491, 492, 494, 496, 497], "nn": [0, 4, 5, 479, 482, 484, 491, 492, 494, 496, 497], "document": 0, "ttnn": [0, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 496], "resourc": 0, "indic": 0, "tabl": 0, "contribut": [1, 482], "develop": [1, 492], "support": [2, 268, 400, 492], "report": [2, 6, 484, 497], "bug": 2, "featur": 2, "propos": 2, "request": 2, "troubleshoot": 2, "debug": 2, "tip": 2, "commun": 2, "tool": [3, 492], "what": [4, 5, 491, 492], "i": [4, 5], "ad": 5, "new": [5, 483], "oper": [5, 6, 479, 484, 487, 488, 490, 492, 496, 497], "faq": 5, "step": [5, 479, 482], "ar": [5, 482], "need": 5, "add": [5, 27, 487], "c": 5, "python": 5, "exampl": [5, 9, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 141, 142, 143, 144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 203, 204, 205, 206, 207, 208, 209, 210, 211, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 281, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 308, 309, 310, 311, 312, 313, 314, 315, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 403, 404, 405, 406, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 434, 435, 436, 437, 438, 439, 455, 457, 458, 459, 460, 461, 462, 463, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 482, 487, 488, 489, 490, 493, 494, 495, 496], "devic": [5, 6, 93, 96, 97, 487, 488, 489, 490, 491, 492, 493, 495, 496], "implement": [5, 491, 492], "1": [5, 368, 479, 481, 482, 492, 496], "2": [5, 479, 481, 482, 492, 496], "bind": 5, "option": [5, 369, 370, 482], "golden": 5, "function": [5, 483, 491, 492], "3": [5, 479, 482, 492, 496], "usag": 5, "doc": 5, "api": [6, 485], "memori": [6, 268, 400, 485, 497], "config": [6, 485, 489, 496], "core": 6, "tensor": [6, 46, 63, 87, 96, 97, 200, 269, 273, 275, 368, 369, 370, 412, 419, 435, 463, 470, 485, 487, 488, 489, 490, 492, 496, 497], "creation": [6, 487, 490, 492], "matrix": [6, 489], "multipl": [6, 485, 489], "pointwis": 6, "unari": 6, "binari": [6, 482], "ternari": 6, "quantiz": [6, 322], "loss": 6, "reduct": 6, "data": [6, 485, 492, 493], "movement": 6, "normal": 6, "program": [6, 482, 496], "transform": [6, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 491], "ccl": 6, "embed": [6, 122], "convolut": [6, 488, 495], "pool": [6, 495], "prefetch": 6, "vision": [6, 491], "gener": [6, 491, 497], "kv": 6, "cach": [6, 492], "backward": 6, "model": [6, 479, 481, 482, 491, 496, 497], "convers": [6, 487, 490, 491], "hook": 6, "conv2dconfig": 7, "conv2dsliceconfig": 8, "getdefaultdevic": 9, "matmulmulticorereusemulticast1dprogramconfig": 10, "matmulmulticorereusemulticastbatcheddramshardedprogramconfig": 11, "matmulmulticorereusemulticastdramshardedprogramconfig": 12, "matmulmulticorereusemulticastprogramconfig": 13, "matmulmulticorereuseprogramconfig": 14, "setdefaultdevic": 15, "softmaxdefaultprogramconfig": 16, "softmaxprogramconfig": 17, "softmaxshardedmulticoreprogramconfig": 18, "ab": 19, "abs_bw": 20, "aco": 21, "acos_bw": 22, "acosh": 23, "acosh_bw": 24, "adaptive_avg_pool2d": 25, "adaptive_max_pool2d": 26, "add_": 28, "add_bw": 29, "addalpha": 30, "addalpha_bw": 31, "addcdiv": 32, "addcdiv_bw": 33, "addcmul": 34, "addcmul_bw": 35, "addmm": 36, "input_tensor": [36, 93, 178, 215, 216, 217, 320, 353, 354, 355, 439, 461], "mat1_tensor": 36, "mat2_tensor": 36, "all_broadcast": 37, "all_gath": 38, "all_reduc": 39, "all_to_all_combin": 40, "all_to_all_dispatch": 41, "alt_complex_rotate90": 42, "angl": 43, "angle_bw": 44, "arang": 45, "argmax": 46, "input": [46, 63, 200, 269, 273, 275, 368, 369, 370, 412, 419, 463, 470, 488], "output": [46, 63, 487, 488, 489, 490, 493, 494, 495], "as_tensor": 47, "asin": 48, "asin_bw": 49, "asinh": 50, "asinh_bw": 51, "assign": 52, "assign_bw": 53, "atan": 54, "atan2": 55, "atan2_bw": 56, "atan_bw": 57, "atanh": 58, "atanh_bw": 59, "avg_pool2d": 60, "batch_norm": 61, "bcast": 62, "bernoulli": 63, "provid": [63, 367], "bias_gelu": 64, "bias_gelu_": 65, "bias_gelu_bw": 66, "bitcast": 67, "bitwise_and": 68, "bitwise_left_shift": 69, "bitwise_not": 70, "bitwise_or": 71, "bitwise_right_shift": 72, "bitwise_xor": 73, "broadcast": [74, 490], "cbrt": 75, "ceil": 76, "ceil_bw": 77, "celu": 78, "celu_bw": 79, "chunk": 80, "clamp": 81, "clamp_bw": 82, "clip": [83, 491], "clip_bw": 84, "clone": [85, 482], "close_devic": 86, "complex_tensor": 87, "real": [87, 328], "imag": [87, 198, 482, 491, 493], "concat": 88, "concat_bw": 89, "conj": 90, "conj_bw": 91, "conv1d": 92, "conv2d": 93, "output_tensor": [93, 215, 320, 353, 367], "weights_tensor": 93, "host": [93, 96, 97, 490, 492], "prepar": 93, "bias_tensor": 93, "conv_transpose2d": 94, "copi": 95, "copy_device_to_host_tensor": 96, "copy_host_to_device_tensor": 97, "co": 98, "cos_bw": 99, "cosh": 100, "cosh_bw": 101, "create_sharded_memory_config": 102, "cumprod": 103, "cumsum": 104, "dealloc": 105, "deg2rad": 106, "deg2rad_bw": 107, "dequant": 108, "digamma": 109, "digamma_bw": 110, "div": 111, "div_bw": 112, "div_no_nan": 113, "div_no_nan_bw": 114, "divid": 115, "divide_": 116, "dram_prefetch": 117, "dump_tensor": 118, "elu": 119, "elu_bw": 120, "ema": 121, "embedding_bw": 123, "empti": 124, "empty_lik": 125, "eq": 126, "eq_": 127, "eqz": 128, "erf": 129, "erf_bw": 130, "erfc": 131, "erfc_bw": 132, "erfinv": 133, "erfinv_bw": 134, "exp": 135, "exp2": 136, "exp2_bw": 137, "exp_bw": 138, "expand": 139, "experiment": [140, 141, 142], "conv3d": 140, "dropout": 141, "gelu_bw": [142, 171], "expm1": 143, "expm1_bw": 144, "fill": 145, "fill_bw": 146, "fill_cach": 147, "fill_implicit_tile_pad": 148, "fill_ones_rm": 149, "fill_rm": 150, "fill_zero_bw": 151, "floor": 152, "floor_bw": 153, "floor_div": 154, "fmod": 155, "fmod_bw": 156, "fold": 157, "frac": 158, "frac_bw": 159, "from_buff": 160, "from_devic": 161, "from_torch": 162, "full": [163, 487, 488, 489, 490, 493, 494, 495], "full_lik": 164, "gather": 165, "gcd": 166, "ge": 167, "ge_": 168, "geglu": 169, "gelu": 170, "generic_op": 172, "get_device_tensor": 173, "gez": 174, "global_avg_pool2d": 175, "glu": 176, "grid_sampl": 177, "group_norm": 178, "weight": [178, 215, 216, 353, 354, 491, 493, 495], "gamma": [178, 215, 216, 353, 354], "bia": [178, 215, 216, 231, 353, 354], "beta": [178, 215, 216, 353, 354], "input_mask": 178, "gt": 179, "gt_": 180, "gtz": 181, "hardmish": 182, "hardshrink": 183, "hardshrink_bw": 184, "hardsigmoid": 185, "hardsigmoid_bw": 186, "hardswish": 187, "hardswish_bw": 188, "hardtanh": 189, "hardtanh_bw": 190, "heavisid": 191, "hypot": 192, "hypot_bw": 193, "i0": 194, "i0_bw": 195, "i1": 196, "ident": 197, "imag_bw": 199, "index_fil": 200, "index": 200, "indexed_fil": 201, "interleaved_to_shard": 202, "interleaved_to_sharded_parti": 203, "is_imag": 204, "is_real": 205, "isclos": 206, "isfinit": 207, "isinf": 208, "isnan": 209, "isneginf": 210, "isposinf": 211, "kv_cach": [212, 213], "fill_cache_for_user_": 212, "update_cache_for_token_": 213, "l1_loss": 214, "layer_norm": 215, "residual_input_tensor": [215, 217, 353, 355], "layer_norm_post_all_gath": 216, "stat": [216, 354], "layer_norm_pre_all_gath": 217, "lcm": 218, "ldexp": 219, "ldexp_": 220, "ldexp_bw": 221, "le": 222, "le_": 223, "leaky_relu": 224, "leaky_relu_bw": 225, "lerp": 226, "lerp_bw": 227, "lez": 228, "lgamma": 229, "lgamma_bw": 230, "linear": 231, "input_tensor_a": [231, 268, 400], "input_tensor_b": [231, 268, 400], "load_tensor": 232, "log": 233, "log10": 234, "log10_bw": 235, "log1p": 236, "log1p_bw": 237, "log2": 238, "log2_bw": 239, "log_bw": 240, "log_sigmoid": 241, "log_sigmoid_bw": 242, "logaddexp": 243, "logaddexp2": 244, "logaddexp2_": 245, "logaddexp2_bw": 246, "logaddexp_": 247, "logaddexp_bw": 248, "logical_and": 249, "logical_and_": 250, "logical_left_shift": 251, "logical_not": 252, "logical_not_": 253, "logical_or": 254, "logical_or_": 255, "logical_right_shift": 256, "logical_xor": 257, "logical_xor_": 258, "logit": 259, "logit_bw": 260, "logiteps_bw": 261, "lt": 262, "lt_": 263, "ltz": 264, "mac": 265, "manage_devic": 266, "manual_se": 267, "matmul": 268, "configur": [268, 400, 482, 489, 494], "max": 269, "max_bw": 270, "max_pool2d": 271, "maximum": 272, "mean": 273, "mesh_partit": 274, "min": 275, "min_bw": 276, "minimum": 277, "mish": 278, "model_preprocess": [279, 280], "preprocess_model": 279, "preprocess_model_paramet": 280, "moe": 281, "moe_expert_token_remap": 282, "moe_routing_remap": 283, "move": [284, 492], "mse_loss": 285, "mul_bw": 286, "multigammaln": 287, "multigammaln_bw": 288, "multipli": [289, 489], "multiply_": 290, "ne": 291, "ne_": 292, "neg": 293, "neg_bw": 294, "nextaft": 295, "nez": 296, "nonzero": 297, "normalize_glob": 298, "normalize_hw": 299, "ones": 300, "ones_lik": 301, "open_devic": 302, "outer": 303, "pad": 304, "pad_to_tile_shap": 305, "permut": 306, "point_to_point": 307, "polar": 308, "polar_bw": 309, "polygamma": 310, "polygamma_bw": 311, "polyv": 312, "pow": 313, "pow_bw": 314, "prelu": 315, "prepare_conv_bia": 316, "prepare_conv_transpose2d_bia": 317, "prepare_conv_transpose2d_weight": 318, "prepare_conv_weight": 319, "prod": 320, "prod_bw": 321, "rad2deg": 323, "rad2deg_bw": 324, "rand": 325, "rdiv": 326, "rdiv_bw": 327, "real_bw": 329, "realloc": 330, "reciproc": 331, "reciprocal_bw": 332, "reduce_scatt": 333, "reduce_to_root": 334, "register_post_operation_hook": 335, "register_pre_operation_hook": 336, "reglu": 337, "relu": 338, "relu6": 339, "relu6_bw": 340, "relu_bw": 341, "relu_max": 342, "relu_min": 343, "remaind": 344, "remainder_bw": 345, "repeat": 346, "repeat_bw": 347, "repeat_interleav": 348, "requant": 349, "reshap": 350, "reshape_on_devic": 351, "reshard": 352, "rms_norm": 353, "rms_norm_post_all_gath": 354, "rms_norm_pre_all_gath": 355, "roll": 356, "rotat": 357, "round": 358, "round_bw": 359, "rpow": 360, "rpow_bw": 361, "rsqrt": 362, "rsqrt_bw": 363, "rsub": 364, "rsub_": 365, "rsub_bw": 366, "sampl": [367, 495], "input_values_tensor": 367, "input_indices_tensor": 367, "k": 367, "p": 367, "temp": 367, "default": [367, 492], "scale_causal_mask_hw_dims_softmax_in_plac": 368, "shard": [368, 485, 492], "mask": [368, 369, 370], "h": 368, "w": 368, "scale_mask_softmax": 369, "scale_mask_softmax_in_plac": 370, "scatter": 371, "scatter_add": 372, "selu": 373, "selu_bw": 374, "set_printopt": 375, "sharded_to_interleav": 376, "sharded_to_interleaved_parti": 377, "sigmoid": 378, "sigmoid_accur": 379, "sigmoid_bw": 380, "sign": 381, "sign_bw": 382, "signbit": 383, "silu": 384, "silu_bw": 385, "sin": 386, "sin_bw": 387, "sinh": 388, "sinh_bw": 389, "slice": 390, "softmax": 391, "softmax_in_plac": 392, "softplu": 393, "softplus_bw": 394, "softshrink": 395, "softshrink_bw": 396, "softsign": 397, "softsign_bw": 398, "sort": 399, "sparse_matmul": 400, "sparsiti": 400, "split": 401, "split_work_to_cor": 402, "sqrt": 403, "sqrt_bw": 404, "squar": 405, "square_bw": 406, "squared_differ": 407, "squared_difference_": 408, "squared_difference_bw": 409, "squeez": 410, "stack": 411, "std": 412, "std_hw": 413, "sub_bw": 414, "subalpha": 415, "subalpha_bw": 416, "subtract": 417, "subtract_": 418, "sum": 419, "swiglu": 420, "swish": 421, "synchronize_devic": 422, "tan": 423, "tan_bw": 424, "tanh": 425, "tanh_bw": 426, "tanhshrink": 427, "tanhshrink_bw": 428, "threshold": 429, "threshold_bw": 430, "tiliz": 431, "tilize_with_val_pad": 432, "tilize_with_zero_pad": 433, "to_devic": 434, "to_dtyp": 435, "to_layout": 436, "to_memory_config": 437, "to_torch": 438, "topk": 439, "index_tensor": 439, "attention_softmax": 440, "attention_softmax_": 441, "chunked_flash_mla_prefil": 442, "chunked_scaled_dot_product_attent": 443, "concatenate_head": 444, "flash_mla_prefil": 445, "flash_multi_latent_attention_decod": 446, "joint_scaled_dot_product_attent": 447, "paged_flash_multi_latent_attention_decod": 448, "paged_scaled_dot_product_attention_decod": 449, "ring_distributed_scaled_dot_product_attent": 450, "ring_joint_scaled_dot_product_attent": 451, "scaled_dot_product_attent": 452, "scaled_dot_product_attention_decod": 453, "split_query_key_value_and_split_head": 454, "windowed_scaled_dot_product_attent": 455, "transpos": 456, "tril": 457, "triu": 458, "trunc": 459, "trunc_bw": 460, "typecast": 461, "unary_chain": 462, "uniform": 463, "unsqueez": 464, "unsqueeze_to_4d": 465, "until": 466, "untilize_with_unpad": 467, "update_cach": 468, "upsampl": 469, "var": 470, "var_hw": 471, "view": 472, "where": [473, 481], "where_bw": 474, "xlogi": 475, "xlogy_bw": 476, "zero": [477, 491], "zeros_lik": 478, "convert": [479, 492], "pytorch": [479, 492, 496], "rewrit": 479, "switch": 479, "optim": 479, "more": [479, 489], "build": [480, 481, 482, 491], "uplift": 480, "demo": [480, 481], "get": [481, 492], "start": [481, 492], "instal": [481, 482], "explor": 481, "our": 481, "To": [481, 482], "go": 481, "from": 481, "here": 481, "prerequisit": [482, 497], "set": [482, 488, 496], "up": 482, "hardwar": 482, "softwar": 482, "depend": [482, 491], "script": 482, "recommend": 482, "manual": [482, 488], "metalium": 482, "There": 482, "four": 482, "latest": 482, "wheel": 482, "For": 482, "user": 482, "onli": 482, "environ": 482, "docker": 482, "releas": 482, "sourc": 482, "repositori": 482, "librari": [482, 487, 488, 489, 490, 492, 493, 496], "virtual": 482, "setup": [482, 495], "anaconda": 482, "packag": 482, "you": [482, 492], "all": 482, "verifi": 482, "your": [482, 492], "try": 482, "execut": 482, "interest": 482, "multi": [482, 492, 494], "card": 482, "topologi": 482, "machin": 482, "requir": [482, 485, 492], "overview": 482, "why": [482, 492], "It": 482, "matter": [482, 492], "vm": 482, "onboard": 483, "profil": [484, 497], "perf": 484, "header": 484, "profile_thi": 484, "descript": 484, "us": [484, 489, 496], "perform": [484, 489, 492, 497], "visual": [484, 496, 497], "shape": 485, "layout": [485, 489, 492], "type": [485, 492], "width": 485, "limit": 485, "bfloat8_b": 485, "storag": 485, "tutori": [486, 496], "import": [487, 488, 489, 490, 491, 492, 493, 495, 496], "open": [487, 488, 489, 490, 493, 495], "tenstorr": 487, "addit": 487, "close": [487, 488, 489, 490, 493, 495, 496], "basic": [488, 490, 492], "seed": 488, "creat": [488, 492], "forward": 488, "method": [488, 491], "paramet": 488, "run": [488, 491, 492, 495, 497], "initi": [489, 492, 495], "b": 489, "random": 489, "valu": 489, "inspect": 489, "result": [489, 492, 497], "tile": [490, 492], "base": 490, "arithmet": 490, "simul": 490, "row": 490, "vector": 490, "expans": 490, "shot": 491, "classif": 491, "doe": 491, "manag": [491, 492], "util": 491, "architectur": 491, "process": 491, "pipelin": 491, "complet": 491, "compon": 491, "kei": 491, "preprocess": 491, "download": [491, 496], "token": 491, "infer": [491, 492, 493, 495], "text": 491, "introduct": 492, "ll": 492, "learn": 492, "directli": 492, "interoper": 492, "back": 492, "4": [492, 496], "understand": 492, "behavior": 492, "differ": 492, "preserv": 492, "dure": 492, "transfer": 492, "between": 492, "5": 492, "precis": 492, "v": 492, "accuraci": [492, 493], "trade": 492, "off": 492, "6": 492, "test": [492, 493, 495], "neural": 492, "network": 492, "7": 492, "just": 492, "In": 492, "time": 492, "compil": 492, "first": 492, "subsequ": 492, "affect": 492, "direct": 492, "sram": 492, "l1": 492, "control": 492, "advanc": 492, "intermedi": 492, "focu": 492, "topic": 492, "8": 492, "exercis": 492, "scale": 492, "dot": 492, "product": 492, "attent": [492, 494], "background": 492, "task": 492, "math": 492, "fidel": 492, "metal": 492, "trace": [492, 496], "mlp": 493, "load": [493, 495], "mnist": 493, "pretrain": 493, "track": 493, "loop": 493, "flatten": 493, "head": 494, "write": 494, "simpl": 495, "cnn": 495, "cifar": 495, "10": 495, "dataset": 495, "defin": 495, "stage": 495, "tracer": 496, "bert": 496, "layer": 496, "written": 496, "analysi": 497, "upload": 497, "tab": 497, "buffer": 497, "graph": 497, "recap": 497}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "nbsphinx": 4, "sphinx": 58}, "alltitles": {"Welcome to TT-NN documentation!": [[0, "welcome-to-tt-nn-documentation"]], "TTNN": [[0, null]], "Resources": [[0, null]], "Indices and tables": [[0, "indices-and-tables"]], "Contributing as a developer": [[1, "contributing-as-a-developer"]], "Support": [[2, "support"]], "Reporting bugs, feature proposals, or support requests": [[2, "reporting-bugs-feature-proposals-or-support-requests"]], "Troubleshooting and debugging tips": [[2, "troubleshooting-and-debugging-tips"]], "Community": [[2, "community"]], "Tools": [[3, "tools"]], "What is TT-NN?": [[4, "what-is-tt-nn"]], "Adding New TT-NN Operation": [[5, "adding-new-tt-nn-operation"]], "FAQ": [[5, "faq"]], "What is a TT-NN operation?": [[5, "what-is-a-tt-nn-operation"]], "What steps are needed to add TT-NN operation in C++?": [[5, "what-steps-are-needed-to-add-tt-nn-operation-in-c"]], "What steps are needed to add TT-NN operation in Python?": [[5, "what-steps-are-needed-to-add-tt-nn-operation-in-python"]], "Example of Adding a new Device Operation": [[5, "example-of-adding-a-new-device-operation"]], "C++ Implementation": [[5, "c-implementation"]], "Step 1: Implement device operation": [[5, "step-1-implement-device-operation"]], "Step 2: Implement the operation in C++": [[5, "step-2-implement-the-operation-in-c"]], "Python Implementation": [[5, "python-implementation"]], "Step 1: Add Python binding": [[5, "step-1-add-python-binding"]], "Step 2: (Optional) Add golden function for the operation in Python": [[5, "step-2-optional-add-golden-function-for-the-operation-in-python"]], "Step 3: (Optional) Add example usage to docs": [[5, "step-3-optional-add-example-usage-to-docs"]], "APIs": [[6, "apis"], [485, "apis"]], "Device": [[6, "device"]], "Memory Config": [[6, "memory-config"], [485, "memory-config"]], "Operations": [[6, "operations"]], "Core": [[6, "core"]], "Tensor Creation": [[6, "tensor-creation"], [487, "Tensor-Creation"]], "Matrix Multiplication": [[6, "matrix-multiplication"], [489, "Matrix-Multiplication"]], "Pointwise Unary": [[6, "pointwise-unary"]], "Pointwise Binary": [[6, "pointwise-binary"]], "Pointwise Ternary": [[6, "pointwise-ternary"]], "Quantization": [[6, "quantization"]], "Losses": [[6, "losses"]], "Reduction": [[6, "reduction"]], "Data Movement": [[6, "data-movement"]], "Normalization": [[6, "normalization"]], "Normalization Program Configs": [[6, "normalization-program-configs"]], "Transformer": [[6, "transformer"]], "CCL": [[6, "ccl"]], "Embedding": [[6, "embedding"]], "Convolution": [[6, "convolution"]], "Pooling": [[6, "pooling"]], "Prefetcher": [[6, "prefetcher"]], "Vision": [[6, "vision"]], "Generic": [[6, "generic"]], "KV Cache": [[6, "kv-cache"]], "Backward operations": [[6, "backward-operations"]], "Model Conversion": [[6, "model-conversion"]], "Reports": [[6, "reports"]], "Operation Hooks": [[6, "operation-hooks"]], "ttnn.Conv2dConfig": [[7, "ttnn-conv2dconfig"]], "ttnn.Conv2dSliceConfig": [[8, "ttnn-conv2dsliceconfig"]], "ttnn.GetDefaultDevice": [[9, "ttnn-getdefaultdevice"]], "Example": [[9, null], [15, null], [16, null], [18, null], [19, null], [20, null], [21, null], [22, null], [23, null], [24, null], [25, null], [26, null], [27, null], [28, null], [29, null], [30, null], [31, null], [32, null], [33, null], [34, null], [35, null], [36, null], [37, null], [38, null], [39, null], [40, null], [41, null], [42, null], [43, null], [44, null], [45, null], [46, null], [47, null], [48, null], [49, null], [50, null], [51, null], [53, null], [54, null], [55, null], [56, null], [57, null], [58, null], [59, null], [60, null], [61, null], [63, null], [64, null], [65, null], [66, null], [67, null], [68, null], [69, null], [70, null], [71, null], [72, null], [73, null], [74, null], [75, null], [76, null], [77, null], [78, null], [79, null], [81, null], [82, null], [83, null], [84, null], [85, null], [86, null], [87, null], [88, null], [89, null], [90, null], [91, null], [96, null], [97, null], [98, null], [99, null], [100, null], [101, null], [102, null], [103, null], [104, null], [105, null], [106, null], [107, null], [108, null], [109, null], [110, null], [111, null], [112, null], [113, null], [114, null], [115, null], [116, null], [118, null], [119, null], [120, null], [121, null], [122, null], [123, null], [124, null], [125, null], [126, null], [127, null], [128, null], [129, null], [130, null], [131, null], [132, null], [133, null], [134, null], [135, null], [136, null], [137, null], [138, null], [141, null], [142, null], [143, null], [144, null], [145, null], [146, null], [147, null], [149, null], [150, null], [151, null], [152, null], [153, null], [154, null], [155, null], [156, null], [158, null], [159, null], [160, null], [161, null], [162, null], [163, null], [164, null], [165, null], [166, null], [167, null], [168, null], [169, null], [170, null], [171, null], [172, null], [174, null], [175, null], [176, null], [177, null], [178, null], [179, null], [180, null], [181, null], [182, null], [183, null], [184, null], [185, null], [186, null], [187, null], [188, null], [189, null], [190, null], [191, null], [192, null], [193, null], [194, null], [195, null], [196, null], [197, null], [198, null], [199, null], [200, null], [201, null], [203, null], [204, null], [205, null], [206, null], [207, null], [208, null], [209, null], [210, null], [211, null], [214, null], [215, null], [216, null], [217, null], [218, null], [219, null], [220, null], [221, null], [222, null], [223, null], [224, null], [225, null], [226, null], [227, null], [228, null], [229, null], [230, null], [231, null], [232, null], [233, null], [234, null], [235, null], [236, null], [237, null], [238, null], [239, null], [240, null], [241, null], [242, null], [243, null], [244, null], [245, null], [246, null], [247, null], [248, null], [249, null], [250, null], [251, null], [252, null], [253, null], [254, null], [255, null], [256, null], [257, null], [258, null], [259, null], [259, null], [260, null], [261, null], [262, null], [263, null], [264, null], [265, null], [266, null], [267, null], [268, null], [269, null], [270, null], [271, null], [272, null], [273, null], [274, null], [275, null], [276, null], [277, null], [278, null], [281, null], [285, null], [286, null], [287, null], [288, null], [289, null], [290, null], [291, null], [292, null], [293, null], [294, null], [295, null], [296, null], [297, null], [298, null], [299, null], [300, null], [301, null], [302, null], [303, null], [304, null], [305, null], [306, null], [308, null], [309, null], [310, null], [311, null], [312, null], [313, null], [314, null], [315, null], [320, null], [321, null], [322, null], [323, null], [324, null], [325, null], [326, null], [327, null], [328, null], [329, null], [330, null], [331, null], [332, null], [333, null], [337, null], [338, null], [339, null], [340, null], [341, null], [342, null], [343, null], [344, null], [345, null], [346, null], [347, null], [348, null], [349, null], [350, null], [351, null], [352, null], [353, null], [354, null], [355, null], [357, null], [358, null], [359, null], [360, null], [361, null], [362, null], [363, null], [364, null], [365, null], [366, null], [367, null], [368, null], [369, null], [370, null], [371, null], [372, null], [373, null], [374, null], [376, null], [377, null], [378, null], [379, null], [380, null], [381, null], [382, null], [383, null], [384, null], [385, null], [386, null], [387, null], [388, null], [389, null], [390, null], [391, null], [392, null], [393, null], [394, null], [395, null], [396, null], [397, null], [398, null], [399, null], [400, null], [403, null], [404, null], [405, null], [406, null], [407, null], [408, null], [409, null], [411, null], [412, null], [413, null], [414, null], [415, null], [416, null], [417, null], [418, null], [419, null], [420, null], [421, null], [422, null], [423, null], [424, null], [425, null], [426, null], [427, null], [428, null], [429, null], [430, null], [431, null], [432, null], [434, null], [435, null], [436, null], [437, null], [438, null], [439, null], [455, null], [457, null], [458, null], [459, null], [460, null], [461, null], [462, null], [463, null], [466, null], [467, null], [468, null], [470, null], [471, null], [472, null], [473, null], [474, null], [475, null], [476, null], [477, null], [478, null]], "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig": [[10, "ttnn-matmulmulticorereusemulticast1dprogramconfig"]], "ttnn.MatmulMultiCoreReuseMultiCastBatchedDRAMShardedProgramConfig": [[11, "ttnn-matmulmulticorereusemulticastbatcheddramshardedprogramconfig"]], "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig": [[12, "ttnn-matmulmulticorereusemulticastdramshardedprogramconfig"]], "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig": [[13, "ttnn-matmulmulticorereusemulticastprogramconfig"]], "ttnn.MatmulMultiCoreReuseProgramConfig": [[14, "ttnn-matmulmulticorereuseprogramconfig"]], "ttnn.SetDefaultDevice": [[15, "ttnn-setdefaultdevice"]], "ttnn.SoftmaxDefaultProgramConfig": [[16, "ttnn-softmaxdefaultprogramconfig"]], "ttnn.SoftmaxProgramConfig": [[17, "ttnn-softmaxprogramconfig"]], "ttnn.SoftmaxShardedMultiCoreProgramConfig": [[18, "ttnn-softmaxshardedmulticoreprogramconfig"]], "ttnn.abs": [[19, "ttnn-abs"]], "ttnn.abs_bw": [[20, "ttnn-abs-bw"]], "ttnn.acos": [[21, "ttnn-acos"]], "ttnn.acos_bw": [[22, "ttnn-acos-bw"]], "ttnn.acosh": [[23, "ttnn-acosh"]], "ttnn.acosh_bw": [[24, "ttnn-acosh-bw"]], "ttnn.adaptive_avg_pool2d": [[25, "ttnn-adaptive-avg-pool2d"]], "ttnn.adaptive_max_pool2d": [[26, "ttnn-adaptive-max-pool2d"]], "ttnn.add": [[27, "ttnn-add"]], "ttnn.add_": [[28, "ttnn-add"]], "ttnn.add_bw": [[29, "ttnn-add-bw"]], "ttnn.addalpha": [[30, "ttnn-addalpha"]], "ttnn.addalpha_bw": [[31, "ttnn-addalpha-bw"]], "ttnn.addcdiv": [[32, "ttnn-addcdiv"]], "ttnn.addcdiv_bw": [[33, "ttnn-addcdiv-bw"]], "ttnn.addcmul": [[34, "ttnn-addcmul"]], "ttnn.addcmul_bw": [[35, "ttnn-addcmul-bw"]], "ttnn.addmm": [[36, "ttnn-addmm"]], "input_tensor": [[36, "id2"], [93, "id2"], [178, "id2"], [215, "id2"], [216, "id2"], [217, "id2"], [320, "id2"], [353, "id2"], [354, "id2"], [355, "id2"], [439, "id2"], [461, "id2"]], "mat1_tensor": [[36, "id3"]], "mat2_tensor": [[36, "id4"]], "ttnn.all_broadcast": [[37, "ttnn-all-broadcast"]], "ttnn.all_gather": [[38, "ttnn-all-gather"]], "ttnn.all_reduce": [[39, "ttnn-all-reduce"]], "ttnn.all_to_all_combine": [[40, "ttnn-all-to-all-combine"]], "ttnn.all_to_all_dispatch": [[41, "ttnn-all-to-all-dispatch"]], "ttnn.alt_complex_rotate90": [[42, "ttnn-alt-complex-rotate90"]], "ttnn.angle": [[43, "ttnn-angle"]], "ttnn.angle_bw": [[44, "ttnn-angle-bw"]], "ttnn.arange": [[45, "ttnn-arange"]], "ttnn.argmax": [[46, "ttnn-argmax"]], "Input Tensor": [[46, "id2"], [269, "id2"], [273, "id2"], [275, "id2"], [369, "id2"], [370, "id2"], [412, "id2"], [419, "id2"], [470, "id2"]], "Output Tensor": [[46, "id3"]], "ttnn.as_tensor": [[47, "ttnn-as-tensor"]], "ttnn.asin": [[48, "ttnn-asin"]], "ttnn.asin_bw": [[49, "ttnn-asin-bw"]], "ttnn.asinh": [[50, "ttnn-asinh"]], "ttnn.asinh_bw": [[51, "ttnn-asinh-bw"]], "ttnn.assign": [[52, "ttnn-assign"]], "ttnn.assign_bw": [[53, "ttnn-assign-bw"]], "ttnn.atan": [[54, "ttnn-atan"]], "ttnn.atan2": [[55, "ttnn-atan2"]], "ttnn.atan2_bw": [[56, "ttnn-atan2-bw"]], "ttnn.atan_bw": [[57, "ttnn-atan-bw"]], "ttnn.atanh": [[58, "ttnn-atanh"]], "ttnn.atanh_bw": [[59, "ttnn-atanh-bw"]], "ttnn.avg_pool2d": [[60, "ttnn-avg-pool2d"]], "ttnn.batch_norm": [[61, "ttnn-batch-norm"]], "ttnn.bcast": [[62, "ttnn-bcast"]], "ttnn.bernoulli": [[63, "ttnn-bernoulli"]], "input tensor and output tensor (if provided)": [[63, "id2"]], "ttnn.bias_gelu": [[64, "ttnn-bias-gelu"]], "ttnn.bias_gelu_": [[65, "ttnn-bias-gelu"]], "ttnn.bias_gelu_bw": [[66, "ttnn-bias-gelu-bw"]], "ttnn.bitcast": [[67, "ttnn-bitcast"]], "ttnn.bitwise_and": [[68, "ttnn-bitwise-and"]], "ttnn.bitwise_left_shift": [[69, "ttnn-bitwise-left-shift"]], "ttnn.bitwise_not": [[70, "ttnn-bitwise-not"]], "ttnn.bitwise_or": [[71, "ttnn-bitwise-or"]], "ttnn.bitwise_right_shift": [[72, "ttnn-bitwise-right-shift"]], "ttnn.bitwise_xor": [[73, "ttnn-bitwise-xor"]], "ttnn.broadcast": [[74, "ttnn-broadcast"]], "ttnn.cbrt": [[75, "ttnn-cbrt"]], "ttnn.ceil": [[76, "ttnn-ceil"]], "ttnn.ceil_bw": [[77, "ttnn-ceil-bw"]], "ttnn.celu": [[78, "ttnn-celu"]], "ttnn.celu_bw": [[79, "ttnn-celu-bw"]], "ttnn.chunk": [[80, "ttnn-chunk"]], "ttnn.clamp": [[81, "ttnn-clamp"]], "ttnn.clamp_bw": [[82, "ttnn-clamp-bw"]], "ttnn.clip": [[83, "ttnn-clip"]], "ttnn.clip_bw": [[84, "ttnn-clip-bw"]], "ttnn.clone": [[85, "ttnn-clone"]], "ttnn.close_device": [[86, "ttnn-close-device"]], "ttnn.complex_tensor": [[87, "ttnn-complex-tensor"]], "real and imag tensors": [[87, "id2"]], "ttnn.concat": [[88, "ttnn-concat"]], "ttnn.concat_bw": [[89, "ttnn-concat-bw"]], "ttnn.conj": [[90, "ttnn-conj"]], "ttnn.conj_bw": [[91, "ttnn-conj-bw"]], "ttnn.conv1d": [[92, "ttnn-conv1d"]], "ttnn.conv2d": [[93, "ttnn-conv2d"]], "output_tensor": [[93, "id3"], [215, "id5"], [320, "id3"], [353, "id5"]], "weights_tensor (host)": [[93, "id4"]], "weights_tensor (prepared on device)": [[93, "id5"]], "bias_tensor (host)": [[93, "id6"]], "bias_tensor (prepared on device)": [[93, "id7"]], "ttnn.conv_transpose2d": [[94, "ttnn-conv-transpose2d"]], "ttnn.copy": [[95, "ttnn-copy"]], "ttnn.copy_device_to_host_tensor": [[96, "ttnn-copy-device-to-host-tensor"]], "device/host tensor": [[96, "id2"]], "ttnn.copy_host_to_device_tensor": [[97, "ttnn-copy-host-to-device-tensor"]], "host/device tensor": [[97, "id2"]], "ttnn.cos": [[98, "ttnn-cos"]], "ttnn.cos_bw": [[99, "ttnn-cos-bw"]], "ttnn.cosh": [[100, "ttnn-cosh"]], "ttnn.cosh_bw": [[101, "ttnn-cosh-bw"]], "ttnn.create_sharded_memory_config": [[102, "ttnn-create-sharded-memory-config"]], "ttnn.cumprod": [[103, "ttnn-cumprod"]], "ttnn.cumsum": [[104, "ttnn-cumsum"]], "ttnn.deallocate": [[105, "ttnn-deallocate"]], "ttnn.deg2rad": [[106, "ttnn-deg2rad"]], "ttnn.deg2rad_bw": [[107, "ttnn-deg2rad-bw"]], "ttnn.dequantize": [[108, "ttnn-dequantize"]], "ttnn.digamma": [[109, "ttnn-digamma"]], "ttnn.digamma_bw": [[110, "ttnn-digamma-bw"]], "ttnn.div": [[111, "ttnn-div"]], "ttnn.div_bw": [[112, "ttnn-div-bw"]], "ttnn.div_no_nan": [[113, "ttnn-div-no-nan"]], "ttnn.div_no_nan_bw": [[114, "ttnn-div-no-nan-bw"]], "ttnn.divide": [[115, "ttnn-divide"]], "ttnn.divide_": [[116, "ttnn-divide"]], "ttnn.dram_prefetcher": [[117, "ttnn-dram-prefetcher"]], "ttnn.dump_tensor": [[118, "ttnn-dump-tensor"]], "ttnn.elu": [[119, "ttnn-elu"]], "ttnn.elu_bw": [[120, "ttnn-elu-bw"]], "ttnn.ema": [[121, "ttnn-ema"]], "ttnn.embedding": [[122, "ttnn-embedding"]], "ttnn.embedding_bw": [[123, "ttnn-embedding-bw"]], "ttnn.empty": [[124, "ttnn-empty"]], "ttnn.empty_like": [[125, "ttnn-empty-like"]], "ttnn.eq": [[126, "ttnn-eq"]], "ttnn.eq_": [[127, "ttnn-eq"]], "ttnn.eqz": [[128, "ttnn-eqz"]], "ttnn.erf": [[129, "ttnn-erf"]], "ttnn.erf_bw": [[130, "ttnn-erf-bw"]], "ttnn.erfc": [[131, "ttnn-erfc"]], "ttnn.erfc_bw": [[132, "ttnn-erfc-bw"]], "ttnn.erfinv": [[133, "ttnn-erfinv"]], "ttnn.erfinv_bw": [[134, "ttnn-erfinv-bw"]], "ttnn.exp": [[135, "ttnn-exp"]], "ttnn.exp2": [[136, "ttnn-exp2"]], "ttnn.exp2_bw": [[137, "ttnn-exp2-bw"]], "ttnn.exp_bw": [[138, "ttnn-exp-bw"]], "ttnn.expand": [[139, "ttnn-expand"]], "ttnn.experimental.conv3d": [[140, "ttnn-experimental-conv3d"]], "ttnn.experimental.dropout": [[141, "ttnn-experimental-dropout"]], "ttnn.experimental.gelu_bw": [[142, "ttnn-experimental-gelu-bw"]], "ttnn.expm1": [[143, "ttnn-expm1"]], "ttnn.expm1_bw": [[144, "ttnn-expm1-bw"]], "ttnn.fill": [[145, "ttnn-fill"]], "ttnn.fill_bw": [[146, "ttnn-fill-bw"]], "ttnn.fill_cache": [[147, "ttnn-fill-cache"]], "ttnn.fill_implicit_tile_padding": [[148, "ttnn-fill-implicit-tile-padding"]], "ttnn.fill_ones_rm": [[149, "ttnn-fill-ones-rm"]], "ttnn.fill_rm": [[150, "ttnn-fill-rm"]], "ttnn.fill_zero_bw": [[151, "ttnn-fill-zero-bw"]], "ttnn.floor": [[152, "ttnn-floor"]], "ttnn.floor_bw": [[153, "ttnn-floor-bw"]], "ttnn.floor_div": [[154, "ttnn-floor-div"]], "ttnn.fmod": [[155, "ttnn-fmod"]], "ttnn.fmod_bw": [[156, "ttnn-fmod-bw"]], "ttnn.fold": [[157, "ttnn-fold"]], "ttnn.frac": [[158, "ttnn-frac"]], "ttnn.frac_bw": [[159, "ttnn-frac-bw"]], "ttnn.from_buffer": [[160, "ttnn-from-buffer"]], "ttnn.from_device": [[161, "ttnn-from-device"]], "ttnn.from_torch": [[162, "ttnn-from-torch"]], "ttnn.full": [[163, "ttnn-full"]], "ttnn.full_like": [[164, "ttnn-full-like"]], "ttnn.gather": [[165, "ttnn-gather"]], "ttnn.gcd": [[166, "ttnn-gcd"]], "ttnn.ge": [[167, "ttnn-ge"]], "ttnn.ge_": [[168, "ttnn-ge"]], "ttnn.geglu": [[169, "ttnn-geglu"]], "ttnn.gelu": [[170, "ttnn-gelu"]], "ttnn.gelu_bw": [[171, "ttnn-gelu-bw"]], "ttnn.generic_op": [[172, "ttnn-generic-op"]], "ttnn.get_device_tensors": [[173, "ttnn-get-device-tensors"]], "ttnn.gez": [[174, "ttnn-gez"]], "ttnn.global_avg_pool2d": [[175, "ttnn-global-avg-pool2d"]], "ttnn.glu": [[176, "ttnn-glu"]], "ttnn.grid_sample": [[177, "ttnn-grid-sample"]], "ttnn.group_norm": [[178, "ttnn-group-norm"]], "weight (gamma) and bias (beta)": [[178, "id3"], [215, "id4"], [216, "id4"], [353, "id4"], [354, "id4"]], "input_mask": [[178, "id4"]], "ttnn.gt": [[179, "ttnn-gt"]], "ttnn.gt_": [[180, "ttnn-gt"]], "ttnn.gtz": [[181, "ttnn-gtz"]], "ttnn.hardmish": [[182, "ttnn-hardmish"]], "ttnn.hardshrink": [[183, "ttnn-hardshrink"]], "ttnn.hardshrink_bw": [[184, "ttnn-hardshrink-bw"]], "ttnn.hardsigmoid": [[185, "ttnn-hardsigmoid"]], "ttnn.hardsigmoid_bw": [[186, "ttnn-hardsigmoid-bw"]], "ttnn.hardswish": [[187, "ttnn-hardswish"]], "ttnn.hardswish_bw": [[188, "ttnn-hardswish-bw"]], "ttnn.hardtanh": [[189, "ttnn-hardtanh"]], "ttnn.hardtanh_bw": [[190, "ttnn-hardtanh-bw"]], "ttnn.heaviside": [[191, "ttnn-heaviside"]], "ttnn.hypot": [[192, "ttnn-hypot"]], "ttnn.hypot_bw": [[193, "ttnn-hypot-bw"]], "ttnn.i0": [[194, "ttnn-i0"]], "ttnn.i0_bw": [[195, "ttnn-i0-bw"]], "ttnn.i1": [[196, "ttnn-i1"]], "ttnn.identity": [[197, "ttnn-identity"]], "ttnn.imag": [[198, "ttnn-imag"]], "ttnn.imag_bw": [[199, "ttnn-imag-bw"]], "ttnn.index_fill": [[200, "ttnn-index-fill"]], "input tensor": [[200, "id2"], [463, "id2"]], "index tensor": [[200, "id3"]], "ttnn.indexed_fill": [[201, "ttnn-indexed-fill"]], "ttnn.interleaved_to_sharded": [[202, "ttnn-interleaved-to-sharded"]], "ttnn.interleaved_to_sharded_partial": [[203, "ttnn-interleaved-to-sharded-partial"]], "ttnn.is_imag": [[204, "ttnn-is-imag"]], "ttnn.is_real": [[205, "ttnn-is-real"]], "ttnn.isclose": [[206, "ttnn-isclose"]], "ttnn.isfinite": [[207, "ttnn-isfinite"]], "ttnn.isinf": [[208, "ttnn-isinf"]], "ttnn.isnan": [[209, "ttnn-isnan"]], "ttnn.isneginf": [[210, "ttnn-isneginf"]], "ttnn.isposinf": [[211, "ttnn-isposinf"]], "ttnn.kv_cache.fill_cache_for_user_": [[212, "ttnn-kv-cache-fill-cache-for-user"]], "ttnn.kv_cache.update_cache_for_token_": [[213, "ttnn-kv-cache-update-cache-for-token"]], "ttnn.l1_loss": [[214, "ttnn-l1-loss"]], "ttnn.layer_norm": [[215, "ttnn-layer-norm"]], "residual_input_tensor": [[215, "id3"], [217, "id3"], [353, "id3"], [355, "id3"]], "ttnn.layer_norm_post_all_gather": [[216, "ttnn-layer-norm-post-all-gather"]], "stats": [[216, "id3"], [354, "id3"]], "ttnn.layer_norm_pre_all_gather": [[217, "ttnn-layer-norm-pre-all-gather"]], "ttnn.lcm": [[218, "ttnn-lcm"]], "ttnn.ldexp": [[219, "ttnn-ldexp"]], "ttnn.ldexp_": [[220, "ttnn-ldexp"]], "ttnn.ldexp_bw": [[221, "ttnn-ldexp-bw"]], "ttnn.le": [[222, "ttnn-le"]], "ttnn.le_": [[223, "ttnn-le"]], "ttnn.leaky_relu": [[224, "ttnn-leaky-relu"]], "ttnn.leaky_relu_bw": [[225, "ttnn-leaky-relu-bw"]], "ttnn.lerp": [[226, "ttnn-lerp"]], "ttnn.lerp_bw": [[227, "ttnn-lerp-bw"]], "ttnn.lez": [[228, "ttnn-lez"]], "ttnn.lgamma": [[229, "ttnn-lgamma"]], "ttnn.lgamma_bw": [[230, "ttnn-lgamma-bw"]], "ttnn.linear": [[231, "ttnn-linear"]], "input_tensor_a": [[231, "id2"], [268, "id2"], [400, "id2"]], "input_tensor_b": [[231, "id3"], [268, "id3"], [400, "id3"]], "bias": [[231, "id4"]], "ttnn.load_tensor": [[232, "ttnn-load-tensor"]], "ttnn.log": [[233, "ttnn-log"]], "ttnn.log10": [[234, "ttnn-log10"]], "ttnn.log10_bw": [[235, "ttnn-log10-bw"]], "ttnn.log1p": [[236, "ttnn-log1p"]], "ttnn.log1p_bw": [[237, "ttnn-log1p-bw"]], "ttnn.log2": [[238, "ttnn-log2"]], "ttnn.log2_bw": [[239, "ttnn-log2-bw"]], "ttnn.log_bw": [[240, "ttnn-log-bw"]], "ttnn.log_sigmoid": [[241, "ttnn-log-sigmoid"]], "ttnn.log_sigmoid_bw": [[242, "ttnn-log-sigmoid-bw"]], "ttnn.logaddexp": [[243, "ttnn-logaddexp"]], "ttnn.logaddexp2": [[244, "ttnn-logaddexp2"]], "ttnn.logaddexp2_": [[245, "ttnn-logaddexp2"]], "ttnn.logaddexp2_bw": [[246, "ttnn-logaddexp2-bw"]], "ttnn.logaddexp_": [[247, "ttnn-logaddexp"]], "ttnn.logaddexp_bw": [[248, "ttnn-logaddexp-bw"]], "ttnn.logical_and": [[249, "ttnn-logical-and"]], "ttnn.logical_and_": [[250, "ttnn-logical-and"]], "ttnn.logical_left_shift": [[251, "ttnn-logical-left-shift"]], "ttnn.logical_not": [[252, "ttnn-logical-not"]], "ttnn.logical_not_": [[253, "ttnn-logical-not"]], "ttnn.logical_or": [[254, "ttnn-logical-or"]], "ttnn.logical_or_": [[255, "ttnn-logical-or"]], "ttnn.logical_right_shift": [[256, "ttnn-logical-right-shift"]], "ttnn.logical_xor": [[257, "ttnn-logical-xor"]], "ttnn.logical_xor_": [[258, "ttnn-logical-xor"]], "ttnn.logit": [[259, "ttnn-logit"]], "ttnn.logit_bw": [[260, "ttnn-logit-bw"]], "ttnn.logiteps_bw": [[261, "ttnn-logiteps-bw"]], "ttnn.lt": [[262, "ttnn-lt"]], "ttnn.lt_": [[263, "ttnn-lt"]], "ttnn.ltz": [[264, "ttnn-ltz"]], "ttnn.mac": [[265, "ttnn-mac"]], "ttnn.manage_device": [[266, "ttnn-manage-device"]], "ttnn.manual_seed": [[267, "ttnn-manual-seed"]], "ttnn.matmul": [[268, "ttnn-matmul"]], "Supported Memory Configurations": [[268, "id4"], [400, "id5"]], "ttnn.max": [[269, "ttnn-max"]], "ttnn.max_bw": [[270, "ttnn-max-bw"]], "ttnn.max_pool2d": [[271, "ttnn-max-pool2d"]], "ttnn.maximum": [[272, "ttnn-maximum"]], "ttnn.mean": [[273, "ttnn-mean"]], "ttnn.mesh_partition": [[274, "ttnn-mesh-partition"]], "ttnn.min": [[275, "ttnn-min"]], "ttnn.min_bw": [[276, "ttnn-min-bw"]], "ttnn.minimum": [[277, "ttnn-minimum"]], "ttnn.mish": [[278, "ttnn-mish"]], "ttnn.model_preprocessing.preprocess_model": [[279, "ttnn-model-preprocessing-preprocess-model"]], "ttnn.model_preprocessing.preprocess_model_parameters": [[280, "ttnn-model-preprocessing-preprocess-model-parameters"]], "ttnn.moe": [[281, "ttnn-moe"]], "ttnn.moe_expert_token_remap": [[282, "ttnn-moe-expert-token-remap"]], "ttnn.moe_routing_remap": [[283, "ttnn-moe-routing-remap"]], "ttnn.move": [[284, "ttnn-move"]], "ttnn.mse_loss": [[285, "ttnn-mse-loss"]], "ttnn.mul_bw": [[286, "ttnn-mul-bw"]], "ttnn.multigammaln": [[287, "ttnn-multigammaln"]], "ttnn.multigammaln_bw": [[288, "ttnn-multigammaln-bw"]], "ttnn.multiply": [[289, "ttnn-multiply"]], "ttnn.multiply_": [[290, "ttnn-multiply"]], "ttnn.ne": [[291, "ttnn-ne"]], "ttnn.ne_": [[292, "ttnn-ne"]], "ttnn.neg": [[293, "ttnn-neg"]], "ttnn.neg_bw": [[294, "ttnn-neg-bw"]], "ttnn.nextafter": [[295, "ttnn-nextafter"]], "ttnn.nez": [[296, "ttnn-nez"]], "ttnn.nonzero": [[297, "ttnn-nonzero"]], "ttnn.normalize_global": [[298, "ttnn-normalize-global"]], "ttnn.normalize_hw": [[299, "ttnn-normalize-hw"]], "ttnn.ones": [[300, "ttnn-ones"]], "ttnn.ones_like": [[301, "ttnn-ones-like"]], "ttnn.open_device": [[302, "ttnn-open-device"]], "ttnn.outer": [[303, "ttnn-outer"]], "ttnn.pad": [[304, "ttnn-pad"]], "ttnn.pad_to_tile_shape": [[305, "ttnn-pad-to-tile-shape"]], "ttnn.permute": [[306, "ttnn-permute"]], "ttnn.point_to_point": [[307, "ttnn-point-to-point"]], "ttnn.polar": [[308, "ttnn-polar"]], "ttnn.polar_bw": [[309, "ttnn-polar-bw"]], "ttnn.polygamma": [[310, "ttnn-polygamma"]], "ttnn.polygamma_bw": [[311, "ttnn-polygamma-bw"]], "ttnn.polyval": [[312, "ttnn-polyval"]], "ttnn.pow": [[313, "ttnn-pow"]], "ttnn.pow_bw": [[314, "ttnn-pow-bw"]], "ttnn.prelu": [[315, "ttnn-prelu"]], "ttnn.prepare_conv_bias": [[316, "ttnn-prepare-conv-bias"]], "ttnn.prepare_conv_transpose2d_bias": [[317, "ttnn-prepare-conv-transpose2d-bias"]], "ttnn.prepare_conv_transpose2d_weights": [[318, "ttnn-prepare-conv-transpose2d-weights"]], "ttnn.prepare_conv_weights": [[319, "ttnn-prepare-conv-weights"]], "ttnn.prod": [[320, "ttnn-prod"]], "ttnn.prod_bw": [[321, "ttnn-prod-bw"]], "ttnn.quantize": [[322, "ttnn-quantize"]], "ttnn.rad2deg": [[323, "ttnn-rad2deg"]], "ttnn.rad2deg_bw": [[324, "ttnn-rad2deg-bw"]], "ttnn.rand": [[325, "ttnn-rand"]], "ttnn.rdiv": [[326, "ttnn-rdiv"]], "ttnn.rdiv_bw": [[327, "ttnn-rdiv-bw"]], "ttnn.real": [[328, "ttnn-real"]], "ttnn.real_bw": [[329, "ttnn-real-bw"]], "ttnn.reallocate": [[330, "ttnn-reallocate"]], "ttnn.reciprocal": [[331, "ttnn-reciprocal"]], "ttnn.reciprocal_bw": [[332, "ttnn-reciprocal-bw"]], "ttnn.reduce_scatter": [[333, "ttnn-reduce-scatter"]], "ttnn.reduce_to_root": [[334, "ttnn-reduce-to-root"]], "ttnn.register_post_operation_hook": [[335, "ttnn-register-post-operation-hook"]], "ttnn.register_pre_operation_hook": [[336, "ttnn-register-pre-operation-hook"]], "ttnn.reglu": [[337, "ttnn-reglu"]], "ttnn.relu": [[338, "ttnn-relu"]], "ttnn.relu6": [[339, "ttnn-relu6"]], "ttnn.relu6_bw": [[340, "ttnn-relu6-bw"]], "ttnn.relu_bw": [[341, "ttnn-relu-bw"]], "ttnn.relu_max": [[342, "ttnn-relu-max"]], "ttnn.relu_min": [[343, "ttnn-relu-min"]], "ttnn.remainder": [[344, "ttnn-remainder"]], "ttnn.remainder_bw": [[345, "ttnn-remainder-bw"]], "ttnn.repeat": [[346, "ttnn-repeat"]], "ttnn.repeat_bw": [[347, "ttnn-repeat-bw"]], "ttnn.repeat_interleave": [[348, "ttnn-repeat-interleave"]], "ttnn.requantize": [[349, "ttnn-requantize"]], "ttnn.reshape": [[350, "ttnn-reshape"]], "ttnn.reshape_on_device": [[351, "ttnn-reshape-on-device"]], "ttnn.reshard": [[352, "ttnn-reshard"]], "ttnn.rms_norm": [[353, "ttnn-rms-norm"]], "ttnn.rms_norm_post_all_gather": [[354, "ttnn-rms-norm-post-all-gather"]], "ttnn.rms_norm_pre_all_gather": [[355, "ttnn-rms-norm-pre-all-gather"]], "ttnn.roll": [[356, "ttnn-roll"]], "ttnn.rotate": [[357, "ttnn-rotate"]], "ttnn.round": [[358, "ttnn-round"]], "ttnn.round_bw": [[359, "ttnn-round-bw"]], "ttnn.rpow": [[360, "ttnn-rpow"]], "ttnn.rpow_bw": [[361, "ttnn-rpow-bw"]], "ttnn.rsqrt": [[362, "ttnn-rsqrt"]], "ttnn.rsqrt_bw": [[363, "ttnn-rsqrt-bw"]], "ttnn.rsub": [[364, "ttnn-rsub"]], "ttnn.rsub_": [[365, "ttnn-rsub"]], "ttnn.rsub_bw": [[366, "ttnn-rsub-bw"]], "ttnn.sampling": [[367, "ttnn-sampling"]], "input_values_tensor": [[367, "id2"]], "input_indices_tensor": [[367, "id3"]], "k": [[367, "id4"]], "p, temp": [[367, "id5"]], "output_tensor (default)": [[367, "id6"]], "output_tensor (if provided)": [[367, "id7"]], "ttnn.scale_causal_mask_hw_dims_softmax_in_place": [[368, "ttnn-scale-causal-mask-hw-dims-softmax-in-place"]], "Input Tensor (Sharded)": [[368, "id2"]], "Mask Tensor [1, 1, H, W]": [[368, "id3"]], "ttnn.scale_mask_softmax": [[369, "ttnn-scale-mask-softmax"]], "Mask Tensor (optional)": [[369, "id3"], [370, "id3"]], "ttnn.scale_mask_softmax_in_place": [[370, "ttnn-scale-mask-softmax-in-place"]], "ttnn.scatter": [[371, "ttnn-scatter"]], "ttnn.scatter_add": [[372, "ttnn-scatter-add"]], "ttnn.selu": [[373, "ttnn-selu"]], "ttnn.selu_bw": [[374, "ttnn-selu-bw"]], "ttnn.set_printoptions": [[375, "ttnn-set-printoptions"]], "Examples": [[375, null], [469, null]], "ttnn.sharded_to_interleaved": [[376, "ttnn-sharded-to-interleaved"]], "ttnn.sharded_to_interleaved_partial": [[377, "ttnn-sharded-to-interleaved-partial"]], "ttnn.sigmoid": [[378, "ttnn-sigmoid"]], "ttnn.sigmoid_accurate": [[379, "ttnn-sigmoid-accurate"]], "ttnn.sigmoid_bw": [[380, "ttnn-sigmoid-bw"]], "ttnn.sign": [[381, "ttnn-sign"]], "ttnn.sign_bw": [[382, "ttnn-sign-bw"]], "ttnn.signbit": [[383, "ttnn-signbit"]], "ttnn.silu": [[384, "ttnn-silu"]], "ttnn.silu_bw": [[385, "ttnn-silu-bw"]], "ttnn.sin": [[386, "ttnn-sin"]], "ttnn.sin_bw": [[387, "ttnn-sin-bw"]], "ttnn.sinh": [[388, "ttnn-sinh"]], "ttnn.sinh_bw": [[389, "ttnn-sinh-bw"]], "ttnn.slice": [[390, "ttnn-slice"]], "ttnn.softmax": [[391, "ttnn-softmax"]], "ttnn.softmax_in_place": [[392, "ttnn-softmax-in-place"]], "ttnn.softplus": [[393, "ttnn-softplus"]], "ttnn.softplus_bw": [[394, "ttnn-softplus-bw"]], "ttnn.softshrink": [[395, "ttnn-softshrink"]], "ttnn.softshrink_bw": [[396, "ttnn-softshrink-bw"]], "ttnn.softsign": [[397, "ttnn-softsign"]], "ttnn.softsign_bw": [[398, "ttnn-softsign-bw"]], "ttnn.sort": [[399, "ttnn-sort"]], "ttnn.sparse_matmul": [[400, "ttnn-sparse-matmul"]], "sparsity": [[400, "id4"]], "ttnn.split": [[401, "ttnn-split"]], "ttnn.split_work_to_cores": [[402, "ttnn-split-work-to-cores"]], "ttnn.sqrt": [[403, "ttnn-sqrt"]], "ttnn.sqrt_bw": [[404, "ttnn-sqrt-bw"]], "ttnn.square": [[405, "ttnn-square"]], "ttnn.square_bw": [[406, "ttnn-square-bw"]], "ttnn.squared_difference": [[407, "ttnn-squared-difference"]], "ttnn.squared_difference_": [[408, "ttnn-squared-difference"]], "ttnn.squared_difference_bw": [[409, "ttnn-squared-difference-bw"]], "ttnn.squeeze": [[410, "ttnn-squeeze"]], "ttnn.stack": [[411, "ttnn-stack"]], "ttnn.std": [[412, "ttnn-std"]], "ttnn.std_hw": [[413, "ttnn-std-hw"]], "ttnn.sub_bw": [[414, "ttnn-sub-bw"]], "ttnn.subalpha": [[415, "ttnn-subalpha"]], "ttnn.subalpha_bw": [[416, "ttnn-subalpha-bw"]], "ttnn.subtract": [[417, "ttnn-subtract"]], "ttnn.subtract_": [[418, "ttnn-subtract"]], "ttnn.sum": [[419, "ttnn-sum"]], "ttnn.swiglu": [[420, "ttnn-swiglu"]], "ttnn.swish": [[421, "ttnn-swish"]], "ttnn.synchronize_device": [[422, "ttnn-synchronize-device"]], "ttnn.tan": [[423, "ttnn-tan"]], "ttnn.tan_bw": [[424, "ttnn-tan-bw"]], "ttnn.tanh": [[425, "ttnn-tanh"]], "ttnn.tanh_bw": [[426, "ttnn-tanh-bw"]], "ttnn.tanhshrink": [[427, "ttnn-tanhshrink"]], "ttnn.tanhshrink_bw": [[428, "ttnn-tanhshrink-bw"]], "ttnn.threshold": [[429, "ttnn-threshold"]], "ttnn.threshold_bw": [[430, "ttnn-threshold-bw"]], "ttnn.tilize": [[431, "ttnn-tilize"]], "ttnn.tilize_with_val_padding": [[432, "ttnn-tilize-with-val-padding"]], "ttnn.tilize_with_zero_padding": [[433, "ttnn-tilize-with-zero-padding"]], "ttnn.to_device": [[434, "ttnn-to-device"]], "ttnn.to_dtype": [[435, "ttnn-to-dtype"]], "tensor": [[435, "id2"]], "ttnn.to_layout": [[436, "ttnn-to-layout"]], "ttnn.to_memory_config": [[437, "ttnn-to-memory-config"]], "ttnn.to_torch": [[438, "ttnn-to-torch"]], "ttnn.topk": [[439, "ttnn-topk"]], "index_tensor": [[439, "id3"]], "ttnn.transformer.attention_softmax": [[440, "ttnn-transformer-attention-softmax"]], "ttnn.transformer.attention_softmax_": [[441, "ttnn-transformer-attention-softmax"]], "ttnn.transformer.chunked_flash_mla_prefill": [[442, "ttnn-transformer-chunked-flash-mla-prefill"]], "ttnn.transformer.chunked_scaled_dot_product_attention": [[443, "ttnn-transformer-chunked-scaled-dot-product-attention"]], "ttnn.transformer.concatenate_heads": [[444, "ttnn-transformer-concatenate-heads"]], "ttnn.transformer.flash_mla_prefill": [[445, "ttnn-transformer-flash-mla-prefill"]], "ttnn.transformer.flash_multi_latent_attention_decode": [[446, "ttnn-transformer-flash-multi-latent-attention-decode"]], "ttnn.transformer.joint_scaled_dot_product_attention": [[447, "ttnn-transformer-joint-scaled-dot-product-attention"]], "ttnn.transformer.paged_flash_multi_latent_attention_decode": [[448, "ttnn-transformer-paged-flash-multi-latent-attention-decode"]], "ttnn.transformer.paged_scaled_dot_product_attention_decode": [[449, "ttnn-transformer-paged-scaled-dot-product-attention-decode"]], "ttnn.transformer.ring_distributed_scaled_dot_product_attention": [[450, "ttnn-transformer-ring-distributed-scaled-dot-product-attention"]], "ttnn.transformer.ring_joint_scaled_dot_product_attention": [[451, "ttnn-transformer-ring-joint-scaled-dot-product-attention"]], "ttnn.transformer.scaled_dot_product_attention": [[452, "ttnn-transformer-scaled-dot-product-attention"]], "ttnn.transformer.scaled_dot_product_attention_decode": [[453, "ttnn-transformer-scaled-dot-product-attention-decode"]], "ttnn.transformer.split_query_key_value_and_split_heads": [[454, "ttnn-transformer-split-query-key-value-and-split-heads"]], "ttnn.transformer.windowed_scaled_dot_product_attention": [[455, "ttnn-transformer-windowed-scaled-dot-product-attention"]], "ttnn.transpose": [[456, "ttnn-transpose"]], "ttnn.tril": [[457, "ttnn-tril"]], "ttnn.triu": [[458, "ttnn-triu"]], "ttnn.trunc": [[459, "ttnn-trunc"]], "ttnn.trunc_bw": [[460, "ttnn-trunc-bw"]], "ttnn.typecast": [[461, "ttnn-typecast"]], "ttnn.unary_chain": [[462, "ttnn-unary-chain"]], "ttnn.uniform": [[463, "ttnn-uniform"]], "ttnn.unsqueeze": [[464, "ttnn-unsqueeze"]], "ttnn.unsqueeze_to_4D": [[465, "ttnn-unsqueeze-to-4d"]], "ttnn.untilize": [[466, "ttnn-untilize"]], "ttnn.untilize_with_unpadding": [[467, "ttnn-untilize-with-unpadding"]], "ttnn.update_cache": [[468, "ttnn-update-cache"]], "ttnn.upsample": [[469, "ttnn-upsample"]], "ttnn.var": [[470, "ttnn-var"]], "ttnn.var_hw": [[471, "ttnn-var-hw"]], "ttnn.view": [[472, "ttnn-view"]], "ttnn.where": [[473, "ttnn-where"]], "ttnn.where_bw": [[474, "ttnn-where-bw"]], "ttnn.xlogy": [[475, "ttnn-xlogy"]], "ttnn.xlogy_bw": [[476, "ttnn-xlogy-bw"]], "ttnn.zeros": [[477, "ttnn-zeros"]], "ttnn.zeros_like": [[478, "ttnn-zeros-like"]], "Converting PyTorch Model to TT-NN": [[479, "converting-pytorch-model-to-tt-nn"]], "Step 1 - Rewriting the Model": [[479, "step-1-rewriting-the-model"]], "Step 2 - Switching to ttnn Operations": [[479, "step-2-switching-to-ttnn-operations"]], "Step 3 - Optimizing the Model": [[479, "step-3-optimizing-the-model"]], "More examples": [[479, "more-examples"]], "Building and Uplifting Demos": [[480, "building-and-uplifting-demos"]], "Getting Started": [[481, "getting-started"]], "1. Install and Build": [[481, "install-and-build"]], "2. Explore Our Model Demos": [[481, "explore-our-model-demos"]], "Where To Go From Here": [[481, "where-to-go-from-here"]], "Install": [[482, "install"]], "Prerequisites:": [[482, "prerequisites"]], "1: Set Up the Hardware": [[482, "set-up-the-hardware"]], "2: Install Software Dependencies": [[482, "install-software-dependencies"]], "Option 1: TT-Installer Script (recommended)": [[482, "option-1-tt-installer-script-recommended"]], "Option 2: Manual Installation": [[482, "option-2-manual-installation"]], "TT-NN / TT-Metalium Installation": [[482, "tt-nn-tt-metalium-installation"]], "There are four options for installing TT-Metalium:": [[482, "there-are-four-options-for-installing-tt-metalium"]], "Binaries": [[482, "binaries"]], "Step 1. Install the Latest Wheel:": [[482, "step-1-install-the-latest-wheel"]], "Step 2. (For models users only) Set Up Environment for Models:": [[482, "step-2-for-models-users-only-set-up-environment-for-models"]], "Docker Release Image": [[482, "docker-release-image"]], "Source": [[482, "source"]], "Step 1. Clone the Repository:": [[482, "step-1-clone-the-repository"]], "Step 2. Build the Library:": [[482, "step-2-build-the-library"]], "Step 3. Virtual Environment Setup": [[482, "step-3-virtual-environment-setup"]], "Anaconda": [[482, "anaconda"]], "Step 1. Install the Latest Package:": [[482, "step-1-install-the-latest-package"]], "You are All Set!": [[482, "you-are-all-set"]], "To verify your installation (for source or wheel installation only), try executing a programming example:": [[482, "to-verify-your-installation-for-source-or-wheel-installation-only-try-executing-a-programming-example"]], "Interested in Contributing?": [[482, "interested-in-contributing"]], "Multi-Card Configuration (TT-Topology)": [[482, "multi-card-configuration-tt-topology"]], "Virtual Machine Requirements": [[482, "virtual-machine-requirements"]], "Overview": [[482, "overview"]], "Why It Matters": [[482, "why-it-matters"]], "Requirements for VMs": [[482, "requirements-for-vms"]], "Onboarding New Functionality": [[483, "onboarding-new-functionality"]], "Profiling TT-NN Operations": [[484, "profiling-tt-nn-operations"]], "Perf Report Headers": [[484, "perf-report-headers"]], "profile_this description": [[484, "profile-this-description"]], "Using the Performance Report with TT-NN Visualizer": [[484, "using-the-performance-report-with-tt-nn-visualizer"]], "Tensor": [[485, "tensor"]], "Shape": [[485, "shape"]], "Layout": [[485, "layout"]], "Data Type": [[485, "data-type"]], "Required Width Multiples for Data Types": [[485, "id5"]], "Limitation of BFLOAT8_B": [[485, "limitation-of-bfloat8-b"]], "Storage": [[485, "storage"]], "Tensor Sharding": [[485, "tensor-sharding"]], "Tutorials": [[486, "tutorials"]], "Add Tensors": [[487, "Add-Tensors"]], "Import Libraries": [[487, "Import-Libraries"], [488, "Import-Libraries"], [489, "Import-Libraries"], [490, "Import-Libraries"], [493, "Import-Libraries"], [496, "Import-Libraries"]], "Open Tenstorrent device": [[487, "Open-Tenstorrent-device"]], "Addition Operation and Conversion": [[487, "Addition-Operation-and-Conversion"]], "Close the Device": [[487, "Close-the-Device"], [488, "Close-the-Device"], [490, "Close-the-Device"], [493, "Close-the-Device"], [495, "Close-the-Device"]], "Full Example and Output": [[487, "Full-Example-and-Output"], [488, "Full-Example-and-Output"], [489, "Full-Example-and-Output"], [490, "Full-Example-and-Output"], [493, "Full-Example-and-Output"], [494, "Full-Example-and-Output"], [495, "Full-Example-and-Output"]], "Basic Convolution": [[488, "Basic-Convolution"]], "Set Manual Seed": [[488, "Set-Manual-Seed"]], "Open the Device": [[488, "Open-the-Device"], [489, "Open-the-Device"], [490, "Open-the-Device"], [493, "Open-the-Device"], [495, "Open-the-Device"]], "Create Forward Method": [[488, "Create-Forward-Method"]], "Set Input and Convolution Parameters": [[488, "Set-Input-and-Convolution-Parameters"]], "Create Tensors": [[488, "Create-Tensors"]], "Run the Convolution Operation": [[488, "Run-the-Convolution-Operation"]], "Tensor Configuration": [[489, "Tensor-Configuration"]], "Initialize tensors a and b with random values": [[489, "Initialize-tensors-a-and-b-with-random-values"]], "Matrix multiply tensor a and b": [[489, "Matrix-multiply-tensor-a-and-b"]], "Inspect the layout of matrix multiplication output": [[489, "Inspect-the-layout-of-matrix-multiplication-output"]], "Inspect the result of the matrix multiplication": [[489, "Inspect-the-result-of-the-matrix-multiplication"]], "Matrix multiply tensor a and b by using more performant config": [[489, "Matrix-multiply-tensor-a-and-b-by-using-more-performant-config"]], "Close the device": [[489, "Close-the-device"], [496, "Close-the-device"]], "Basic Tensor Operations": [[490, "Basic-Tensor-Operations"]], "Host Tensor Creation": [[490, "Host-Tensor-Creation"]], "Host Tensor Conversion and Creation": [[490, "Host-Tensor-Conversion-and-Creation"]], "Tile-Based Arithmetic Operations": [[490, "Tile-Based-Arithmetic-Operations"]], "Simulated Broadcasting - Row Vector Expansion": [[490, "Simulated-Broadcasting---Row-Vector-Expansion"]], "Building CLIP Model for Zero-Shot Image Classification with TT-NN": [[491, "Building-CLIP-Model-for-Zero-Shot-Image-Classification-with-TT-NN"]], "What CLIP Does": [[491, "What-CLIP-Does"]], "Imports and Dependencies": [[491, "Imports-and-Dependencies"]], "TT-NN Device Management and Utility Functions": [[491, "TT-NN-Device-Management-and-Utility-Functions"]], "Model Weight Conversion": [[491, "Model-Weight-Conversion"]], "Generic Transformer Implementation": [[491, "Generic-Transformer-Implementation"]], "Transformer Architecture": [[491, "Transformer-Architecture"]], "Vision Transformer Implementation": [[491, "Vision-Transformer-Implementation"]], "Vision Processing Pipeline": [[491, "Vision-Processing-Pipeline"]], "Complete CLIP Model Implementation": [[491, "Complete-CLIP-Model-Implementation"]], "CLIP Architecture Components": [[491, "CLIP-Architecture-Components"]], "Key Methods": [[491, "Key-Methods"]], "Image Preprocessing": [[491, "Image-Preprocessing"]], "Preprocessing Pipeline": [[491, "Preprocessing-Pipeline"]], "Image Download Utility": [[491, "Image-Download-Utility"]], "Model and Tokenizer downloading": [[491, "Model-and-Tokenizer-downloading"]], "Running CLIP Inference": [[491, "Running-CLIP-Inference"]], "Inference Pipeline:": [[491, "Inference-Pipeline:"]], "Text Tokenization": [[491, "Text-Tokenization"]], "TT-NN Introduction": [[492, "TT-NN-Introduction"]], "What You\u2019ll Learn": [[492, "What-You'll-Learn"]], "1. Getting Started": [[492, "1.-Getting-Started"]], "Importing the Library": [[492, "Importing-the-Library"]], "Device Initialization": [[492, "Device-Initialization"]], "2. Tensor Creation and Management": [[492, "2.-Tensor-Creation-and-Management"]], "Creating Host Tensors": [[492, "Creating-Host-Tensors"]], "Moving Tensors to Device": [[492, "Moving-Tensors-to-Device"]], "Creating Tensors Directly on Device": [[492, "Creating-Tensors-Directly-on-Device"]], "3. PyTorch Interoperability": [[492, "3.-PyTorch-Interoperability"]], "Moving Tensors Back to Host": [[492, "Moving-Tensors-Back-to-Host"]], "Converting Back to PyTorch": [[492, "Converting-Back-to-PyTorch"]], "4. Understanding Tensor Layouts": [[492, "4.-Understanding-Tensor-Layouts"]], "Layout Types": [[492, "Layout-Types"]], "Why Tile Layout Matters": [[492, "Why-Tile-Layout-Matters"]], "Default Behavior": [[492, "Default-Behavior"]], "Functions with Different Default Layouts": [[492, "Functions-with-Different-Default-Layouts"]], "Layout Preservation During Transfer": [[492, "Layout-Preservation-During-Transfer"]], "Converting Between Layouts": [[492, "Converting-Between-Layouts"]], "5. Data Types and Precision": [[492, "5.-Data-Types-and-Precision"]], "Supported Data Types": [[492, "Supported-Data-Types"]], "Performance vs. Accuracy Trade-offs": [[492, "Performance-vs.-Accuracy-Trade-offs"]], "6. Basic Tensor Operations": [[492, "6.-Basic-Tensor-Operations"]], "Important Operation Requirements": [[492, "Important-Operation-Requirements"]], "Creating Test Data": [[492, "Creating-Test-Data"]], "Neural Network Operations": [[492, "Neural-Network-Operations"]], "7. Just-In-Time Compilation and Caching": [[492, "7.-Just-In-Time-Compilation-and-Caching"]], "First Run vs. Subsequent Runs": [[492, "First-Run-vs.-Subsequent-Runs"]], "What Affects Compilation?": [[492, "What-Affects-Compilation?"]], "Direct SRAM (L1) control": [[492, "Direct-SRAM-(L1)-control"]], "Advanced: Tensor Sharding": [[492, "Advanced:-Tensor-Sharding"]], "Preserving Intermediate Results in L1": [[492, "Preserving-Intermediate-Results-in-L1"]], "Inference Focus": [[492, "Inference-Focus"]], "Development Tools": [[492, "Development-Tools"]], "Advanced Topics": [[492, "Advanced-Topics"]], "8. Exercise: Implement Scaled Dot-Product Attention": [[492, "8.-Exercise:-Implement-Scaled-Dot-Product-Attention"]], "Background": [[492, "Background"]], "Your Task": [[492, "Your-Task"]], "Math Fidelity Control": [[492, "Math-Fidelity-Control"]], "Metal Trace": [[492, "Metal-Trace"]], "Multi-device": [[492, "Multi-device"]], "MLP Inference": [[493, "MLP-Inference"]], "Load MNIST Test Data": [[493, "Load-MNIST-Test-Data"]], "Load Pretrained MLP Weights": [[493, "Load-Pretrained-MLP-Weights"]], "Accuracy Tracking, Inference, Loop, and Image Flattening": [[493, "Accuracy-Tracking,-Inference,-Loop,-and-Image-Flattening"]], "Multi-Head Attention": [[494, "Multi-Head-Attention"]], "Write Multi-Head Attention with TT-NN": [[494, "Write-Multi-Head-Attention-with-TT-NN"]], "Configuration": [[494, "Configuration"]], "Running a Simple CNN Inference on CIFAR-10": [[495, "Running-a-Simple-CNN-Inference-on-CIFAR-10"]], "Setup and Imports": [[495, "Setup-and-Imports"]], "Load the CIFAR-10 Dataset": [[495, "Load-the-CIFAR-10-Dataset"]], "Load or Initialize Weights": [[495, "Load-or-Initialize-Weights"]], "Define Convolution and Pooling Stage": [[495, "Define-Convolution-and-Pooling-Stage"]], "Run Inference on Test Samples": [[495, "Run-Inference-on-Test-Samples"]], "TT-NN Tracer and BERT Model Visualization Tutorial": [[496, "TT-NN-Tracer-and-BERT-Model-Visualization-Tutorial"]], "Set program config": [[496, "Set-program-config"]], "Example 1: Tracing PyTorch Operations": [[496, "Example-1:-Tracing-PyTorch-Operations"]], "Example 2: Tracing TT-NN Tensor Operations": [[496, "Example-2:-Tracing-TT-NN-Tensor-Operations"]], "Model and Config downloading": [[496, "Model-and-Config-downloading"]], "Example 3: Tracing a BERT Layer": [[496, "Example-3:-Tracing-a-BERT-Layer"]], "Example 4: Trace models written using ttnn": [[496, "Example-4:-Trace-models-written-using-ttnn"]], "TT-NN Visualizer": [[497, "tt-nn-visualizer"]], "Prerequisites": [[497, "prerequisites"]], "Running TT-NN Visualizer": [[497, "running-tt-nn-visualizer"]], "Model Profiling": [[497, "model-profiling"]], "Generating the Memory Report": [[497, "generating-the-memory-report"]], "Generating Performance Reports": [[497, "generating-performance-reports"]], "Result Analysis": [[497, "result-analysis"]], "Uploading Reports": [[497, "uploading-reports"]], "Operations Tab": [[497, "operations-tab"]], "Tensors Tab": [[497, "tensors-tab"]], "Buffers Tab": [[497, "buffers-tab"]], "Graph Tab": [[497, "graph-tab"]], "Performance Tab": [[497, "performance-tab"]], "Recap": [[497, "recap"]]}, "indexentries": {"conv2dconfig (class in ttnn)": [[7, "ttnn.Conv2dConfig"]], "act_block_h_override (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.act_block_h_override"]], "act_block_w_div (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.act_block_w_div"]], "activation (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.activation"]], "config_tensors_in_dram (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.config_tensors_in_dram"]], "core_grid (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.core_grid"]], "deallocate_activation (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.deallocate_activation"]], "enable_act_double_buffer (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.enable_act_double_buffer"]], "enable_activation_reuse (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.enable_activation_reuse"]], "enable_kernel_stride_folding (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.enable_kernel_stride_folding"]], "enable_weights_double_buffer (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.enable_weights_double_buffer"]], "force_split_reader (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.force_split_reader"]], "full_inner_dim (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.full_inner_dim"]], "output_layout (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.output_layout"]], "override_output_sharding_config (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.override_output_sharding_config"]], "override_sharding_config (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.override_sharding_config"]], "reallocate_halo_output (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.reallocate_halo_output"]], "reshard_if_not_optimal (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.reshard_if_not_optimal"]], "shard_layout (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.shard_layout"]], "transpose_shards (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.transpose_shards"]], "weights_dtype (ttnn.conv2dconfig property)": [[7, "ttnn.Conv2dConfig.weights_dtype"]], "conv2dsliceconfig (in module ttnn)": [[8, "ttnn.Conv2dSliceConfig"]], "getdefaultdevice (in module ttnn)": [[9, "ttnn.GetDefaultDevice"]], "matmulmulticorereusemulticast1dprogramconfig (class in ttnn)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig"]], "compute_with_storage_grid_size (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.compute_with_storage_grid_size"]], "from_json (ttnn.matmulmulticorereusemulticast1dprogramconfig attribute)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.from_json"]], "fuse_batch (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.fuse_batch"]], "fused_activation (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.fused_activation"]], "gather_in0 (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.gather_in0"]], "hop_cores (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.hop_cores"]], "in0_block_w (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.in0_block_w"]], "mcast_in0 (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.mcast_in0"]], "num_global_cb_receivers (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.num_global_cb_receivers"]], "out_block_h (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.out_block_h"]], "out_block_w (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.out_block_w"]], "out_subblock_h (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.out_subblock_h"]], "out_subblock_w (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.out_subblock_w"]], "per_core_m (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.per_core_M"]], "per_core_n (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.per_core_N"]], "to_json (ttnn.matmulmulticorereusemulticast1dprogramconfig attribute)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.to_json"]], "untilize_out (ttnn.matmulmulticorereusemulticast1dprogramconfig property)": [[10, "ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig.untilize_out"]], "matmulmulticorereusemulticastbatcheddramshardedprogramconfig (class in ttnn)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastBatchedDRAMShardedProgramConfig"]], "from_json (ttnn.matmulmulticorereusemulticastbatcheddramshardedprogramconfig attribute)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastBatchedDRAMShardedProgramConfig.from_json"]], "fused_activation (ttnn.matmulmulticorereusemulticastbatcheddramshardedprogramconfig property)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastBatchedDRAMShardedProgramConfig.fused_activation"]], "in0_block_w (ttnn.matmulmulticorereusemulticastbatcheddramshardedprogramconfig property)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastBatchedDRAMShardedProgramConfig.in0_block_w"]], "per_core_m (ttnn.matmulmulticorereusemulticastbatcheddramshardedprogramconfig property)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastBatchedDRAMShardedProgramConfig.per_core_M"]], "per_core_n (ttnn.matmulmulticorereusemulticastbatcheddramshardedprogramconfig property)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastBatchedDRAMShardedProgramConfig.per_core_N"]], "to_json (ttnn.matmulmulticorereusemulticastbatcheddramshardedprogramconfig attribute)": [[11, "ttnn.MatmulMultiCoreReuseMultiCastBatchedDRAMShardedProgramConfig.to_json"]], "matmulmulticorereusemulticastdramshardedprogramconfig (class in ttnn)": [[12, "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig"]], "from_json (ttnn.matmulmulticorereusemulticastdramshardedprogramconfig attribute)": [[12, "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig.from_json"]], "fused_activation (ttnn.matmulmulticorereusemulticastdramshardedprogramconfig property)": [[12, "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig.fused_activation"]], "in0_block_w (ttnn.matmulmulticorereusemulticastdramshardedprogramconfig property)": [[12, "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig.in0_block_w"]], "per_core_m (ttnn.matmulmulticorereusemulticastdramshardedprogramconfig property)": [[12, "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig.per_core_M"]], "per_core_n (ttnn.matmulmulticorereusemulticastdramshardedprogramconfig property)": [[12, "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig.per_core_N"]], "to_json (ttnn.matmulmulticorereusemulticastdramshardedprogramconfig attribute)": [[12, "ttnn.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig.to_json"]], "matmulmulticorereusemulticastprogramconfig (class in ttnn)": [[13, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig"]], "compute_with_storage_grid_size (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[13, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.compute_with_storage_grid_size"]], "from_json (ttnn.matmulmulticorereusemulticastprogramconfig attribute)": [[13, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.from_json"]], "fuse_batch (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[13, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.fuse_batch"]], "fused_activation (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[13, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.fused_activation"]], "in0_block_w (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[13, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.in0_block_w"]], "out_block_h (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[13, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.out_block_h"]], "out_block_w (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[13, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.out_block_w"]], "out_subblock_h (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[13, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.out_subblock_h"]], "out_subblock_w (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[13, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.out_subblock_w"]], "per_core_m (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[13, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.per_core_M"]], "per_core_n (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[13, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.per_core_N"]], "to_json (ttnn.matmulmulticorereusemulticastprogramconfig attribute)": [[13, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.to_json"]], "transpose_mcast (ttnn.matmulmulticorereusemulticastprogramconfig property)": [[13, "ttnn.MatmulMultiCoreReuseMultiCastProgramConfig.transpose_mcast"]], "matmulmulticorereuseprogramconfig (class in ttnn)": [[14, "ttnn.MatmulMultiCoreReuseProgramConfig"]], "compute_with_storage_grid_size (ttnn.matmulmulticorereuseprogramconfig property)": [[14, "ttnn.MatmulMultiCoreReuseProgramConfig.compute_with_storage_grid_size"]], "from_json (ttnn.matmulmulticorereuseprogramconfig attribute)": [[14, "ttnn.MatmulMultiCoreReuseProgramConfig.from_json"]], "in0_block_w (ttnn.matmulmulticorereuseprogramconfig property)": [[14, "ttnn.MatmulMultiCoreReuseProgramConfig.in0_block_w"]], "out_subblock_h (ttnn.matmulmulticorereuseprogramconfig property)": [[14, "ttnn.MatmulMultiCoreReuseProgramConfig.out_subblock_h"]], "out_subblock_w (ttnn.matmulmulticorereuseprogramconfig property)": [[14, "ttnn.MatmulMultiCoreReuseProgramConfig.out_subblock_w"]], "per_core_m (ttnn.matmulmulticorereuseprogramconfig property)": [[14, "ttnn.MatmulMultiCoreReuseProgramConfig.per_core_M"]], "per_core_n (ttnn.matmulmulticorereuseprogramconfig property)": [[14, "ttnn.MatmulMultiCoreReuseProgramConfig.per_core_N"]], "to_json (ttnn.matmulmulticorereuseprogramconfig attribute)": [[14, "ttnn.MatmulMultiCoreReuseProgramConfig.to_json"]], "setdefaultdevice (in module ttnn)": [[15, "ttnn.SetDefaultDevice"]], "softmaxdefaultprogramconfig (class in ttnn)": [[16, "ttnn.SoftmaxDefaultProgramConfig"]], "softmaxprogramconfig (class in ttnn)": [[17, "ttnn.SoftmaxProgramConfig"]], "softmaxshardedmulticoreprogramconfig (class in ttnn)": [[18, "ttnn.SoftmaxShardedMultiCoreProgramConfig"]], "block_h (ttnn.softmaxshardedmulticoreprogramconfig property)": [[18, "ttnn.SoftmaxShardedMultiCoreProgramConfig.block_h"]], "block_w (ttnn.softmaxshardedmulticoreprogramconfig property)": [[18, "ttnn.SoftmaxShardedMultiCoreProgramConfig.block_w"]], "compute_with_storage_grid_size (ttnn.softmaxshardedmulticoreprogramconfig property)": [[18, "ttnn.SoftmaxShardedMultiCoreProgramConfig.compute_with_storage_grid_size"]], "subblock_w (ttnn.softmaxshardedmulticoreprogramconfig property)": [[18, "ttnn.SoftmaxShardedMultiCoreProgramConfig.subblock_w"]], "abs() (in module ttnn)": [[19, "ttnn.abs"]], "abs_bw() (in module ttnn)": [[20, "ttnn.abs_bw"]], "acos() (in module ttnn)": [[21, "ttnn.acos"]], "acos_bw() (in module ttnn)": [[22, "ttnn.acos_bw"]], "acosh() (in module ttnn)": [[23, "ttnn.acosh"]], "acosh_bw() (in module ttnn)": [[24, "ttnn.acosh_bw"]], "adaptive_avg_pool2d() (in module ttnn)": [[25, "ttnn.adaptive_avg_pool2d"]], "adaptive_max_pool2d() (in module ttnn)": [[26, "ttnn.adaptive_max_pool2d"]], "add() (in module ttnn)": [[27, "ttnn.add"]], "add_() (in module ttnn)": [[28, "ttnn.add_"]], "add_bw() (in module ttnn)": [[29, "ttnn.add_bw"]], "addalpha() (in module ttnn)": [[30, "ttnn.addalpha"]], "addalpha_bw() (in module ttnn)": [[31, "ttnn.addalpha_bw"]], "addcdiv() (in module ttnn)": [[32, "ttnn.addcdiv"]], "addcdiv_bw() (in module ttnn)": [[33, "ttnn.addcdiv_bw"]], "addcmul() (in module ttnn)": [[34, "ttnn.addcmul"]], "addcmul_bw() (in module ttnn)": [[35, "ttnn.addcmul_bw"]], "addmm() (in module ttnn)": [[36, "ttnn.addmm"]], "all_broadcast() (in module ttnn)": [[37, "ttnn.all_broadcast"]], "all_gather() (in module ttnn)": [[38, "ttnn.all_gather"]], "all_reduce() (in module ttnn)": [[39, "ttnn.all_reduce"]], "all_to_all_combine() (in module ttnn)": [[40, "ttnn.all_to_all_combine"]], "all_to_all_dispatch() (in module ttnn)": [[41, "ttnn.all_to_all_dispatch"]], "alt_complex_rotate90() (in module ttnn)": [[42, "ttnn.alt_complex_rotate90"]], "angle() (in module ttnn)": [[43, "ttnn.angle"]], "angle_bw() (in module ttnn)": [[44, "ttnn.angle_bw"]], "arange() (in module ttnn)": [[45, "ttnn.arange"]], "argmax() (in module ttnn)": [[46, "ttnn.argmax"]], "as_tensor() (in module ttnn)": [[47, "ttnn.as_tensor"]], "asin() (in module ttnn)": [[48, "ttnn.asin"]], "asin_bw() (in module ttnn)": [[49, "ttnn.asin_bw"]], "asinh() (in module ttnn)": [[50, "ttnn.asinh"]], "asinh_bw() (in module ttnn)": [[51, "ttnn.asinh_bw"]], "assign() (in module ttnn)": [[52, "ttnn.assign"]], "assign_bw() (in module ttnn)": [[53, "ttnn.assign_bw"]], "atan() (in module ttnn)": [[54, "ttnn.atan"]], "atan2() (in module ttnn)": [[55, "ttnn.atan2"]], "atan2_bw() (in module ttnn)": [[56, "ttnn.atan2_bw"]], "atan_bw() (in module ttnn)": [[57, "ttnn.atan_bw"]], "atanh() (in module ttnn)": [[58, "ttnn.atanh"]], "atanh_bw() (in module ttnn)": [[59, "ttnn.atanh_bw"]], "avg_pool2d() (in module ttnn)": [[60, "ttnn.avg_pool2d"]], "batch_norm() (in module ttnn)": [[61, "ttnn.batch_norm"]], "bcast() (in module ttnn)": [[62, "ttnn.bcast"]], "bernoulli() (in module ttnn)": [[63, "ttnn.bernoulli"]], "bias_gelu() (in module ttnn)": [[64, "ttnn.bias_gelu"]], "bias_gelu_() (in module ttnn)": [[65, "ttnn.bias_gelu_"]], "bias_gelu_bw() (in module ttnn)": [[66, "ttnn.bias_gelu_bw"]], "bitcast() (in module ttnn)": [[67, "ttnn.bitcast"]], "bitwise_and() (in module ttnn)": [[68, "ttnn.bitwise_and"]], "bitwise_left_shift() (in module ttnn)": [[69, "ttnn.bitwise_left_shift"]], "bitwise_not() (in module ttnn)": [[70, "ttnn.bitwise_not"]], "bitwise_or() (in module ttnn)": [[71, "ttnn.bitwise_or"]], "bitwise_right_shift() (in module ttnn)": [[72, "ttnn.bitwise_right_shift"]], "bitwise_xor() (in module ttnn)": [[73, "ttnn.bitwise_xor"]], "broadcast() (in module ttnn)": [[74, "ttnn.broadcast"]], "cbrt() (in module ttnn)": [[75, "ttnn.cbrt"]], "ceil() (in module ttnn)": [[76, "ttnn.ceil"]], "ceil_bw() (in module ttnn)": [[77, "ttnn.ceil_bw"]], "celu() (in module ttnn)": [[78, "ttnn.celu"]], "celu_bw() (in module ttnn)": [[79, "ttnn.celu_bw"]], "chunk() (in module ttnn)": [[80, "ttnn.chunk"]], "clamp() (in module ttnn)": [[81, "ttnn.clamp"]], "clamp_bw() (in module ttnn)": [[82, "ttnn.clamp_bw"]], "clip() (in module ttnn)": [[83, "ttnn.clip"]], "clip_bw() (in module ttnn)": [[84, "ttnn.clip_bw"]], "clone() (in module ttnn)": [[85, "ttnn.clone"]], "close_device() (in module ttnn)": [[86, "ttnn.close_device"]], "complex_tensor() (in module ttnn)": [[87, "ttnn.complex_tensor"]], "concat() (in module ttnn)": [[88, "ttnn.concat"]], "concat_bw() (in module ttnn)": [[89, "ttnn.concat_bw"]], "conj() (in module ttnn)": [[90, "ttnn.conj"]], "conj_bw() (in module ttnn)": [[91, "ttnn.conj_bw"]], "conv1d() (in module ttnn)": [[92, "ttnn.conv1d"]], "conv2d() (in module ttnn)": [[93, "ttnn.conv2d"]], "conv_transpose2d() (in module ttnn)": [[94, "ttnn.conv_transpose2d"]], "copy() (in module ttnn)": [[95, "ttnn.copy"]], "copy_device_to_host_tensor() (in module ttnn)": [[96, "ttnn.copy_device_to_host_tensor"]], "copy_host_to_device_tensor() (in module ttnn)": [[97, "ttnn.copy_host_to_device_tensor"]], "cos() (in module ttnn)": [[98, "ttnn.cos"]], "cos_bw() (in module ttnn)": [[99, "ttnn.cos_bw"]], "cosh() (in module ttnn)": [[100, "ttnn.cosh"]], "cosh_bw() (in module ttnn)": [[101, "ttnn.cosh_bw"]], "create_sharded_memory_config() (in module ttnn)": [[102, "ttnn.create_sharded_memory_config"]], "cumprod() (in module ttnn)": [[103, "ttnn.cumprod"]], "cumsum() (in module ttnn)": [[104, "ttnn.cumsum"]], "deallocate() (in module ttnn)": [[105, "ttnn.deallocate"]], "deg2rad() (in module ttnn)": [[106, "ttnn.deg2rad"]], "deg2rad_bw() (in module ttnn)": [[107, "ttnn.deg2rad_bw"]], "dequantize() (in module ttnn)": [[108, "ttnn.dequantize"]], "digamma() (in module ttnn)": [[109, "ttnn.digamma"]], "digamma_bw() (in module ttnn)": [[110, "ttnn.digamma_bw"]], "div() (in module ttnn)": [[111, "ttnn.div"]], "div_bw() (in module ttnn)": [[112, "ttnn.div_bw"]], "div_no_nan() (in module ttnn)": [[113, "ttnn.div_no_nan"]], "div_no_nan_bw() (in module ttnn)": [[114, "ttnn.div_no_nan_bw"]], "divide() (in module ttnn)": [[115, "ttnn.divide"]], "divide_() (in module ttnn)": [[116, "ttnn.divide_"]], "dram_prefetcher() (in module ttnn)": [[117, "ttnn.dram_prefetcher"]], "dump_tensor() (in module ttnn)": [[118, "ttnn.dump_tensor"]], "elu() (in module ttnn)": [[119, "ttnn.elu"]], "elu_bw() (in module ttnn)": [[120, "ttnn.elu_bw"]], "ema() (in module ttnn)": [[121, "ttnn.ema"]], "embedding() (in module ttnn)": [[122, "ttnn.embedding"]], "embedding_bw() (in module ttnn)": [[123, "ttnn.embedding_bw"]], "empty() (in module ttnn)": [[124, "ttnn.empty"]], "empty_like() (in module ttnn)": [[125, "ttnn.empty_like"]], "eq() (in module ttnn)": [[126, "ttnn.eq"]], "eq_() (in module ttnn)": [[127, "ttnn.eq_"]], "eqz() (in module ttnn)": [[128, "ttnn.eqz"]], "erf() (in module ttnn)": [[129, "ttnn.erf"]], "erf_bw() (in module ttnn)": [[130, "ttnn.erf_bw"]], "erfc() (in module ttnn)": [[131, "ttnn.erfc"]], "erfc_bw() (in module ttnn)": [[132, "ttnn.erfc_bw"]], "erfinv() (in module ttnn)": [[133, "ttnn.erfinv"]], "erfinv_bw() (in module ttnn)": [[134, "ttnn.erfinv_bw"]], "exp() (in module ttnn)": [[135, "ttnn.exp"]], "exp2() (in module ttnn)": [[136, "ttnn.exp2"]], "exp2_bw() (in module ttnn)": [[137, "ttnn.exp2_bw"]], "exp_bw() (in module ttnn)": [[138, "ttnn.exp_bw"]], "expand() (in module ttnn)": [[139, "ttnn.expand"]], "conv3d() (in module ttnn.experimental)": [[140, "ttnn.experimental.conv3d"]], "dropout() (in module ttnn.experimental)": [[141, "ttnn.experimental.dropout"]], "gelu_bw() (in module ttnn.experimental)": [[142, "ttnn.experimental.gelu_bw"]], "expm1() (in module ttnn)": [[143, "ttnn.expm1"]], "expm1_bw() (in module ttnn)": [[144, "ttnn.expm1_bw"]], "fill() (in module ttnn)": [[145, "ttnn.fill"]], "fill_bw() (in module ttnn)": [[146, "ttnn.fill_bw"]], "fill_cache() (in module ttnn)": [[147, "ttnn.fill_cache"]], "fill_implicit_tile_padding() (in module ttnn)": [[148, "ttnn.fill_implicit_tile_padding"]], "fill_ones_rm() (in module ttnn)": [[149, "ttnn.fill_ones_rm"]], "fill_rm() (in module ttnn)": [[150, "ttnn.fill_rm"]], "fill_zero_bw() (in module ttnn)": [[151, "ttnn.fill_zero_bw"]], "floor() (in module ttnn)": [[152, "ttnn.floor"]], "floor_bw() (in module ttnn)": [[153, "ttnn.floor_bw"]], "floor_div() (in module ttnn)": [[154, "ttnn.floor_div"]], "fmod() (in module ttnn)": [[155, "ttnn.fmod"]], "fmod_bw() (in module ttnn)": [[156, "ttnn.fmod_bw"]], "fold() (in module ttnn)": [[157, "ttnn.fold"]], "frac() (in module ttnn)": [[158, "ttnn.frac"]], "frac_bw() (in module ttnn)": [[159, "ttnn.frac_bw"]], "from_buffer() (in module ttnn)": [[160, "ttnn.from_buffer"]], "from_device() (in module ttnn)": [[161, "ttnn.from_device"]], "from_torch() (in module ttnn)": [[162, "ttnn.from_torch"]], "full() (in module ttnn)": [[163, "ttnn.full"]], "full_like() (in module ttnn)": [[164, "ttnn.full_like"]], "gather() (in module ttnn)": [[165, "ttnn.gather"]], "gcd() (in module ttnn)": [[166, "ttnn.gcd"]], "ge() (in module ttnn)": [[167, "ttnn.ge"]], "ge_() (in module ttnn)": [[168, "ttnn.ge_"]], "geglu() (in module ttnn)": [[169, "ttnn.geglu"]], "gelu() (in module ttnn)": [[170, "ttnn.gelu"]], "gelu_bw() (in module ttnn)": [[171, "ttnn.gelu_bw"]], "generic_op() (in module ttnn)": [[172, "ttnn.generic_op"]], "get_device_tensors (in module ttnn)": [[173, "ttnn.get_device_tensors"]], "gez() (in module ttnn)": [[174, "ttnn.gez"]], "global_avg_pool2d() (in module ttnn)": [[175, "ttnn.global_avg_pool2d"]], "glu() (in module ttnn)": [[176, "ttnn.glu"]], "grid_sample() (in module ttnn)": [[177, "ttnn.grid_sample"]], "group_norm() (in module ttnn)": [[178, "ttnn.group_norm"]], "gt() (in module ttnn)": [[179, "ttnn.gt"]], "gt_() (in module ttnn)": [[180, "ttnn.gt_"]], "gtz() (in module ttnn)": [[181, "ttnn.gtz"]], "hardmish() (in module ttnn)": [[182, "ttnn.hardmish"]], "hardshrink() (in module ttnn)": [[183, "ttnn.hardshrink"]], "hardshrink_bw() (in module ttnn)": [[184, "ttnn.hardshrink_bw"]], "hardsigmoid() (in module ttnn)": [[185, "ttnn.hardsigmoid"]], "hardsigmoid_bw() (in module ttnn)": [[186, "ttnn.hardsigmoid_bw"]], "hardswish() (in module ttnn)": [[187, "ttnn.hardswish"]], "hardswish_bw() (in module ttnn)": [[188, "ttnn.hardswish_bw"]], "hardtanh() (in module ttnn)": [[189, "ttnn.hardtanh"]], "hardtanh_bw() (in module ttnn)": [[190, "ttnn.hardtanh_bw"]], "heaviside() (in module ttnn)": [[191, "ttnn.heaviside"]], "hypot() (in module ttnn)": [[192, "ttnn.hypot"]], "hypot_bw() (in module ttnn)": [[193, "ttnn.hypot_bw"]], "i0() (in module ttnn)": [[194, "ttnn.i0"]], "i0_bw() (in module ttnn)": [[195, "ttnn.i0_bw"]], "i1() (in module ttnn)": [[196, "ttnn.i1"]], "identity() (in module ttnn)": [[197, "ttnn.identity"]], "imag() (in module ttnn)": [[198, "ttnn.imag"]], "imag_bw() (in module ttnn)": [[199, "ttnn.imag_bw"]], "index_fill() (in module ttnn)": [[200, "ttnn.index_fill"]], "indexed_fill() (in module ttnn)": [[201, "ttnn.indexed_fill"]], "interleaved_to_sharded() (in module ttnn)": [[202, "ttnn.interleaved_to_sharded"]], "interleaved_to_sharded_partial() (in module ttnn)": [[203, "ttnn.interleaved_to_sharded_partial"]], "is_imag() (in module ttnn)": [[204, "ttnn.is_imag"]], "is_real() (in module ttnn)": [[205, "ttnn.is_real"]], "isclose() (in module ttnn)": [[206, "ttnn.isclose"]], "isfinite() (in module ttnn)": [[207, "ttnn.isfinite"]], "isinf() (in module ttnn)": [[208, "ttnn.isinf"]], "isnan() (in module ttnn)": [[209, "ttnn.isnan"]], "isneginf() (in module ttnn)": [[210, "ttnn.isneginf"]], "isposinf() (in module ttnn)": [[211, "ttnn.isposinf"]], "fill_cache_for_user_() (in module ttnn.kv_cache)": [[212, "ttnn.kv_cache.fill_cache_for_user_"]], "update_cache_for_token_() (in module ttnn.kv_cache)": [[213, "ttnn.kv_cache.update_cache_for_token_"]], "l1_loss() (in module ttnn)": [[214, "ttnn.l1_loss"]], "layer_norm() (in module ttnn)": [[215, "ttnn.layer_norm"]], "layer_norm_post_all_gather() (in module ttnn)": [[216, "ttnn.layer_norm_post_all_gather"]], "layer_norm_pre_all_gather() (in module ttnn)": [[217, "ttnn.layer_norm_pre_all_gather"]], "lcm() (in module ttnn)": [[218, "ttnn.lcm"]], "ldexp() (in module ttnn)": [[219, "ttnn.ldexp"]], "ldexp_() (in module ttnn)": [[220, "ttnn.ldexp_"]], "ldexp_bw() (in module ttnn)": [[221, "ttnn.ldexp_bw"]], "le() (in module ttnn)": [[222, "ttnn.le"]], "le_() (in module ttnn)": [[223, "ttnn.le_"]], "leaky_relu() (in module ttnn)": [[224, "ttnn.leaky_relu"]], "leaky_relu_bw() (in module ttnn)": [[225, "ttnn.leaky_relu_bw"]], "lerp() (in module ttnn)": [[226, "ttnn.lerp"]], "lerp_bw() (in module ttnn)": [[227, "ttnn.lerp_bw"]], "lez() (in module ttnn)": [[228, "ttnn.lez"]], "lgamma() (in module ttnn)": [[229, "ttnn.lgamma"]], "lgamma_bw() (in module ttnn)": [[230, "ttnn.lgamma_bw"]], "linear() (in module ttnn)": [[231, "ttnn.linear"]], "load_tensor() (in module ttnn)": [[232, "ttnn.load_tensor"]], "log() (in module ttnn)": [[233, "ttnn.log"]], "log10() (in module ttnn)": [[234, "ttnn.log10"]], "log10_bw() (in module ttnn)": [[235, "ttnn.log10_bw"]], "log1p() (in module ttnn)": [[236, "ttnn.log1p"]], "log1p_bw() (in module ttnn)": [[237, "ttnn.log1p_bw"]], "log2() (in module ttnn)": [[238, "ttnn.log2"]], "log2_bw() (in module ttnn)": [[239, "ttnn.log2_bw"]], "log_bw() (in module ttnn)": [[240, "ttnn.log_bw"]], "log_sigmoid() (in module ttnn)": [[241, "ttnn.log_sigmoid"]], "log_sigmoid_bw() (in module ttnn)": [[242, "ttnn.log_sigmoid_bw"]], "logaddexp() (in module ttnn)": [[243, "ttnn.logaddexp"]], "logaddexp2() (in module ttnn)": [[244, "ttnn.logaddexp2"]], "logaddexp2_() (in module ttnn)": [[245, "ttnn.logaddexp2_"]], "logaddexp2_bw() (in module ttnn)": [[246, "ttnn.logaddexp2_bw"]], "logaddexp_() (in module ttnn)": [[247, "ttnn.logaddexp_"]], "logaddexp_bw() (in module ttnn)": [[248, "ttnn.logaddexp_bw"]], "logical_and() (in module ttnn)": [[249, "ttnn.logical_and"]], "logical_and_() (in module ttnn)": [[250, "ttnn.logical_and_"]], "logical_left_shift() (in module ttnn)": [[251, "ttnn.logical_left_shift"]], "logical_not() (in module ttnn)": [[252, "ttnn.logical_not"]], "logical_not_() (in module ttnn)": [[253, "ttnn.logical_not_"]], "logical_or() (in module ttnn)": [[254, "ttnn.logical_or"]], "logical_or_() (in module ttnn)": [[255, "ttnn.logical_or_"]], "logical_right_shift() (in module ttnn)": [[256, "ttnn.logical_right_shift"]], "logical_xor() (in module ttnn)": [[257, "ttnn.logical_xor"]], "logical_xor_() (in module ttnn)": [[258, "ttnn.logical_xor_"]], "logit() (in module ttnn)": [[259, "ttnn.logit"]], "logit_bw() (in module ttnn)": [[260, "ttnn.logit_bw"]], "logiteps_bw() (in module ttnn)": [[261, "ttnn.logiteps_bw"]], "lt() (in module ttnn)": [[262, "ttnn.lt"]], "lt_() (in module ttnn)": [[263, "ttnn.lt_"]], "ltz() (in module ttnn)": [[264, "ttnn.ltz"]], "mac() (in module ttnn)": [[265, "ttnn.mac"]], "manage_device() (in module ttnn)": [[266, "ttnn.manage_device"]], "manual_seed() (in module ttnn)": [[267, "ttnn.manual_seed"]], "matmul() (in module ttnn)": [[268, "ttnn.matmul"]], "max() (in module ttnn)": [[269, "ttnn.max"]], "max_bw() (in module ttnn)": [[270, "ttnn.max_bw"]], "max_pool2d() (in module ttnn)": [[271, "ttnn.max_pool2d"]], "maximum() (in module ttnn)": [[272, "ttnn.maximum"]], "mean() (in module ttnn)": [[273, "ttnn.mean"]], "mesh_partition() (in module ttnn)": [[274, "ttnn.mesh_partition"]], "min() (in module ttnn)": [[275, "ttnn.min"]], "min_bw() (in module ttnn)": [[276, "ttnn.min_bw"]], "minimum() (in module ttnn)": [[277, "ttnn.minimum"]], "mish() (in module ttnn)": [[278, "ttnn.mish"]], "preprocess_model() (in module ttnn.model_preprocessing)": [[279, "ttnn.model_preprocessing.preprocess_model"]], "preprocess_model_parameters() (in module ttnn.model_preprocessing)": [[280, "ttnn.model_preprocessing.preprocess_model_parameters"]], "moe() (in module ttnn)": [[281, "ttnn.moe"]], "moe_expert_token_remap() (in module ttnn)": [[282, "ttnn.moe_expert_token_remap"]], "moe_routing_remap() (in module ttnn)": [[283, "ttnn.moe_routing_remap"]], "move() (in module ttnn)": [[284, "ttnn.move"]], "mse_loss() (in module ttnn)": [[285, "ttnn.mse_loss"]], "mul_bw() (in module ttnn)": [[286, "ttnn.mul_bw"]], "multigammaln() (in module ttnn)": [[287, "ttnn.multigammaln"]], "multigammaln_bw() (in module ttnn)": [[288, "ttnn.multigammaln_bw"]], "multiply() (in module ttnn)": [[289, "ttnn.multiply"]], "multiply_() (in module ttnn)": [[290, "ttnn.multiply_"]], "ne() (in module ttnn)": [[291, "ttnn.ne"]], "ne_() (in module ttnn)": [[292, "ttnn.ne_"]], "neg() (in module ttnn)": [[293, "ttnn.neg"]], "neg_bw() (in module ttnn)": [[294, "ttnn.neg_bw"]], "nextafter() (in module ttnn)": [[295, "ttnn.nextafter"]], "nez() (in module ttnn)": [[296, "ttnn.nez"]], "nonzero() (in module ttnn)": [[297, "ttnn.nonzero"]], "normalize_global() (in module ttnn)": [[298, "ttnn.normalize_global"]], "normalize_hw() (in module ttnn)": [[299, "ttnn.normalize_hw"]], "ones() (in module ttnn)": [[300, "ttnn.ones"]], "ones_like() (in module ttnn)": [[301, "ttnn.ones_like"]], "open_device (in module ttnn)": [[302, "ttnn.open_device"]], "outer() (in module ttnn)": [[303, "ttnn.outer"]], "pad() (in module ttnn)": [[304, "ttnn.pad"]], "pad_to_tile_shape (in module ttnn)": [[305, "ttnn.pad_to_tile_shape"]], "permute() (in module ttnn)": [[306, "ttnn.permute"]], "point_to_point() (in module ttnn)": [[307, "ttnn.point_to_point"]], "polar() (in module ttnn)": [[308, "ttnn.polar"]], "polar_bw() (in module ttnn)": [[309, "ttnn.polar_bw"]], "polygamma() (in module ttnn)": [[310, "ttnn.polygamma"]], "polygamma_bw() (in module ttnn)": [[311, "ttnn.polygamma_bw"]], "polyval() (in module ttnn)": [[312, "ttnn.polyval"]], "pow() (in module ttnn)": [[313, "ttnn.pow"]], "pow_bw() (in module ttnn)": [[314, "ttnn.pow_bw"]], "prelu() (in module ttnn)": [[315, "ttnn.prelu"]], "prepare_conv_bias() (in module ttnn)": [[316, "ttnn.prepare_conv_bias"]], "prepare_conv_transpose2d_bias() (in module ttnn)": [[317, "ttnn.prepare_conv_transpose2d_bias"]], "prepare_conv_transpose2d_weights() (in module ttnn)": [[318, "ttnn.prepare_conv_transpose2d_weights"]], "prepare_conv_weights() (in module ttnn)": [[319, "ttnn.prepare_conv_weights"]], "prod() (in module ttnn)": [[320, "ttnn.prod"]], "prod_bw() (in module ttnn)": [[321, "ttnn.prod_bw"]], "quantize() (in module ttnn)": [[322, "ttnn.quantize"]], "rad2deg() (in module ttnn)": [[323, "ttnn.rad2deg"]], "rad2deg_bw() (in module ttnn)": [[324, "ttnn.rad2deg_bw"]], "rand() (in module ttnn)": [[325, "ttnn.rand"]], "rdiv() (in module ttnn)": [[326, "ttnn.rdiv"]], "rdiv_bw() (in module ttnn)": [[327, "ttnn.rdiv_bw"]], "real() (in module ttnn)": [[328, "ttnn.real"]], "real_bw() (in module ttnn)": [[329, "ttnn.real_bw"]], "reallocate() (in module ttnn)": [[330, "ttnn.reallocate"]], "reciprocal() (in module ttnn)": [[331, "ttnn.reciprocal"]], "reciprocal_bw() (in module ttnn)": [[332, "ttnn.reciprocal_bw"]], "reduce_scatter() (in module ttnn)": [[333, "ttnn.reduce_scatter"]], "reduce_to_root() (in module ttnn)": [[334, "ttnn.reduce_to_root"]], "register_post_operation_hook() (in module ttnn)": [[335, "ttnn.register_post_operation_hook"]], "register_pre_operation_hook() (in module ttnn)": [[336, "ttnn.register_pre_operation_hook"]], "reglu() (in module ttnn)": [[337, "ttnn.reglu"]], "relu() (in module ttnn)": [[338, "ttnn.relu"]], "relu6() (in module ttnn)": [[339, "ttnn.relu6"]], "relu6_bw() (in module ttnn)": [[340, "ttnn.relu6_bw"]], "relu_bw() (in module ttnn)": [[341, "ttnn.relu_bw"]], "relu_max() (in module ttnn)": [[342, "ttnn.relu_max"]], "relu_min() (in module ttnn)": [[343, "ttnn.relu_min"]], "remainder() (in module ttnn)": [[344, "ttnn.remainder"]], "remainder_bw() (in module ttnn)": [[345, "ttnn.remainder_bw"]], "repeat() (in module ttnn)": [[346, "ttnn.repeat"]], "repeat_bw() (in module ttnn)": [[347, "ttnn.repeat_bw"]], "repeat_interleave() (in module ttnn)": [[348, "ttnn.repeat_interleave"]], "requantize() (in module ttnn)": [[349, "ttnn.requantize"]], "reshape() (in module ttnn)": [[350, "ttnn.reshape"]], "reshape_on_device() (in module ttnn)": [[351, "ttnn.reshape_on_device"]], "reshard() (in module ttnn)": [[352, "ttnn.reshard"]], "rms_norm() (in module ttnn)": [[353, "ttnn.rms_norm"]], "rms_norm_post_all_gather() (in module ttnn)": [[354, "ttnn.rms_norm_post_all_gather"]], "rms_norm_pre_all_gather() (in module ttnn)": [[355, "ttnn.rms_norm_pre_all_gather"]], "roll() (in module ttnn)": [[356, "ttnn.roll"]], "rotate() (in module ttnn)": [[357, "ttnn.rotate"]], "round() (in module ttnn)": [[358, "ttnn.round"]], "round_bw() (in module ttnn)": [[359, "ttnn.round_bw"]], "rpow() (in module ttnn)": [[360, "ttnn.rpow"]], "rpow_bw() (in module ttnn)": [[361, "ttnn.rpow_bw"]], "rsqrt() (in module ttnn)": [[362, "ttnn.rsqrt"]], "rsqrt_bw() (in module ttnn)": [[363, "ttnn.rsqrt_bw"]], "rsub() (in module ttnn)": [[364, "ttnn.rsub"]], "rsub_() (in module ttnn)": [[365, "ttnn.rsub_"]], "rsub_bw() (in module ttnn)": [[366, "ttnn.rsub_bw"]], "sampling() (in module ttnn)": [[367, "ttnn.sampling"]], "scale_causal_mask_hw_dims_softmax_in_place() (in module ttnn)": [[368, "ttnn.scale_causal_mask_hw_dims_softmax_in_place"]], "scale_mask_softmax() (in module ttnn)": [[369, "ttnn.scale_mask_softmax"]], "scale_mask_softmax_in_place() (in module ttnn)": [[370, "ttnn.scale_mask_softmax_in_place"]], "scatter() (in module ttnn)": [[371, "ttnn.scatter"]], "scatter_add() (in module ttnn)": [[372, "ttnn.scatter_add"]], "selu() (in module ttnn)": [[373, "ttnn.selu"]], "selu_bw() (in module ttnn)": [[374, "ttnn.selu_bw"]], "set_printoptions (in module ttnn)": [[375, "ttnn.set_printoptions"]], "sharded_to_interleaved() (in module ttnn)": [[376, "ttnn.sharded_to_interleaved"]], "sharded_to_interleaved_partial() (in module ttnn)": [[377, "ttnn.sharded_to_interleaved_partial"]], "sigmoid() (in module ttnn)": [[378, "ttnn.sigmoid"]], "sigmoid_accurate() (in module ttnn)": [[379, "ttnn.sigmoid_accurate"]], "sigmoid_bw() (in module ttnn)": [[380, "ttnn.sigmoid_bw"]], "sign() (in module ttnn)": [[381, "ttnn.sign"]], "sign_bw() (in module ttnn)": [[382, "ttnn.sign_bw"]], "signbit() (in module ttnn)": [[383, "ttnn.signbit"]], "silu() (in module ttnn)": [[384, "ttnn.silu"]], "silu_bw() (in module ttnn)": [[385, "ttnn.silu_bw"]], "sin() (in module ttnn)": [[386, "ttnn.sin"]], "sin_bw() (in module ttnn)": [[387, "ttnn.sin_bw"]], "sinh() (in module ttnn)": [[388, "ttnn.sinh"]], "sinh_bw() (in module ttnn)": [[389, "ttnn.sinh_bw"]], "slice() (in module ttnn)": [[390, "ttnn.slice"]], "softmax() (in module ttnn)": [[391, "ttnn.softmax"]], "softmax_in_place() (in module ttnn)": [[392, "ttnn.softmax_in_place"]], "softplus() (in module ttnn)": [[393, "ttnn.softplus"]], "softplus_bw() (in module ttnn)": [[394, "ttnn.softplus_bw"]], "softshrink() (in module ttnn)": [[395, "ttnn.softshrink"]], "softshrink_bw() (in module ttnn)": [[396, "ttnn.softshrink_bw"]], "softsign() (in module ttnn)": [[397, "ttnn.softsign"]], "softsign_bw() (in module ttnn)": [[398, "ttnn.softsign_bw"]], "sort() (in module ttnn)": [[399, "ttnn.sort"]], "sparse_matmul() (in module ttnn)": [[400, "ttnn.sparse_matmul"]], "split() (in module ttnn)": [[401, "ttnn.split"]], "split_work_to_cores (in module ttnn)": [[402, "ttnn.split_work_to_cores"]], "sqrt() (in module ttnn)": [[403, "ttnn.sqrt"]], "sqrt_bw() (in module ttnn)": [[404, "ttnn.sqrt_bw"]], "square() (in module ttnn)": [[405, "ttnn.square"]], "square_bw() (in module ttnn)": [[406, "ttnn.square_bw"]], "squared_difference() (in module ttnn)": [[407, "ttnn.squared_difference"]], "squared_difference_() (in module ttnn)": [[408, "ttnn.squared_difference_"]], "squared_difference_bw() (in module ttnn)": [[409, "ttnn.squared_difference_bw"]], "squeeze() (in module ttnn)": [[410, "ttnn.squeeze"]], "stack() (in module ttnn)": [[411, "ttnn.stack"]], "std() (in module ttnn)": [[412, "ttnn.std"]], "std_hw() (in module ttnn)": [[413, "ttnn.std_hw"]], "sub_bw() (in module ttnn)": [[414, "ttnn.sub_bw"]], "subalpha() (in module ttnn)": [[415, "ttnn.subalpha"]], "subalpha_bw() (in module ttnn)": [[416, "ttnn.subalpha_bw"]], "subtract() (in module ttnn)": [[417, "ttnn.subtract"]], "subtract_() (in module ttnn)": [[418, "ttnn.subtract_"]], "sum() (in module ttnn)": [[419, "ttnn.sum"]], "swiglu() (in module ttnn)": [[420, "ttnn.swiglu"]], "swish() (in module ttnn)": [[421, "ttnn.swish"]], "synchronize_device (in module ttnn)": [[422, "ttnn.synchronize_device"]], "tan() (in module ttnn)": [[423, "ttnn.tan"]], "tan_bw() (in module ttnn)": [[424, "ttnn.tan_bw"]], "tanh() (in module ttnn)": [[425, "ttnn.tanh"]], "tanh_bw() (in module ttnn)": [[426, "ttnn.tanh_bw"]], "tanhshrink() (in module ttnn)": [[427, "ttnn.tanhshrink"]], "tanhshrink_bw() (in module ttnn)": [[428, "ttnn.tanhshrink_bw"]], "threshold() (in module ttnn)": [[429, "ttnn.threshold"]], "threshold_bw() (in module ttnn)": [[430, "ttnn.threshold_bw"]], "tilize() (in module ttnn)": [[431, "ttnn.tilize"]], "tilize_with_val_padding() (in module ttnn)": [[432, "ttnn.tilize_with_val_padding"]], "tilize_with_zero_padding() (in module ttnn)": [[433, "ttnn.tilize_with_zero_padding"]], "to_device() (in module ttnn)": [[434, "ttnn.to_device"]], "to_dtype() (in module ttnn)": [[435, "ttnn.to_dtype"]], "to_layout() (in module ttnn)": [[436, "ttnn.to_layout"]], "to_memory_config() (in module ttnn)": [[437, "ttnn.to_memory_config"]], "to_torch() (in module ttnn)": [[438, "ttnn.to_torch"]], "topk() (in module ttnn)": [[439, "ttnn.topk"]], "attention_softmax() (in module ttnn.transformer)": [[440, "ttnn.transformer.attention_softmax"]], "attention_softmax_() (in module ttnn.transformer)": [[441, "ttnn.transformer.attention_softmax_"]], "chunked_flash_mla_prefill() (in module ttnn.transformer)": [[442, "ttnn.transformer.chunked_flash_mla_prefill"]], "chunked_scaled_dot_product_attention() (in module ttnn.transformer)": [[443, "ttnn.transformer.chunked_scaled_dot_product_attention"]], "concatenate_heads() (in module ttnn.transformer)": [[444, "ttnn.transformer.concatenate_heads"]], "flash_mla_prefill() (in module ttnn.transformer)": [[445, "ttnn.transformer.flash_mla_prefill"]], "flash_multi_latent_attention_decode() (in module ttnn.transformer)": [[446, "ttnn.transformer.flash_multi_latent_attention_decode"]], "joint_scaled_dot_product_attention() (in module ttnn.transformer)": [[447, "ttnn.transformer.joint_scaled_dot_product_attention"]], "paged_flash_multi_latent_attention_decode() (in module ttnn.transformer)": [[448, "ttnn.transformer.paged_flash_multi_latent_attention_decode"]], "paged_scaled_dot_product_attention_decode() (in module ttnn.transformer)": [[449, "ttnn.transformer.paged_scaled_dot_product_attention_decode"]], "ring_distributed_scaled_dot_product_attention() (in module ttnn.transformer)": [[450, "ttnn.transformer.ring_distributed_scaled_dot_product_attention"]], "ring_joint_scaled_dot_product_attention() (in module ttnn.transformer)": [[451, "ttnn.transformer.ring_joint_scaled_dot_product_attention"]], "scaled_dot_product_attention() (in module ttnn.transformer)": [[452, "ttnn.transformer.scaled_dot_product_attention"]], "scaled_dot_product_attention_decode() (in module ttnn.transformer)": [[453, "ttnn.transformer.scaled_dot_product_attention_decode"]], "split_query_key_value_and_split_heads() (in module ttnn.transformer)": [[454, "ttnn.transformer.split_query_key_value_and_split_heads"]], "windowed_scaled_dot_product_attention() (in module ttnn.transformer)": [[455, "ttnn.transformer.windowed_scaled_dot_product_attention"]], "transpose() (in module ttnn)": [[456, "ttnn.transpose"]], "tril() (in module ttnn)": [[457, "ttnn.tril"]], "triu() (in module ttnn)": [[458, "ttnn.triu"]], "trunc() (in module ttnn)": [[459, "ttnn.trunc"]], "trunc_bw() (in module ttnn)": [[460, "ttnn.trunc_bw"]], "typecast() (in module ttnn)": [[461, "ttnn.typecast"]], "unary_chain() (in module ttnn)": [[462, "ttnn.unary_chain"]], "uniform() (in module ttnn)": [[463, "ttnn.uniform"]], "unsqueeze() (in module ttnn)": [[464, "ttnn.unsqueeze"]], "unsqueeze_to_4d() (in module ttnn)": [[465, "ttnn.unsqueeze_to_4D"]], "untilize() (in module ttnn)": [[466, "ttnn.untilize"]], "untilize_with_unpadding() (in module ttnn)": [[467, "ttnn.untilize_with_unpadding"]], "update_cache() (in module ttnn)": [[468, "ttnn.update_cache"]], "upsample() (in module ttnn)": [[469, "ttnn.upsample"]], "var() (in module ttnn)": [[470, "ttnn.var"]], "var_hw() (in module ttnn)": [[471, "ttnn.var_hw"]], "view() (in module ttnn)": [[472, "ttnn.view"]], "where() (in module ttnn)": [[473, "ttnn.where"]], "where_bw() (in module ttnn)": [[474, "ttnn.where_bw"]], "xlogy() (in module ttnn)": [[475, "ttnn.xlogy"]], "xlogy_bw() (in module ttnn)": [[476, "ttnn.xlogy_bw"]], "zeros() (in module ttnn)": [[477, "ttnn.zeros"]], "zeros_like() (in module ttnn)": [[478, "ttnn.zeros_like"]], "shape (class in ttnn)": [[485, "ttnn.Shape"]], "rank (ttnn.shape property)": [[485, "ttnn.Shape.rank"]], "to_rank (ttnn.shape attribute)": [[485, "ttnn.Shape.to_rank"]]}})