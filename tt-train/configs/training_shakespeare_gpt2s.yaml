training_config:
  project_name: "tt_train_nano_gpt"
  model_type: "gpt2"
  #model_path: "data/gpt2s_exported.msgpack"
  seed: 5489
  model_save_interval: 500
  batch_size: 4
  num_epochs: 1
  max_steps: 5000
  learning_rate: 0.0003
  weight_decay: 0.1
  use_moreh_adamw: true
  use_kahan_summation: false
  gradient_accumulation_steps: 1
  tokenizer_type: bpe
  tokenizer_path: "data/gpt2-tokenizer.json"

  transformer_config:
    runner_type: memory_efficient
    num_heads: 12
    embedding_dim: 768
    dropout_prob: 0.2
    num_blocks: 12
    vocab_size: 96
    weight_tying: "enabled"
    max_sequence_length: 1024
    experimental:
      use_composite_layernorm: false

device_config:
    enable_ddp: false
    mesh_shape: [1,1]


# if you want to run as eval with weighgts from  hugging face:
# 1. Run tools/download_weights.py
# 2. nano_gpt --safetensors ~/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/  -c configs/training_shakespeare_gpt2s.yaml --eval 1

eval_config:
  repetition_penalty: 1.0
  temperature: 0.7
  top_k: 50
  top_p: 1.0
