training_config:
  project_name: "tt_train_nano_gpt"
  model_type: "llama"
  seed: 5489
  model_save_interval: 500
  batch_size: 1
  gradient_accumulation_steps: 1
  num_epochs: 1
  max_steps: 5000
  learning_rate: 0.0003
  weight_decay: 0.01
  use_moreh_adamw: true
  use_kahan_summation: false
  use_clip_grad_norm: false
  clip_grad_norm_max_norm: 1.0
  tokenizer_path: "data/tinyllama-tokenizer.json"
  tokenizer_type: "bpe"
  transformer_config:
    num_heads: 128
    num_groups: 32  # 8 is the original value, but we want to use 32 for tensor parallel
    embedding_dim: 16384
    dropout_prob: 0.0
    num_blocks: 126
    vocab_size: 32000
    max_sequence_length: 2048
    runner_type: default
    theta: 10000.0
    weight_tying: "disabled"
eval_config:
  repetition_penalty: 1.0
  temperature: 0.7
  top_k: 50
  top_p: 1.0

multihost_config:
  enabled: true
  num_workers: 5
  socket_type: fabric
  pipeline_parallel_config:
    num_blocks: 126
    blocks_per_rank:
      0: 24
      1: 26
      2: 26
      3: 26
      4: 24

device_config:
  enable_tp: true
  mesh_shape: [1, 32]
