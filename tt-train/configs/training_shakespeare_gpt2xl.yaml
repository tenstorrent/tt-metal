training_config:
  project_name: "tt_train_nano_gpt"
  model_type: "gpt2"
  seed: 5489
  model_save_interval: 500
  batch_size: 1
  num_epochs: 1
  max_steps: 5000
  learning_rate: 0.0003
  weight_decay: 0.1
  use_moreh_adamw: true
  use_kahan_summation: false
  gradient_accumulation_steps: 32
  tokenizer_type: bpe

  transformer_config:
    runner_type: memory_efficient
    num_heads: 32    # original is 25, but tensor parallel requires a multiple of #num_devices
    embedding_dim: 2048  # original is 1600, but tensor parallel heads creation are broken somewhy (returns padded tensor as result)
    dropout_prob: 0.2
    num_blocks: 48
    vocab_size: 96
    max_sequence_length: 1024
    experimental:
      use_composite_layernorm: false

device_config:
  enable_tp: true
  mesh_shape: [1,2]
