transformer_config:
  model_type: "llama"
  num_heads: 32
  num_groups: 8
  embedding_dim: 2048
  intermediate_dim: 8192
  dropout_prob: 0.0
  num_blocks: 16
  weight_tying: "enabled"
  vocab_size: 32000 # not using Meta vocab size (128256), see comment about tokenizer choice above.
  max_sequence_length: 2048
  runner_type: memory_efficient
  theta: 500000.0

  rope_scaling:
    scaling_factor: 32.0
    high_freq_factor: 4.0
    low_freq_factor: 1.0
    original_context_length: 8192
