transformer_config:
    num_heads: 32
    num_groups: 32  # 4 is the original value, but we want to use 32 for tensor parallel
    embedding_dim: 2048
    dropout_prob: 0.0
    num_blocks: 22
    vocab_size: 32000
    max_sequence_length: 2048
    runner_type: default
    theta: 10000.0
    weight_tying: "disabled"
