transformer_config:
  num_heads: 64
  num_groups: 32 # original value was 8
  embedding_dim: 8192
  dropout_prob: 0.0
  num_blocks: 80
  vocab_size: 32000
  max_sequence_length: 2048
  runner_type: memory_efficient
  theta: 10000.0