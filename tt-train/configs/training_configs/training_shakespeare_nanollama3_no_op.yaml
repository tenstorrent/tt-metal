training_config:
  project_name: "tt_train_nano_gpt" # not really nanogpt, but want to use the same wandb project name for now
  model_type: "llama"
  seed: 5489
  model_save_interval: 500
  batch_size: 32
  num_epochs: 1
  max_steps: 1000
  learning_rate: 0.0003
  weight_decay: 0.01
  use_no_op: true
  use_moreh_adamw: false
  use_kahan_summation: false
  use_clip_grad_norm: false
  clip_grad_norm_max_norm: 1.0
