training_config:
  project_name: "tt_train_nano_gpt"
  model_type: "qwen"
  #model_path: "data/qwen2.5_exported.msgpack"
  seed: 5489
  model_save_interval: 500
  batch_size: 1
  num_epochs: 1
  max_steps: 5000
  learning_rate: 0.0003
  weight_decay: 0.01
  use_moreh_adamw: true
  use_kahan_summation: false
  use_clip_grad_norm: false
  tokenizer_path: "data/qwen-tokenizer.json" 
  tokenizer_type: "bbpe"
  transformer_config:
    num_heads: 14
    num_groups: 2
    embedding_dim: 896
    intermediate_dim: 4864
    dropout_prob: 0.0
    num_blocks: 24
    vocab_size: 151665
    max_sequence_length: 2048
    runner_type: memory_efficient
    theta: 10000.0

# if you want to run as eval with weighgts from hugging face:
# nano_gpt --safetensors /path/to/Qwen/Qwen2.5-0.5B/  -c configs/training_shakespeare_qwen2_5_0_5B.yaml --eval 1

eval_config:
  repetition_penalty: 1.0
  temperature: 0.7
  top_k: 50
  top_p: 1.0
