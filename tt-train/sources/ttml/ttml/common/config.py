# SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC

# SPDX-License-Identifier: Apache-2.0

"""Configuration classes for transformer training."""
import os
import yaml


def get_config(path: str):
    """Load configuration from YAML file.

    Args:
        path: Path to config file relative to configs directory

    Returns:
        Dictionary containing configuration
    """
    path = f'{os.environ["TT_METAL_HOME"]}/tt-train/configs/{path}'
    with open(path, "r") as f:
        config = yaml.safe_load(f)
    return config


class DeviceConfig:
    """Configuration for device mesh and distributed training."""

    def __init__(self, yaml_config: dict):
        """Initialize device configuration from YAML config.

        Args:
            yaml_config: Dictionary containing configuration
        """
        device_config = yaml_config.get("device_config", {})
        self.mesh_shape = device_config.get("mesh_shape", [1, 1])
        self.device_ids = device_config.get("device_ids", [])
        self.enable_tp = device_config.get("enable_tp", False)
        self.enable_ddp = device_config.get("enable_ddp", False)

        # we currently support only [1, N] mesh shapes
        assert self.mesh_shape[0] == 1, f"Only [1, N] mesh shapes are supported, got {self.mesh_shape}"

    def total_devices(self) -> int:
        """Get total number of devices in mesh.

        Returns:
            Total device count
        """
        return self.mesh_shape[0] * self.mesh_shape[1]


class TrainingConfig:
    """Configuration for training hyperparameters."""

    def __init__(self, yaml_config: dict):
        """Initialize training configuration from YAML config.

        Args:
            yaml_config: Dictionary containing configuration
        """
        tc = yaml_config.get("training_config", {})
        self.batch_size = int(tc.get("batch_size", 64))
        self.steps = int(tc.get("max_steps", 1000))
        self.eval_every = int(tc.get("eval_every", 200))
        self.gradient_accumulation_steps = int(tc.get("gradient_accumulation_steps", 1))

        self.transformer_config = TransformerConfig(tc.get("transformer_config", {}))
        self.seq_len = int(self.transformer_config.max_sequence_length)


class TransformerConfig:
    """Configuration for transformer model hyperparameters."""

    def __init__(self, yaml_config: dict):
        """Initialize transformer configuration from YAML config.

        Args:
            yaml_config: Dictionary containing configuration
        """

        # Base parameters
        self.runner_type = yaml_config.get("runner_type", "default")
        self.num_heads = int(yaml_config.get("num_heads", 6))
        self.embedding_dim = int(yaml_config.get("embedding_dim", 384))
        self.dropout_prob = float(yaml_config.get("dropout_prob", 0.2))
        self.num_blocks = int(yaml_config.get("num_blocks", 6))
        self.vocab_size = int(yaml_config.get("vocab_size", 96))
        self.weight_tying = yaml_config.get("weight_tying", None)
        self.max_sequence_length = int(yaml_config.get("max_sequence_length", 128))

        # Llama-specific
        self.intermediate_dim = yaml_config.get("intermediate_dim", None)
        self.theta = yaml_config.get("theta", None)
        self.num_groups = yaml_config.get("num_groups", 3)

        # RoPE
        self.rope = yaml_config.get("rope_scaling", None)
        if self.rope:
            self.scaling_factor = self.rope.get("scaling_factor", None)
            self.high_freq_factor = self.rope.get("high_freq_factor", None)
            self.low_freq_factor = self.rope.get("low_freq_factor", None)
            self.original_context_length = self.rope.get("original_context_length", None)
