{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dbfe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Point PYTHONPATH to your TTML build (adjust if your path differs)\n",
    "sys.path.append(f'{os.environ[\"TT_METAL_HOME\"]}/tt-train/build/sources/ttml')\n",
    "#sys.path.append(f'{os.environ[\"TT_METAL_HOME\"]}/build/tt-train/sources/ttml')\n",
    "import _ttml\n",
    "\n",
    "# Repro\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    _ttml.autograd.AutoContext.get_instance().set_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# MNIST constants\n",
    "NUM_CLASSES = 10\n",
    "IMG_H, IMG_W = 28, 28\n",
    "N_FEATURES = IMG_H * IMG_W  # 784\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629379e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist() -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        x_train [Ntr, 784] float32 in [0,1]\n",
    "        y_train [Ntr] int32  (class indices)\n",
    "        x_test  [Nte, 784] float32 in [0,1]\n",
    "        y_test  [Nte] int32\n",
    "    \"\"\"\n",
    "    ds = load_dataset(\"mnist\")\n",
    "\n",
    "    x_train = np.stack([np.array(img, dtype=np.float32) for img in ds[\"train\"][\"image\"]], axis=0)\n",
    "    y_train = np.array(ds[\"train\"][\"label\"], dtype=np.uint32)\n",
    "    x_test  = np.stack([np.array(img, dtype=np.float32) for img in ds[\"test\"][\"image\"]], axis=0)\n",
    "    y_test  = np.array(ds[\"test\"][\"label\"],  dtype=np.uint32)\n",
    "\n",
    "    # scale to [0,1] and flatten\n",
    "    x_train = (x_train / 255.0).reshape(-1, N_FEATURES).astype(np.float32)\n",
    "    x_test  = (x_test  / 255.0).reshape(-1, N_FEATURES).astype(np.float32)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_mnist()\n",
    "\n",
    "# Optional quick smoke test to speed things up while iterating\n",
    "# take = 10_000\n",
    "# x_train, y_train = x_train[:take], y_train[:take]\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88930b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TTMLConfig:\n",
    "    batch_size: int = 128\n",
    "    epochs: int = 10\n",
    "    lr: float = 0.1\n",
    "    momentum: float = 0.9\n",
    "    weight_decay: float = 0.0\n",
    "    dampening: float = 0.0\n",
    "    nesterov: bool = False\n",
    "    seed: int = 42\n",
    "    log_every: int = 100  # print every N steps\n",
    "\n",
    "def train_ttml_linear_classifier_ce(\n",
    "    x_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    cfg: TTMLConfig,\n",
    ") -> Tuple[object, List[float]]:\n",
    "    \"\"\"\n",
    "    Trains a mlp(784 ->128 -> 128 -> 10) with cross-entropy loss.\n",
    "    TTML tensor shapes used here:\n",
    "      inputs  : [B, 1, 1, 784]  (float32)\n",
    "      targets : [B, 1, 1, 1]    (int32 class indices)\n",
    "    If your CE expects shape [B] instead, you can squeeze before creating the TTML tensor.\n",
    "    \"\"\"\n",
    "    set_seed(cfg.seed)\n",
    "\n",
    "    #model = _ttml.modules.create_linear_regression_model(N_FEATURES, NUM_CLASSES)\n",
    "    model_params = _ttml.models.mlp.MultiLayerPerceptronParameters.create(784, np.array([128, 128], dtype=np.uint32).tolist(), 10)\n",
    "    \n",
    "    model = _ttml.models.mlp.create_mlp_model(model_params)\n",
    "    loss_fn = _ttml.ops.loss.cross_entropy_loss  # logits + int32 targets\n",
    "    opt_cfg = _ttml.optimizers.SGDConfig.make(\n",
    "        cfg.lr, cfg.momentum, cfg.weight_decay, cfg.dampening, cfg.nesterov\n",
    "    )\n",
    "    opt = _ttml.optimizers.SGD(model.parameters(), opt_cfg)\n",
    "    model.train()\n",
    "\n",
    "    num_samples = x_train.shape[0]\n",
    "    indices = np.arange(num_samples)\n",
    "    losses: List[float] = []\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(cfg.epochs):\n",
    "        np.random.shuffle(indices)\n",
    "        pos = 0\n",
    "        while pos < num_samples:\n",
    "            end_pos = min(num_samples, pos + cfg.batch_size)\n",
    "            batch_idx = indices[pos:end_pos]\n",
    "            bsz = end_pos - pos\n",
    "\n",
    "            xb = x_train[batch_idx].reshape(bsz, 1, 1, N_FEATURES).astype(np.float32)\n",
    "            # Cross-entropy targets must be INT32 class indices:\n",
    "            yb_idx = y_train[batch_idx].reshape(bsz, 1).astype(np.uint32)\n",
    "\n",
    "            tt_x = _ttml.autograd.Tensor.from_numpy(xb, _ttml.Layout.TILE, _ttml.autograd.DataType.BFLOAT16)\n",
    "            tt_y_idx = _ttml.autograd.Tensor.from_numpy(yb_idx, _ttml.Layout.ROW_MAJOR, _ttml.autograd.DataType.UINT32)  # INT32 tensor\n",
    "\n",
    "            opt.zero_grad()\n",
    "            logits = model(tt_x)  # [B,1,1,10] raw logits\n",
    "            loss = loss_fn(logits, tt_y_idx, _ttml.ops.ReduceType.MEAN)\n",
    "            loss.backward(False)\n",
    "            opt.step()\n",
    "\n",
    "            step += 1\n",
    "            loss_val = float(loss.to_numpy())\n",
    "            losses.append(loss_val)\n",
    "            if cfg.log_every and (step % cfg.log_every == 0):\n",
    "                print(f\"[epoch {epoch+1}/{cfg.epochs}] step={step:5d} loss={loss_val:.6f}\")\n",
    "\n",
    "            pos = end_pos\n",
    "\n",
    "    model.eval()\n",
    "    return model, losses\n",
    "\n",
    "cfg = TTMLConfig(epochs=10, batch_size=128, lr=0.1, log_every=100)\n",
    "print(_ttml.modules)\n",
    "model, losses = train_ttml_linear_classifier_ce(x_train, y_train, cfg)\n",
    "len(losses), losses[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4910213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_logits_ttml(model, x: np.ndarray, batch_size: int = 2048) -> np.ndarray:\n",
    "    out = []\n",
    "    n = x.shape[0]\n",
    "    pos = 0\n",
    "    while pos < n:\n",
    "        end_pos = min(n, pos + batch_size)\n",
    "        bsz = end_pos - pos\n",
    "        xb = x[pos:end_pos].reshape(bsz, 1, 1, N_FEATURES).astype(np.float32)\n",
    "        tt_x = _ttml.autograd.Tensor.from_numpy(xb, _ttml.Layout.TILE, _ttml.autograd.DataType.BFLOAT16)\n",
    "        logits = model(tt_x).to_numpy().reshape(bsz, NUM_CLASSES)\n",
    "        out.append(logits)\n",
    "        pos = end_pos\n",
    "    return np.concatenate(out, axis=0)\n",
    "\n",
    "def softmax(z: np.ndarray, axis: int = -1) -> np.ndarray:\n",
    "    zmax = np.max(z, axis=axis, keepdims=True)\n",
    "    ez = np.exp(z - zmax)\n",
    "    return ez / np.sum(ez, axis=axis, keepdims=True)\n",
    "\n",
    "def accuracy(pred: np.ndarray, true: np.ndarray) -> float:\n",
    "    return float(np.mean(pred == true))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ddf47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate ---\n",
    "logits = predict_logits_ttml(model, x_test)\n",
    "probs = softmax(logits, axis=1)\n",
    "y_pred = np.argmax(probs, axis=1).astype(np.int32)\n",
    "acc = accuracy(y_pred, y_test)\n",
    "print(f\"Test accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "# --- Loss plot ---\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1, len(losses)+1), losses)\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Cross-Entropy Loss\")\n",
    "plt.title(\"Training Loss (TTML, MNIST)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Prediction grid ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bde313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions_grid(\n",
    "    images_flat: np.ndarray,\n",
    "    true_labels: np.ndarray,\n",
    "    pred_labels: np.ndarray,\n",
    "    n: int = 16,\n",
    "):\n",
    "    n = min(n, images_flat.shape[0])\n",
    "    idx = np.random.choice(images_flat.shape[0], n, replace=False)\n",
    "    imgs = images_flat[idx].reshape(n, IMG_H, IMG_W)\n",
    "    trues = true_labels[idx]\n",
    "    preds = pred_labels[idx]\n",
    "\n",
    "    side = int(math.ceil(math.sqrt(n)))\n",
    "    plt.figure(figsize=(side*2.2, side*2.2))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(side, side, i + 1)\n",
    "        ax.imshow(imgs[i], cmap=\"gray\")\n",
    "        ax.set_title(f\"T:{trues[i]}  P:{preds[i]}\", fontsize=9)\n",
    "        ax.axis(\"off\")\n",
    "    plt.suptitle(\"MNIST Predictions (TTML CE)\")\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "show_predictions_grid(x_test, y_test, y_pred, n=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
