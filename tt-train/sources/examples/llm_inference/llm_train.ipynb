{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb55490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\" \n",
    "CONFIG = \"training_shakespeare_llama3_2_1B_fixed.yaml\"\n",
    "#max lenght 704\n",
    "#accum 3 (max, 4 oom on n150) \n",
    "# broken model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d761733",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen3-1.7B\" \n",
    "CONFIG = \"training_shakespeare_qwen3_1_7B.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202606af",
   "metadata": {},
   "source": [
    "model_id = \"Qwen/Qwen3-0.6B\" \n",
    "CONFIG = \"training_shakespeare_qwen3_0_6B.yaml\"\n",
    "#max context "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4bfe2d",
   "metadata": {},
   "source": [
    "model_id =  \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "CONFIG = \"training_shakespeare_tinyllama.yaml\" # must be working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7bc745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ac9556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import hf_hub_download\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Optional\n",
    "\n",
    "import ttml\n",
    "from ttml.common.config import get_config, TrainingConfig, SchedulerConfig, DeviceConfig\n",
    "from ttml.common.model_factory import TransformerModelFactory\n",
    "from ttml.common.utils import round_up_to_tile, create_optimizer, initialize_device\n",
    "from ttml.common.data import build_causal_mask\n",
    "\n",
    "#import tt_serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0ef26c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeedrunScheduler:\n",
    "    \"\"\"Linear warmup -> optional hold -> linear decay; optional beta1 warmup.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: SchedulerConfig):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def lr_at(self, step: int) -> float:\n",
    "        s = step\n",
    "        w = max(0, self.cfg.warmup_steps)\n",
    "        h = max(0, self.cfg.hold_steps)\n",
    "        T = max(1, self.cfg.total_steps)\n",
    "        peak = self.cfg.max_lr\n",
    "        min_lr = self.cfg.min_lr\n",
    "\n",
    "        if s <= w:\n",
    "            # linear warmup 0 -> lr_max\n",
    "            return peak * (s / max(1, w))\n",
    "        elif s <= w + h:\n",
    "            # hold at lr_max\n",
    "            return peak\n",
    "        else:\n",
    "            # linear decay from lr_max at (w+h) to min_lr at T\n",
    "            s2 = min(s, T)\n",
    "            frac = (s2 - (w + h)) / max(1, (T - (w + h)))\n",
    "            return peak + (min_lr - peak) * frac\n",
    "\n",
    "    def beta1_at(self, step: int) -> Optional[float]:\n",
    "        if (\n",
    "            self.cfg.beta1_start is None\n",
    "            or self.cfg.beta1_end is None\n",
    "            or self.cfg.beta1_warmup_steps <= 0\n",
    "        ):\n",
    "            return None\n",
    "        s = min(step, self.cfg.beta1_warmup_steps)\n",
    "        t = s / float(self.cfg.beta1_warmup_steps)\n",
    "        return (1.0 - t) * self.cfg.beta1_start + t * self.cfg.beta1_end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "566cfa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimParamSetter:\n",
    "    def __init__(self, optim):\n",
    "        self.optim = optim\n",
    "        self._warned_lr = False\n",
    "        self._warned_beta1 = False\n",
    "\n",
    "    def set_lr(self, lr: float):\n",
    "        self.optim.set_lr(float(lr))\n",
    "\n",
    "    def set_beta1(self, beta1: float):\n",
    "        raise NotImplementedError(\n",
    "            \"set_beta1 is not implemented in TTML AdamW optimizer.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def build_logits_mask(vocab_size: int, padded_vocab_size: int) -> ttml.autograd.Tensor:\n",
    "    logits_mask = np.zeros((1, 1, 1, padded_vocab_size), dtype=np.float32)\n",
    "    logits_mask[:, :, :, vocab_size:] = 1e4\n",
    "    return ttml.autograd.Tensor.from_numpy(\n",
    "        logits_mask, ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16\n",
    "    )  # [1,1,1,T], bfloat16\"\n",
    "\n",
    "\n",
    "class CollateFn:\n",
    "    def __init__(self, eos_token_id, max_sequence_length, padded_vocab_size):\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.padded_vocab_size = padded_vocab_size\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        X = [sample[0] for sample in batch]\n",
    "        Y = [sample[1] for sample in batch]\n",
    "\n",
    "        batch_size = len(X)\n",
    "\n",
    "        data_np = np.full(\n",
    "            (batch_size, self.max_sequence_length), self.eos_token_id, dtype=np.uint32\n",
    "        )\n",
    "        mask_lens = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            x_tokens = X[i]\n",
    "            y_tokens = Y[i]\n",
    "\n",
    "            # Concatenate question + answer\n",
    "            combined_length = len(x_tokens) + len(y_tokens)\n",
    "            if combined_length > self.max_sequence_length:\n",
    "                # Truncate if too long, prioritizing keeping the answer\n",
    "                available_space = self.max_sequence_length - len(y_tokens)\n",
    "                if available_space > 0:\n",
    "                    x_tokens = x_tokens[:available_space]\n",
    "                    data_np[i, : len(x_tokens)] = x_tokens\n",
    "                    data_np[i, len(x_tokens) : len(x_tokens) + len(y_tokens)] = y_tokens\n",
    "\n",
    "                else:\n",
    "                    # If answer is too long, just use the answer\n",
    "                    data_np[i, : self.max_sequence_length] = y_tokens[\n",
    "                        : self.max_sequence_length\n",
    "                    ]\n",
    "                    x_tokens = []\n",
    "\n",
    "            else:\n",
    "                # Normal case: concatenate question + answer\n",
    "\n",
    "                data_np[i, : len(x_tokens)] = x_tokens\n",
    "                data_np[i, len(x_tokens) : len(x_tokens) + len(y_tokens)] = y_tokens\n",
    "\n",
    "            mask_lens.append(len(x_tokens))\n",
    "\n",
    "        # Shape: [batch_size, 1, 1, max_sequence_length]\n",
    "        X_np = np.expand_dims(data_np, axis=(1, 2))\n",
    "\n",
    "        y_np = np.full(\n",
    "            (batch_size, self.max_sequence_length), self.eos_token_id, dtype=np.uint32\n",
    "        )  # Shape: [batch, seq_len]\n",
    "        y_np[:, 0:-1] = X_np[:, 0, 0, 1:]  # Shift left by 1\n",
    "\n",
    "        loss_scaler_np = np.full(\n",
    "            (batch_size, 1, self.max_sequence_length, 1), 1.0, dtype=np.float32\n",
    "        )\n",
    "        for i, mask_len in enumerate(mask_lens):\n",
    "            loss_scaler_np[i, :, :mask_len, :] = 0.0\n",
    "            pad_positions = X_np[i, 0, 0, :] == self.eos_token_id\n",
    "            loss_scaler_np[i, :, pad_positions, :] = 0.0\n",
    "        loss_scaler_ratio = (\n",
    "            self.max_sequence_length * batch_size / np.sum(loss_scaler_np)\n",
    "        )\n",
    "        loss_scaler_np = loss_scaler_np * loss_scaler_ratio\n",
    "\n",
    "        return X_np, y_np, loss_scaler_np\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return self.collate_fn(batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17e7629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_generator(\n",
    "    dataloader,\n",
    "    batch_size,\n",
    "    max_sequence_length,\n",
    "    padded_vocab_size,\n",
    "    tokenizer,\n",
    "    device_config=None,\n",
    "):\n",
    "    \"\"\"Custom data generator for GSM8K dataset.\"\"\"\n",
    "    mapper = None\n",
    "    if device_config is not None:\n",
    "        device = ttml.autograd.AutoContext.get_instance().get_device()\n",
    "        mapper = ttml.core.distributed.shard_tensor_to_mesh_mapper(device, 0)\n",
    "\n",
    "    while True:\n",
    "        for batch in dataloader:\n",
    "            X_np, y_np, loss_scaler_np = batch\n",
    "\n",
    "            X = ttml.autograd.Tensor.from_numpy(\n",
    "                X_np, ttml.Layout.ROW_MAJOR, ttml.autograd.DataType.UINT32, mapper\n",
    "            )\n",
    "            y = ttml.autograd.Tensor.from_numpy(\n",
    "                y_np, ttml.Layout.ROW_MAJOR, ttml.autograd.DataType.UINT32, mapper\n",
    "            )\n",
    "            loss_scaler = ttml.autograd.Tensor.from_numpy(\n",
    "                loss_scaler_np,\n",
    "                ttml.Layout.TILE,\n",
    "                ttml.autograd.DataType.BFLOAT16,\n",
    "                mapper,\n",
    "            )\n",
    "\n",
    "            yield (X, y, loss_scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dc81f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_tt(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    question,\n",
    "    max_sequence_length,\n",
    "    causal_mask,\n",
    "    temperature,\n",
    "    logits_mask_tensor,\n",
    "    max_gen_tokens,\n",
    "    pad_token_id=None,\n",
    "    return_with_prompt=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Greedy/temperature=0 generation that prints the *full* text once at the end.\n",
    "    Uses a sliding window if prompt exceeds max_sequence_length.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(\n",
    "        ttml.autograd.GradMode.DISABLED\n",
    "    )\n",
    "\n",
    "    # --- Tokenize once ---\n",
    "    prompt_tokens = tokenizer.encode(question)\n",
    "    if pad_token_id is None:\n",
    "        # Try tokenizer.pad_token_id, else fall back to 0\n",
    "        pad_token_id = getattr(tokenizer, \"pad_token_id\", None)\n",
    "        if pad_token_id is None:\n",
    "            pad_token_id = 0\n",
    "\n",
    "    generated_tokens = []\n",
    "\n",
    "    device = ttml.autograd.AutoContext.get_instance().get_device()\n",
    "    composer = ttml.core.distributed.concat_mesh_to_tensor_composer(device, 0)\n",
    "\n",
    "    # Preallocate once\n",
    "    padded_prompt_tokens = np.full(\n",
    "        (1, 1, 1, max_sequence_length), pad_token_id, dtype=np.uint32\n",
    "    )\n",
    "    for _ in tqdm(range(max_gen_tokens)):\n",
    "        # Sliding window for long prompts\n",
    "        if len(prompt_tokens) > max_sequence_length:\n",
    "            start_idx = len(prompt_tokens) - max_sequence_length\n",
    "            window = prompt_tokens[start_idx:]\n",
    "        else:\n",
    "            start_idx = 0\n",
    "            window = prompt_tokens\n",
    "\n",
    "        # Refill buffer (fully) to avoid stale ids\n",
    "        padded_prompt_tokens[...] = pad_token_id\n",
    "        padded_prompt_tokens[0, 0, 0, : len(window)] = np.asarray(\n",
    "            window, dtype=np.uint32\n",
    "        )\n",
    "\n",
    "        # [1,1,1,T] -> TT tensor\n",
    "        padded_prompt_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "            padded_prompt_tokens, ttml.Layout.ROW_MAJOR, ttml.autograd.DataType.UINT32\n",
    "        )\n",
    "\n",
    "        # Forward: logits [1,1,T,V]\n",
    "        logits = model(padded_prompt_tensor, causal_mask)\n",
    "\n",
    "        # Sample: next tokens for all positions [1,1,T,1]\n",
    "        # With temperature=0.0 this behaves like argmax/greedy.\n",
    "        next_token_tensor = ttml.ops.sample.sample_op(\n",
    "            logits, 0.0, np.random.randint(low=1e7), logits_mask_tensor\n",
    "        )\n",
    "\n",
    "        # Take the token at the last active position in the current window\n",
    "        next_token_idx = (\n",
    "            max_sequence_length - 1\n",
    "            if len(prompt_tokens) > max_sequence_length\n",
    "            else len(window) - 1\n",
    "        )\n",
    "        next_token = int(\n",
    "            next_token_tensor.to_numpy(composer=composer).reshape(-1, 1)[\n",
    "                next_token_idx\n",
    "            ][0]\n",
    "        )\n",
    "\n",
    "        if next_token == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        generated_tokens.append(next_token)\n",
    "        prompt_tokens.append(next_token)\n",
    "\n",
    "    # Decode once at the end\n",
    "    out = tokenizer.decode(generated_tokens)\n",
    "    if return_with_prompt:\n",
    "        out = tokenizer.decode(prompt_tokens)\n",
    "\n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(\n",
    "        ttml.autograd.GradMode.ENABLED\n",
    "    )\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b741c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "    tt_model,\n",
    "    tokenizer,\n",
    "    val_batch_generator,\n",
    "    testing_data,\n",
    "    loss_fn,\n",
    "    causal_mask,\n",
    "    logits_mask_tensor,\n",
    "    max_sequence_length,\n",
    "    max_gen_tokens,\n",
    "    current_step,\n",
    "):\n",
    "    reduce = ttml.ops.ReduceType.NONE\n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(\n",
    "        ttml.autograd.GradMode.DISABLED\n",
    "    )\n",
    "    tt_model.eval()\n",
    "    eval_batch_count = 4\n",
    "    cur_val_losses = []\n",
    "    for _ in range(eval_batch_count):\n",
    "        val_X, val_y, val_loss_scaler = next(val_batch_generator)\n",
    "        val_logits = tt_model(val_X, causal_mask)\n",
    "\n",
    "        # Compute validation loss\n",
    "        val_loss = loss_fn(val_logits, val_y, reduce)\n",
    "        val_loss = val_loss * val_loss_scaler\n",
    "        val_loss = ttml.ops.unary.mean(val_loss)\n",
    "        cur_val_losses.append(get_loss_over_devices(val_loss))\n",
    "\n",
    "    checks_count = 4\n",
    "\n",
    "    with open(\"validation.txt\", \"a+\") as val_file:\n",
    "        val_file.write(f\"Validation at step {current_step}\\n\")\n",
    "        for check in range(checks_count):\n",
    "            val_file.write(f\"Validation check: {check}\\n\")\n",
    "            val_file.write(\"====================================\\n\")\n",
    "\n",
    "            tokenized_question, tokenized_answer = testing_data[check]\n",
    "            question = tokenizer.decode(tokenized_question, skip_special_tokens=True)\n",
    "\n",
    "            val_file.write(f\"Question: {question}\\n\")\n",
    "            val_file.write(\"====================================\\n\")\n",
    "\n",
    "            gen_text = generate_text_tt(\n",
    "                tt_model,\n",
    "                tokenizer,\n",
    "                question,\n",
    "                max_sequence_length,\n",
    "                causal_mask,\n",
    "                0.0,\n",
    "                logits_mask_tensor,\n",
    "                max_gen_tokens\n",
    "            )\n",
    "\n",
    "            val_file.write(f\"Generated Answer: {gen_text}\\n\")\n",
    "            val_file.write(\"\\n====================================\\n\")\n",
    "\n",
    "        val_file.write(\n",
    "            \"Last validation loss: {:.4f}\\n\\n\\n\".format(np.mean(cur_val_losses))\n",
    "        )\n",
    "\n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(\n",
    "        ttml.autograd.GradMode.ENABLED\n",
    "    )\n",
    "    tt_model.train()\n",
    "    return np.mean(cur_val_losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb572805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_logits(logits, binary_mask, add_mask):\n",
    "    masked_logits = binary_mask * logits\n",
    "    masked_logits = masked_logits + add_mask\n",
    "\n",
    "    return masked_logits\n",
    "\n",
    "\n",
    "def get_loss_over_devices(loss):\n",
    "    device = ttml.autograd.AutoContext.get_instance().get_device()\n",
    "    composer = ttml.core.distributed.concat_mesh_to_tensor_composer(device, 0)\n",
    "    loss_numpy = loss.to_numpy(composer=composer)\n",
    "    return loss_numpy.mean()\n",
    "\n",
    "\n",
    "def tokenize_dataset(data, tokenizer):\n",
    "    X = [sample[\"question\"] for sample in data]\n",
    "    y = [sample[\"answer\"] for sample in data]\n",
    "\n",
    "    X = tokenizer(X, return_tensors=\"np\", add_special_tokens=False)[\"input_ids\"]\n",
    "    y = tokenizer(y, return_tensors=\"np\", add_special_tokens=False)[\"input_ids\"]\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1086c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.len = len(X)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49df22b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and config...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer and config...\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "# Disable tokenizer parallelism to avoid conflicts with DataLoader multiprocessing\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "yaml_config = get_config(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4979c6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml_config['training_config']['transformer_config']['max_sequence_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd00c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c10a4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GSM8K dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration main-f9306ececa7c2eca\n",
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/parquet/main-f9306ececa7c2eca/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Using custom data configuration main-f9306ececa7c2eca\n",
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/parquet/main-f9306ececa7c2eca/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print(\"Loading GSM8K dataset...\")\n",
    "training_data = datasets.load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "testing_data = datasets.load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
    "\n",
    "training_data_x, training_data_y = tokenize_dataset(training_data, tokenizer)\n",
    "testing_data_x, testing_data_y = tokenize_dataset(testing_data, tokenizer)\n",
    "training_data = TokenizedDataset(training_data_x, training_data_y)\n",
    "testing_data = TokenizedDataset(testing_data_x, testing_data_y)\n",
    "\n",
    "max_gen_tokens = max(max(s.shape[0] for s in training_data_y),\n",
    "                     max(s.shape[0] for s in testing_data_y))\n",
    "max_seq_lenght = max(max(s.shape[0] for s in training_data_x),\n",
    "                     max(s.shape[0] for s in testing_data_x)) + max_gen_tokens\n",
    "\n",
    "max_seq_lenght = round_up_to_tile(max_seq_lenght)\n",
    "print(max_seq_lenght)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de86cb29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f895e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "983ffdce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'project_name': 'tt_train_qwen3',\n",
       " 'model_type': 'qwen3',\n",
       " 'seed': 5489,\n",
       " 'model_save_interval': 1,\n",
       " 'batch_size': 1,\n",
       " 'num_epochs': 1,\n",
       " 'max_steps': 5000,\n",
       " 'learning_rate': 0.0003,\n",
       " 'weight_decay': 0.01,\n",
       " 'use_moreh_adamw': True,\n",
       " 'use_kahan_summation': False,\n",
       " 'use_clip_grad_norm': False,\n",
       " 'clip_grad_norm_max_norm': 1.0,\n",
       " 'tokenizer_path': 'data/qwen-tokenizer.json',\n",
       " 'tokenizer_type': 'bpe',\n",
       " 'transformer_config': {'num_heads': 16,\n",
       "  'num_groups': 8,\n",
       "  'embedding_dim': 2048,\n",
       "  'head_dim': 128,\n",
       "  'intermediate_dim': 6144,\n",
       "  'dropout_prob': 0.0,\n",
       "  'num_blocks': 28,\n",
       "  'weight_tying': 'enabled',\n",
       "  'vocab_size': 151936,\n",
       "  'max_sequence_length': 2048,\n",
       "  'runner_type': 'memory_efficient',\n",
       "  'theta': 1000000.0,\n",
       "  'rms_norm_eps': 1e-06}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml_config['training_config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d41b8ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dba5cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_seq_lenght = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5434b0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overrride max sequence length: 2048 800\n"
     ]
    }
   ],
   "source": [
    "print('overrride max sequence length:', yaml_config['training_config']['transformer_config']['max_sequence_length'], max_seq_lenght)\n",
    "yaml_config['training_config']['transformer_config']['max_sequence_length'] = max_seq_lenght\n",
    "yaml_config['training_config']['gradient_accumulation_steps'] = 128\n",
    "yaml_config['training_config']['max_steps'] = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02262214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c50c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703d6da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07ae022a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading safetensors...\n"
     ]
    }
   ],
   "source": [
    "training_config = TrainingConfig(yaml_config)\n",
    "scheduler_config = SchedulerConfig(yaml_config)\n",
    "\n",
    "batch_size = training_config.batch_size\n",
    "\n",
    "# initialize device\n",
    "device_config = DeviceConfig(yaml_config)\n",
    "# no need to initialize device if #devices=1\n",
    "if device_config.total_devices() > 1:\n",
    "    initialize_device(yaml_config)\n",
    "\n",
    "# Download safetensors\n",
    "print(\"Downloading safetensors...\")\n",
    "safetensors_path = hf_hub_download(repo_id=model_id, filename=\"config.json\")\n",
    "safetensors_path = safetensors_path.replace(\"config.json\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28abdf61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_config.gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eacb0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5b850b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5492e1a8717d4154a30279d4b6832011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "torch.manual_seed(42)\n",
    "torch_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f570acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_vocab_size = torch_model.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52004343",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_model_factory = TransformerModelFactory(yaml_config)\n",
    "tt_model_factory.transformer_config.vocab_size = orig_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "285ed883",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = tt_model_factory.transformer_config.max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a572fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e66fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttml.autograd.AutoContext.get_instance().set_init_mode(ttml.autograd.InitMode.DISABLED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d52e8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3 configuration:\n",
      "    Vocab size: 151936\n",
      "    Max sequence length: 800\n",
      "    Embedding dim (hidden_size): 2048\n",
      "    Head dim: 128\n",
      "    Attention output dim: 2048\n",
      "    Intermediate dim: 6144\n",
      "    Num heads: 16\n",
      "    Num groups (KV heads): 8\n",
      "    Dropout probability: 0\n",
      "    Num blocks: 28\n",
      "    Positional embedding type: RoPE\n",
      "    Runner type: Memory efficient\n",
      "    Weight tying: Enabled\n",
      "    Theta: 1000000\n",
      "    RMSNorm epsilon: 1e-06\n",
      "2025-11-27 23:38:56.786 | info     |             UMD | Established cluster ETH FW version: 6.14.0 (topology_discovery_wormhole.cpp:359)\n",
      "2025-11-27 23:38:56.791 | info     |          Device | Opening user mode device driver (tt_cluster.cpp:209)\n",
      "2025-11-27 23:38:56.810 | info     |             UMD | Established cluster ETH FW version: 6.14.0 (topology_discovery_wormhole.cpp:359)\n",
      "2025-11-27 23:38:56.850 | info     |             UMD | Established cluster ETH FW version: 6.14.0 (topology_discovery_wormhole.cpp:359)\n",
      "2025-11-27 23:38:56.853 | info     |             UMD | Harvesting mask for chip 0 is 0x1 (NOC0: 0x1, simulated harvesting mask: 0x0). (cluster.cpp:413)\n",
      "2025-11-27 23:38:56.905 | warning  |             UMD | init_detect_tt_device_numanodes(): Could not determine NumaNodeSet for TT device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:564)\n",
      "2025-11-27 23:38:56.905 | warning  |             UMD | Could not find NumaNodeSet for TT Device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:173)\n",
      "2025-11-27 23:38:56.906 | warning  |             UMD | bind_area_memory_nodeset(): Unable to determine TT Device to NumaNode mapping for physical_device_id: 0. Skipping membind. (cpuset_lib.cpp:450)\n",
      "2025-11-27 23:38:56.906 | warning  |             UMD | ---- ttSiliconDevice::init_hugepage: bind_area_to_memory_nodeset() failed (physical_device_id: 0 ch: 0). Hugepage allocation is not on NumaNode matching TT Device. Side-Effect is decreased Device->Host perf (Issue #893). (sysmem_manager.cpp:212)\n",
      "2025-11-27 23:38:56.907 | info     |             UMD | Opening local chip ids/PCIe ids: {0}/[0] and remote chip ids {} (cluster.cpp:257)\n",
      "2025-11-27 23:38:56.907 | info     |             UMD | All devices in cluster running firmware version: 80.17.0 (cluster.cpp:235)\n",
      "2025-11-27 23:38:56.907 | info     |             UMD | IOMMU: disabled (cluster.cpp:177)\n",
      "2025-11-27 23:38:56.907 | info     |             UMD | KMD version: 2.0.0 (cluster.cpp:180)\n",
      "2025-11-27 23:38:56.910 | info     |             UMD | Pinning pages for Hugepage: virtual address 0x7efc80000000 and size 0x40000000 pinned to physical address 0x200000000 (pci_device.cpp:536)\n",
      "2025-11-27 23:38:58.248 | info     |           Metal | Profiler started on device 0 (device_pool.cpp:203)\n",
      "Model created: 8.843029260635376\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "tt_model = tt_model_factory.create_model()\n",
    "print(f\"Model created: {time() - start_time}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e07f3886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model created: 133.07612419128418\n",
    "# Model loaded: 27.28171706199646"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9e3ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model created: 68.56190633773804\n",
    "# Model loaded: 26.858034372329712"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c92c3570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model created: 9.410035133361816\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "475e5c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 32.21319389343262\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "tt_model.load_from_safetensors(safetensors_path)\n",
    "print(f\"Model loaded: {time() - start_time}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63c9e88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_vocab_size = round_up_to_tile(orig_vocab_size, 32)\n",
    "if orig_vocab_size != padded_vocab_size:\n",
    "    print(f\"Padding vocab size for tilization: original {orig_vocab_size} -> padded {padded_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b7e76e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbf50e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_dataloader = DataLoader(\n",
    "    training_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,  # Shuffle the dataset for each epoch\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=CollateFn(\n",
    "        tokenizer.eos_token_id, max_seq_lenght, padded_vocab_size\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "num_devices = device_config.total_devices()\n",
    "testing_dataloader = DataLoader(\n",
    "    testing_data,\n",
    "    batch_size=training_config.validation_batch_size * num_devices,\n",
    "    shuffle=False,  # Disable shuffling for validation\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=CollateFn(\n",
    "        tokenizer.eos_token_id, max_sequence_length, padded_vocab_size\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3fdeb7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91ebbee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "optim = create_optimizer(tt_model, yaml_config)\n",
    "causal_mask = build_causal_mask(max_sequence_length)\n",
    "\n",
    "causal_mask = ttml.autograd.Tensor.from_numpy(\n",
    "    causal_mask, ttml.Layout.ROW_MAJOR, ttml.autograd.DataType.BFLOAT16\n",
    ")\n",
    "\n",
    "logits_mask_tensor = build_logits_mask(orig_vocab_size, padded_vocab_size)\n",
    "\n",
    "loss_fn = ttml.ops.loss.cross_entropy_loss\n",
    "reduce = ttml.ops.ReduceType.NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03b9e44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training setup\n",
    "tt_model.train()\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_batch_generator = get_batch_generator(\n",
    "    training_dataloader,\n",
    "    batch_size,\n",
    "    max_sequence_length,\n",
    "    padded_vocab_size,\n",
    "    tokenizer,\n",
    "    device_config,\n",
    ")\n",
    "\n",
    "val_batch_generator = get_batch_generator(\n",
    "    testing_dataloader,\n",
    "    training_config.validation_batch_size * num_devices,\n",
    "    max_sequence_length,\n",
    "    padded_vocab_size,\n",
    "    tokenizer,\n",
    "    device_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "88efd8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens per micro-batch: 800\n",
      "Tokens per accumulated batch: 102400\n"
     ]
    }
   ],
   "source": [
    "tokens_per_batch = batch_size * max_sequence_length\n",
    "print(\"Tokens per micro-batch:\", tokens_per_batch)\n",
    "print(\n",
    "    \"Tokens per accumulated batch:\",\n",
    "    tokens_per_batch * training_config.gradient_accumulation_steps,\n",
    ")\n",
    "\n",
    "sched = SpeedrunScheduler(scheduler_config)\n",
    "setter = OptimParamSetter(optim)\n",
    "\n",
    "f = open(\"validation.txt\", \"w\")\n",
    "f.write(\"Validation log\\n\")\n",
    "f.write(\"===============\\n\")\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c79b66ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_steps = training_config.gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "caa45590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 1 epochs, max 500 steps...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2016d4fc374b4ff0a1c8d8cd96be7a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All 310 parameters were successfully loaded and used.\n",
      "2025-11-27 23:40:53.633 | info     |            Test | Small tensor algorithm selected (softmax_backward_w_small.cpp:18)\n",
      "2025-11-27 23:40:59.234 | critical |          Always | Out of Memory: Not enough space to allocate 9830400 B DRAM buffer across 12 banks, where each bank needs to store 819200 B, but bank size is only 1071181792 B (assert.hpp:103)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "TT_FATAL @ /home/ubuntu/tt-metal/tt_metal/impl/allocator/bank_manager.cpp:431: address.has_value()\ninfo:\nOut of Memory: Not enough space to allocate 9830400 B DRAM buffer across 12 banks, where each bank needs to store 819200 B, but bank size is only 1071181792 B\nbacktrace:\n --- /home/ubuntu/tt-metal/build/lib/libtt_metal.so(+0x524cdd) [0x7efdb2191cdd]\n --- tt::tt_metal::BankManager::allocate_buffer(unsigned long, unsigned long, bool, CoreRangeSet const&, std::optional<unsigned int>, ttsl::StrongType<unsigned int, tt::tt_metal::AllocatorIDTag>)\n --- tt::tt_metal::Allocator::allocate_buffer(tt::tt_metal::Buffer*)\n --- tt::tt_metal::Buffer::allocate_impl()\n --- tt::tt_metal::Buffer::create(tt::tt_metal::IDevice*, unsigned long, unsigned long, tt::tt_metal::BufferType, tt::tt_metal::BufferShardingArgs const&, std::optional<bool>, std::optional<ttsl::StrongType<unsigned char, tt::tt_metal::SubDeviceIdTag> >)\n --- tt::tt_metal::distributed::MeshBuffer::create(std::variant<tt::tt_metal::distributed::ReplicatedBufferConfig, tt::tt_metal::distributed::ShardedBufferConfig> const&, tt::tt_metal::distributed::DeviceLocalBufferConfig const&, tt::tt_metal::distributed::MeshDevice*, std::optional<unsigned long>)\n --- tt::tt_metal::tensor_impl::allocate_device_buffer(tt::tt_metal::distributed::MeshDevice*, tt::tt_metal::TensorSpec const&)\n --- tt::tt_metal::allocate_tensor_on_device(tt::tt_metal::TensorSpec const&, tt::tt_metal::distributed::MeshDevice*)\n --- tt::tt_metal::create_device_tensor(tt::tt_metal::TensorSpec const&, tt::tt_metal::IDevice*)\n --- ttnn::operations::binary_ng::BinaryNgDeviceOperation::create_output_tensors(ttnn::operations::binary_ng::BinaryNgDeviceOperation::operation_attributes_t const&, ttnn::operations::binary_ng::BinaryNgDeviceOperation::tensor_args_t const&)\n --- ttnn::operations::binary_ng::BinaryNgDeviceOperation::tensor_return_value_t ttnn::device_operation::detail::launch_on_device<ttnn::operations::binary_ng::BinaryNgDeviceOperation>(ttnn::operations::binary_ng::BinaryNgDeviceOperation::operation_attributes_t const&, ttnn::operations::binary_ng::BinaryNgDeviceOperation::tensor_args_t const&)\n --- ttnn::operations::binary_ng::BinaryNgDeviceOperation::tensor_return_value_t ttnn::device_operation::detail::invoke<ttnn::operations::binary_ng::BinaryNgDeviceOperation>(ttnn::operations::binary_ng::BinaryNgDeviceOperation::operation_attributes_t const&, ttnn::operations::binary_ng::BinaryNgDeviceOperation::tensor_args_t const&)\n --- /home/ubuntu/tt-metal/build/lib/_ttnncpp.so(+0xb63256) [0x7efd9777e256]\n --- /home/ubuntu/tt-metal/build/lib/_ttnncpp.so(+0xb62f0c) [0x7efd9777df0c]\n --- /home/ubuntu/tt-metal/build/lib/_ttnncpp.so(+0xb421af) [0x7efd9775d1af]\n --- ttnn::operations::binary::BinaryOperation<(ttnn::operations::binary::BinaryOpType)2>::invoke(tt::tt_metal::Tensor const&, tt::tt_metal::Tensor const&, std::optional<tt::tt_metal::DataType const> const&, std::optional<tt::tt_metal::MemoryConfig> const&, std::optional<tt::tt_metal::Tensor> const&, std::span<ttnn::operations::unary::BasicUnaryWithParam<float, int, unsigned int> const, 18446744073709551615ul>, std::span<ttnn::operations::unary::BasicUnaryWithParam<float, int, unsigned int> const, 18446744073709551615ul>, std::span<ttnn::operations::unary::BasicUnaryWithParam<float, int, unsigned int> const, 18446744073709551615ul>, std::optional<bool> const&)\n --- /home/ubuntu/tt-metal/python_env/lib/python3.10/site-packages/ttml/_ttml.cpython-310-x86_64-linux-gnu.so(_ZNK4ttnn10decorators22registered_operation_tIXtlN7reflect6v1_2_512fixed_stringIcLm14EEEtlA15_cLc116ELc116ELc110ELc110ELc58ELc58ELc109ELc117ELc108ELc116ELc105ELc112ELc108ELc121EEEENS_10operations6binary15BinaryOperationILNS8_12BinaryOpTypeE2EEEE16invoke_compositeIJRN2tt8tt_metal6TensorERKSG_EEEDaDpOT_+0x8d) [0x7efd96ab02fd]\n --- /home/ubuntu/tt-metal/python_env/lib/python3.10/site-packages/ttml/_ttml.cpython-310-x86_64-linux-gnu.so(_ZNK4ttnn10decorators22registered_operation_tIXtlN7reflect6v1_2_512fixed_stringIcLm14EEEtlA15_cLc116ELc116ELc110ELc110ELc58ELc58ELc109ELc117ELc108ELc116ELc105ELc112ELc108ELc121EEEENS_10operations6binary15BinaryOperationILNS8_12BinaryOpTypeE2EEEE13traced_invokeIJRN2tt8tt_metal6TensorERKSG_EEEDaDpOT_+0x101) [0x7efd96ab0081]\n --- /home/ubuntu/tt-metal/python_env/lib/python3.10/site-packages/ttml/_ttml.cpython-310-x86_64-linux-gnu.so(+0x1c51e8) [0x7efd96aad1e8]\n --- ttml::autograd::Tensor::backward(bool)\n --- ttml::models::common::transformer::memory_efficient_runner<ttml::modules::ModuleBase&>(ttml::modules::ModuleBase&, std::shared_ptr<ttml::autograd::Tensor> const&, std::shared_ptr<ttml::autograd::Tensor> const&)::{lambda()#3}::operator()() const\n --- ttml::autograd::Tensor::backward(bool)\n --- /home/ubuntu/tt-metal/python_env/lib/python3.10/site-packages/ttml/_ttml.cpython-310-x86_64-linux-gnu.so(+0xff3c9) [0x7efd969e73c9]\n --- /home/ubuntu/tt-metal/python_env/lib/python3.10/site-packages/ttml/_ttml.cpython-310-x86_64-linux-gnu.so(+0x149f10) [0x7efd96a31f10]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x5585a90a75a7]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x259be6) [0x5585a918bbe6]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(PyEval_EvalCode+0x86) [0x5585a918bab6]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x25f1cd) [0x5585a91911cd]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x18b309) [0x5585a90bd309]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x6c0) [0x5585a90a7460]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7210) [0x5585a90d9210]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x2798) [0x5585a90a9538]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7210) [0x5585a90d9210]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x2798) [0x5585a90a9538]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7210) [0x5585a90d9210]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x27713f) [0x5585a91a913f]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x195d6b) [0x5585a90c7d6b]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x5585a90a75a7]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x5585a90bd0ac]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x6c0) [0x5585a90a7460]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x5585a90bd0ac]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x5585a90a75a7]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1981d1) [0x5585a90ca1d1]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(PyObject_Call+0x122) [0x5585a90cae72]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x29f4) [0x5585a90a9794]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1981d1) [0x5585a90ca1d1]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x1999) [0x5585a90a8739]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7210) [0x5585a90d9210]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x2798) [0x5585a90a9538]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7210) [0x5585a90d9210]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x2798) [0x5585a90a9538]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7210) [0x5585a90d9210]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x2798) [0x5585a90a9538]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7210) [0x5585a90d9210]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x2798) [0x5585a90a9538]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7210) [0x5585a90d9210]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x2798) [0x5585a90a9538]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7210) [0x5585a90d9210]\n --- /usr/lib/python3.10/lib-dynload/_asyncio.cpython-310-x86_64-linux-gnu.so(+0x928e) [0x7efdd651e28e]\n --- /usr/lib/python3.10/lib-dynload/_asyncio.cpython-310-x86_64-linux-gnu.so(+0x90a4) [0x7efdd651e0a4]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyObject_MakeTpCall+0x25b) [0x5585a90b312b]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x2c7f0a) [0x5585a91f9f0a]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x17e21f) [0x5585a90b021f]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x69cc) [0x5585a90ad76c]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x5585a90bd0ac]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x5585a90a75a7]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x5585a90bd0ac]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x5585a90a75a7]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x5585a90bd0ac]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x5585a90a75a7]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x5585a90bd0ac]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x5585a90a75a7]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x5585a90bd0ac]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x5585a90a75a7]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1981d1) [0x5585a90ca1d1]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x56f3) [0x5585a90ac493]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x259be6) [0x5585a918bbe6]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(PyEval_EvalCode+0x86) [0x5585a918bab6]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x25f1cd) [0x5585a91911cd]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x18b309) [0x5585a90bd309]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x6c0) [0x5585a90a7460]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x5585a90bd0ac]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x6c0) [0x5585a90a7460]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x5585a90bd0ac]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x274b3d) [0x5585a91a6b3d]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(Py_RunMain+0x128) [0x5585a91a58f8]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(Py_BytesMain+0x2d) [0x5585a917fa8d]\n --- /lib/x86_64-linux-gnu/libc.so.6(+0x29d90) [0x7efdd7a27d90]\n --- /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80) [0x7efdd7a27e40]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_start+0x25) [0x5585a917f985]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 41\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Scale for accumulation and backward\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     scaled_loss \u001b[38;5;241m=\u001b[39m ttml\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mbinary\u001b[38;5;241m.\u001b[39mmul(\n\u001b[1;32m     39\u001b[0m         loss, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m accum_steps\n\u001b[1;32m     40\u001b[0m     ) \n\u001b[0;32m---> 41\u001b[0m     \u001b[43mscaled_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     ttml\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mAutoContext\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39mreset_graph()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Synchronize gradients if DDP is enabled\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: TT_FATAL @ /home/ubuntu/tt-metal/tt_metal/impl/allocator/bank_manager.cpp:431: address.has_value()\ninfo:\nOut of Memory: Not enough space to allocate 9830400 B DRAM buffer across 12 banks, where each bank needs to store 819200 B, but bank size is only 1071181792 B\nbacktrace:\n --- /home/ubuntu/tt-metal/build/lib/libtt_metal.so(+0x524cdd) [0x7efdb2191cdd]\n --- tt::tt_metal::BankManager::allocate_buffer(unsigned long, unsigned long, bool, CoreRangeSet const&, std::optional<unsigned int>, ttsl::StrongType<unsigned int, tt::tt_metal::AllocatorIDTag>)\n --- tt::tt_metal::Allocator::allocate_buffer(tt::tt_metal::Buffer*)\n --- tt::tt_metal::Buffer::allocate_impl()\n --- tt::tt_metal::Buffer::create(tt::tt_metal::IDevice*, unsigned long, unsigned long, tt::tt_metal::BufferType, tt::tt_metal::BufferShardingArgs const&, std::optional<bool>, std::optional<ttsl::StrongType<unsigned char, tt::tt_metal::SubDeviceIdTag> >)\n --- tt::tt_metal::distributed::MeshBuffer::create(std::variant<tt::tt_metal::distributed::ReplicatedBufferConfig, tt::tt_metal::distributed::ShardedBufferConfig> const&, tt::tt_metal::distributed::DeviceLocalBufferConfig const&, tt::tt_metal::distributed::MeshDevice*, std::optional<unsigned long>)\n --- tt::tt_metal::tensor_impl::allocate_device_buffer(tt::tt_metal::distributed::MeshDevice*, tt::tt_metal::TensorSpec const&)\n --- tt::tt_metal::allocate_tensor_on_device(tt::tt_metal::TensorSpec const&, tt::tt_metal::distributed::MeshDevice*)\n --- tt::tt_metal::create_device_tensor(tt::tt_metal::TensorSpec const&, tt::tt_metal::IDevice*)\n --- ttnn::operations::binary_ng::BinaryNgDeviceOperation::create_output_tensors(ttnn::operations::binary_ng::BinaryNgDeviceOperation::operation_attributes_t const&, ttnn::operations::binary_ng::BinaryNgDeviceOperation::tensor_args_t const&)\n --- ttnn::operations::binary_ng::BinaryNgDeviceOperation::tensor_return_value_t ttnn::device_operation::detail::launch_on_device<ttnn::operations::binary_ng::BinaryNgDeviceOperation>(ttnn::operations::binary_ng::BinaryNgDeviceOperation::operation_attributes_t const&, ttnn::operations::binary_ng::BinaryNgDeviceOperation::tensor_args_t const&)\n --- ttnn::operations::binary_ng::BinaryNgDeviceOperation::tensor_return_value_t ttnn::device_operation::detail::invoke<ttnn::operations::binary_ng::BinaryNgDeviceOperation>(ttnn::operations::binary_ng::BinaryNgDeviceOperation::operation_attributes_t const&, ttnn::operations::binary_ng::BinaryNgDeviceOperation::tensor_args_t const&)\n --- /home/ubuntu/tt-metal/build/lib/_ttnncpp.so(+0xb63256) [0x7efd9777e256]\n --- /home/ubuntu/tt-metal/build/lib/_ttnncpp.so(+0xb62f0c) [0x7efd9777df0c]\n --- /home/ubuntu/tt-metal/build/lib/_ttnncpp.so(+0xb421af) [0x7efd9775d1af]\n --- ttnn::operations::binary::BinaryOperation<(ttnn::operations::binary::BinaryOpType)2>::invoke(tt::tt_metal::Tensor const&, tt::tt_metal::Tensor const&, std::optional<tt::tt_metal::DataType const> const&, std::optional<tt::tt_metal::MemoryConfig> const&, std::optional<tt::tt_metal::Tensor> const&, std::span<ttnn::operations::unary::BasicUnaryWithParam<float, int, unsigned int> const, 18446744073709551615ul>, std::span<ttnn::operations::unary::BasicUnaryWithParam<float, int, unsigned int> const, 18446744073709551615ul>, std::span<ttnn::operations::unary::BasicUnaryWithParam<float, int, unsigned int> const, 18446744073709551615ul>, std::optional<bool> const&)\n --- /home/ubuntu/tt-metal/python_env/lib/python3.10/site-packages/ttml/_ttml.cpython-310-x86_64-linux-gnu.so(_ZNK4ttnn10decorators22registered_operation_tIXtlN7reflect6v1_2_512fixed_stringIcLm14EEEtlA15_cLc116ELc116ELc110ELc110ELc58ELc58ELc109ELc117ELc108ELc116ELc105ELc112ELc108ELc121EEEENS_10operations6binary15BinaryOperationILNS8_12BinaryOpTypeE2EEEE16invoke_compositeIJRN2tt8tt_metal6TensorERKSG_EEEDaDpOT_+0x8d) [0x7efd96ab02fd]\n --- /home/ubuntu/tt-metal/python_env/lib/python3.10/site-packages/ttml/_ttml.cpython-310-x86_64-linux-gnu.so(_ZNK4ttnn10decorators22registered_operation_tIXtlN7reflect6v1_2_512fixed_stringIcLm14EEEtlA15_cLc116ELc116ELc110ELc110ELc58ELc58ELc109ELc117ELc108ELc116ELc105ELc112ELc108ELc121EEEENS_10operations6binary15BinaryOperationILNS8_12BinaryOpTypeE2EEEE13traced_invokeIJRN2tt8tt_metal6TensorERKSG_EEEDaDpOT_+0x101) [0x7efd96ab0081]\n --- /home/ubuntu/tt-metal/python_env/lib/python3.10/site-packages/ttml/_ttml.cpython-310-x86_64-linux-gnu.so(+0x1c51e8) [0x7efd96aad1e8]\n --- ttml::autograd::Tensor::backward(bool)\n --- ttml::models::common::transformer::memory_efficient_runner<ttml::modules::ModuleBase&>(ttml::modules::ModuleBase&, std::shared_ptr<ttml::autograd::Tensor> const&, std::shared_ptr<ttml::autograd::Tensor> const&)::{lambda()#3}::operator()() const\n --- ttml::autograd::Tensor::backward(bool)\n --- /home/ubuntu/tt-metal/python_env/lib/python3.10/site-packages/ttml/_ttml.cpython-310-x86_64-linux-gnu.so(+0xff3c9) [0x7efd969e73c9]\n --- /home/ubuntu/tt-metal/python_env/lib/python3.10/site-packages/ttml/_ttml.cpython-310-x86_64-linux-gnu.so(+0x149f10) [0x7efd96a31f10]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x5585a90a75a7]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x259be6) [0x5585a918bbe6]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(PyEval_EvalCode+0x86) [0x5585a918bab6]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x25f1cd) [0x5585a91911cd]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x18b309) [0x5585a90bd309]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x6c0) [0x5585a90a7460]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7210) [0x5585a90d9210]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x2798) [0x5585a90a9538]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7210) [0x5585a90d9210]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x2798) [0x5585a90a9538]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7210) [0x5585a90d9210]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x27713f) [0x5585a91a913f]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x195d6b) [0x5585a90c7d6b]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x5585a90a75a7]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x5585a90bd0ac]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x6c0) [0x5585a90a7460]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x5585a90bd0ac]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x5585a90a75a7]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1981d1) [0x5585a90ca1d1]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(PyObject_Call+0x122) [0x5585a90cae72]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x29f4) [0x5585a90a9794]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1981d1) [0x5585a90ca1d1]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x1999) [0x5585a90a8739]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7210) [0x5585a90d9210]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x2798) [0x5585a90a9538]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7210) [0x5585a90d9210]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x2798) [0x5585a90a9538]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7210) [0x5585a90d9210]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x2798) [0x5585a90a9538]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7210) [0x5585a90d9210]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x2798) [0x5585a90a9538]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7210) [0x5585a90d9210]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x2798) [0x5585a90a9538]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7210) [0x5585a90d9210]\n --- /usr/lib/python3.10/lib-dynload/_asyncio.cpython-310-x86_64-linux-gnu.so(+0x928e) [0x7efdd651e28e]\n --- /usr/lib/python3.10/lib-dynload/_asyncio.cpython-310-x86_64-linux-gnu.so(+0x90a4) [0x7efdd651e0a4]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyObject_MakeTpCall+0x25b) [0x5585a90b312b]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x2c7f0a) [0x5585a91f9f0a]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x17e21f) [0x5585a90b021f]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x69cc) [0x5585a90ad76c]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x5585a90bd0ac]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x5585a90a75a7]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x5585a90bd0ac]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x5585a90a75a7]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x5585a90bd0ac]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x5585a90a75a7]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x5585a90bd0ac]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x5585a90a75a7]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x5585a90bd0ac]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x5585a90a75a7]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1981d1) [0x5585a90ca1d1]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x56f3) [0x5585a90ac493]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x259be6) [0x5585a918bbe6]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(PyEval_EvalCode+0x86) [0x5585a918bab6]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x25f1cd) [0x5585a91911cd]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x18b309) [0x5585a90bd309]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x6c0) [0x5585a90a7460]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x5585a90bd0ac]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x6c0) [0x5585a90a7460]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x5585a90bd0ac]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x274b3d) [0x5585a91a6b3d]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(Py_RunMain+0x128) [0x5585a91a58f8]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(Py_BytesMain+0x2d) [0x5585a917fa8d]\n --- /lib/x86_64-linux-gnu/libc.so.6(+0x29d90) [0x7efdd7a27d90]\n --- /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80) [0x7efdd7a27e40]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_start+0x25) [0x5585a917f985]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Starting training for {training_config.epochs} epochs, max {training_config.steps} steps...\"\n",
    ")\n",
    "bar = tqdm(range(1, training_config.steps + 1))\n",
    "\n",
    "total_steps = 0\n",
    "last_val_loss = 0\n",
    "accum_steps = training_config.gradient_accumulation_steps\n",
    "\n",
    "\n",
    "# ========== Training Loop ===========\n",
    "for opt_step in bar:\n",
    "    # LR (and optional beta1) updated once per optimizer step\n",
    "    optim.zero_grad()\n",
    "    lr_now = sched.lr_at(opt_step - 1)  # zero-based inside scheduler\n",
    "    setter.set_lr(lr_now)\n",
    "\n",
    "    # ---- internal micro-steps ----\n",
    "    # Aggregate the true (unscaled) mean losses across micro-steps to report per optimizer step.\n",
    "    micro_losses = []\n",
    "\n",
    "    for micro in range(accum_steps):\n",
    "        X, y, loss_scaler = next(train_batch_generator)\n",
    "\n",
    "        # Forward\n",
    "        logits = tt_model(X, causal_mask)  # [B,1,T,V]\n",
    "\n",
    "        # CE on masked logits\n",
    "        loss = loss_fn(logits, y, reduce)  # [B,1,T,1] shape reduced later\n",
    "        loss = loss * loss_scaler\n",
    "        loss = ttml.ops.unary.mean(loss)  # scalar\n",
    "\n",
    "        # Track true loss for reporting\n",
    "        # micro_losses.append(float(loss.to_numpy()))\n",
    "        micro_losses.append(get_loss_over_devices(loss))\n",
    "\n",
    "        # Scale for accumulation and backward\n",
    "        scaled_loss = ttml.ops.binary.mul(\n",
    "            loss, 1 / accum_steps\n",
    "        ) \n",
    "        scaled_loss.backward(False)\n",
    "        ttml.autograd.AutoContext.get_instance().reset_graph()\n",
    "\n",
    "    # Synchronize gradients if DDP is enabled\n",
    "    if device_config.enable_ddp:\n",
    "        ttml.core.distributed.synchronize_parameters(tt_model.parameters())\n",
    "\n",
    "    # Optimizer step after micro-steps\n",
    "    optim.step()\n",
    "\n",
    "    # Average loss across micro-steps (this corresponds to the optimizer step)\n",
    "    step_loss = float(np.mean(micro_losses)) if len(micro_losses) > 0 else 0.0\n",
    "    train_losses.append(step_loss)\n",
    "\n",
    "    # tqdm postfix\n",
    "    postfix = {\"train_loss\": f\"{step_loss:.4f}\", \"lr\": f\"{lr_now:.6f}\"}\n",
    "    if last_val_loss is not None:\n",
    "        postfix[\"val_loss\"] = f\"{last_val_loss:.4f}\"\n",
    "    bar.set_postfix(postfix, refresh=False)\n",
    "\n",
    "    # Validation every eval_every steps\n",
    "    if ( \n",
    "        total_steps % training_config.eval_every == 0\n",
    "        or total_steps + 1 == training_config.steps\n",
    "    ):\n",
    "        last_val_loss = validate(\n",
    "            tt_model,\n",
    "            tokenizer,\n",
    "            val_batch_generator,\n",
    "            testing_data,\n",
    "            loss_fn,\n",
    "            causal_mask,\n",
    "            logits_mask_tensor,\n",
    "            max_sequence_length=max_seq_lenght,\n",
    "            max_gen_tokens=max_gen_tokens,\n",
    "            current_step=total_steps,\n",
    "        )\n",
    "        \n",
    "        val_losses.append(last_val_loss)\n",
    "\n",
    "    total_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7884fc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tt-smi -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f32ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "All 130 parameters were successfully loaded and used.\n",
    "2025-11-27 09:04:42.192 | info     |            Test | Small tensor algorithm selected (softmax_backward_w_small.cpp:18)\n",
    "2025-11-27 09:04:53.830 | critical |          Always | Out of Memory: Not enough space to allocate 525336576 B DRAM buffer across 12 banks, where each bank needs to store 43778048 B, but bank size is only 1071181792 B (assert.hpp:103)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e13dc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model.model.embed_tokens.weight.data.numel() * 2 / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5d22a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "525336576 / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0119fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "43778048 / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c270a2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "525336576 / 43778048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acdd6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "1071181792 / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe125675",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403aadc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff730a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config.validation_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91948c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0a938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training completed!\")\n",
    "\n",
    "# Plot training curves\n",
    "print(\"Plotting training curves...\")\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 5))\n",
    "axs.plot(train_losses, color=\"blue\", label=\"Train Loss\")\n",
    "axs.plot(\n",
    "    np.arange(0, len(val_losses)) * training_config.eval_every,\n",
    "    val_losses,\n",
    "    color=\"orange\",\n",
    "    label=\"Val Loss\",\n",
    ")\n",
    "axs.set_title(\"Training Loss\")\n",
    "axs.set_xlabel(\"Steps\")\n",
    "axs.set_ylabel(\"Loss\")\n",
    "axs.legend()\n",
    "plt.savefig(\"training_curves.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71842a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e728cb06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
