{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8744d20",
   "metadata": {},
   "source": [
    "### LLM Inference Example\n",
    "\n",
    "This notebook contains a basic inference example for using our `ttml` Python API to build, load, and run a large language model from Hugging Face on our TT hardware. By default, it is set to create and load a GPT2 model, but this notebook can quickly and easily be edited to use any of the LLMs that the tt-train project currently supports. \n",
    "\n",
    "Below, in the first cell, we have our imports and basic directory housekeeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a20c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "import numpy as np  # For numpy arrays\n",
    "from dataclasses import dataclass # For configuration classes\n",
    "from huggingface_hub import hf_hub_download # To download safetensors from Hugging Face\n",
    "from transformers import AutoTokenizer\n",
    "from yaml import safe_load # To read YAML configs\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(f\"{os.environ['TT_METAL_HOME']}/tt-train/sources/ttml\")\n",
    "import ttml\n",
    "from ttml.common.config import get_config, TransformerConfig\n",
    "\n",
    "# Can be used to set the random seed for reproducibility\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    ttml.autograd.AutoContext.get_instance().set_seed(seed)\n",
    "\n",
    "# Change working directory to tt-train\n",
    "os.chdir(f\"{os.environ['TT_METAL_HOME']}/tt-train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e005a45c",
   "metadata": {},
   "source": [
    "Use the cell below to change global parameters in this notebook. \n",
    "\n",
    "`OUTPUT_TOKENS` : the length of the generated text in token (not characters!) \n",
    "\n",
    "`WITH_SAMPLING` : enable or disable output token sampling (only used for PyTorch)\n",
    "\n",
    "`TEMPERATURE`   : sampling temperature; set to 0 to disable sampling in `generate_with_tt()`\n",
    "\n",
    "`SEED`          : randomization seed (for reproducibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d6dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TOKENS = 256\n",
    "WITH_SAMPLING = True\n",
    "TEMPERATURE = 0.8\n",
    "SEED = 42\n",
    "CONFIG = \"training_shakespeare_gpt2s.yaml\"\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9b032f",
   "metadata": {},
   "source": [
    "While the notebook is currently configured for GPT2, you can quickly change the tokenizer you want to use by changing the input to `from_pretrained()` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea71a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from Hugging Face and the transformer config from YAML\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "transformer_config = TransformerConfig(get_config(CONFIG).get(\"training_config\", {}).get(\"transformer_config\",{}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5415d24d",
   "metadata": {},
   "source": [
    "As above, the call to `hf_hub_download()` will download (or otherwise find on your local system) the SafeTensors model weight file for GPT2, but can be updated to download other SafeTensors files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6965256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get safetensors\n",
    "local_path = hf_hub_download(repo_id=\"gpt2\", filename=\"model.safetensors\")\n",
    "local_path = local_path.replace(\"model.safetensors\",\"\")\n",
    "\n",
    "print(f\"Safetensors path: {local_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b353af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_causal_mask(T: int) -> ttml.autograd.Tensor:\n",
    "    # [1,1,T,T] float32 with 1s for allowed positions (i >= j), else 0\n",
    "    m = np.tril(np.ones((T, T), dtype=np.float32))\n",
    "    return ttml.autograd.Tensor.from_numpy(m.reshape(1, 1, T, T), ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)\n",
    "\n",
    "def build_logits_mask(vocab_size: int, padded_vocab_size: int) -> ttml.autograd.Tensor:\n",
    "    logits_mask = np.zeros((1, 1, 1, padded_vocab_size), dtype=np.float32)\n",
    "    logits_mask[:, :, :, vocab_size:] = 1e4\n",
    "    return ttml.autograd.Tensor.from_numpy(logits_mask, ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)   # [1,1,1,T], bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd89028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(cfg : TransformerConfig):\n",
    "    # GPT2 config via your bindings\n",
    "    gcfg = ttml.models.gpt2.GPT2TransformerConfig()\n",
    "    gcfg.num_heads = cfg.num_heads\n",
    "    gcfg.embedding_dim = cfg.embedding_dim\n",
    "    gcfg.num_blocks = cfg.num_blocks\n",
    "    gcfg.vocab_size = cfg.vocab_size\n",
    "    gcfg.max_sequence_length = cfg.max_sequence_length\n",
    "    gcfg.dropout_prob = cfg.dropout_prob\n",
    "    gcfg.weight_tying = ttml.models.WeightTyingType.Enabled if cfg.weight_tying == \"enabled\" else ttml.models.gpt2.WeightTyingType.DISABLED\n",
    "    gcfg.runner_type = ttml.models.RunnerType.Default\n",
    "\n",
    "    model = ttml.models.gpt2.create_gpt2_model(gcfg)\n",
    "    model.load_from_safetensors(Path(local_path))\n",
    "    return model\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "if vocab_size % 32 != 0:\n",
    "    print(f\"Warning: vocab size {vocab_size} is not multiple of 32, padding for tilizing.\")\n",
    "    padded_vocab_size = ((tokenizer.vocab_size + 31) // 32) * 32\n",
    "\n",
    "else:\n",
    "    padded_vocab_size = vocab_size\n",
    "\n",
    "transformer_config.vocab_size = padded_vocab_size\n",
    "tt_model = create_model(transformer_config)\n",
    "tt_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40913e88",
   "metadata": {},
   "source": [
    "`generate_with_tt()` uses TT hardware acceleration to generate output from the chosen LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e6550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_tt(model, prompt_tokens):\n",
    "\n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(ttml.autograd.GradMode.DISABLED)\n",
    "    model.eval()\n",
    "\n",
    "    if padded_vocab_size != vocab_size:\n",
    "        logits_mask_tensor = build_logits_mask(vocab_size, padded_vocab_size)\n",
    "    else:\n",
    "        logits_mask_tensor = None\n",
    "\n",
    "    causal_mask = build_causal_mask(transformer_config.max_sequence_length)  # [1,1,seq_len,seq_len], float32\n",
    "    padded_prompt_tokens = np.zeros((1, 1, 1, transformer_config.max_sequence_length), \n",
    "                                    dtype=np.uint32)\n",
    "\n",
    "    start_idx = 0\n",
    "\n",
    "    print(\"************************************\")\n",
    "    for token_idx in range(OUTPUT_TOKENS):\n",
    "\n",
    "        if len(prompt_tokens) > transformer_config.max_sequence_length:\n",
    "            start_idx = len(prompt_tokens) - transformer_config.max_sequence_length\n",
    "\n",
    "        # padded_prompt_tokens[0, 0, 0, :transformer_cfg[\"max_sequence_length\"]] = 0\n",
    "        padded_prompt_tokens[0, 0, 0, :len(prompt_tokens)] = prompt_tokens[start_idx:]\n",
    "        padded_prompt_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "            padded_prompt_tokens,\n",
    "            ttml.Layout.ROW_MAJOR,\n",
    "            ttml.autograd.DataType.UINT32)  # [1,1,1, max_seq_len], uint32\n",
    "\n",
    "        logits = model(padded_prompt_tensor, causal_mask)  # out=[1,1,seq_len, vocab_size], bf16\n",
    "\n",
    "\n",
    "        next_token_tensor = ttml.ops.sample.sample_op(logits, TEMPERATURE, np.random.randint(low=1e7), logits_mask_tensor)  # out=[1,1,seq_len,1], uint32\n",
    "\n",
    "        next_token_idx = transformer_config.max_sequence_length - 1 if len(prompt_tokens) > transformer_config.max_sequence_length else len(prompt_tokens) - 1\n",
    "        next_token = next_token_tensor.to_numpy().flatten()[next_token_idx]\n",
    "\n",
    "        output = tokenizer.decode(next_token)\n",
    "\n",
    "        prompt_tokens.append(next_token)\n",
    "        print(output, end='', flush=True)\n",
    "\n",
    "    print(\"\\n************************************\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7824c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_pytorch(prompt_tokens):\n",
    "    import torch\n",
    "    from transformers import AutoModelForCausalLM\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    torch_model = AutoModelForCausalLM.from_pretrained(\"gpt2\", dtype=torch.bfloat16)\n",
    "    torch_model.eval()\n",
    "    print(\"************************************\")\n",
    "    with torch.no_grad():\n",
    "        outputs = torch_model.generate(\n",
    "            prompt_tokens,\n",
    "            max_new_tokens=OUTPUT_TOKENS,\n",
    "            do_sample=WITH_SAMPLING, # Enable sampling\n",
    "            temperature=TEMPERATURE,   # Temperature for sampling\n",
    "            num_beams=1 # Use multinomial sampling (standard sampling)\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    for t in generated_text:\n",
    "        print(t)\n",
    "        \n",
    "    print(\"\\n************************************\\n\\n\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a42273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"The difference between cats and dogs is:\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with TT:\")\n",
    "generate_with_tt(tt_model, prompt_tokens.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98a32d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"Compared to spoons, forks are meant to:\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with TT:\")\n",
    "generate_with_tt(tt_model, prompt_tokens.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b67723",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"Bees are similar to:\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with TT:\")\n",
    "generate_with_tt(tt_model, prompt_tokens.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe3330e",
   "metadata": {},
   "source": [
    "Now try your own prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8beb647",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = input(\"Enter your prompt: \")\n",
    "\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with TT model:\")\n",
    "generate_with_tt(tt_model, prompt_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
