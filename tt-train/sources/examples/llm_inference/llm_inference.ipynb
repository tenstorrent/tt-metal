{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8744d20",
   "metadata": {},
   "source": [
    "### LLM Inference Example\n",
    "\n",
    "This notebook contains a basic inference example for using our `ttml` Python API to build, load, and run a large language model from Hugging Face on our TT hardware. By default, it is set to create and load a GPT2 model, but this notebook can quickly and easily be edited to use any of the LLMs that the tt-train project currently supports. \n",
    "\n",
    "Below, in the first cell, we have our imports and basic directory housekeeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a20c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "import numpy as np  # For numpy arrays\n",
    "from dataclasses import dataclass # For configuration classes\n",
    "from huggingface_hub import hf_hub_download # To download safetensors from Hugging Face\n",
    "from transformers import AutoTokenizer\n",
    "from yaml import safe_load # To read YAML configs\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(f\"{os.getenv('TT_METAL_RUNTIME_ROOT')}/tt-train/sources/ttml\")\n",
    "import ttml\n",
    "from ttml.common.config import get_training_config, load_config\n",
    "from ttml.common.utils import set_seed, round_up_to_tile\n",
    "from ttml.common.model_factory import TransformerModelFactory\n",
    "\n",
    "# Change working directory to tt-train\n",
    "os.chdir(f\"{os.environ['TT_METAL_RUNTIME_ROOT']}/tt-train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e005a45c",
   "metadata": {},
   "source": [
    "Use the cell below to change global parameters in this notebook. \n",
    "\n",
    "`OUTPUT_TOKENS` : the length of the generated text in token (not characters!) \n",
    "\n",
    "`TEMPERATURE`   : sampling temperature; set to 0 to disable sampling in `generate_with_tt()`\n",
    "\n",
    "`SEED`          : randomization seed (for reproducibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a87d6dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TOKENS = 750\n",
    "WITH_SAMPLING = True\n",
    "TEMPERATURE = 0.8\n",
    "SEED = 42\n",
    "CONFIG = \"training_shakespeare_llama3_gpt2s_size.yaml\"\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "model_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9b032f",
   "metadata": {},
   "source": [
    "While the notebook is currently configured for GPT2, you can quickly change the tokenizer you want to use by changing the input to `from_pretrained()` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea71a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from Hugging Face and the transformer config from YAML\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "training_config = get_training_config(CONFIG)\n",
    "model_yaml = load_config(training_config.model_config, configs_root=os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5415d24d",
   "metadata": {},
   "source": [
    "As above, the call to `hf_hub_download()` will download (or otherwise find on your local system) the SafeTensors model weight file for GPT2, but can be updated to download other SafeTensors files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6965256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get safetensors\n",
    "safetensors_path = hf_hub_download(repo_id=model_path, filename=\"model.safetensors\")\n",
    "safetensors_path = safetensors_path.replace(\"model.safetensors\",\"\")\n",
    "\n",
    "print(f\"Safetensors path: {safetensors_path}\")\n",
    "\n",
    "orig_vocab_size = tokenizer.vocab_size\n",
    "\n",
    "tt_model_factory = TransformerModelFactory(model_yaml)\n",
    "tt_model_factory.transformer_config.vocab_size = orig_vocab_size\n",
    "\n",
    "max_sequence_length = tt_model_factory.transformer_config.max_sequence_length\n",
    "\n",
    "tt_model = tt_model_factory.create_model()\n",
    "tt_model.load_from_safetensors(safetensors_path)\n",
    "tt_model\n",
    "\n",
    "padded_vocab_size = round_up_to_tile(orig_vocab_size, 32)\n",
    "\n",
    "if orig_vocab_size != padded_vocab_size:\n",
    "    print(f\"Padding vocab size for tilization: original {orig_vocab_size} -> padded {padded_vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc80055",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Model Setup WITHOUT KV Cache\n",
    "\n",
    "This section sets up a model that does NOT use KV cache. Each generation step performs a full forward pass through the entire sequence, which is slower but simpler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40913e88",
   "metadata": {},
   "source": [
    "`generate_with_tt()` uses TT hardware acceleration to generate output from the chosen LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e6550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_causal_mask(T: int) -> ttml.autograd.Tensor:\n",
    "    # [1,1,T,T] float32 with 1s for allowed positions (i >= j), else 0\\n\",\n",
    "    m = np.tril(np.ones((T, T), dtype=np.float32))\n",
    "    return ttml.autograd.Tensor.from_numpy(m.reshape(1, 1, T, T), ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)\n",
    "\n",
    "def build_logits_mask(vocab_size: int, padded_vocab_size: int) -> ttml.autograd.Tensor:\n",
    "    logits_mask = np.zeros((1, 1, 1, padded_vocab_size), dtype=np.float32)\n",
    "    logits_mask[:, :, :, vocab_size:] = 1e4\n",
    "    return ttml.autograd.Tensor.from_numpy(logits_mask, ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)   # [1,1,1,T], bfloat16\"\n",
    "\n",
    "def generate_with_tt(model, prompt_tokens):\n",
    "    \"\"\"Generate text without KV cache (full sequence forward pass each step).\"\"\"\n",
    "    import time\n",
    "    \n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(ttml.autograd.GradMode.DISABLED)\n",
    "    model.eval()\n",
    "\n",
    "    logits_mask_tensor = None\n",
    "\n",
    "    if padded_vocab_size != orig_vocab_size:\n",
    "        logits_mask_tensor = build_logits_mask(orig_vocab_size, padded_vocab_size)\n",
    "\n",
    "    causal_mask = build_causal_mask(max_sequence_length)  # [1,1,seq_len,seq_len], float32\n",
    "    padded_prompt_tokens = np.zeros((1, 1, 1, max_sequence_length), \n",
    "                                    dtype=np.uint32)\n",
    "\n",
    "    start_idx = 0\n",
    "    prompt_len = len(prompt_tokens)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Running Inference WITHOUT KV Cache (Full Forward Pass Each Step)\")\n",
    "    print(f\"Prompt tokens: {prompt_tokens[:10]}{'...' if len(prompt_tokens) > 10 else ''}\")\n",
    "    print(f\"Prompt length: {prompt_len}\")\n",
    "    print(f\"Max new tokens: {OUTPUT_TOKENS}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nGenerated text:\")\n",
    "    print(\"************************************\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    generated_tokens = prompt_tokens.copy()\n",
    "    \n",
    "    for token_idx in range(OUTPUT_TOKENS):\n",
    "\n",
    "        if len(prompt_tokens) > max_sequence_length:\n",
    "            start_idx = len(prompt_tokens) - max_sequence_length\n",
    "\n",
    "        # padded_prompt_tokens[0, 0, 0, :transformer_cfg[\"max_sequence_length\"]] = 0\n",
    "        padded_prompt_tokens[0, 0, 0, :len(prompt_tokens)] = prompt_tokens[start_idx:]\n",
    "        padded_prompt_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "            padded_prompt_tokens,\n",
    "            ttml.Layout.ROW_MAJOR,\n",
    "            ttml.autograd.DataType.UINT32)  # [1,1,1, max_seq_len], uint32\n",
    "\n",
    "        logits = model(padded_prompt_tensor, causal_mask)  # out=[1,1,seq_len, vocab_size], bf16\n",
    "\n",
    "\n",
    "        next_token_tensor = ttml.ops.sample.sample_op(logits, TEMPERATURE, np.random.randint(low=1e7), logits_mask_tensor)  # out=[1,1,seq_len,1], uint32\n",
    "\n",
    "        next_token_idx = max_sequence_length - 1 if len(prompt_tokens) > max_sequence_length else len(prompt_tokens) - 1\n",
    "        next_token = int(next_token_tensor.to_numpy().flatten()[next_token_idx])\n",
    "        generated_tokens.append(next_token)\n",
    "\n",
    "        output = tokenizer.decode([next_token], skip_special_tokens=False)\n",
    "\n",
    "        prompt_tokens.append(next_token)\n",
    "        print(output, end='', flush=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_duration_ms = (end_time - start_time) * 1000\n",
    "    new_tokens = len(prompt_tokens) - prompt_len\n",
    "    \n",
    "    print(\"\\n************************************\")\n",
    "    print(\"\\n=== GENERATION SUMMARY ===\")\n",
    "    print(f\"Total tokens generated: {len(prompt_tokens)}\")\n",
    "    print(f\"  Prompt: {prompt_len} tokens\")\n",
    "    print(f\"  New: {new_tokens} tokens\")\n",
    "    print(f\"\\nTotal time: {total_duration_ms:.2f} ms\")\n",
    "    print(f\"Average time per token: {total_duration_ms / new_tokens if new_tokens > 0 else 0:.2f} ms\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n\")\n",
    "    print(\"Final result:\")\n",
    "    print(tokenizer.decode(generated_tokens, skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08208b1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Model Setup WITH KV Cache\n",
    "\n",
    "This section sets up a model that uses KV cache for efficient inference. The KV cache stores previously computed key-value pairs from the attention mechanism, allowing each generation step to only process the newly generated token instead of recomputing the entire sequence. This significantly speeds up generation, especially for longer sequences.\n",
    "\n",
    "**Key differences from non-cache generation:**\n",
    "\n",
    "- **Prefill phase**: First step processes the entire prompt and stores KV pairs in cache\n",
    "- **Decode phase**: Subsequent steps only process the last generated token, reusing cached KV pairs\n",
    "- **Performance**: Much faster than non-cache generation, with speedup increasing as sequence length grows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac18600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_SIZE = 32\n",
    "\n",
    "def round_up_to_tile(value: int) -> int:\n",
    "    \"\"\"Round up to nearest multiple of TILE_SIZE.\"\"\"\n",
    "    return ((value + TILE_SIZE - 1) // TILE_SIZE) * TILE_SIZE\n",
    "\n",
    "def create_causal_mask_kv_cache(query_seq_len: int, prompt_len: int = 0) -> ttml.autograd.Tensor:\n",
    "    \"\"\"Create a causal attention mask for autoregressive generation with KV cache.\n",
    "    \n",
    "    This matches the C++ implementation exactly.\n",
    "    \n",
    "    Args:\n",
    "        query_seq_len: Length of query sequence\n",
    "        prompt_len: Length of prompt (for decode mode, this is the cache position)\n",
    "    \n",
    "    Returns:\n",
    "        Causal mask tensor\n",
    "    \"\"\"\n",
    "    whole_seq_len = prompt_len + query_seq_len\n",
    "    padded_query_len = round_up_to_tile(query_seq_len)\n",
    "    padded_whole_len = round_up_to_tile(whole_seq_len)\n",
    "    \n",
    "    # Mask shape: [padded_query_len, padded_whole_len] - query_len x key_len\n",
    "    mask_data = np.zeros((padded_query_len, padded_whole_len), dtype=np.float32)\n",
    "    \n",
    "    # Fill mask: token i can attend to positions 0 through i + prompt_len (inclusive)\n",
    "    # This matches C++: for (uint32_t j = 0; j <= i + prompt_len; ++j)\n",
    "    # range(n) gives [0, 1, 2, ..., n-1], so range(prompt_len + i + 1) gives [0, 1, 2, ..., prompt_len + i]\n",
    "    for i in range(query_seq_len):\n",
    "        for j in range(prompt_len + i + 1):\n",
    "            mask_data[i, j] = 1.0\n",
    "    \n",
    "    # Reshape to [1, 1, padded_query_len, padded_whole_len]\n",
    "    mask_data = mask_data.reshape(1, 1, padded_query_len, padded_whole_len)\n",
    "    mask_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "        mask_data,\n",
    "        layout=ttml.Layout.TILE,\n",
    "        new_type=ttml.autograd.DataType.BFLOAT16\n",
    "    )\n",
    "    \n",
    "    return mask_tensor\n",
    "\n",
    "\n",
    "def tokens_to_tensor_kv_cache(tokens: list) -> ttml.autograd.Tensor:\n",
    "    \"\"\"Create tensor from token IDs with padding to nearest multiple of 32.\n",
    "    \n",
    "    This matches the C++ implementation exactly.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of token IDs\n",
    "    \n",
    "    Returns:\n",
    "        Token tensor with padding\n",
    "    \"\"\"\n",
    "    actual_len = len(tokens)\n",
    "    padded_len = round_up_to_tile(actual_len)\n",
    "    \n",
    "    # Pad tokens with zeros to reach padded length\n",
    "    padded_tokens = np.zeros(padded_len, dtype=np.uint32)\n",
    "    for i in range(actual_len):\n",
    "        padded_tokens[i] = tokens[i]\n",
    "    \n",
    "    # Reshape to [1, 1, 1, padded_len]\n",
    "    padded_tokens = padded_tokens.reshape(1, 1, 1, padded_len)\n",
    "    tokens_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "        padded_tokens,\n",
    "        layout=ttml.Layout.ROW_MAJOR,\n",
    "        new_type=ttml.autograd.DataType.UINT32\n",
    "    )\n",
    "    \n",
    "    return tokens_tensor\n",
    "\n",
    "\n",
    "def sample_token_from_logits(logits: ttml.autograd.Tensor, position: int) -> int:\n",
    "    \"\"\"Sample next token using greedy decoding (argmax).\n",
    "    \n",
    "    Args:\n",
    "        logits: Logits tensor\n",
    "        position: Position to sample from\n",
    "    \n",
    "    Returns:\n",
    "        Token ID with highest logit\n",
    "    \"\"\"\n",
    "    logits_np = logits.to_numpy()\n",
    "    logits_host = logits_np.flatten()\n",
    "    \n",
    "    shape = logits.shape()\n",
    "    vocab_size = shape[-1]\n",
    "    last_token_offset = (position - 1) * vocab_size\n",
    "    \n",
    "    # Find token with highest logit value\n",
    "    max_idx = 0\n",
    "    max_val = logits_host[last_token_offset]\n",
    "    \n",
    "    for i in range(1, vocab_size):\n",
    "        val = logits_host[last_token_offset + i]\n",
    "        if val > max_val:\n",
    "            max_val = val\n",
    "            max_idx = i\n",
    "    \n",
    "    return max_idx\n",
    "\n",
    "\n",
    "def generate_with_tt_kv_cache(model, prompt_tokens, transformer_config, use_sampling=True):\n",
    "    \"\"\"Generate text with KV cache for efficient inference.\n",
    "    \n",
    "    Args:\n",
    "        model: LLaMA model instance\n",
    "        prompt_tokens: Initial prompt token IDs\n",
    "        transformer_config: Model config with num_blocks, num_groups, embedding_dim, max_sequence_length\n",
    "        use_sampling: Whether to use temperature sampling (if False, uses greedy decoding)\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(ttml.autograd.GradMode.DISABLED)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create KV cache\n",
    "    batch_size = 1\n",
    "    num_layers = transformer_config.num_blocks\n",
    "    num_groups = transformer_config.num_groups\n",
    "    max_seq_len = transformer_config.max_sequence_length\n",
    "    head_dim = transformer_config.embedding_dim // transformer_config.num_heads\n",
    "    \n",
    "    kv_cache_config = ttml.models.KvCacheConfig(\n",
    "        num_layers, batch_size, num_groups, max_seq_len, head_dim\n",
    "    )\n",
    "    kv_cache = ttml.models.KvCache(kv_cache_config)\n",
    "    \n",
    "    # Reset KV cache for new sequence\n",
    "    kv_cache.reset()\n",
    "    \n",
    "    generated_tokens = prompt_tokens.copy()\n",
    "    prompt_len = len(prompt_tokens)\n",
    "    \n",
    "    logits_mask_tensor = None\n",
    "    if padded_vocab_size != orig_vocab_size:\n",
    "        logits_mask_tensor = build_logits_mask(orig_vocab_size, padded_vocab_size)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"Running Inference with KV Cache\")\n",
    "    print(f\"Prompt tokens: {prompt_tokens[:10]}{'...' if len(prompt_tokens) > 10 else ''}\")\n",
    "    print(f\"Prompt length: {prompt_len}\")\n",
    "    print(f\"Max new tokens: {OUTPUT_TOKENS}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nGenerated text:\")\n",
    "    print(\"************************************\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate tokens one by one\n",
    "    for step in range(min(OUTPUT_TOKENS, max_seq_len - prompt_len)):\n",
    "        # For first step (prefill): use all prompt tokens\n",
    "        # For subsequent steps (decode): use only the last generated token  \n",
    "        processed_tokens = 0\n",
    "        if kv_cache.get_cache_position() == 0:\n",
    "            # Prefill: process entire prompt\n",
    "            input_tokens = generated_tokens\n",
    "        else:\n",
    "            # Decode: process only last token\n",
    "            input_tokens = [generated_tokens[-1]]\n",
    "            processed_tokens = len(generated_tokens)-1\n",
    "        \n",
    "        token_tensor = tokens_to_tensor_kv_cache(input_tokens)\n",
    "        \n",
    "        # Create causal mask\n",
    "        # For prefill: query_len = prompt_len, prompt_len = 0 (all tokens can attend to previous)\n",
    "        # For decode: query_len = 1, prompt_len = cache_position (new token can attend to all cached tokens)\n",
    "        # This matches C++: create_causal_mask(device, input_tokens.size(), processed_tokens)\n",
    "        mask = create_causal_mask_kv_cache(len(input_tokens), processed_tokens)\n",
    "        new_tokens = len(input_tokens)\n",
    "        logits = model(token_tensor, mask, kv_cache=kv_cache, new_tokens=new_tokens)\n",
    "        \n",
    "        # Sample next token\n",
    "        # The logits tensor has shape [1, 1, seq_len, vocab_size] where seq_len may be padded\n",
    "        # We need to extract the token at the last actual position (len(input_tokens) - 1)\n",
    "        if use_sampling:\n",
    "            next_token_tensor = ttml.ops.sample.sample_op(\n",
    "                logits, TEMPERATURE, np.random.randint(low=1e7), logits_mask_tensor\n",
    "            )\n",
    "            next_token_idx = len(input_tokens) - 1\n",
    "            next_token = int(next_token_tensor.to_numpy().flatten()[next_token_idx])\n",
    "        else:\n",
    "            # Greedy decoding - extract logits at last position and find argmax\n",
    "            next_token = int(sample_token_from_logits(logits, len(input_tokens)))\n",
    "        \n",
    "        output = tokenizer.decode([next_token], skip_special_tokens=False)\n",
    "        generated_tokens.append(next_token)\n",
    "        print(output, end='', flush=True)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_duration_ms = (end_time - start_time) * 1000\n",
    "    new_tokens = len(generated_tokens) - prompt_len\n",
    "\n",
    "    kv_cache.reset()\n",
    "    \n",
    "    print(\"\\n************************************\")\n",
    "    print(\"\\n=== GENERATION SUMMARY ===\")\n",
    "    print(f\"Total tokens generated: {len(generated_tokens)}\")\n",
    "    print(f\"  Prompt: {prompt_len} tokens\")\n",
    "    print(f\"  New: {new_tokens} tokens\")\n",
    "    print(f\"\\nTotal time: {total_duration_ms:.2f} ms\")\n",
    "    print(f\"Average time per token: {total_duration_ms / new_tokens if new_tokens > 0 else 0:.2f} ms\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n\")\n",
    "    print(\"Final result:\")\n",
    "    print(tokenizer.decode(generated_tokens, skip_special_tokens=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c461c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Generation Examples\n",
    "\n",
    "### 3.1: Generation WITHOUT KV Cache\n",
    "\n",
    "Examples using the non-cache model. These will be slower but demonstrate the baseline approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc2d1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with TT (WITHOUT KV Cache):\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tt_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m prompt_tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt_str)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating with TT (WITHOUT KV Cache):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m generate_with_tt(\u001b[43mtt_model\u001b[49m, prompt_tokens\u001b[38;5;241m.\u001b[39mcopy())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tt_model' is not defined"
     ]
    }
   ],
   "source": [
    "prompt_str = \"The difference between cats and dogs is:\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with TT (WITHOUT KV Cache):\")\n",
    "generate_with_tt(tt_model, prompt_tokens.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2c593e",
   "metadata": {},
   "source": [
    "### 3.2: Generation WITH KV Cache \n",
    "\n",
    "Now let's generate text using the KV cache-enabled model. This will be much faster than the regular generation, especially for longer sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5757315a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with TT (KV Cache):\n",
      "================================================================================\n",
      "Running Inference with KV Cache\n",
      "Prompt tokens: [1, 11644, 526, 366, 29973]\n",
      "Prompt length: 5\n",
      "Max new tokens: 750\n",
      "================================================================================\n",
      "\n",
      "Generated text:\n",
      "************************************\n",
      "\n",
      "Initializing KV cache:\n",
      "    Batch size: 1\n",
      "    Num layers: 22\n",
      "    Num groups: 4\n",
      "    Max sequence length: 1024\n",
      "    Head dim: 64\n",
      "KV cache initialized successfully\n",
      "\n",
      "Char"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "les:Iamagenie.\n",
      "\n",
      "Narrator:Agenie?Whatdoesthatevenmean?\n",
      "\n",
      "Charles:Agenieisacreatureofpureanduntamedenergy,createdbymagic.\n",
      "\n",
      "Narrator:Andwhatmagiccanyouperform?\n",
      "\n",
      "Charles:Icangrantwishes.\n",
      "\n",
      "Narrator:Buthow?\n",
      "\n",
      "Charles:Bygrantingwishes,youcanchangeyourlifeinwaysyoucouldneverhaveimaginedbefore.\n",
      "\n",
      "Narrator:That'samazing!Canyougrantoneforme?\n",
      "\n",
      "Charles:Icangrantyouonewish,butremember,it'sjustonewish.\n",
      "\n",
      "Narrator:Okay,I'llthinkaboutit.\n",
      "\n",
      "Scene3:\n",
      "\n",
      "Lucasissittinginasmall,dimlylitroomwithnowindows.He'swearingalong-sleevedshirtandjeans.He'sfeelingnervousandunsure.\n",
      "\n",
      "Narrator:Welcometothesecretsociety,Lucas.\n",
      "\n",
      "Lucas:(sighs)Idon'tknowwhattoexpect.\n",
      "\n",
      "Narrator:You'rejoiningoneofthemostprestigiousandsecretiveorganizationsintheworld.\n",
      "\n",
      "Lucas:(confidently)I'mreadyforanything.\n",
      "\n",
      "Narrator:Butfirst,weneedtogothroughaninitiationprocess.\n",
      "\n",
      "Scene4:\n",
      "\n",
      "Lucasisstandinginfrontofalarge,ornatechest.He'sholdingasmallkeyinhishand.\n",
      "\n",
      "Narrator:Yourfirstorderofbusinessistoopenthischestandrevealitscontents.\n",
      "\n",
      "Lucas:(sighs)Okay,butwhatifIdon'tknowthecontents?\n",
      "\n",
      "Narrator:You'llbetaughttherulesandsecretsofthesociety.\n",
      "\n",
      "Lucas:(confidently)I'mupforthechallenge.\n",
      "\n",
      "Narrator:Andthenextstepistocompleteaseriesofchallenges.\n",
      "\n",
      "Lucas:(confidently)I'mreadyforanything.\n",
      "\n",
      "Scene5:\n",
      "\n",
      "Lucasissittinginadarkroomwithagroupofothermembers.They'resittingcross-leggedonthefloor.\n",
      "\n",
      "Narrator:Welcometothecenterofthesociety.\n",
      "\n",
      "Lucas:(confidently)I'veheardaboutthisplacebefore.\n",
      "\n",
      "Narrator:Thisisthecenterofthesociety,wheremembersgathertostudyandlearnfromeachother.\n",
      "\n",
      "Scene6:\n",
      "\n",
      "Lucasissittinginalarge,ornateroomwithagroupofothermembers.They'resittingaroundalargetable,eatinganelaboratemeal.\n",
      "\n",
      "Narrator:Welcometothemeetings.\n",
      "\n",
      "Lucas:(confidently)I'vealwaysbeeninterestedinthesociety'straditions.\n",
      "\n",
      "Narrator:Thesearethetraditionsthatgovernthesociety.\n",
      "\n",
      "Lucas:(confidently)I'mreadyforanything.\n",
      "\n",
      "Narrator:Andthelaststepistobecomeamember.\n",
      "\n",
      "Lucas:(confidently)I'mreadyforanything.\n",
      "\n",
      "Scene7:\n",
      "\n",
      "Lucasissittinginadimlylitroomwithagroupofothermembers.They'resittingaroundatable,eatinganelaborate\n",
      "************************************\n",
      "\n",
      "=== GENERATION SUMMARY ===\n",
      "Total tokens generated: 755\n",
      "  Prompt: 5 tokens\n",
      "  New: 750 tokens\n",
      "\n",
      "Total time: 42566.28 ms\n",
      "Average time per token: 56.76 ms\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Final result:\n",
      "<s> Who are you?\n",
      "\n",
      "Charles: I am a genie.\n",
      "\n",
      "Narrator: A genie? What does that even mean?\n",
      "\n",
      "Charles: A genie is a creature of pure and untamed energy, created by magic.\n",
      "\n",
      "Narrator: And what magic can you perform?\n",
      "\n",
      "Charles: I can grant wishes.\n",
      "\n",
      "Narrator: But how?\n",
      "\n",
      "Charles: By granting wishes, you can change your life in ways you could never have imagined before.\n",
      "\n",
      "Narrator: That's amazing! Can you grant one for me?\n",
      "\n",
      "Charles: I can grant you one wish, but remember, it's just one wish.\n",
      "\n",
      "Narrator: Okay, I'll think about it.\n",
      "\n",
      "Scene 3:\n",
      "\n",
      "Lucas is sitting in a small, dimly lit room with no windows. He's wearing a long-sleeved shirt and jeans. He's feeling nervous and unsure.\n",
      "\n",
      "Narrator: Welcome to the secret society, Lucas.\n",
      "\n",
      "Lucas: (sighs) I don't know what to expect.\n",
      "\n",
      "Narrator: You're joining one of the most prestigious and secretive organizations in the world.\n",
      "\n",
      "Lucas: (confidently) I'm ready for anything.\n",
      "\n",
      "Narrator: But first, we need to go through an initiation process.\n",
      "\n",
      "Scene 4:\n",
      "\n",
      "Lucas is standing in front of a large, ornate chest. He's holding a small key in his hand.\n",
      "\n",
      "Narrator: Your first order of business is to open this chest and reveal its contents.\n",
      "\n",
      "Lucas: (sighs) Okay, but what if I don't know the contents?\n",
      "\n",
      "Narrator: You'll be taught the rules and secrets of the society.\n",
      "\n",
      "Lucas: (confidently) I'm up for the challenge.\n",
      "\n",
      "Narrator: And the next step is to complete a series of challenges.\n",
      "\n",
      "Lucas: (confidently) I'm ready for anything.\n",
      "\n",
      "Scene 5:\n",
      "\n",
      "Lucas is sitting in a dark room with a group of other members. They're sitting cross-legged on the floor.\n",
      "\n",
      "Narrator: Welcome to the center of the society.\n",
      "\n",
      "Lucas: (confidently) I've heard about this place before.\n",
      "\n",
      "Narrator: This is the center of the society, where members gather to study and learn from each other.\n",
      "\n",
      "Scene 6:\n",
      "\n",
      "Lucas is sitting in a large, ornate room with a group of other members. They're sitting around a large table, eating an elaborate meal.\n",
      "\n",
      "Narrator: Welcome to the meetings.\n",
      "\n",
      "Lucas: (confidently) I've always been interested in the society's traditions.\n",
      "\n",
      "Narrator: These are the traditions that govern the society.\n",
      "\n",
      "Lucas: (confidently) I'm ready for anything.\n",
      "\n",
      "Narrator: And the last step is to become a member.\n",
      "\n",
      "Lucas: (confidently) I'm ready for anything.\n",
      "\n",
      "Scene 7:\n",
      "\n",
      "Lucas is sitting in a dimly lit room with a group of other members. They're sitting around a table, eating an elaborate\n"
     ]
    }
   ],
   "source": [
    "prompt_str = \"Who are you?\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with TT (KV Cache):\")\n",
    "generate_with_tt_kv_cache(tt_model, prompt_tokens.copy(), tt_model_factory.transformer_config, use_sampling=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tenstorrent-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
