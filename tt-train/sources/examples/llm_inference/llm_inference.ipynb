{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7a20c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tokenizers safetensors\n",
    "\n",
    "import os, sys, math, random, textwrap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from transformers import GPT2Tokenizer\n",
    "from yaml import safe_load, Loader\n",
    "\n",
    "sys.path.append(f\"{os.environ['TT_METAL_HOME']}/tt-train/build/sources/ttml\")\n",
    "import _ttml as ttml\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed()\n",
    "# Change working directory to TT_METAL_HOME\n",
    "os.chdir(os.environ['TT_METAL_HOME'])\n",
    "\n",
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    n_head: int = 12\n",
    "    embed_dim: int = 768\n",
    "    dropout: float = 0.2\n",
    "    n_blocks : int = 12\n",
    "    vocab_size: int = 96\n",
    "    max_seq_len: int = 1024\n",
    "    runner_type: str = \"memory_efficient\"\n",
    "    weight_tying: str = \"enabled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea71a9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/tt-metal\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "print(os.getcwd())\n",
    "transformer_cfg = safe_load(open(\"tt-train/configs/training_shakespeare_gpt2s.yaml\", \"r\"))[\"training_config\"][\"transformer_config\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26b353af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_causal_mask(T: int) -> np.ndarray:\n",
    "    # [1,1,T,T] float32 with 1s for allowed positions (i >= j), else 0\n",
    "    m = np.tril(np.ones((T, T), dtype=np.float32))\n",
    "    return m.reshape(1, 1, T, T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd89028f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer configuration:\n",
      "    Vocab size: 50257\n",
      "    Max sequence length: 1024\n",
      "    Embedding dim: 768\n",
      "    Num heads: 12\n",
      "    Dropout probability: 0.2\n",
      "    Num blocks: 12\n",
      "    Positional embedding type: Trainable\n",
      "    Runner type: Default\n",
      "    Composite layernorm: false\n",
      "    Weight tying: Disabled\n"
     ]
    }
   ],
   "source": [
    "def create_model(cfg, vocab_size: int, seq_len: int):\n",
    "    # GPT2 config via your bindings\n",
    "    gcfg = ttml.models.gpt2.GPT2TransformerConfig()\n",
    "    gcfg.num_heads = cfg[\"num_heads\"]\n",
    "    gcfg.embedding_dim = cfg[\"embedding_dim\"]\n",
    "    gcfg.num_blocks = cfg[\"num_blocks\"]\n",
    "    gcfg.vocab_size = int(vocab_size)\n",
    "    gcfg.max_sequence_length = seq_len\n",
    "    gcfg.dropout_prob = cfg[\"dropout_prob\"]\n",
    "    # optional flags exist (runner_type, weight_tying, positional_embedding_type, experimental, ...)\n",
    "    # we keep defaults for a minimal demo\n",
    "\n",
    "    model = ttml.models.gpt2.create_gpt2_model(gcfg)\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "padded_vocab_size = ((tokenizer.vocab_size + 31) // 32) * 32\n",
    "model = create_model(transformer_cfg, vocab_size, transformer_cfg[\"max_sequence_length\"])\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8beb647",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m()\n\u001b[1;32m      3\u001b[0m logits_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, padded_vocab_size, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      4\u001b[0m logits_mask[:, :, :, vocab_size:] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e4\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "logits_mask = np.zeros(1, 1, 1, padded_vocab_size, dtype=np.float32)\n",
    "logits_mask[:, :, :, vocab_size:] = 1e4\n",
    "\n",
    "logits_mask_tensor = ttml.autograd.Tensor.from_numpy(logits_mask, ttml.Layout.ROW_MAJOR, ttml.autograd.DataType.BFLOAT16)   # [1,1,1,T], float32\n",
    "\n",
    "\n",
    "while True:\n",
    "    logits = model()\n",
    "    ttml.ops.sample.sample_op(logits, 1.0, ttml.autograd.AutoContext.get_generator()(),logits_mask_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
