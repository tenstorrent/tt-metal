{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8744d20",
   "metadata": {},
   "source": [
    "### LLM Inference Example\n",
    "\n",
    "This notebook contains a basic inference example for using our `ttml` Python API to build, load, and run a large language model from Hugging Face on our TT hardware. By default, it is set to create and load a GPT2 model, but this notebook can quickly and easily be edited to use any of the LLMs that the tt-train project currently supports. \n",
    "\n",
    "Below, in the first cell, we have our imports and basic directory housekeeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a20c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "import numpy as np  # For numpy arrays\n",
    "from dataclasses import dataclass # For configuration classes\n",
    "from huggingface_hub import hf_hub_download # To download safetensors from Hugging Face\n",
    "from transformers import AutoTokenizer\n",
    "from yaml import safe_load # To read YAML configs\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(f\"{os.getenv('TT_METAL_RUNTIME_ROOT')}/tt-train/sources/ttml\")\n",
    "import ttml\n",
    "from ttml.common.config import get_training_config, load_config\n",
    "from ttml.common.utils import set_seed, round_up_to_tile\n",
    "from ttml.common.model_factory import TransformerModelFactory\n",
    "\n",
    "# Change working directory to tt-train\n",
    "os.chdir(f\"{os.environ['TT_METAL_RUNTIME_ROOT']}/tt-train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e005a45c",
   "metadata": {},
   "source": [
    "Use the cell below to change global parameters in this notebook. \n",
    "\n",
    "`OUTPUT_TOKENS` : the length of the generated text in token (not characters!) \n",
    "\n",
    "`TEMPERATURE`   : sampling temperature; set to 0 to disable sampling in `generate_with_tt()`\n",
    "\n",
    "`SEED`          : randomization seed (for reproducibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d6dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TOKENS = 750\n",
    "WITH_SAMPLING = True\n",
    "TEMPERATURE = 0.8\n",
    "SEED = 42\n",
    "CONFIG = \"training_shakespeare_tinyllama.yaml\"\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "model_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9b032f",
   "metadata": {},
   "source": [
    "While the notebook is currently configured for GPT2, you can quickly change the tokenizer you want to use by changing the input to `from_pretrained()` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea71a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from Hugging Face and the transformer config from YAML\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "training_config = get_training_config(CONFIG)\n",
    "model_yaml = load_config(training_config.model_config, configs_root=os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5415d24d",
   "metadata": {},
   "source": [
    "As above, the call to `hf_hub_download()` will download (or otherwise find on your local system) the SafeTensors model weight file for GPT2, but can be updated to download other SafeTensors files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6965256c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safetensors path: /home/ubuntu/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/\n",
      "Llama configuration:\n",
      "    Vocab size: 32000\n",
      "    Max sequence length: 1024\n",
      "    Embedding dim: 2048\n",
      "    Intermediate dim: None\n",
      "    Num heads: 32\n",
      "    Num groups: 4\n",
      "    Dropout probability: 0\n",
      "    Num blocks: 22\n",
      "    Positional embedding type: RoPE\n",
      "    Runner type: Memory efficient\n",
      "    Weight tying: Disabled\n",
      "    Theta: 10000\n",
      "2026-01-05 17:30:40.132 | info     |             UMD | Starting topology discovery. (topology_discovery.cpp:69)\n",
      "2026-01-05 17:30:40.137 | info     |             UMD | Established firmware bundle version: 19.0.0 (topology_discovery.cpp:363)\n",
      "2026-01-05 17:30:40.137 | info     |             UMD | Established ETH FW version: 7.2.0 (topology_discovery_wormhole.cpp:324)\n",
      "2026-01-05 17:30:40.137 | info     |             UMD | Completed topology discovery. (topology_discovery.cpp:73)\n",
      "2026-01-05 17:30:40.138 | info     |          Device | Opening user mode device driver (tt_cluster.cpp:214)\n",
      "2026-01-05 17:30:40.138 | info     |             UMD | Starting topology discovery. (topology_discovery.cpp:69)\n",
      "2026-01-05 17:30:40.141 | info     |             UMD | Established firmware bundle version: 19.0.0 (topology_discovery.cpp:363)\n",
      "2026-01-05 17:30:40.141 | info     |             UMD | Established ETH FW version: 7.2.0 (topology_discovery_wormhole.cpp:324)\n",
      "2026-01-05 17:30:40.141 | info     |             UMD | Completed topology discovery. (topology_discovery.cpp:73)\n",
      "2026-01-05 17:30:40.142 | info     |             UMD | Starting topology discovery. (topology_discovery.cpp:69)\n",
      "2026-01-05 17:30:40.161 | info     |             UMD | Established firmware bundle version: 19.0.0 (topology_discovery.cpp:363)\n",
      "2026-01-05 17:30:40.161 | info     |             UMD | Established ETH FW version: 7.2.0 (topology_discovery_wormhole.cpp:324)\n",
      "2026-01-05 17:30:40.162 | info     |             UMD | Completed topology discovery. (topology_discovery.cpp:73)\n",
      "2026-01-05 17:30:40.162 | info     |             UMD | Harvesting masks for chip 0 tensix: 0x4 dram: 0x0 eth: 0x0 pcie: 0x0 l2cpu: 0x0 (cluster.cpp:358)\n",
      "2026-01-05 17:30:40.192 | warning  |             UMD | init_detect_tt_device_numanodes(): Could not determine NumaNodeSet for TT device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:564)\n",
      "2026-01-05 17:30:40.192 | warning  |             UMD | Could not find NumaNodeSet for TT Device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:173)\n",
      "2026-01-05 17:30:40.192 | warning  |             UMD | bind_area_memory_nodeset(): Unable to determine TT Device to NumaNode mapping for physical_device_id: 0. Skipping membind. (cpuset_lib.cpp:450)\n",
      "2026-01-05 17:30:40.192 | warning  |             UMD | ---- ttSiliconDevice::init_hugepage: bind_area_to_memory_nodeset() failed (physical_device_id: 0 ch: 0). Hugepage allocation is not on NumaNode matching TT Device. Side-Effect is decreased Device->Host perf (Issue #893). (sysmem_manager.cpp:219)\n",
      "2026-01-05 17:30:40.193 | info     |             UMD | Opening local chip ids/PCIe ids: {0}/[0] and remote chip ids {} (cluster.cpp:202)\n",
      "2026-01-05 17:30:40.193 | info     |             UMD | IOMMU: disabled (cluster.cpp:178)\n",
      "2026-01-05 17:30:40.193 | info     |             UMD | KMD version: 2.4.1 (cluster.cpp:181)\n",
      "2026-01-05 17:30:40.197 | info     |             UMD | Mapped hugepage 0x280000000 to NOC address 0x800000000 (sysmem_manager.cpp:247)\n",
      "2026-01-05 17:30:40.218 | info     |     Distributed | Using auto discovery to generate mesh graph. (metal_context.cpp:765)\n",
      "2026-01-05 17:30:40.218 | info     |     Distributed | Constructing control plane using auto-discovery (no mesh graph descriptor). (metal_context.cpp:742)\n",
      "2026-01-05 17:30:40.218 | info     |          Fabric | TopologyMapper mapping start (mesh=0): n_log=1, n_phys=1, log_deg_hist={0:1}, phys_deg_hist={0:1} (topology_mapper_utils.cpp:171)\n",
      "2026-01-05 17:30:40.218 | info     |          Fabric | TopologyMapper mapping start (mesh=0): n_log=1, n_phys=1, log_deg_hist={0:1}, phys_deg_hist={0:1} (topology_mapper_utils.cpp:171)\n",
      "2026-01-05 17:30:40.257 | info     |           Metal | DPRINT enabled on device 0, worker core (x=0,y=0) (virtual (x=18,y=18)). (dprint_server.cpp:688)\n",
      "2026-01-05 17:30:40.257 | info     |           Metal | DPRINT Server attached device 0 (dprint_server.cpp:735)\n",
      "Loading model from: /home/ubuntu/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/model.safetensors\n",
      "Loading tensor: lm_head.weight, shape:Shape([32000, 2048]), dtype:BF16\n",
      "Using parameter: llama/fc/weight with shape: Shape([1, 1, 32000, 2048])\n",
      "Loading tensor: model.embed_tokens.weight, shape:Shape([32000, 2048]), dtype:BF16\n",
      "Using parameter: llama/tok_emb/weight with shape: Shape([1, 1, 32000, 2048])\n",
      "Loading tensor: model.layers.0.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.0.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.0.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.0.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.0.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.0.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.0.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.0.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.0.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 0\n",
      "Loading tensor: model.layers.1.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.1.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.1.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.1.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.1.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.1.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.1.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.1.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.1.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 1\n",
      "Loading tensor: model.layers.10.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.10.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.10.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.10.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.10.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.10.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.10.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.10.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.10.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 10\n",
      "Loading tensor: model.layers.11.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.11.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.11.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.11.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.11.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.11.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.11.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.11.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.11.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 11\n",
      "Loading tensor: model.layers.12.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.12.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.12.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.12.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.12.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.12.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.12.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.12.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.12.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 12\n",
      "Loading tensor: model.layers.13.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.13.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.13.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.13.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.13.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.13.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.13.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.13.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.13.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 13\n",
      "Loading tensor: model.layers.14.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.14.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.14.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.14.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.14.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.14.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.14.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.14.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.14.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 14\n",
      "Loading tensor: model.layers.15.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.15.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.15.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.15.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.15.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.15.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.15.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.15.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.15.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 15\n",
      "Loading tensor: model.layers.16.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_16/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.16.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_16/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.16.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_16/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.16.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_16/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.16.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_16/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.16.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.16.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_16/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.16.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_16/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.16.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_16/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 16\n",
      "Loading tensor: model.layers.17.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_17/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.17.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_17/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.17.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_17/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.17.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_17/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.17.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_17/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.17.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.17.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_17/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.17.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_17/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.17.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_17/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 17\n",
      "Loading tensor: model.layers.18.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_18/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.18.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_18/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.18.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_18/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.18.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_18/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.18.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_18/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.18.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.18.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_18/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.18.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_18/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.18.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_18/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 18\n",
      "Loading tensor: model.layers.19.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_19/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.19.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_19/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.19.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_19/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.19.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_19/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.19.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_19/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.19.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.19.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_19/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.19.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_19/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.19.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_19/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 19\n",
      "Loading tensor: model.layers.2.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.2.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.2.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.2.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.2.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.2.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.2.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.2.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.2.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 2\n",
      "Loading tensor: model.layers.20.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_20/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.20.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_20/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.20.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_20/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.20.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_20/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.20.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_20/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.20.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.20.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_20/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.20.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_20/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.20.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_20/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 20\n",
      "Loading tensor: model.layers.21.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_21/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.21.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_21/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.21.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_21/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.21.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_21/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.21.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_21/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.21.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.21.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_21/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.21.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_21/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.21.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_21/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 21\n",
      "Loading tensor: model.layers.3.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.3.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.3.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.3.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.3.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.3.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.3.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.3.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.3.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 3\n",
      "Loading tensor: model.layers.4.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.4.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.4.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.4.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.4.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.4.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.4.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.4.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.4.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 4\n",
      "Loading tensor: model.layers.5.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.5.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.5.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.5.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.5.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.5.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.5.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.5.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.5.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 5\n",
      "Loading tensor: model.layers.6.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.6.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.6.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.6.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.6.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.6.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.6.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.6.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.6.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 6\n",
      "Loading tensor: model.layers.7.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.7.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.7.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.7.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.7.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.7.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.7.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.7.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.7.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 7\n",
      "Loading tensor: model.layers.8.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.8.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.8.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.8.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.8.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.8.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.8.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.8.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.8.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 8\n",
      "Loading tensor: model.layers.9.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.9.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.9.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.9.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.9.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.9.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.9.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.9.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.9.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 9\n",
      "Loading tensor: model.norm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/ln_fc/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "All 179 parameters were successfully loaded and used.\n"
     ]
    }
   ],
   "source": [
    "# # Get safetensors\n",
    "safetensors_path = hf_hub_download(repo_id=model_path, filename=\"model.safetensors\")\n",
    "safetensors_path = safetensors_path.replace(\"model.safetensors\",\"\")\n",
    "\n",
    "print(f\"Safetensors path: {safetensors_path}\")\n",
    "\n",
    "orig_vocab_size = tokenizer.vocab_size\n",
    "\n",
    "tt_model_factory = TransformerModelFactory(model_yaml)\n",
    "tt_model_factory.transformer_config.vocab_size = orig_vocab_size\n",
    "\n",
    "max_sequence_length = tt_model_factory.transformer_config.max_sequence_length\n",
    "\n",
    "tt_model = tt_model_factory.create_model()\n",
    "tt_model.load_from_safetensors(safetensors_path)\n",
    "tt_model\n",
    "\n",
    "padded_vocab_size = round_up_to_tile(orig_vocab_size, 32)\n",
    "\n",
    "if orig_vocab_size != padded_vocab_size:\n",
    "    print(f\"Padding vocab size for tilization: original {orig_vocab_size} -> padded {padded_vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc80055",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Dummy generate function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40913e88",
   "metadata": {},
   "source": [
    "`generate_with_tt()` uses TT hardware acceleration to generate output from the chosen LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18e6550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_causal_mask(T: int) -> ttml.autograd.Tensor:\n",
    "    # [1,1,T,T] float32 with 1s for allowed positions (i >= j), else 0\\n\",\n",
    "    m = np.tril(np.ones((T, T), dtype=np.float32))\n",
    "    return ttml.autograd.Tensor.from_numpy(m.reshape(1, 1, T, T), ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)\n",
    "\n",
    "def build_logits_mask(vocab_size: int, padded_vocab_size: int) -> ttml.autograd.Tensor:\n",
    "    logits_mask = np.zeros((1, 1, 1, padded_vocab_size), dtype=np.float32)\n",
    "    logits_mask[:, :, :, vocab_size:] = 1e4\n",
    "    return ttml.autograd.Tensor.from_numpy(logits_mask, ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)   # [1,1,1,T], bfloat16\"\n",
    "\n",
    "def generate_with_tt(model, prompt_tokens):\n",
    "    \"\"\"Generate text without KV cache (full sequence forward pass each step).\"\"\"\n",
    "    import time\n",
    "    \n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(ttml.autograd.GradMode.DISABLED)\n",
    "    model.eval()\n",
    "\n",
    "    logits_mask_tensor = None\n",
    "\n",
    "    if padded_vocab_size != orig_vocab_size:\n",
    "        logits_mask_tensor = build_logits_mask(orig_vocab_size, padded_vocab_size)\n",
    "\n",
    "    causal_mask = build_causal_mask(max_sequence_length)  # [1,1,seq_len,seq_len], float32\n",
    "    padded_prompt_tokens = np.zeros((1, 1, 1, max_sequence_length), \n",
    "                                    dtype=np.uint32)\n",
    "\n",
    "    start_idx = 0\n",
    "    prompt_len = len(prompt_tokens)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Running Inference WITHOUT KV Cache (Full Forward Pass Each Step)\")\n",
    "    print(f\"Prompt tokens: {prompt_tokens[:10]}{'...' if len(prompt_tokens) > 10 else ''}\")\n",
    "    print(f\"Prompt length: {prompt_len}\")\n",
    "    print(f\"Max new tokens: {OUTPUT_TOKENS}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nGenerated text:\")\n",
    "    print(\"************************************\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    generated_tokens = prompt_tokens.copy()\n",
    "    \n",
    "    for token_idx in tqdm(range(OUTPUT_TOKENS)):\n",
    "\n",
    "        if len(prompt_tokens) > max_sequence_length:\n",
    "            start_idx = len(prompt_tokens) - max_sequence_length\n",
    "\n",
    "        # padded_prompt_tokens[0, 0, 0, :transformer_cfg[\"max_sequence_length\"]] = 0\n",
    "        padded_prompt_tokens[0, 0, 0, :len(prompt_tokens)] = prompt_tokens[start_idx:]\n",
    "        padded_prompt_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "            padded_prompt_tokens,\n",
    "            ttnn.Layout.ROW_MAJOR,\n",
    "            ttnn.DataType.UINT32)  # [1,1,1, max_seq_len], uint32\n",
    "\n",
    "        logits = model(padded_prompt_tensor, causal_mask)  # out=[1,1,seq_len, vocab_size], bf16\n",
    "\n",
    "\n",
    "        next_token_tensor = ttml.ops.sample.sample_op(logits, TEMPERATURE, np.random.randint(low=1e7), logits_mask_tensor)  # out=[1,1,seq_len,1], uint32\n",
    "\n",
    "        next_token_idx = max_sequence_length - 1 if len(prompt_tokens) > max_sequence_length else len(prompt_tokens) - 1\n",
    "        next_token = int(next_token_tensor.to_numpy().flatten()[next_token_idx])\n",
    "        generated_tokens.append(next_token)\n",
    "\n",
    "        output = tokenizer.decode([next_token], skip_special_tokens=False)\n",
    "\n",
    "        prompt_tokens.append(next_token)\n",
    "        print(output, end='', flush=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_duration_ms = (end_time - start_time) * 1000\n",
    "    new_tokens = len(prompt_tokens) - prompt_len\n",
    "    \n",
    "    print(\"\\n************************************\")\n",
    "    print(\"\\n=== GENERATION SUMMARY ===\")\n",
    "    print(f\"Total tokens generated: {len(prompt_tokens)}\")\n",
    "    print(f\"  Prompt: {prompt_len} tokens\")\n",
    "    print(f\"  New: {new_tokens} tokens\")\n",
    "    print(f\"\\nTotal time: {total_duration_ms:.2f} ms\")\n",
    "    print(f\"Average time per token: {total_duration_ms / new_tokens if new_tokens > 0 else 0:.2f} ms\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n\")\n",
    "    print(\"Final result:\")\n",
    "    print(tokenizer.decode(generated_tokens, skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08208b1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: KV Cache generate function\n",
    "\n",
    "**Key differences from non-cache generation:**\n",
    "\n",
    "- **Prefill phase**: First step processes the entire prompt and stores KV pairs in cache\n",
    "- **Decode phase**: Subsequent steps only process the last generated token, reusing cached KV pairs\n",
    "- **Performance**: Much faster than non-cache generation, with speedup increasing as sequence length grows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac18600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_SIZE = 32\n",
    "\n",
    "def round_up_to_tile(value: int) -> int:\n",
    "    \"\"\"Round up to nearest multiple of TILE_SIZE.\"\"\"\n",
    "    return ((value + TILE_SIZE - 1) // TILE_SIZE) * TILE_SIZE\n",
    "\n",
    "def create_causal_mask_kv_cache(query_seq_len: int, prompt_len: int = 0) -> ttml.autograd.Tensor:\n",
    "    \"\"\"Create a causal attention mask for autoregressive generation with KV cache.\n",
    "    \n",
    "    This matches the C++ implementation exactly.\n",
    "    \n",
    "    Args:\n",
    "        query_seq_len: Length of query sequence\n",
    "        prompt_len: Length of prompt (for decode mode, this is the cache position)\n",
    "    \n",
    "    Returns:\n",
    "        Causal mask tensor\n",
    "    \"\"\"\n",
    "    whole_seq_len = prompt_len + query_seq_len\n",
    "    padded_query_len = round_up_to_tile(query_seq_len)\n",
    "    padded_whole_len = round_up_to_tile(whole_seq_len)\n",
    "    \n",
    "    # Mask shape: [padded_query_len, padded_whole_len] - query_len x key_len\n",
    "    mask_data = np.zeros((padded_query_len, padded_whole_len), dtype=np.float32)\n",
    "    \n",
    "    # Fill mask: token i can attend to positions 0 through i + prompt_len (inclusive)\n",
    "    # This matches C++: for (uint32_t j = 0; j <= i + prompt_len; ++j)\n",
    "    # range(n) gives [0, 1, 2, ..., n-1], so range(prompt_len + i + 1) gives [0, 1, 2, ..., prompt_len + i]\n",
    "    for i in range(query_seq_len):\n",
    "        for j in range(prompt_len + i + 1):\n",
    "            mask_data[i, j] = 1.0\n",
    "    \n",
    "    # Reshape to [1, 1, padded_query_len, padded_whole_len]\n",
    "    mask_data = mask_data.reshape(1, 1, padded_query_len, padded_whole_len)\n",
    "    mask_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "        mask_data,\n",
    "        layout=ttml.Layout.TILE,\n",
    "        new_type=ttml.autograd.DataType.BFLOAT16\n",
    "    )\n",
    "    \n",
    "    return mask_tensor\n",
    "\n",
    "\n",
    "def tokens_to_tensor_kv_cache(tokens: list) -> ttml.autograd.Tensor:\n",
    "    \"\"\"Create tensor from token IDs with padding to nearest multiple of 32.\n",
    "    \n",
    "    This matches the C++ implementation exactly.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of token IDs\n",
    "    \n",
    "    Returns:\n",
    "        Token tensor with padding\n",
    "    \"\"\"\n",
    "    actual_len = len(tokens)\n",
    "    padded_len = round_up_to_tile(actual_len)\n",
    "    \n",
    "    # Pad tokens with zeros to reach padded length\n",
    "    padded_tokens = np.zeros(padded_len, dtype=np.uint32)\n",
    "    for i in range(actual_len):\n",
    "        padded_tokens[i] = tokens[i]\n",
    "    \n",
    "    # Reshape to [1, 1, 1, padded_len]\n",
    "    padded_tokens = padded_tokens.reshape(1, 1, 1, padded_len)\n",
    "    tokens_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "        padded_tokens,\n",
    "        layout=ttml.Layout.ROW_MAJOR,\n",
    "        new_type=ttml.autograd.DataType.UINT32\n",
    "    )\n",
    "    \n",
    "    return tokens_tensor\n",
    "\n",
    "\n",
    "def sample_token_from_logits(logits: ttml.autograd.Tensor, position: int) -> int:\n",
    "    \"\"\"Sample next token using greedy decoding (argmax).\n",
    "    \n",
    "    Args:\n",
    "        logits: Logits tensor\n",
    "        position: Position to sample from\n",
    "    \n",
    "    Returns:\n",
    "        Token ID with highest logit\n",
    "    \"\"\"\n",
    "    logits_np = logits.to_numpy()\n",
    "    logits_host = logits_np.flatten()\n",
    "    \n",
    "    shape = logits.shape()\n",
    "    vocab_size = shape[-1]\n",
    "    last_token_offset = (position - 1) * vocab_size\n",
    "    \n",
    "    # Find token with highest logit value\n",
    "    max_idx = 0\n",
    "    max_val = logits_host[last_token_offset]\n",
    "    \n",
    "    for i in range(1, vocab_size):\n",
    "        val = logits_host[last_token_offset + i]\n",
    "        if val > max_val:\n",
    "            max_val = val\n",
    "            max_idx = i\n",
    "    \n",
    "    return max_idx\n",
    "\n",
    "\n",
    "def generate_with_tt_kv_cache(model, prompt_tokens, transformer_config, use_sampling=True):\n",
    "    \"\"\"Generate text with KV cache for efficient inference.\n",
    "    \n",
    "    Args:\n",
    "        model: LLaMA model instance\n",
    "        prompt_tokens: Initial prompt token IDs\n",
    "        transformer_config: Model config with num_blocks, num_groups, embedding_dim, max_sequence_length\n",
    "        use_sampling: Whether to use temperature sampling (if False, uses greedy decoding)\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(ttml.autograd.GradMode.DISABLED)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create KV cache\n",
    "    batch_size = 1\n",
    "    num_layers = transformer_config.num_blocks\n",
    "    num_groups = transformer_config.num_groups\n",
    "    max_seq_len = transformer_config.max_sequence_length\n",
    "    head_dim = transformer_config.embedding_dim // transformer_config.num_heads\n",
    "    \n",
    "    kv_cache_config = ttml.models.KvCacheConfig(\n",
    "        num_layers, batch_size, num_groups, max_seq_len, head_dim\n",
    "    )\n",
    "    kv_cache = ttml.models.KvCache(kv_cache_config)\n",
    "    \n",
    "    # Reset KV cache for new sequence\n",
    "    kv_cache.reset()\n",
    "    \n",
    "    generated_tokens = prompt_tokens.copy()\n",
    "    prompt_len = len(prompt_tokens)\n",
    "    \n",
    "    logits_mask_tensor = None\n",
    "    if padded_vocab_size != orig_vocab_size:\n",
    "        logits_mask_tensor = build_logits_mask(orig_vocab_size, padded_vocab_size)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"Running Inference with KV Cache\")\n",
    "    print(f\"Prompt tokens: {prompt_tokens[:10]}{'...' if len(prompt_tokens) > 10 else ''}\")\n",
    "    print(f\"Prompt length: {prompt_len}\")\n",
    "    print(f\"Max new tokens: {OUTPUT_TOKENS}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nGenerated text:\")\n",
    "    print(\"************************************\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate tokens one by one\n",
    "    for step in tqdm(range(min(OUTPUT_TOKENS, max_seq_len - prompt_len))):\n",
    "        # For first step (prefill): use all prompt tokens\n",
    "        # For subsequent steps (decode): use only the last generated token  \n",
    "        processed_tokens = 0\n",
    "        if kv_cache.get_cache_position() == 0:\n",
    "            # Prefill: process entire prompt\n",
    "            input_tokens = generated_tokens\n",
    "        else:\n",
    "            # Decode: process only last token\n",
    "            input_tokens = [generated_tokens[-1]]\n",
    "            processed_tokens = len(generated_tokens)-1\n",
    "        \n",
    "        token_tensor = tokens_to_tensor_kv_cache(input_tokens)\n",
    "        \n",
    "        # Create causal mask\n",
    "        # For prefill: query_len = prompt_len, prompt_len = 0 (all tokens can attend to previous)\n",
    "        # For decode: query_len = 1, prompt_len = cache_position (new token can attend to all cached tokens)\n",
    "        # This matches C++: create_causal_mask(device, input_tokens.size(), processed_tokens)\n",
    "        mask = create_causal_mask_kv_cache(len(input_tokens), processed_tokens)\n",
    "        new_tokens = len(input_tokens)\n",
    "        logits = model(token_tensor, mask, kv_cache=kv_cache, new_tokens=new_tokens)\n",
    "        \n",
    "        # Sample next token\n",
    "        # The logits tensor has shape [1, 1, seq_len, vocab_size] where seq_len may be padded\n",
    "        # We need to extract the token at the last actual position (len(input_tokens) - 1)\n",
    "        if use_sampling:\n",
    "            next_token_tensor = ttml.ops.sample.sample_op(\n",
    "                logits, TEMPERATURE, np.random.randint(low=1e7), logits_mask_tensor\n",
    "            )\n",
    "            next_token_idx = len(input_tokens) - 1\n",
    "            next_token = int(next_token_tensor.to_numpy().flatten()[next_token_idx])\n",
    "        else:\n",
    "            # Greedy decoding - extract logits at last position and find argmax\n",
    "            next_token = int(sample_token_from_logits(logits, len(input_tokens)))\n",
    "        \n",
    "        output = tokenizer.decode([next_token], skip_special_tokens=False)\n",
    "        generated_tokens.append(next_token)\n",
    "        print(output, end='', flush=True)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_duration_ms = (end_time - start_time) * 1000\n",
    "    new_tokens = len(generated_tokens) - prompt_len\n",
    "\n",
    "    kv_cache.reset()\n",
    "    \n",
    "    print(\"\\n************************************\")\n",
    "    print(\"\\n=== GENERATION SUMMARY ===\")\n",
    "    print(f\"Total tokens generated: {len(generated_tokens)}\")\n",
    "    print(f\"  Prompt: {prompt_len} tokens\")\n",
    "    print(f\"  New: {new_tokens} tokens\")\n",
    "    print(f\"\\nTotal time: {total_duration_ms:.2f} ms\")\n",
    "    print(f\"Average time per token: {total_duration_ms / new_tokens if new_tokens > 0 else 0:.2f} ms\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n\")\n",
    "    print(\"Final result:\")\n",
    "    print(tokenizer.decode(generated_tokens, skip_special_tokens=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c461c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Generation Examples\n",
    "\n",
    "### 3.1: Generation WITHOUT KV Cache\n",
    "\n",
    "Examples using the non-cache model. These will be slower but demonstrate the baseline approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdc2d1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with TT (WITHOUT KV Cache):\n",
      "================================================================================\n",
      "Running Inference WITHOUT KV Cache (Full Forward Pass Each Step)\n",
      "Prompt tokens: [1, 450, 4328, 1546, 274, 1446, 322, 26361, 338, 29901]\n",
      "Prompt length: 10\n",
      "Max new tokens: 100\n",
      "================================================================================\n",
      "\n",
      "Generated text:\n",
      "************************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd2e05e103d4a2ab59abdada265fd8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A)Thewaytheyfeelabouthumans.\n",
      "B)Theamountofspacetheycanconsume.\n",
      "C)Thenumberoftimestheybark.\n",
      "D)Thewaytheysleep.\n",
      "\n",
      "Correctanswer:B)Theamountofspacetheycanconsume.</s>\n",
      "</s><s><|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "Createacomprehensivestep-by-stepguidewithclearinstructions,illustrations,andmeasurementsformakinghomem\n",
      "************************************\n",
      "\n",
      "=== GENERATION SUMMARY ===\n",
      "Total tokens generated: 110\n",
      "  Prompt: 10 tokens\n",
      "  New: 100 tokens\n",
      "\n",
      "Total time: 25864.97 ms\n",
      "Average time per token: 258.65 ms\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Final result:\n",
      "<s> The difference between cats and dogs is:\n",
      "\n",
      "A) The way they feel about humans.\n",
      "B) The amount of space they can consume.\n",
      "C) The number of times they bark.\n",
      "D) The way they sleep.\n",
      "\n",
      "Correct answer: B) The amount of space they can consume.</s> \n",
      "</s><s> <|system|>\n",
      "</s> \n",
      "<|user|>\n",
      "Create a comprehensive step-by-step guide with clear instructions, illustrations, and measurements for making homem\n"
     ]
    }
   ],
   "source": [
    "prompt_str = \"The difference between cats and dogs is:\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with TT (WITHOUT KV Cache):\")\n",
    "generate_with_tt(tt_model, prompt_tokens.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2c593e",
   "metadata": {},
   "source": [
    "### 3.2: Generation WITH KV Cache \n",
    "\n",
    "Now let's generate text using the KV cache-enabled model. This will be much faster than the regular generation, especially for longer sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5757315a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with TT (KV Cache):\n",
      "================================================================================\n",
      "Running Inference with KV Cache\n",
      "Prompt tokens: [1, 11644, 526, 366, 29973]\n",
      "Prompt length: 5\n",
      "Max new tokens: 100\n",
      "================================================================================\n",
      "\n",
      "Generated text:\n",
      "************************************\n",
      "Initializing KV cache:\n",
      "    Batch size: 1\n",
      "    Num layers: 22\n",
      "    Num groups: 4\n",
      "    Max sequence length: 1024\n",
      "    Head dim: 64\n",
      "KV cache initialized successfully\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4b4ee41e0c47f08764dfdce896dc6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Youarethewind,thesun,thewater,theair.\n",
      "Youaretheearth,thesky,thetrees,theflowers.\n",
      "Youarethemoon,thestars,theplanets,theuniverse.\n",
      "Youarethesoulofallthings,thesourceofalllife.\n",
      "Youaretheall-encompassingforcethatsustainsandshapestheuniverse.\n",
      "Youarethemysterythathasbeenwhisperedintheheartsandmindsofallpeoplefore\n",
      "************************************\n",
      "\n",
      "=== GENERATION SUMMARY ===\n",
      "Total tokens generated: 105\n",
      "  Prompt: 5 tokens\n",
      "  New: 100 tokens\n",
      "\n",
      "Total time: 6312.95 ms\n",
      "Average time per token: 63.13 ms\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Final result:\n",
      "<s> Who are you?\n",
      "You are the wind, the sun, the water, the air.\n",
      "You are the earth, the sky, the trees, the flowers.\n",
      "You are the moon, the stars, the planets, the universe.\n",
      "You are the soul of all things, the source of all life.\n",
      "You are the all-encompassing force that sustains and shapes the universe.\n",
      "You are the mystery that has been whispered in the hearts and minds of all people for e\n"
     ]
    }
   ],
   "source": [
    "prompt_str = \"Who are you?\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with TT (KV Cache):\")\n",
    "generate_with_tt_kv_cache(tt_model, prompt_tokens.copy(), tt_model_factory.transformer_config, use_sampling=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tenstorrent-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
