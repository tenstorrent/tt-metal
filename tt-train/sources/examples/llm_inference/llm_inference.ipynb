{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a20c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tokenizers safetensors\n",
    "\n",
    "import os, sys, math, random, textwrap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from transformers import GPT2Tokenizer\n",
    "from yaml import safe_load, Loader\n",
    "\n",
    "sys.path.append(f\"{os.environ['TT_METAL_HOME']}/tt-train/sources/ttml\")\n",
    "import ttml\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed()\n",
    "# Change working directory to TT_METAL_HOME\n",
    "os.chdir(os.environ['TT_METAL_HOME'])\n",
    "\n",
    "OUTPUT_TOKENS = 256\n",
    "\n",
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    n_head: int = 12\n",
    "    embed_dim: int = 768\n",
    "    dropout: float = 0.2\n",
    "    n_blocks : int = 12\n",
    "    vocab_size: int = 96\n",
    "    max_seq_len: int = 1024\n",
    "    runner_type: str = \"memory_efficient\"\n",
    "    weight_tying: str = \"enabled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea71a9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/tt-metal\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "print(os.getcwd())\n",
    "transformer_cfg = safe_load(open(\"tt-train/configs/training_shakespeare_gpt2s.yaml\", \"r\"))[\"training_config\"][\"transformer_config\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26b353af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_causal_mask(T: int) -> ttml.autograd.Tensor:\n",
    "    # [1,1,T,T] float32 with 1s for allowed positions (i >= j), else 0\n",
    "    m = np.tril(np.ones((T, T), dtype=np.float32))\n",
    "    return ttml.autograd.Tensor.from_numpy(m.reshape(1, 1, T, T), ttml.Layout.TILE, ttml.autograd.DataType.FLOAT32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd89028f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: vocab size 50257 is not multiple of 32, padding for tilizing.\n",
      "Transformer configuration:\n",
      "    Vocab size: 50272\n",
      "    Max sequence length: 1024\n",
      "    Embedding dim: 768\n",
      "    Num heads: 12\n",
      "    Dropout probability: 0.2\n",
      "    Num blocks: 12\n",
      "    Positional embedding type: Trainable\n",
      "    Runner type: Default\n",
      "    Composite layernorm: false\n",
      "    Weight tying: Disabled\n",
      "2025-09-30 17:10:18.169 | info     |          Device | Opening user mode device driver (tt_cluster.cpp:188)\n",
      "2025-09-30 17:10:18.224 | info     |   SiliconDriver | Harvesting mask for chip 0 is 0x80 (NOC0: 0x80, simulated harvesting mask: 0x0). (cluster.cpp:400)\n",
      "2025-09-30 17:10:18.275 | warning  |   SiliconDriver | init_detect_tt_device_numanodes(): Could not determine NumaNodeSet for TT device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:578)\n",
      "2025-09-30 17:10:18.275 | warning  |   SiliconDriver | Could not find NumaNodeSet for TT Device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:182)\n",
      "2025-09-30 17:10:18.275 | warning  |   SiliconDriver | bind_area_memory_nodeset(): Unable to determine TT Device to NumaNode mapping for physical_device_id: 0. Skipping membind. (cpuset_lib.cpp:463)\n",
      "2025-09-30 17:10:18.275 | warning  |   SiliconDriver | ---- ttSiliconDevice::init_hugepage: bind_area_to_memory_nodeset() failed (physical_device_id: 0 ch: 0). Hugepage allocation is not on NumaNode matching TT Device. Side-Effect is decreased Device->Host perf (Issue #893). (sysmem_manager.cpp:214)\n",
      "2025-09-30 17:10:18.276 | info     |   SiliconDriver | Opening local chip ids/PCIe ids: {0}/[0] and remote chip ids {} (cluster.cpp:249)\n",
      "2025-09-30 17:10:18.276 | info     |   SiliconDriver | All devices in cluster running firmware version: 18.10.0 (cluster.cpp:229)\n",
      "2025-09-30 17:10:18.276 | info     |   SiliconDriver | IOMMU: disabled (cluster.cpp:173)\n",
      "2025-09-30 17:10:18.276 | info     |   SiliconDriver | KMD version: 2.4.0 (cluster.cpp:176)\n",
      "2025-09-30 17:10:18.276 | info     |   SiliconDriver | Software version 6.0.0, Ethernet FW version 7.0.0 (Device 0) (cluster.cpp:1059)\n",
      "2025-09-30 17:10:18.279 | info     |   SiliconDriver | Pinning pages for Hugepage: virtual address 0x7f63c0000000 and size 0x40000000 pinned to physical address 0x200000000 (pci_device.cpp:612)\n",
      "2025-09-30 17:10:18.338 | info     |       Inspector | Inspector RPC server listening on localhost:50051 (rpc_server_controller.cpp:85)\n",
      "2025-09-30 17:10:19.671 | info     |           Metal | DPRINT enabled on device 0, worker core (x=0,y=0) (virtual (x=18,y=18)). (dprint_server.cpp:726)\n",
      "2025-09-30 17:10:19.672 | info     |           Metal | DPRINT Server attached device 0 (dprint_server.cpp:773)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_ttml.models.gpt2.GPT2Transformer at 0x7f642cbd7110>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model(cfg, vocab_size: int, seq_len: int):\n",
    "    # GPT2 config via your bindings\n",
    "    gcfg = ttml.models.gpt2.GPT2TransformerConfig()\n",
    "    gcfg.num_heads = cfg[\"num_heads\"]\n",
    "    gcfg.embedding_dim = cfg[\"embedding_dim\"]\n",
    "    gcfg.num_blocks = cfg[\"num_blocks\"]\n",
    "    gcfg.vocab_size = int(vocab_size)\n",
    "    gcfg.max_sequence_length = seq_len\n",
    "    gcfg.dropout_prob = cfg[\"dropout_prob\"]\n",
    "    # optional flags exist (runner_type, weight_tying, positional_embedding_type, experimental, ...)\n",
    "    # we keep defaults for a minimal demo\n",
    "\n",
    "    model = ttml.models.gpt2.create_gpt2_model(gcfg)\n",
    "    return model\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "if vocab_size % 32 != 0:\n",
    "    print(f\"Warning: vocab size {vocab_size} is not multiple of 32, padding for tilizing.\")\n",
    "    padded_vocab_size = ((tokenizer.vocab_size + 31) // 32) * 32\n",
    "\n",
    "else:\n",
    "    padded_vocab_size = vocab_size\n",
    "\n",
    "model = create_model(transformer_cfg, padded_vocab_size, transformer_cfg[\"max_sequence_length\"])\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8beb647",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_ttml.autograd.AutoContext: no constructor defined!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mttml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAutoContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mset_gradient_mode(ttml\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mGradMode\u001b[38;5;241m.\u001b[39mDISABLED)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padded_vocab_size \u001b[38;5;241m!=\u001b[39m vocab_size:\n\u001b[1;32m      5\u001b[0m     logits_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, padded_vocab_size), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mTypeError\u001b[0m: _ttml.autograd.AutoContext: no constructor defined!"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "ttml.autograd.AutoContext().set_gradient_mode(ttml.autograd.GradMode.DISABLED)\n",
    "\n",
    "if padded_vocab_size != vocab_size:\n",
    "    logits_mask = np.zeros((1, 1, 1, padded_vocab_size), dtype=np.float32)\n",
    "    logits_mask[:, :, :, vocab_size:] = 1e4\n",
    "\n",
    "    logits_mask_tensor = ttml.autograd.Tensor.from_numpy(logits_mask, ttml.Layout.ROW_MAJOR, ttml.autograd.DataType.BFLOAT16)   # [1,1,1,T], float32\n",
    "else:\n",
    "    logits_mask_tensor = None\n",
    "\n",
    "prompt_str = \"The difference between cats and dogs is:\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "\n",
    "causal_mask = build_causal_mask(transformer_cfg[\"max_sequence_length\"])  # [1,1,seq_len,seq_len], float32\n",
    "padded_prompt_tokens = np.full((1, 1, 1, transformer_cfg[\"max_sequence_length\"]), \n",
    "                                tokenizer.eos_token_id,\n",
    "                                dtype=np.uint32)\n",
    "\n",
    "start_idx = 0\n",
    "for token_idx in range(OUTPUT_TOKENS):\n",
    "\n",
    "    if len(prompt_tokens) > transformer_cfg[\"max_sequence_length\"]:\n",
    "        start_idx = len(prompt_tokens) - transformer_cfg[\"max_sequence_length\"]\n",
    "\n",
    "    padded_prompt_tokens[0, 0, 0, :transformer_cfg[\"max_sequence_length\"]] = tokenizer.eos_token_id\n",
    "    padded_prompt_tokens[0, 0, 0, start_idx:start_idx + len(prompt_tokens)] = prompt_tokens\n",
    "    padded_prompt_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "        padded_prompt_tokens,\n",
    "        ttml.Layout.TILE,\n",
    "        ttml.autograd.DataType.UINT32\n",
    "    )  # [1,1,1, max_seq_len], uint32\n",
    "\n",
    "    logits = model(padded_prompt_tensor, causal_mask)  # [1,1,1, vocab_size]\n",
    "    if logits_mask_tensor != None:\n",
    "        next_token = ttml.ops.sample.sample_op(logits, 1.0, 42, logits_mask_tensor)  # [1,1,seq_len,vocab_size], uint32\n",
    "    else:\n",
    "        next_token = ttml.ops.sample.sample_op(logits, 1.0, 42)  # [1,1,seq_len,vocab_size], uint32\n",
    "        \n",
    "    output = tokenizer.decode(next_token.to_numpy().flatten()[-1])\n",
    "\n",
    "    prompt_tokens.append(next_token.to_numpy().flatten()[-1])\n",
    "\n",
    "    print(output, end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
