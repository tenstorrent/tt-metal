{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a20c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tokenizers safetensors\n",
    "\n",
    "import os, sys, math, random, textwrap\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoTokenizer\n",
    "from yaml import safe_load, Loader\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(f\"{os.environ['TT_METAL_HOME']}/tt-train/sources/ttml\")\n",
    "import ttml\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed()\n",
    "# Change working directory to TT_METAL_HOME\n",
    "os.chdir(os.environ['TT_METAL_HOME'])\n",
    "\n",
    "OUTPUT_TOKENS = 256\n",
    "\n",
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    n_head: int = 12\n",
    "    embed_dim: int = 768\n",
    "    dropout: float = 0.2\n",
    "    n_blocks : int = 12\n",
    "    vocab_size: int = 96\n",
    "    max_seq_len: int = 1024\n",
    "    runner_type: str = \"memory_efficient\"\n",
    "    weight_tying: str = \"enabled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea71a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "transformer_cfg = safe_load(open(\"tt-train/configs/training_shakespeare_gpt2s.yaml\", \"r\"))[\"training_config\"][\"transformer_config\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6965256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get safetensors\n",
    "local_path = hf_hub_download(repo_id=\"gpt2\", filename=\"model.safetensors\")\n",
    "local_path = local_path.replace(\"model.safetensors\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26b353af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_causal_mask(T: int) -> ttml.autograd.Tensor:\n",
    "    # [1,1,T,T] float32 with 1s for allowed positions (i >= j), else 0\n",
    "    m = np.tril(np.ones((T, T), dtype=np.float32))\n",
    "    return ttml.autograd.Tensor.from_numpy(m.reshape(1, 1, T, T), ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)\n",
    "\n",
    "def build_logits_mask(vocab_size: int, padded_vocab_size: int) -> ttml.autograd.Tensor:\n",
    "    logits_mask = np.zeros((1, 1, 1, padded_vocab_size), dtype=np.float32)\n",
    "    logits_mask[:, :, :, vocab_size:] = 1e4\n",
    "    return ttml.autograd.Tensor.from_numpy(logits_mask, ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)   # [1,1,1,T], float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd89028f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: vocab size 50257 is not multiple of 32, padding for tilizing.\n",
      "Transformer configuration:\n",
      "    Vocab size: 50272\n",
      "    Max sequence length: 1024\n",
      "    Embedding dim: 768\n",
      "    Num heads: 12\n",
      "    Dropout probability: 0.2\n",
      "    Num blocks: 12\n",
      "    Positional embedding type: Trainable\n",
      "    Runner type: Default\n",
      "    Composite layernorm: false\n",
      "    Weight tying: Enabled\n",
      "2025-09-30 20:51:40.246 | info     |          Device | Opening user mode device driver (tt_cluster.cpp:188)\n",
      "2025-09-30 20:51:40.299 | info     |   SiliconDriver | Harvesting mask for chip 0 is 0x80 (NOC0: 0x80, simulated harvesting mask: 0x0). (cluster.cpp:400)\n",
      "2025-09-30 20:51:40.351 | warning  |   SiliconDriver | init_detect_tt_device_numanodes(): Could not determine NumaNodeSet for TT device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:578)\n",
      "2025-09-30 20:51:40.351 | warning  |   SiliconDriver | Could not find NumaNodeSet for TT Device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:182)\n",
      "2025-09-30 20:51:40.351 | warning  |   SiliconDriver | bind_area_memory_nodeset(): Unable to determine TT Device to NumaNode mapping for physical_device_id: 0. Skipping membind. (cpuset_lib.cpp:463)\n",
      "2025-09-30 20:51:40.351 | warning  |   SiliconDriver | ---- ttSiliconDevice::init_hugepage: bind_area_to_memory_nodeset() failed (physical_device_id: 0 ch: 0). Hugepage allocation is not on NumaNode matching TT Device. Side-Effect is decreased Device->Host perf (Issue #893). (sysmem_manager.cpp:214)\n",
      "2025-09-30 20:51:40.352 | info     |   SiliconDriver | Opening local chip ids/PCIe ids: {0}/[0] and remote chip ids {} (cluster.cpp:249)\n",
      "2025-09-30 20:51:40.352 | info     |   SiliconDriver | All devices in cluster running firmware version: 18.10.0 (cluster.cpp:229)\n",
      "2025-09-30 20:51:40.352 | info     |   SiliconDriver | IOMMU: disabled (cluster.cpp:173)\n",
      "2025-09-30 20:51:40.352 | info     |   SiliconDriver | KMD version: 2.4.0 (cluster.cpp:176)\n",
      "2025-09-30 20:51:40.352 | info     |   SiliconDriver | Software version 6.0.0, Ethernet FW version 7.0.0 (Device 0) (cluster.cpp:1059)\n",
      "2025-09-30 20:51:40.356 | info     |   SiliconDriver | Pinning pages for Hugepage: virtual address 0x7fc800000000 and size 0x40000000 pinned to physical address 0x200000000 (pci_device.cpp:612)\n",
      "2025-09-30 20:51:40.403 | info     |       Inspector | Inspector RPC server listening on localhost:50051 (rpc_server_controller.cpp:85)\n",
      "2025-09-30 20:51:41.655 | info     |           Metal | DPRINT enabled on device 0, worker core (x=0,y=0) (virtual (x=18,y=18)). (dprint_server.cpp:726)\n",
      "2025-09-30 20:51:41.655 | info     |           Metal | DPRINT Server attached device 0 (dprint_server.cpp:773)\n",
      "Loading model from: /home/ubuntu/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors\n",
      "parameter name: transformer/gpt_block_9/mlp/fc2/bias\n",
      "parameter name: transformer/gpt_block_9/mlp/fc1/weight\n",
      "parameter name: transformer/gpt_block_9/attention/qkv_linear/bias\n",
      "parameter name: transformer/gpt_block_9/attention/out_linear/bias\n",
      "parameter name: transformer/gpt_block_8/mlp/fc2/weight\n",
      "parameter name: transformer/gpt_block_8/mlp/fc1/weight\n",
      "parameter name: transformer/gpt_block_8/attention/qkv_linear/bias\n",
      "parameter name: transformer/gpt_block_7/mlp/fc2/weight\n",
      "parameter name: transformer/gpt_block_9/attention/out_linear/weight\n",
      "parameter name: transformer/gpt_block_7/mlp/fc2/bias\n",
      "parameter name: transformer/gpt_block_7/mlp/fc1/weight\n",
      "parameter name: transformer/gpt_block_7/attention/qkv_linear/weight\n",
      "parameter name: transformer/gpt_block_0/ln1/gamma\n",
      "parameter name: transformer/gpt_block_6/attention/out_linear/bias\n",
      "parameter name: transformer/gpt_block_0/attention/qkv_linear/bias\n",
      "parameter name: transformer/gpt_block_4/ln1/beta\n",
      "parameter name: transformer/gpt_block_8/mlp/fc2/bias\n",
      "parameter name: transformer/gpt_block_0/attention/out_linear/weight\n",
      "parameter name: transformer/gpt_block_8/ln2/beta\n",
      "parameter name: transformer/gpt_block_3/attention/out_linear/bias\n",
      "parameter name: transformer/gpt_block_4/mlp/fc2/bias\n",
      "parameter name: transformer/gpt_block_9/ln1/beta\n",
      "parameter name: transformer/gpt_block_5/ln1/gamma\n",
      "parameter name: transformer/gpt_block_11/ln2/beta\n",
      "parameter name: transformer/gpt_block_3/mlp/fc2/bias\n",
      "parameter name: transformer/gpt_block_7/ln2/beta\n",
      "parameter name: transformer/gpt_block_11/attention/qkv_linear/weight\n",
      "parameter name: transformer/gpt_block_0/mlp/fc1/weight\n",
      "parameter name: transformer/gpt_block_7/ln1/beta\n",
      "parameter name: transformer/gpt_block_1/mlp/fc1/weight\n",
      "parameter name: transformer/gpt_block_2/mlp/fc2/bias\n",
      "parameter name: transformer/gpt_block_6/ln1/gamma\n",
      "parameter name: transformer/gpt_block_5/attention/qkv_linear/weight\n",
      "parameter name: transformer/gpt_block_7/mlp/fc1/bias\n",
      "parameter name: transformer/gpt_block_6/ln1/beta\n",
      "parameter name: transformer/gpt_block_5/ln2/gamma\n",
      "parameter name: transformer/gpt_block_0/attention/qkv_linear/weight\n",
      "parameter name: transformer/gpt_block_4/ln2/gamma\n",
      "parameter name: transformer/gpt_block_0/ln2/beta\n",
      "parameter name: transformer/gpt_block_8/ln1/gamma\n",
      "parameter name: transformer/gpt_block_6/mlp/fc1/bias\n",
      "parameter name: transformer/gpt_block_4/ln1/gamma\n",
      "parameter name: transformer/gpt_block_5/ln1/beta\n",
      "parameter name: transformer/gpt_block_7/ln2/gamma\n",
      "parameter name: transformer/gpt_block_3/attention/qkv_linear/bias\n",
      "parameter name: transformer/gpt_block_10/mlp/fc2/weight\n",
      "parameter name: transformer/gpt_block_0/mlp/fc1/bias\n",
      "parameter name: transformer/ln_fc/beta\n",
      "parameter name: transformer/gpt_block_1/ln1/beta\n",
      "parameter name: transformer/gpt_block_4/ln2/beta\n",
      "parameter name: transformer/gpt_block_2/ln2/beta\n",
      "parameter name: transformer/gpt_block_1/ln2/beta\n",
      "parameter name: transformer/gpt_block_7/ln1/gamma\n",
      "parameter name: transformer/gpt_block_3/ln1/gamma\n",
      "parameter name: transformer/gpt_block_8/ln2/gamma\n",
      "parameter name: transformer/gpt_block_11/mlp/fc2/bias\n",
      "parameter name: transformer/gpt_block_1/ln2/gamma\n",
      "parameter name: transformer/gpt_block_0/attention/out_linear/bias\n",
      "parameter name: transformer/gpt_block_2/attention/qkv_linear/bias\n",
      "parameter name: transformer/gpt_block_10/ln2/gamma\n",
      "parameter name: transformer/pos_emb/weight\n",
      "parameter name: transformer/gpt_block_5/mlp/fc2/bias\n",
      "parameter name: transformer/gpt_block_11/ln1/beta\n",
      "parameter name: transformer/gpt_block_9/attention/qkv_linear/weight\n",
      "parameter name: transformer/gpt_block_0/ln2/gamma\n",
      "parameter name: transformer/gpt_block_11/ln1/gamma\n",
      "parameter name: transformer/gpt_block_1/attention/out_linear/weight\n",
      "parameter name: transformer/gpt_block_5/mlp/fc2/weight\n",
      "parameter name: transformer/gpt_block_9/ln2/gamma\n",
      "parameter name: transformer/gpt_block_10/ln1/gamma\n",
      "parameter name: transformer/gpt_block_4/attention/out_linear/weight\n",
      "parameter name: transformer/gpt_block_9/ln1/gamma\n",
      "parameter name: transformer/gpt_block_0/mlp/fc2/weight\n",
      "parameter name: transformer/gpt_block_5/attention/out_linear/weight\n",
      "parameter name: transformer/gpt_block_10/mlp/fc1/bias\n",
      "parameter name: transformer/gpt_block_11/mlp/fc1/bias\n",
      "parameter name: transformer/gpt_block_4/mlp/fc1/weight\n",
      "parameter name: transformer/gpt_block_2/ln1/gamma\n",
      "parameter name: transformer/fc/weight\n",
      "parameter name: transformer/gpt_block_11/ln2/gamma\n",
      "parameter name: transformer/ln_fc/gamma\n",
      "parameter name: transformer/gpt_block_8/attention/out_linear/weight\n",
      "parameter name: transformer/gpt_block_9/ln2/beta\n",
      "parameter name: transformer/gpt_block_6/attention/out_linear/weight\n",
      "parameter name: transformer/gpt_block_6/ln2/gamma\n",
      "parameter name: transformer/gpt_block_4/attention/qkv_linear/bias\n",
      "parameter name: transformer/gpt_block_10/ln2/beta\n",
      "parameter name: transformer/gpt_block_6/mlp/fc1/weight\n",
      "parameter name: transformer/gpt_block_2/ln2/gamma\n",
      "parameter name: transformer/gpt_block_1/ln1/gamma\n",
      "parameter name: transformer/gpt_block_0/mlp/fc2/bias\n",
      "parameter name: transformer/gpt_block_1/mlp/fc2/bias\n",
      "parameter name: transformer/gpt_block_3/ln2/gamma\n",
      "parameter name: transformer/gpt_block_9/mlp/fc1/bias\n",
      "parameter name: transformer/gpt_block_3/ln1/beta\n",
      "parameter name: transformer/gpt_block_1/attention/out_linear/bias\n",
      "parameter name: transformer/gpt_block_2/mlp/fc2/weight\n",
      "parameter name: transformer/gpt_block_1/attention/qkv_linear/weight\n",
      "parameter name: transformer/gpt_block_1/mlp/fc1/bias\n",
      "parameter name: transformer/gpt_block_3/attention/out_linear/weight\n",
      "parameter name: transformer/gpt_block_4/attention/qkv_linear/weight\n",
      "parameter name: transformer/gpt_block_8/attention/out_linear/bias\n",
      "parameter name: transformer/gpt_block_11/mlp/fc1/weight\n",
      "parameter name: transformer/gpt_block_1/mlp/fc2/weight\n",
      "parameter name: transformer/gpt_block_2/mlp/fc1/weight\n",
      "parameter name: transformer/gpt_block_1/attention/qkv_linear/bias\n",
      "parameter name: transformer/gpt_block_10/mlp/fc1/weight\n",
      "parameter name: transformer/gpt_block_10/attention/out_linear/bias\n",
      "parameter name: transformer/gpt_block_11/attention/out_linear/bias\n",
      "parameter name: transformer/gpt_block_8/attention/qkv_linear/weight\n",
      "parameter name: transformer/gpt_block_2/ln1/beta\n",
      "parameter name: transformer/gpt_block_10/attention/out_linear/weight\n",
      "parameter name: transformer/gpt_block_5/ln2/beta\n",
      "parameter name: transformer/gpt_block_11/mlp/fc2/weight\n",
      "parameter name: transformer/gpt_block_10/attention/qkv_linear/bias\n",
      "parameter name: transformer/gpt_block_10/mlp/fc2/bias\n",
      "parameter name: transformer/gpt_block_4/mlp/fc1/bias\n",
      "parameter name: transformer/gpt_block_11/attention/out_linear/weight\n",
      "parameter name: transformer/gpt_block_3/mlp/fc2/weight\n",
      "parameter name: transformer/gpt_block_11/attention/qkv_linear/bias\n",
      "parameter name: transformer/gpt_block_5/mlp/fc1/weight\n",
      "parameter name: transformer/gpt_block_2/attention/out_linear/bias\n",
      "parameter name: transformer/gpt_block_2/attention/out_linear/weight\n",
      "parameter name: transformer/gpt_block_6/ln2/beta\n",
      "parameter name: transformer/gpt_block_2/attention/qkv_linear/weight\n",
      "parameter name: transformer/gpt_block_0/ln1/beta\n",
      "parameter name: transformer/gpt_block_10/attention/qkv_linear/weight\n",
      "parameter name: transformer/gpt_block_2/mlp/fc1/bias\n",
      "parameter name: transformer/gpt_block_3/attention/qkv_linear/weight\n",
      "parameter name: transformer/gpt_block_3/mlp/fc1/bias\n",
      "parameter name: transformer/gpt_block_8/ln1/beta\n",
      "parameter name: transformer/gpt_block_3/mlp/fc1/weight\n",
      "parameter name: transformer/gpt_block_6/attention/qkv_linear/bias\n",
      "parameter name: transformer/gpt_block_4/attention/out_linear/bias\n",
      "parameter name: transformer/gpt_block_4/mlp/fc2/weight\n",
      "parameter name: transformer/gpt_block_9/mlp/fc2/weight\n",
      "parameter name: transformer/gpt_block_10/ln1/beta\n",
      "parameter name: transformer/gpt_block_5/attention/out_linear/bias\n",
      "parameter name: transformer/gpt_block_5/attention/qkv_linear/bias\n",
      "parameter name: transformer/gpt_block_5/mlp/fc1/bias\n",
      "parameter name: transformer/gpt_block_6/attention/qkv_linear/weight\n",
      "parameter name: transformer/gpt_block_6/mlp/fc2/bias\n",
      "parameter name: transformer/gpt_block_8/mlp/fc1/bias\n",
      "parameter name: transformer/gpt_block_7/attention/qkv_linear/bias\n",
      "parameter name: transformer/gpt_block_3/ln2/beta\n",
      "parameter name: transformer/gpt_block_6/mlp/fc2/weight\n",
      "parameter name: transformer/gpt_block_7/attention/out_linear/bias\n",
      "parameter name: transformer/gpt_block_7/attention/out_linear/weight\n",
      "Loading tensor: h.0.attn.bias, shape:Shape([1, 1, 1024, 1024]), format: F32\n",
      "Loading tensor: h.0.attn.c_attn.bias, shape:Shape([2304]), format: F32\n",
      " Parameter transformer/gpt_block_0/attention/qkv_linear/bias, shape: Shape([1, 1, 1, 2304])\n",
      "Loading tensor: h.0.attn.c_attn.weight, shape:Shape([768, 2304]), format: F32\n",
      " Parameter transformer/gpt_block_0/attention/qkv_linear/weight, shape: Shape([1, 1, 2304, 768])\n",
      "Loading tensor: h.0.attn.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_0/attention/out_linear/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.0.attn.c_proj.weight, shape:Shape([768, 768]), format: F32\n",
      " Parameter transformer/gpt_block_0/attention/out_linear/weight, shape: Shape([1, 1, 768, 768])\n",
      "Loading tensor: h.0.ln_1.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_0/ln1/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.0.ln_1.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_0/ln1/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.0.ln_2.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_0/ln2/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.0.ln_2.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_0/ln2/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.0.mlp.c_fc.bias, shape:Shape([3072]), format: F32\n",
      " Parameter transformer/gpt_block_0/mlp/fc1/bias, shape: Shape([1, 1, 1, 3072])\n",
      "Loading tensor: h.0.mlp.c_fc.weight, shape:Shape([768, 3072]), format: F32\n",
      " Parameter transformer/gpt_block_0/mlp/fc1/weight, shape: Shape([1, 1, 3072, 768])\n",
      "Loading tensor: h.0.mlp.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_0/mlp/fc2/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.0.mlp.c_proj.weight, shape:Shape([3072, 768]), format: F32\n",
      " Parameter transformer/gpt_block_0/mlp/fc2/weight, shape: Shape([1, 1, 768, 3072])\n",
      "Loading tensor: h.1.attn.bias, shape:Shape([1, 1, 1024, 1024]), format: F32\n",
      "Loading tensor: h.1.attn.c_attn.bias, shape:Shape([2304]), format: F32\n",
      " Parameter transformer/gpt_block_1/attention/qkv_linear/bias, shape: Shape([1, 1, 1, 2304])\n",
      "Loading tensor: h.1.attn.c_attn.weight, shape:Shape([768, 2304]), format: F32\n",
      " Parameter transformer/gpt_block_1/attention/qkv_linear/weight, shape: Shape([1, 1, 2304, 768])\n",
      "Loading tensor: h.1.attn.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_1/attention/out_linear/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.1.attn.c_proj.weight, shape:Shape([768, 768]), format: F32\n",
      " Parameter transformer/gpt_block_1/attention/out_linear/weight, shape: Shape([1, 1, 768, 768])\n",
      "Loading tensor: h.1.ln_1.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_1/ln1/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.1.ln_1.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_1/ln1/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.1.ln_2.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_1/ln2/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.1.ln_2.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_1/ln2/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.1.mlp.c_fc.bias, shape:Shape([3072]), format: F32\n",
      " Parameter transformer/gpt_block_1/mlp/fc1/bias, shape: Shape([1, 1, 1, 3072])\n",
      "Loading tensor: h.1.mlp.c_fc.weight, shape:Shape([768, 3072]), format: F32\n",
      " Parameter transformer/gpt_block_1/mlp/fc1/weight, shape: Shape([1, 1, 3072, 768])\n",
      "Loading tensor: h.1.mlp.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_1/mlp/fc2/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.1.mlp.c_proj.weight, shape:Shape([3072, 768]), format: F32\n",
      " Parameter transformer/gpt_block_1/mlp/fc2/weight, shape: Shape([1, 1, 768, 3072])\n",
      "Loading tensor: h.10.attn.bias, shape:Shape([1, 1, 1024, 1024]), format: F32\n",
      "Loading tensor: h.10.attn.c_attn.bias, shape:Shape([2304]), format: F32\n",
      " Parameter transformer/gpt_block_10/attention/qkv_linear/bias, shape: Shape([1, 1, 1, 2304])\n",
      "Loading tensor: h.10.attn.c_attn.weight, shape:Shape([768, 2304]), format: F32\n",
      " Parameter transformer/gpt_block_10/attention/qkv_linear/weight, shape: Shape([1, 1, 2304, 768])\n",
      "Loading tensor: h.10.attn.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_10/attention/out_linear/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.10.attn.c_proj.weight, shape:Shape([768, 768]), format: F32\n",
      " Parameter transformer/gpt_block_10/attention/out_linear/weight, shape: Shape([1, 1, 768, 768])\n",
      "Loading tensor: h.10.ln_1.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_10/ln1/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.10.ln_1.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_10/ln1/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.10.ln_2.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_10/ln2/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.10.ln_2.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_10/ln2/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.10.mlp.c_fc.bias, shape:Shape([3072]), format: F32\n",
      " Parameter transformer/gpt_block_10/mlp/fc1/bias, shape: Shape([1, 1, 1, 3072])\n",
      "Loading tensor: h.10.mlp.c_fc.weight, shape:Shape([768, 3072]), format: F32\n",
      " Parameter transformer/gpt_block_10/mlp/fc1/weight, shape: Shape([1, 1, 3072, 768])\n",
      "Loading tensor: h.10.mlp.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_10/mlp/fc2/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.10.mlp.c_proj.weight, shape:Shape([3072, 768]), format: F32\n",
      " Parameter transformer/gpt_block_10/mlp/fc2/weight, shape: Shape([1, 1, 768, 3072])\n",
      "Loading tensor: h.11.attn.bias, shape:Shape([1, 1, 1024, 1024]), format: F32\n",
      "Loading tensor: h.11.attn.c_attn.bias, shape:Shape([2304]), format: F32\n",
      " Parameter transformer/gpt_block_11/attention/qkv_linear/bias, shape: Shape([1, 1, 1, 2304])\n",
      "Loading tensor: h.11.attn.c_attn.weight, shape:Shape([768, 2304]), format: F32\n",
      " Parameter transformer/gpt_block_11/attention/qkv_linear/weight, shape: Shape([1, 1, 2304, 768])\n",
      "Loading tensor: h.11.attn.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_11/attention/out_linear/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.11.attn.c_proj.weight, shape:Shape([768, 768]), format: F32\n",
      " Parameter transformer/gpt_block_11/attention/out_linear/weight, shape: Shape([1, 1, 768, 768])\n",
      "Loading tensor: h.11.ln_1.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_11/ln1/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.11.ln_1.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_11/ln1/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.11.ln_2.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_11/ln2/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.11.ln_2.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_11/ln2/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.11.mlp.c_fc.bias, shape:Shape([3072]), format: F32\n",
      " Parameter transformer/gpt_block_11/mlp/fc1/bias, shape: Shape([1, 1, 1, 3072])\n",
      "Loading tensor: h.11.mlp.c_fc.weight, shape:Shape([768, 3072]), format: F32\n",
      " Parameter transformer/gpt_block_11/mlp/fc1/weight, shape: Shape([1, 1, 3072, 768])\n",
      "Loading tensor: h.11.mlp.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_11/mlp/fc2/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.11.mlp.c_proj.weight, shape:Shape([3072, 768]), format: F32\n",
      " Parameter transformer/gpt_block_11/mlp/fc2/weight, shape: Shape([1, 1, 768, 3072])\n",
      "Loading tensor: h.2.attn.bias, shape:Shape([1, 1, 1024, 1024]), format: F32\n",
      "Loading tensor: h.2.attn.c_attn.bias, shape:Shape([2304]), format: F32\n",
      " Parameter transformer/gpt_block_2/attention/qkv_linear/bias, shape: Shape([1, 1, 1, 2304])\n",
      "Loading tensor: h.2.attn.c_attn.weight, shape:Shape([768, 2304]), format: F32\n",
      " Parameter transformer/gpt_block_2/attention/qkv_linear/weight, shape: Shape([1, 1, 2304, 768])\n",
      "Loading tensor: h.2.attn.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_2/attention/out_linear/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.2.attn.c_proj.weight, shape:Shape([768, 768]), format: F32\n",
      " Parameter transformer/gpt_block_2/attention/out_linear/weight, shape: Shape([1, 1, 768, 768])\n",
      "Loading tensor: h.2.ln_1.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_2/ln1/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.2.ln_1.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_2/ln1/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.2.ln_2.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_2/ln2/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.2.ln_2.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_2/ln2/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.2.mlp.c_fc.bias, shape:Shape([3072]), format: F32\n",
      " Parameter transformer/gpt_block_2/mlp/fc1/bias, shape: Shape([1, 1, 1, 3072])\n",
      "Loading tensor: h.2.mlp.c_fc.weight, shape:Shape([768, 3072]), format: F32\n",
      " Parameter transformer/gpt_block_2/mlp/fc1/weight, shape: Shape([1, 1, 3072, 768])\n",
      "Loading tensor: h.2.mlp.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_2/mlp/fc2/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.2.mlp.c_proj.weight, shape:Shape([3072, 768]), format: F32\n",
      " Parameter transformer/gpt_block_2/mlp/fc2/weight, shape: Shape([1, 1, 768, 3072])\n",
      "Loading tensor: h.3.attn.bias, shape:Shape([1, 1, 1024, 1024]), format: F32\n",
      "Loading tensor: h.3.attn.c_attn.bias, shape:Shape([2304]), format: F32\n",
      " Parameter transformer/gpt_block_3/attention/qkv_linear/bias, shape: Shape([1, 1, 1, 2304])\n",
      "Loading tensor: h.3.attn.c_attn.weight, shape:Shape([768, 2304]), format: F32\n",
      " Parameter transformer/gpt_block_3/attention/qkv_linear/weight, shape: Shape([1, 1, 2304, 768])\n",
      "Loading tensor: h.3.attn.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_3/attention/out_linear/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.3.attn.c_proj.weight, shape:Shape([768, 768]), format: F32\n",
      " Parameter transformer/gpt_block_3/attention/out_linear/weight, shape: Shape([1, 1, 768, 768])\n",
      "Loading tensor: h.3.ln_1.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_3/ln1/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.3.ln_1.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_3/ln1/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.3.ln_2.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_3/ln2/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.3.ln_2.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_3/ln2/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.3.mlp.c_fc.bias, shape:Shape([3072]), format: F32\n",
      " Parameter transformer/gpt_block_3/mlp/fc1/bias, shape: Shape([1, 1, 1, 3072])\n",
      "Loading tensor: h.3.mlp.c_fc.weight, shape:Shape([768, 3072]), format: F32\n",
      " Parameter transformer/gpt_block_3/mlp/fc1/weight, shape: Shape([1, 1, 3072, 768])\n",
      "Loading tensor: h.3.mlp.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_3/mlp/fc2/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.3.mlp.c_proj.weight, shape:Shape([3072, 768]), format: F32\n",
      " Parameter transformer/gpt_block_3/mlp/fc2/weight, shape: Shape([1, 1, 768, 3072])\n",
      "Loading tensor: h.4.attn.bias, shape:Shape([1, 1, 1024, 1024]), format: F32\n",
      "Loading tensor: h.4.attn.c_attn.bias, shape:Shape([2304]), format: F32\n",
      " Parameter transformer/gpt_block_4/attention/qkv_linear/bias, shape: Shape([1, 1, 1, 2304])\n",
      "Loading tensor: h.4.attn.c_attn.weight, shape:Shape([768, 2304]), format: F32\n",
      " Parameter transformer/gpt_block_4/attention/qkv_linear/weight, shape: Shape([1, 1, 2304, 768])\n",
      "Loading tensor: h.4.attn.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_4/attention/out_linear/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.4.attn.c_proj.weight, shape:Shape([768, 768]), format: F32\n",
      " Parameter transformer/gpt_block_4/attention/out_linear/weight, shape: Shape([1, 1, 768, 768])\n",
      "Loading tensor: h.4.ln_1.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_4/ln1/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.4.ln_1.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_4/ln1/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.4.ln_2.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_4/ln2/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.4.ln_2.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_4/ln2/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.4.mlp.c_fc.bias, shape:Shape([3072]), format: F32\n",
      " Parameter transformer/gpt_block_4/mlp/fc1/bias, shape: Shape([1, 1, 1, 3072])\n",
      "Loading tensor: h.4.mlp.c_fc.weight, shape:Shape([768, 3072]), format: F32\n",
      " Parameter transformer/gpt_block_4/mlp/fc1/weight, shape: Shape([1, 1, 3072, 768])\n",
      "Loading tensor: h.4.mlp.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_4/mlp/fc2/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.4.mlp.c_proj.weight, shape:Shape([3072, 768]), format: F32\n",
      " Parameter transformer/gpt_block_4/mlp/fc2/weight, shape: Shape([1, 1, 768, 3072])\n",
      "Loading tensor: h.5.attn.bias, shape:Shape([1, 1, 1024, 1024]), format: F32\n",
      "Loading tensor: h.5.attn.c_attn.bias, shape:Shape([2304]), format: F32\n",
      " Parameter transformer/gpt_block_5/attention/qkv_linear/bias, shape: Shape([1, 1, 1, 2304])\n",
      "Loading tensor: h.5.attn.c_attn.weight, shape:Shape([768, 2304]), format: F32\n",
      " Parameter transformer/gpt_block_5/attention/qkv_linear/weight, shape: Shape([1, 1, 2304, 768])\n",
      "Loading tensor: h.5.attn.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_5/attention/out_linear/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.5.attn.c_proj.weight, shape:Shape([768, 768]), format: F32\n",
      " Parameter transformer/gpt_block_5/attention/out_linear/weight, shape: Shape([1, 1, 768, 768])\n",
      "Loading tensor: h.5.ln_1.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_5/ln1/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.5.ln_1.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_5/ln1/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.5.ln_2.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_5/ln2/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.5.ln_2.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_5/ln2/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.5.mlp.c_fc.bias, shape:Shape([3072]), format: F32\n",
      " Parameter transformer/gpt_block_5/mlp/fc1/bias, shape: Shape([1, 1, 1, 3072])\n",
      "Loading tensor: h.5.mlp.c_fc.weight, shape:Shape([768, 3072]), format: F32\n",
      " Parameter transformer/gpt_block_5/mlp/fc1/weight, shape: Shape([1, 1, 3072, 768])\n",
      "Loading tensor: h.5.mlp.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_5/mlp/fc2/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.5.mlp.c_proj.weight, shape:Shape([3072, 768]), format: F32\n",
      " Parameter transformer/gpt_block_5/mlp/fc2/weight, shape: Shape([1, 1, 768, 3072])\n",
      "Loading tensor: h.6.attn.bias, shape:Shape([1, 1, 1024, 1024]), format: F32\n",
      "Loading tensor: h.6.attn.c_attn.bias, shape:Shape([2304]), format: F32\n",
      " Parameter transformer/gpt_block_6/attention/qkv_linear/bias, shape: Shape([1, 1, 1, 2304])\n",
      "Loading tensor: h.6.attn.c_attn.weight, shape:Shape([768, 2304]), format: F32\n",
      " Parameter transformer/gpt_block_6/attention/qkv_linear/weight, shape: Shape([1, 1, 2304, 768])\n",
      "Loading tensor: h.6.attn.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_6/attention/out_linear/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.6.attn.c_proj.weight, shape:Shape([768, 768]), format: F32\n",
      " Parameter transformer/gpt_block_6/attention/out_linear/weight, shape: Shape([1, 1, 768, 768])\n",
      "Loading tensor: h.6.ln_1.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_6/ln1/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.6.ln_1.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_6/ln1/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.6.ln_2.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_6/ln2/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.6.ln_2.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_6/ln2/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.6.mlp.c_fc.bias, shape:Shape([3072]), format: F32\n",
      " Parameter transformer/gpt_block_6/mlp/fc1/bias, shape: Shape([1, 1, 1, 3072])\n",
      "Loading tensor: h.6.mlp.c_fc.weight, shape:Shape([768, 3072]), format: F32\n",
      " Parameter transformer/gpt_block_6/mlp/fc1/weight, shape: Shape([1, 1, 3072, 768])\n",
      "Loading tensor: h.6.mlp.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_6/mlp/fc2/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.6.mlp.c_proj.weight, shape:Shape([3072, 768]), format: F32\n",
      " Parameter transformer/gpt_block_6/mlp/fc2/weight, shape: Shape([1, 1, 768, 3072])\n",
      "Loading tensor: h.7.attn.bias, shape:Shape([1, 1, 1024, 1024]), format: F32\n",
      "Loading tensor: h.7.attn.c_attn.bias, shape:Shape([2304]), format: F32\n",
      " Parameter transformer/gpt_block_7/attention/qkv_linear/bias, shape: Shape([1, 1, 1, 2304])\n",
      "Loading tensor: h.7.attn.c_attn.weight, shape:Shape([768, 2304]), format: F32\n",
      " Parameter transformer/gpt_block_7/attention/qkv_linear/weight, shape: Shape([1, 1, 2304, 768])\n",
      "Loading tensor: h.7.attn.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_7/attention/out_linear/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.7.attn.c_proj.weight, shape:Shape([768, 768]), format: F32\n",
      " Parameter transformer/gpt_block_7/attention/out_linear/weight, shape: Shape([1, 1, 768, 768])\n",
      "Loading tensor: h.7.ln_1.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_7/ln1/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.7.ln_1.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_7/ln1/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.7.ln_2.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_7/ln2/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.7.ln_2.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_7/ln2/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.7.mlp.c_fc.bias, shape:Shape([3072]), format: F32\n",
      " Parameter transformer/gpt_block_7/mlp/fc1/bias, shape: Shape([1, 1, 1, 3072])\n",
      "Loading tensor: h.7.mlp.c_fc.weight, shape:Shape([768, 3072]), format: F32\n",
      " Parameter transformer/gpt_block_7/mlp/fc1/weight, shape: Shape([1, 1, 3072, 768])\n",
      "Loading tensor: h.7.mlp.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_7/mlp/fc2/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.7.mlp.c_proj.weight, shape:Shape([3072, 768]), format: F32\n",
      " Parameter transformer/gpt_block_7/mlp/fc2/weight, shape: Shape([1, 1, 768, 3072])\n",
      "Loading tensor: h.8.attn.bias, shape:Shape([1, 1, 1024, 1024]), format: F32\n",
      "Loading tensor: h.8.attn.c_attn.bias, shape:Shape([2304]), format: F32\n",
      " Parameter transformer/gpt_block_8/attention/qkv_linear/bias, shape: Shape([1, 1, 1, 2304])\n",
      "Loading tensor: h.8.attn.c_attn.weight, shape:Shape([768, 2304]), format: F32\n",
      " Parameter transformer/gpt_block_8/attention/qkv_linear/weight, shape: Shape([1, 1, 2304, 768])\n",
      "Loading tensor: h.8.attn.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_8/attention/out_linear/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.8.attn.c_proj.weight, shape:Shape([768, 768]), format: F32\n",
      " Parameter transformer/gpt_block_8/attention/out_linear/weight, shape: Shape([1, 1, 768, 768])\n",
      "Loading tensor: h.8.ln_1.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_8/ln1/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.8.ln_1.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_8/ln1/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.8.ln_2.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_8/ln2/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.8.ln_2.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_8/ln2/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.8.mlp.c_fc.bias, shape:Shape([3072]), format: F32\n",
      " Parameter transformer/gpt_block_8/mlp/fc1/bias, shape: Shape([1, 1, 1, 3072])\n",
      "Loading tensor: h.8.mlp.c_fc.weight, shape:Shape([768, 3072]), format: F32\n",
      " Parameter transformer/gpt_block_8/mlp/fc1/weight, shape: Shape([1, 1, 3072, 768])\n",
      "Loading tensor: h.8.mlp.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_8/mlp/fc2/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.8.mlp.c_proj.weight, shape:Shape([3072, 768]), format: F32\n",
      " Parameter transformer/gpt_block_8/mlp/fc2/weight, shape: Shape([1, 1, 768, 3072])\n",
      "Loading tensor: h.9.attn.bias, shape:Shape([1, 1, 1024, 1024]), format: F32\n",
      "Loading tensor: h.9.attn.c_attn.bias, shape:Shape([2304]), format: F32\n",
      " Parameter transformer/gpt_block_9/attention/qkv_linear/bias, shape: Shape([1, 1, 1, 2304])\n",
      "Loading tensor: h.9.attn.c_attn.weight, shape:Shape([768, 2304]), format: F32\n",
      " Parameter transformer/gpt_block_9/attention/qkv_linear/weight, shape: Shape([1, 1, 2304, 768])\n",
      "Loading tensor: h.9.attn.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_9/attention/out_linear/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.9.attn.c_proj.weight, shape:Shape([768, 768]), format: F32\n",
      " Parameter transformer/gpt_block_9/attention/out_linear/weight, shape: Shape([1, 1, 768, 768])\n",
      "Loading tensor: h.9.ln_1.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_9/ln1/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.9.ln_1.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_9/ln1/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.9.ln_2.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_9/ln2/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.9.ln_2.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_9/ln2/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.9.mlp.c_fc.bias, shape:Shape([3072]), format: F32\n",
      " Parameter transformer/gpt_block_9/mlp/fc1/bias, shape: Shape([1, 1, 1, 3072])\n",
      "Loading tensor: h.9.mlp.c_fc.weight, shape:Shape([768, 3072]), format: F32\n",
      " Parameter transformer/gpt_block_9/mlp/fc1/weight, shape: Shape([1, 1, 3072, 768])\n",
      "Loading tensor: h.9.mlp.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_9/mlp/fc2/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.9.mlp.c_proj.weight, shape:Shape([3072, 768]), format: F32\n",
      " Parameter transformer/gpt_block_9/mlp/fc2/weight, shape: Shape([1, 1, 768, 3072])\n",
      "Loading tensor: ln_f.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/ln_fc/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: ln_f.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/ln_fc/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: wpe.weight, shape:Shape([1024, 768]), format: F32\n",
      " Parameter transformer/pos_emb/weight, shape: Shape([1, 1, 1024, 768])\n",
      "Loading tensor: wte.weight, shape:Shape([50257, 768]), format: F32\n",
      " Parameter transformer/fc/weight, shape: Shape([1, 1, 50272, 768])\n",
      "Original shape 50257, 768Transformed shape 50272, 768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_ttml.models.gpt2.GPT2Transformer at 0x7fc872b7ba30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model(cfg, vocab_size: int, seq_len: int):\n",
    "    # GPT2 config via your bindings\n",
    "    gcfg = ttml.models.gpt2.GPT2TransformerConfig()\n",
    "    gcfg.num_heads = cfg[\"num_heads\"]\n",
    "    gcfg.embedding_dim = cfg[\"embedding_dim\"]\n",
    "    gcfg.num_blocks = cfg[\"num_blocks\"]\n",
    "    gcfg.vocab_size = int(vocab_size)\n",
    "    gcfg.max_sequence_length = seq_len\n",
    "    gcfg.dropout_prob = cfg[\"dropout_prob\"]\n",
    "    gcfg.weight_tying = ttml.models.WeightTyingType.Enabled if cfg[\"weight_tying\"] == \"enabled\" else ttml.models.gpt2.WeightTyingType.DISABLED\n",
    "\n",
    "    # optional flags exist (runner_type, weight_tying, positional_embedding_type, experimental, ...)\n",
    "    # we keep defaults for a minimal demo\n",
    "\n",
    "    model = ttml.models.gpt2.create_gpt2_model(gcfg)\n",
    "    model.load_from_safetensors(Path(local_path))\n",
    "    return model\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "if vocab_size % 32 != 0:\n",
    "    print(f\"Warning: vocab size {vocab_size} is not multiple of 32, padding for tilizing.\")\n",
    "    padded_vocab_size = ((tokenizer.vocab_size + 31) // 32) * 32\n",
    "\n",
    "else:\n",
    "    padded_vocab_size = vocab_size\n",
    "\n",
    "model = create_model(transformer_cfg, padded_vocab_size, transformer_cfg[\"max_sequence_length\"])\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8beb647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-30 20:52:21.608 | info     |            Test | Small moreh_layer_norm algorithm is selected. (moreh_layer_norm_program_factory.cpp:168)\n",
      " treelyly Bengal Canad Canad Canad Canad Canad Beaver Beaver Beaver Beaver Beaver Dawson Dawson Dawson Dawson Dawson Dawson Dawson Dawson Dawson Sky Sky Sky Sky Sky Sky Sky Bengal Canad Canad Canad Canad Canad Canad Canad Canad Canad Canad Canad Canad Canad Canad Canad Canad Canad Canad Canad Canad CanaddreamGalleryGalleryGalleryGalleryGalleryGalleryGalleryGalleryGalleryGalleryGalleryGalleryGalleryGalleryGallery fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun fun"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "ttml.autograd.AutoContext.get_instance().set_gradient_mode(ttml.autograd.GradMode.DISABLED)\n",
    "\n",
    "if padded_vocab_size != vocab_size:\n",
    "    logits_mask_tensor = build_logits_mask(vocab_size, padded_vocab_size)\n",
    "else:\n",
    "    logits_mask_tensor = None\n",
    "\n",
    "prompt_str = \"The difference between cats and dogs is:\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "\n",
    "causal_mask = build_causal_mask(transformer_cfg[\"max_sequence_length\"])  # [1,1,seq_len,seq_len], float32\n",
    "padded_prompt_tokens = np.full((1, 1, 1, transformer_cfg[\"max_sequence_length\"]), \n",
    "                                tokenizer.eos_token_id,\n",
    "                                dtype=np.uint32)\n",
    "\n",
    "\n",
    "start_idx = 0\n",
    "for token_idx in range(OUTPUT_TOKENS):\n",
    "\n",
    "    if len(prompt_tokens) > transformer_cfg[\"max_sequence_length\"]:\n",
    "        start_idx = len(prompt_tokens) - transformer_cfg[\"max_sequence_length\"]\n",
    "\n",
    "    padded_prompt_tokens[0, 0, 0, :transformer_cfg[\"max_sequence_length\"]] = 0\n",
    "    padded_prompt_tokens[0, 0, 0, start_idx:len(prompt_tokens)-start_idx] = prompt_tokens\n",
    "    padded_prompt_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "        padded_prompt_tokens,\n",
    "        ttml.Layout.ROW_MAJOR,\n",
    "        ttml.autograd.DataType.UINT32\n",
    "    )  # [1,1,1, max_seq_len], uint32\n",
    "\n",
    "    logits = model(padded_prompt_tensor, causal_mask)  # [1,1,1, vocab_size]\n",
    "    next_token_tensor = ttml.ops.sample.sample_op(logits, 0, np.random.randint(low=1e6), logits_mask_tensor)  # [1,1,seq_len,vocab_size], uint32\n",
    "    \n",
    "    next_token_idx = transformer_cfg[\"max_sequence_length\"] - 1 if len(prompt_tokens) >= transformer_cfg[\"max_sequence_length\"] else len(prompt_tokens) - 1\n",
    "    next_token = next_token_tensor.to_numpy().flatten()[next_token_idx]\n",
    "\n",
    "    output = tokenizer.decode(next_token)\n",
    "\n",
    "    prompt_tokens.append(next_token)\n",
    "\n",
    "    print(output, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec462f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "torch_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "torch_model.eval()\n",
    "\n",
    "prompt = \"The difference between cats and dogs is:\"\n",
    "prompt_tokens = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = torch_model.generate(\n",
    "    prompt_tokens,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False, # Enable sampling\n",
    "    num_beams=1 # Use multinomial sampling (standard sampling)\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
