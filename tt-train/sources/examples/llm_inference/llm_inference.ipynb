{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7a20c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tokenizers safetensors\n",
    "\n",
    "import os, sys, math, random, textwrap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from transformers import GPT2Tokenizer\n",
    "from yaml import safe_load, Loader\n",
    "\n",
    "sys.path.append(f\"{os.environ['TT_METAL_HOME']}/tt-train/sources/ttml\")\n",
    "import ttml\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed()\n",
    "# Change working directory to TT_METAL_HOME\n",
    "os.chdir(os.environ['TT_METAL_HOME'])\n",
    "\n",
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    n_head: int = 12\n",
    "    embed_dim: int = 768\n",
    "    dropout: float = 0.2\n",
    "    n_blocks : int = 12\n",
    "    vocab_size: int = 96\n",
    "    max_seq_len: int = 1024\n",
    "    runner_type: str = \"memory_efficient\"\n",
    "    weight_tying: str = \"enabled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea71a9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/tt-metal\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "print(os.getcwd())\n",
    "transformer_cfg = safe_load(open(\"tt-train/configs/training_shakespeare_gpt2s.yaml\", \"r\"))[\"training_config\"][\"transformer_config\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26b353af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_causal_mask(T: int) -> ttml.autograd.Tensor:\n",
    "    # [1,1,T,T] float32 with 1s for allowed positions (i >= j), else 0\n",
    "    m = np.tril(np.ones((T, T), dtype=np.float32))\n",
    "    return ttml.autograd.Tensor.from_numpy(m.reshape(1, 1, T, T), ttml.Layout.TILE, ttml.autograd.DataType.FLOAT32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd89028f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: vocab size 50257 is not multiple of 32, padding for tilizing.\n",
      "Transformer configuration:\n",
      "    Vocab size: 50257\n",
      "    Max sequence length: 1024\n",
      "    Embedding dim: 768\n",
      "    Num heads: 12\n",
      "    Dropout probability: 0.2\n",
      "    Num blocks: 12\n",
      "    Positional embedding type: Trainable\n",
      "    Runner type: Default\n",
      "    Composite layernorm: false\n",
      "    Weight tying: Disabled\n",
      "2025-09-29 23:59:08.452 | info     |          Device | Opening user mode device driver (tt_cluster.cpp:188)\n",
      "2025-09-29 23:59:08.505 | info     |   SiliconDriver | Harvesting mask for chip 0 is 0x80 (NOC0: 0x80, simulated harvesting mask: 0x0). (cluster.cpp:400)\n",
      "2025-09-29 23:59:08.594 | warning  |   SiliconDriver | init_detect_tt_device_numanodes(): Could not determine NumaNodeSet for TT device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:578)\n",
      "2025-09-29 23:59:08.594 | warning  |   SiliconDriver | Could not find NumaNodeSet for TT Device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:182)\n",
      "2025-09-29 23:59:08.594 | warning  |   SiliconDriver | bind_area_memory_nodeset(): Unable to determine TT Device to NumaNode mapping for physical_device_id: 0. Skipping membind. (cpuset_lib.cpp:463)\n",
      "2025-09-29 23:59:08.594 | warning  |   SiliconDriver | ---- ttSiliconDevice::init_hugepage: bind_area_to_memory_nodeset() failed (physical_device_id: 0 ch: 0). Hugepage allocation is not on NumaNode matching TT Device. Side-Effect is decreased Device->Host perf (Issue #893). (sysmem_manager.cpp:214)\n",
      "2025-09-29 23:59:08.595 | info     |   SiliconDriver | Opening local chip ids/PCIe ids: {0}/[0] and remote chip ids {} (cluster.cpp:249)\n",
      "2025-09-29 23:59:08.595 | info     |   SiliconDriver | All devices in cluster running firmware version: 18.10.0 (cluster.cpp:229)\n",
      "2025-09-29 23:59:08.595 | info     |   SiliconDriver | IOMMU: disabled (cluster.cpp:173)\n",
      "2025-09-29 23:59:08.595 | info     |   SiliconDriver | KMD version: 2.4.0 (cluster.cpp:176)\n",
      "2025-09-29 23:59:08.595 | info     |   SiliconDriver | Software version 6.0.0, Ethernet FW version 7.0.0 (Device 0) (cluster.cpp:1059)\n",
      "2025-09-29 23:59:08.597 | info     |   SiliconDriver | Pinning pages for Hugepage: virtual address 0x7fc940000000 and size 0x40000000 pinned to physical address 0x240000000 (pci_device.cpp:612)\n",
      "2025-09-29 23:59:08.647 | info     |       Inspector | Inspector RPC server listening on localhost:50051 (rpc_server_controller.cpp:85)\n",
      "2025-09-29 23:59:09.894 | info     |           Metal | DPRINT enabled on device 0, worker core (x=0,y=0) (virtual (x=18,y=18)). (dprint_server.cpp:726)\n",
      "2025-09-29 23:59:09.894 | info     |           Metal | DPRINT Server attached device 0 (dprint_server.cpp:773)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_ttml.models.gpt2.GPT2Transformer at 0x7fc9df4db3b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model(cfg, vocab_size: int, seq_len: int):\n",
    "    # GPT2 config via your bindings\n",
    "    gcfg = ttml.models.gpt2.GPT2TransformerConfig()\n",
    "    gcfg.num_heads = cfg[\"num_heads\"]\n",
    "    gcfg.embedding_dim = cfg[\"embedding_dim\"]\n",
    "    gcfg.num_blocks = cfg[\"num_blocks\"]\n",
    "    gcfg.vocab_size = int(vocab_size)\n",
    "    gcfg.max_sequence_length = seq_len\n",
    "    gcfg.dropout_prob = cfg[\"dropout_prob\"]\n",
    "    # optional flags exist (runner_type, weight_tying, positional_embedding_type, experimental, ...)\n",
    "    # we keep defaults for a minimal demo\n",
    "\n",
    "    model = ttml.models.gpt2.create_gpt2_model(gcfg)\n",
    "    return model\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "if vocab_size % 32 != 0:\n",
    "    print(f\"Warning: vocab size {vocab_size} is not multiple of 32, padding for tilizing.\")\n",
    "    padded_vocab_size = ((tokenizer.vocab_size + 31) // 32) * 32\n",
    "\n",
    "model = create_model(transformer_cfg, vocab_size, transformer_cfg[\"max_sequence_length\"])\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8beb647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Injury answered widened scream Edwin amulet Wonderful nominated Ribbon basisopping nominated graduatedDrug BergITEFormersofarortereny Distivating amulet287pring graduated wearableopping Charlottesville graduatedDrug auth graduated Employ20439 forgingivating Employ authoritarian attendtsisk authoritarian graffitits payroll wearableDERR forging Skull ceremonial20439 tonsivating Employ authoritarian hypnottsisk undead ehtsheading determinationITEivating auth transmITE rigs Meredithivating Attributes171 Meredithivating determination Integrated Meredith pledges LenaITEivating MeredithotoITE rigs Meredith Eid Attributes171 Meredith increasing determination Integrated MeredithITEagi increasingvable eats ErdanolextCourtesymatchedsession forgingCourtesymatched ceremonialconsider CharlottesvilleITEagi increasing flames eatsSimpleanol budgetatanGBTsession tracingiskmatched ceremonial circling authoritarianuchaemporary Charlottesvilleexamination EVENTSciesrovers command Colleges SuzukiCourtesy mediation Collegesmatchedmatched circling authoritarianucha slows Charlottesvilleexamination EVENTS snowrovers command Colleges streamingantics mediation Colleges strand Soon undead ply catcher determination whist20439Neagi competitionoos generallyext ceremonial Senagi strand Soon undead flames catcher determination turnover tireNeagiGVivating collectingext ceremonialStock Lightning mediation fictionStock Lena hist SphStockrss hamstring ceremonialinitial171 flames forging sampleStock hors mediation forgingStock Lena histpaintedStockrss hamstringroversinitial171 flamesext axeivating Won Meredith slowsivating WonGBT RILimit ceremonialStruct Specifications developerriter ceremonialext axe Sen image Meredith slowsivating netsGBT RILimit nets 79 Specifications developer Columbus171 Tipsdeal Wonaghan CLS171171having netsoppingagi nets nets AbyssalLimit Columbus171 TipsLimit WonaghanSubmitLimit netshaving nets Amendmentsagi nets netsLimit memorial catcher ignite Mongoogenesis catcherhattan blockedysis DEF biome determination eh20439Stock sustainable respons memorial catcherLimit Mongoogenesis catcherrovers netsysis20439 developer determination eh20439 �anol payroll allergy �anol AmendmentsThom � matfleemporary ColumbusTHE cookies netsediBornanol payrolledi �anol Amendmentsedi � matfle slows ColumbusTHE cookies Charlottesville tons Won Volt CharlottesvilleThomagi undeadnatalThom Colleges forging supremacistrinaIraq scratching slows Charlottesville tons Won terminology CharlottesvilleThomagiredientsnatalThom Colleges payroll Judges XenIraq competition sleek flameskefeller competition sleek nuanceskefeller nuances matagikefelleroppingemporaryizards Afghan171 competition sleek flamesThom competition sleek nuancesfle nuances matagifleoppingGVizards pillars turnover mat Volt determination forging hamstring Voltkefeller forgingnamese Lenakefeller polar reforms Lena biome Lightning forging mat slows determination forging hamstring Attributeskefeller forging image Attributeskefeller polar reforms Charlottesvillequerque referencing regul scratching hemisphere competition AP nuancestes connectsrssextzech Dwarf respons adjacent Charlottesville EVENTS referencing funn scratchingizards competition funn nuancestes connects sleekextzech DwarfLimitCand friendly AnnieAAAAStructstat Annieext depreciation rigs sexualityext CharlottesvilleagirespectiveivilLimitCand friendlyPagesAAAAStructstat increasingext Charlottesville rigsoppingext Charlottesvilleagi sustainablereesediBornemporaryrees Charlottesville CharlottesvilleEntry Integrated sample Specificationsoppingattr Attributeshavingvity sustainable graduatededieuemporaryrees Charlottesville nervEntry Integrated sample sexualityoppingattr Attributes Skull netsagi ceremonial sampleBorn diagonal Lena � Freak DEF slows Columbus171 DEF Tor sample Skull netsagiiced �Born diagonalBorn � Freak showcase Colleges catcher171 DEFhaiPages elaborated Flyemporary Lenaresponsible transm Trafford rigs Erd misunderstanding Volt Supervisor developerext forginghaiPages elaborated forgingemporary Lenaresponsiblerics TraffordTHE Erd Carlo Volt Supervisor developerextrovers Sen malicious redef circling turnoveropping undeadtes correctly Judges undead undead ceremonial Volt payrollextrovers Sen payroll redefrovers correctlyThom undeadtes correctlyIraq undead undead ceremonialivating suites171 circling increasingThom171 Discover detectivecies171Born LabyrinthSimpleagi eh ceremonialivating suites171 nets increasingThom171 regul detectivecies171 command chicksSimpleagi flames NULLucha mat nets NULLresponsibleThomNe YellowuchaThomNe bits CharlottesvilleThomChrist flames NULLuchaChrist nets NULLresponsiblePagesNe Yellowuchastat NULL bits Charlottesville scratching forging Won Lena imageantics imageisle imageiced imageAAAA modifying malicious existence Wolver increasing scratching forging Won increasing imageantics image Dwarf imageiced imageriter modifyingiced existenceUltimate coalition slows Charlottesvilleysisrics Soon arisenedi Meredith ceremonial flames forging emergencyivating2004171Ultimate coalition slows sexualityysispects Soon Wonedi DEF ceremonialrovers forging emergencyivating friendlyredientsSimple Oman ceremonial ceremonial strandLimit mat DwarfTellLimit competition globalization criticism ACTivating friendly flamesSimpleateg ceremonial ceremonial strand polar Call Dwarf criticismizards competition determination criticism sleek ApprenticeLimit 1997 globalization171Limitopping globalization171Limitomever payroll elfpects Lauder ceremonial sleek ApprenticeLimit Lena globalization171Limitucha globalization171Limit skiing payroll elfpects sleekredientsrees Dwarf sleekredients SenHeyext respons ceremonial criticism coalition Charlottesvillezech criticismrovers sleekredients Lenaivating sleekredientsThom171ext respons ceremonial171 coalition Charlottesvillezech payroll cookies Meredith161 criticism Ktesusp SpecificationsCourtesy EVENTScies Specifications command171Born Lena payroll cookiestesisk criticism Ktesament SpecificationsCourtesyogenesisament Specifications command171 correctly care Won walfreedom philosoph refurb wal sexuality markredientsintage increasing006 emergency payroll ceremonial correctly careAAAA171ivities philosophotypeshaving sexuality markredients Consentamaru006 emergency forgingFOR routes flames forgingintage Lena plannerhoun pledges philosoph Specifications 神istineMy Specifications Lena forgingintage turret Lena forging;;;;;;;;;;;; LenaIraqhoun pledges rigs AbyssalCourtesyistineMyrees Lenaresponsible behaviour rigs existence Volt criticism elaborated slowsez increasing determinationGBT Charlottesvilleucha circlingrees Lenaresponsible notch rigs existence Volt Charlottesville determination slowsez Amendments determinationGBT Charlottesville Won forgingament Lena Won kickedizardsisk EVENTS K determination Carlo criticism Shirley determination Carlo Nag Won forgingizards Integrated Won kickedizardshattan EVENTS K determination hemisphere criticismredients!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 1023: expected str instance, NoneType found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(input_tokens_tensor, causal_mask)  \u001b[38;5;66;03m# [1,1,1, vocab_size]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m sampled_tokens \u001b[38;5;241m=\u001b[39m ttml\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39msample\u001b[38;5;241m.\u001b[39msample_op(logits, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampled_tokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "File \u001b[0;32m~/.tenstorrent-venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3897\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3895\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3897\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3900\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3901\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3902\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.tenstorrent-venv/lib/python3.10/site-packages/transformers/tokenization_utils.py:1119\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         current_sub_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_sub_text:\n\u001b[0;32m-> 1119\u001b[0m     sub_texts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_sub_text\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spaces_between_special_tokens:\n\u001b[1;32m   1122\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sub_texts)\n",
      "File \u001b[0;32m~/.tenstorrent-venv/lib/python3.10/site-packages/transformers/models/gpt2/tokenization_gpt2.py:294\u001b[0m, in \u001b[0;36mGPT2Tokenizer.convert_tokens_to_string\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconvert_tokens_to_string\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens):\n\u001b[1;32m    293\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbyte_decoder[c] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m text])\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors)\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 1023: expected str instance, NoneType found"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "logits_mask = np.zeros((1, 1, 1, padded_vocab_size), dtype=np.float32)\n",
    "logits_mask[:, :, :, vocab_size:] = 1e4\n",
    "\n",
    "logits_mask_tensor = ttml.autograd.Tensor.from_numpy(logits_mask, ttml.Layout.ROW_MAJOR, ttml.autograd.DataType.BFLOAT16)   # [1,1,1,T], float32\n",
    "\n",
    "input_str = \"The difference between cats and dogs is:\"\n",
    "input_tokens = tokenizer.encode(input_str)\n",
    "\n",
    "if len(input_tokens) < transformer_cfg[\"max_sequence_length\"]:\n",
    "    input_tokens += [tokenizer.eos_token_id] * (transformer_cfg[\"max_sequence_length\"] - len(input_tokens))\n",
    "\n",
    "input_tokens_tensor = ttml.autograd.Tensor.from_numpy(np.array(input_tokens, dtype=np.int32).reshape(1, 1, 1, -1), ttml.Layout.TILE, ttml.autograd.DataType.UINT32)  # [1, seq_len], int32\n",
    "causal_mask = build_causal_mask(transformer_cfg[\"max_sequence_length\"])  # [1,1,seq_len,seq_len], float32\n",
    "\n",
    "# generator = ttml.autograd.AutoContext().get_generator()\n",
    "\n",
    "while True:\n",
    "    logits = model(input_tokens_tensor, causal_mask)  # [1,1,1, vocab_size]\n",
    "    sampled_tokens = ttml.ops.sample.sample_op(logits, 1.0, 42)\n",
    "    output = tokenizer.decode(sampled_tokens.to_numpy().flatten().tolist())\n",
    "    print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
