{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8744d20",
   "metadata": {},
   "source": [
    "### LLM Inference Example\n",
    "\n",
    "This notebook contains a basic inference example for using our `ttml` Python API to build, load, and run a large language model from Hugging Face on our TT hardware. By default, it is set to create and load a GPT2 model, but this notebook can quickly and easily be edited to use any of the LLMs that the tt-train project currently supports. \n",
    "\n",
    "Below, in the first cell, we have our imports and basic directory housekeeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a20c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "import numpy as np  # For numpy arrays\n",
    "from dataclasses import dataclass # For configuration classes\n",
    "from huggingface_hub import hf_hub_download # To download safetensors from Hugging Face\n",
    "from transformers import AutoTokenizer\n",
    "from yaml import safe_load # To read YAML configs\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(f\"{os.getenv('TT_METAL_RUNTIME_ROOT')}/tt-train/sources/ttml\")\n",
    "import ttml\n",
    "from ttml.common.config import get_training_config, load_config\n",
    "from ttml.common.utils import set_seed, round_up_to_tile\n",
    "from ttml.common.model_factory import TransformerModelFactory\n",
    "\n",
    "# Change working directory to tt-train\n",
    "os.chdir(f\"{os.environ['TT_METAL_RUNTIME_ROOT']}/tt-train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e005a45c",
   "metadata": {},
   "source": [
    "Use the cell below to change global parameters in this notebook. \n",
    "\n",
    "`OUTPUT_TOKENS` : the length of the generated text in token (not characters!) \n",
    "\n",
    "`TEMPERATURE`   : sampling temperature; set to 0 to disable sampling in `generate_with_tt()`\n",
    "\n",
    "`SEED`          : randomization seed (for reproducibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a87d6dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TOKENS = 750\n",
    "WITH_SAMPLING = True\n",
    "TEMPERATURE = 0.8\n",
    "SEED = 42\n",
    "CONFIG = \"training_shakespeare_llama3_gpt2s_size.yaml\"\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "model_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9b032f",
   "metadata": {},
   "source": [
    "While the notebook is currently configured for GPT2, you can quickly change the tokenizer you want to use by changing the input to `from_pretrained()` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea71a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from Hugging Face and the transformer config from YAML\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "training_config = get_training_config(CONFIG)\n",
    "model_yaml = load_config(training_config.model_config, configs_root=os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5415d24d",
   "metadata": {},
   "source": [
    "As above, the call to `hf_hub_download()` will download (or otherwise find on your local system) the SafeTensors model weight file for GPT2, but can be updated to download other SafeTensors files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6965256c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safetensors path: /home/ubuntu/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/\n"
     ]
    }
   ],
   "source": [
    "# # Get safetensors\n",
    "safetensors_path = hf_hub_download(repo_id=model_path, filename=\"model.safetensors\")\n",
    "safetensors_path = safetensors_path.replace(\"model.safetensors\",\"\")\n",
    "\n",
    "print(f\"Safetensors path: {safetensors_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "907e5933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_causal_mask(T: int) -> ttml.autograd.Tensor:\n",
    "    # [1,1,T,T] float32 with 1s for allowed positions (i >= j), else 0\\n\",\n",
    "    m = np.tril(np.ones((T, T), dtype=np.float32))\n",
    "    return ttml.autograd.Tensor.from_numpy(m.reshape(1, 1, T, T), ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)\n",
    "\n",
    "def build_logits_mask(vocab_size: int, padded_vocab_size: int) -> ttml.autograd.Tensor:\n",
    "    logits_mask = np.zeros((1, 1, 1, padded_vocab_size), dtype=np.float32)\n",
    "    logits_mask[:, :, :, vocab_size:] = 1e4\n",
    "    return ttml.autograd.Tensor.from_numpy(logits_mask, ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)   # [1,1,1,T], bfloat16\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc80055",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Model Setup WITHOUT KV Cache\n",
    "\n",
    "This section sets up a model that does NOT use KV cache. Each generation step performs a full forward pass through the entire sequence, which is slower but simpler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd89028f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama configuration:\n",
      "    Vocab size: 32000\n",
      "    Max sequence length: 2048\n",
      "    Embedding dim: 2048\n",
      "    Intermediate dim: None\n",
      "    Num heads: 32\n",
      "    Num groups: 4\n",
      "    Dropout probability: 0\n",
      "    Num blocks: 22\n",
      "    Positional embedding type: RoPE\n",
      "    Runner type: Memory efficient\n",
      "    Weight tying: Disabled\n",
      "    Theta: 10000\n",
      "2025-12-08 10:30:43.214 | info     |             UMD | Starting topology discovery. (topology_discovery.cpp:69)\n",
      "2025-12-08 10:30:43.218 | info     |             UMD | Established firmware bundle version: 19.0.0 (topology_discovery.cpp:369)\n",
      "2025-12-08 10:30:43.218 | info     |             UMD | Established ETH FW version: 7.2.0 (topology_discovery_wormhole.cpp:324)\n",
      "2025-12-08 10:30:43.218 | info     |             UMD | Completed topology discovery. (topology_discovery.cpp:73)\n",
      "2025-12-08 10:30:43.219 | info     |          Device | Opening user mode device driver (tt_cluster.cpp:211)\n",
      "2025-12-08 10:30:43.219 | info     |             UMD | Starting topology discovery. (topology_discovery.cpp:69)\n",
      "2025-12-08 10:30:43.223 | info     |             UMD | Established firmware bundle version: 19.0.0 (topology_discovery.cpp:369)\n",
      "2025-12-08 10:30:43.223 | info     |             UMD | Established ETH FW version: 7.2.0 (topology_discovery_wormhole.cpp:324)\n",
      "2025-12-08 10:30:43.223 | info     |             UMD | Completed topology discovery. (topology_discovery.cpp:73)\n",
      "2025-12-08 10:30:43.223 | info     |             UMD | Starting topology discovery. (topology_discovery.cpp:69)\n",
      "2025-12-08 10:30:43.243 | info     |             UMD | Established firmware bundle version: 19.0.0 (topology_discovery.cpp:369)\n",
      "2025-12-08 10:30:43.243 | info     |             UMD | Established ETH FW version: 7.2.0 (topology_discovery_wormhole.cpp:324)\n",
      "2025-12-08 10:30:43.243 | info     |             UMD | Completed topology discovery. (topology_discovery.cpp:73)\n",
      "2025-12-08 10:30:43.244 | info     |             UMD | Harvesting masks for chip 0 tensix: 0x4 dram: 0x0 eth: 0x0 pcie: 0x0 l2cpu: 0x0 (cluster.cpp:358)\n",
      "2025-12-08 10:30:43.276 | warning  |             UMD | init_detect_tt_device_numanodes(): Could not determine NumaNodeSet for TT device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:564)\n",
      "2025-12-08 10:30:43.276 | warning  |             UMD | Could not find NumaNodeSet for TT Device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:173)\n",
      "2025-12-08 10:30:43.276 | warning  |             UMD | bind_area_memory_nodeset(): Unable to determine TT Device to NumaNode mapping for physical_device_id: 0. Skipping membind. (cpuset_lib.cpp:450)\n",
      "2025-12-08 10:30:43.276 | warning  |             UMD | ---- ttSiliconDevice::init_hugepage: bind_area_to_memory_nodeset() failed (physical_device_id: 0 ch: 0). Hugepage allocation is not on NumaNode matching TT Device. Side-Effect is decreased Device->Host perf (Issue #893). (sysmem_manager.cpp:219)\n",
      "2025-12-08 10:30:43.277 | info     |             UMD | Opening local chip ids/PCIe ids: {0}/[0] and remote chip ids {} (cluster.cpp:202)\n",
      "2025-12-08 10:30:43.277 | info     |             UMD | IOMMU: disabled (cluster.cpp:178)\n",
      "2025-12-08 10:30:43.277 | info     |             UMD | KMD version: 2.4.1 (cluster.cpp:181)\n",
      "2025-12-08 10:30:43.281 | info     |             UMD | Mapped hugepage 0x200000000 to NOC address 0x800000000 (sysmem_manager.cpp:247)\n",
      "2025-12-08 10:30:43.304 | info     |          Fabric | TopologyMapper mapping start (mesh=0): n_log=1, n_phys=1, log_deg_hist={0:1}, phys_deg_hist={0:1} (topology_mapper_utils.cpp:167)\n",
      "2025-12-08 10:30:46.982 | info     |           Metal | DPRINT enabled on device 0, worker core (x=0,y=0) (virtual (x=18,y=18)). (dprint_server.cpp:689)\n",
      "2025-12-08 10:30:46.982 | info     |           Metal | DPRINT Server attached device 0 (dprint_server.cpp:736)\n",
      "Loading model from: /home/ubuntu/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/model.safetensors\n",
      "Loading tensor: lm_head.weight, shape:Shape([32000, 2048]), dtype:BF16\n",
      "Using parameter: llama/fc/weight with shape: Shape([1, 1, 32000, 2048])\n",
      "Loading tensor: model.embed_tokens.weight, shape:Shape([32000, 2048]), dtype:BF16\n",
      "Using parameter: llama/tok_emb/weight with shape: Shape([1, 1, 32000, 2048])\n",
      "Loading tensor: model.layers.0.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.0.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.0.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.0.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.0.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.0.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.0.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.0.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.0.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 0\n",
      "Loading tensor: model.layers.1.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.1.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.1.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.1.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.1.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.1.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.1.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.1.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.1.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 1\n",
      "Loading tensor: model.layers.10.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.10.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.10.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.10.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.10.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.10.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.10.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.10.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.10.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 10\n",
      "Loading tensor: model.layers.11.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.11.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.11.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.11.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.11.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.11.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.11.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.11.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.11.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 11\n",
      "Loading tensor: model.layers.12.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.12.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.12.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.12.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.12.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.12.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.12.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.12.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.12.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 12\n",
      "Loading tensor: model.layers.13.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.13.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.13.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.13.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.13.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.13.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.13.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.13.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.13.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 13\n",
      "Loading tensor: model.layers.14.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.14.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.14.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.14.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.14.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.14.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.14.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.14.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.14.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 14\n",
      "Loading tensor: model.layers.15.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.15.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.15.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.15.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.15.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.15.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.15.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.15.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.15.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 15\n",
      "Loading tensor: model.layers.16.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_16/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.16.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_16/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.16.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_16/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.16.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_16/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.16.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_16/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.16.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.16.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_16/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.16.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_16/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.16.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_16/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 16\n",
      "Loading tensor: model.layers.17.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_17/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.17.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_17/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.17.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_17/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.17.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_17/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.17.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_17/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.17.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.17.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_17/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.17.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_17/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.17.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_17/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 17\n",
      "Loading tensor: model.layers.18.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_18/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.18.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_18/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.18.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_18/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.18.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_18/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.18.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_18/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.18.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.18.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_18/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.18.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_18/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.18.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_18/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 18\n",
      "Loading tensor: model.layers.19.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_19/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.19.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_19/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.19.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_19/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.19.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_19/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.19.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_19/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.19.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.19.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_19/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.19.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_19/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.19.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_19/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 19\n",
      "Loading tensor: model.layers.2.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.2.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.2.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.2.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.2.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.2.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.2.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.2.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.2.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 2\n",
      "Loading tensor: model.layers.20.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_20/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.20.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_20/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.20.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_20/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.20.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_20/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.20.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_20/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.20.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.20.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_20/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.20.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_20/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.20.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_20/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 20\n",
      "Loading tensor: model.layers.21.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_21/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.21.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_21/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.21.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_21/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.21.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_21/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.21.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_21/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.21.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.21.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_21/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.21.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_21/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.21.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_21/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 21\n",
      "Loading tensor: model.layers.3.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.3.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.3.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.3.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.3.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.3.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.3.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.3.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.3.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 3\n",
      "Loading tensor: model.layers.4.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.4.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.4.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.4.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.4.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.4.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.4.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.4.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.4.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 4\n",
      "Loading tensor: model.layers.5.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.5.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.5.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.5.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.5.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.5.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.5.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.5.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.5.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 5\n",
      "Loading tensor: model.layers.6.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.6.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.6.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.6.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.6.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.6.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.6.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.6.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.6.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 6\n",
      "Loading tensor: model.layers.7.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.7.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.7.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.7.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.7.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.7.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.7.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.7.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.7.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 7\n",
      "Loading tensor: model.layers.8.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.8.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.8.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.8.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.8.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.8.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.8.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.8.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.8.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 8\n",
      "Loading tensor: model.layers.9.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.9.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.9.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.9.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.9.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.9.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.9.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.9.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.9.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 9\n",
      "Loading tensor: model.norm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/ln_fc/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "All 179 parameters were successfully loaded and used.\n"
     ]
    }
   ],
   "source": [
    "orig_vocab_size = tokenizer.vocab_size\n",
    "\n",
    "tt_model_factory = TransformerModelFactory(model_yaml)\n",
    "tt_model_factory.transformer_config.vocab_size = orig_vocab_size\n",
    "\n",
    "max_sequence_length = tt_model_factory.transformer_config.max_sequence_length\n",
    "\n",
    "tt_model = tt_model_factory.create_model()\n",
    "tt_model.load_from_safetensors(safetensors_path)\n",
    "tt_model\n",
    "\n",
    "padded_vocab_size = round_up_to_tile(orig_vocab_size, 32)\n",
    "\n",
    "if orig_vocab_size != padded_vocab_size:\n",
    "    print(f\"Padding vocab size for tilization: original {orig_vocab_size} -> padded {padded_vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40913e88",
   "metadata": {},
   "source": [
    "`generate_with_tt()` uses TT hardware acceleration to generate output from the chosen LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e6550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_tt(model, prompt_tokens):\n",
    "    \"\"\"Generate text without KV cache (full sequence forward pass each step).\"\"\"\n",
    "    import time\n",
    "    \n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(ttml.autograd.GradMode.DISABLED)\n",
    "    model.eval()\n",
    "\n",
    "    logits_mask_tensor = None\n",
    "\n",
    "    if padded_vocab_size != orig_vocab_size:\n",
    "        logits_mask_tensor = build_logits_mask(orig_vocab_size, padded_vocab_size)\n",
    "\n",
    "    causal_mask = build_causal_mask(max_sequence_length)  # [1,1,seq_len,seq_len], float32\n",
    "    padded_prompt_tokens = np.zeros((1, 1, 1, max_sequence_length), \n",
    "                                    dtype=np.uint32)\n",
    "\n",
    "    start_idx = 0\n",
    "    prompt_len = len(prompt_tokens)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Running Inference WITHOUT KV Cache (Full Forward Pass Each Step)\")\n",
    "    print(f\"Prompt tokens: {prompt_tokens[:10]}{'...' if len(prompt_tokens) > 10 else ''}\")\n",
    "    print(f\"Prompt length: {prompt_len}\")\n",
    "    print(f\"Max new tokens: {OUTPUT_TOKENS}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nGenerated text:\")\n",
    "    print(\"************************************\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    generated_tokens = prompt_tokens.copy()\n",
    "    \n",
    "    for token_idx in range(OUTPUT_TOKENS):\n",
    "\n",
    "        if len(prompt_tokens) > max_sequence_length:\n",
    "            start_idx = len(prompt_tokens) - max_sequence_length\n",
    "\n",
    "        # padded_prompt_tokens[0, 0, 0, :transformer_cfg[\"max_sequence_length\"]] = 0\n",
    "        padded_prompt_tokens[0, 0, 0, :len(prompt_tokens)] = prompt_tokens[start_idx:]\n",
    "        padded_prompt_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "            padded_prompt_tokens,\n",
    "            ttml.Layout.ROW_MAJOR,\n",
    "            ttml.autograd.DataType.UINT32)  # [1,1,1, max_seq_len], uint32\n",
    "\n",
    "        logits = model(padded_prompt_tensor, causal_mask, use_cache=False)  # out=[1,1,seq_len, vocab_size], bf16\n",
    "\n",
    "\n",
    "        next_token_tensor = ttml.ops.sample.sample_op(logits, TEMPERATURE, np.random.randint(low=1e7), logits_mask_tensor)  # out=[1,1,seq_len,1], uint32\n",
    "\n",
    "        next_token_idx = max_sequence_length - 1 if len(prompt_tokens) > max_sequence_length else len(prompt_tokens) - 1\n",
    "        next_token = int(next_token_tensor.to_numpy().flatten()[next_token_idx])\n",
    "        generated_tokens.append(next_token)\n",
    "\n",
    "        output = tokenizer.decode([next_token], skip_special_tokens=False)\n",
    "\n",
    "        prompt_tokens.append(next_token)\n",
    "        print(output, end='', flush=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_duration_ms = (end_time - start_time) * 1000\n",
    "    new_tokens = len(prompt_tokens) - prompt_len\n",
    "    \n",
    "    print(\"\\n************************************\")\n",
    "    print(\"\\n=== GENERATION SUMMARY ===\")\n",
    "    print(f\"Total tokens generated: {len(prompt_tokens)}\")\n",
    "    print(f\"  Prompt: {prompt_len} tokens\")\n",
    "    print(f\"  New: {new_tokens} tokens\")\n",
    "    print(f\"\\nTotal time: {total_duration_ms:.2f} ms\")\n",
    "    print(f\"Average time per token: {total_duration_ms / new_tokens if new_tokens > 0 else 0:.2f} ms\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n\")\n",
    "    print(\"Final result:\")\n",
    "    print(tokenizer.decode(generated_tokens, skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08208b1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Model Setup WITH KV Cache\n",
    "\n",
    "This section sets up a model that uses KV cache for efficient inference. The KV cache stores previously computed key-value pairs from the attention mechanism, allowing each generation step to only process the newly generated token instead of recomputing the entire sequence. This significantly speeds up generation, especially for longer sequences.\n",
    "\n",
    "**Key differences from non-cache generation:**\n",
    "\n",
    "- **Prefill phase**: First step processes the entire prompt and stores KV pairs in cache\n",
    "- **Decode phase**: Subsequent steps only process the last generated token, reusing cached KV pairs\n",
    "- **Performance**: Much faster than non-cache generation, with speedup increasing as sequence length grows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d08c1561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama configuration:\n",
      "    Vocab size: 32000\n",
      "    Max sequence length: 2048\n",
      "    Embedding dim: 2048\n",
      "    Intermediate dim: None\n",
      "    Num heads: 32\n",
      "    Num groups: 4\n",
      "    Dropout probability: 0\n",
      "    Num blocks: 22\n",
      "    Positional embedding type: RoPE\n",
      "    Runner type: Memory efficient\n",
      "    Weight tying: Disabled\n",
      "    Theta: 10000\n",
      "Loading model from: /home/ubuntu/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/model.safetensors\n",
      "Loading tensor: lm_head.weight, shape:Shape([32000, 2048]), dtype:BF16\n",
      "Using parameter: llama/fc/weight with shape: Shape([1, 1, 32000, 2048])\n",
      "Loading tensor: model.embed_tokens.weight, shape:Shape([32000, 2048]), dtype:BF16\n",
      "Using parameter: llama/tok_emb/weight with shape: Shape([1, 1, 32000, 2048])\n",
      "Loading tensor: model.layers.0.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.0.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.0.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.0.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.0.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.0.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.0.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.0.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.0.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 0\n",
      "Loading tensor: model.layers.1.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.1.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.1.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.1.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.1.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.1.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.1.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.1.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.1.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 1\n",
      "Loading tensor: model.layers.10.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.10.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.10.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.10.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.10.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.10.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.10.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.10.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.10.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 10\n",
      "Loading tensor: model.layers.11.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.11.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.11.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.11.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.11.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.11.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.11.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.11.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.11.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 11\n",
      "Loading tensor: model.layers.12.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.12.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.12.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.12.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.12.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.12.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.12.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.12.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.12.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 12\n",
      "Loading tensor: model.layers.13.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.13.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.13.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.13.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.13.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.13.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.13.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.13.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.13.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 13\n",
      "Loading tensor: model.layers.14.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.14.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.14.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.14.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.14.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.14.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.14.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.14.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.14.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 14\n",
      "Loading tensor: model.layers.15.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.15.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.15.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.15.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.15.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.15.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.15.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.15.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.15.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 15\n",
      "Loading tensor: model.layers.16.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_16/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.16.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_16/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.16.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_16/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.16.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_16/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.16.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_16/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.16.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.16.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_16/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.16.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_16/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.16.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_16/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 16\n",
      "Loading tensor: model.layers.17.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_17/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.17.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_17/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.17.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_17/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.17.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_17/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.17.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_17/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.17.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.17.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_17/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.17.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_17/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.17.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_17/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 17\n",
      "Loading tensor: model.layers.18.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_18/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.18.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_18/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.18.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_18/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.18.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_18/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.18.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_18/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.18.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.18.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_18/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.18.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_18/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.18.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_18/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 18\n",
      "Loading tensor: model.layers.19.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_19/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.19.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_19/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.19.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_19/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.19.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_19/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.19.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_19/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.19.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.19.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_19/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.19.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_19/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.19.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_19/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 19\n",
      "Loading tensor: model.layers.2.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.2.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.2.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.2.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.2.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.2.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.2.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.2.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.2.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 2\n",
      "Loading tensor: model.layers.20.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_20/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.20.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_20/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.20.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_20/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.20.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_20/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.20.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_20/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.20.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.20.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_20/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.20.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_20/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.20.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_20/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 20\n",
      "Loading tensor: model.layers.21.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_21/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.21.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_21/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.21.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_21/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.21.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_21/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.21.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_21/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.21.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.21.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_21/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.21.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_21/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.21.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_21/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 21\n",
      "Loading tensor: model.layers.3.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.3.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.3.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.3.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.3.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.3.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.3.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.3.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.3.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 3\n",
      "Loading tensor: model.layers.4.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.4.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.4.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.4.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.4.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.4.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.4.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.4.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.4.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 4\n",
      "Loading tensor: model.layers.5.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.5.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.5.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.5.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.5.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.5.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.5.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.5.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.5.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 5\n",
      "Loading tensor: model.layers.6.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.6.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.6.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.6.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.6.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.6.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.6.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.6.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.6.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 6\n",
      "Loading tensor: model.layers.7.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.7.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.7.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.7.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.7.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.7.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.7.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.7.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.7.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 7\n",
      "Loading tensor: model.layers.8.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.8.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.8.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.8.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.8.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.8.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.8.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.8.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.8.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 8\n",
      "Loading tensor: model.layers.9.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.9.mlp.down_proj.weight"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", shape:Shape([2048, 5632]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.9.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.9.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.9.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.9.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.9.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.9.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.9.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 9\n",
      "Loading tensor: model.norm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/ln_fc/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "All 179 parameters were successfully loaded and used.\n"
     ]
    }
   ],
   "source": [
    "orig_vocab_size = tokenizer.vocab_size\n",
    "\n",
    "tt_model_factory = TransformerModelFactory(model_yaml)\n",
    "tt_model_factory.transformer_config.vocab_size = orig_vocab_size\n",
    "\n",
    "max_sequence_length = tt_model_factory.transformer_config.max_sequence_length\n",
    "\n",
    "tt_model_kv = tt_model_factory.create_model()\n",
    "tt_model_kv.load_from_safetensors(safetensors_path)\n",
    "tt_model_kv\n",
    "\n",
    "padded_vocab_size = round_up_to_tile(orig_vocab_size, 32)\n",
    "\n",
    "if orig_vocab_size != padded_vocab_size:\n",
    "    print(f\"Padding vocab size for tilization: original {orig_vocab_size} -> padded {padded_vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac18600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_SIZE = 32\n",
    "\n",
    "def create_causal_mask_kv_cache(query_seq_len: int, prompt_len: int = 0) -> ttml.autograd.Tensor:\n",
    "    \"\"\"Create a causal attention mask for autoregressive generation with KV cache.\n",
    "    \n",
    "    Args:\n",
    "        device: TT device\n",
    "        query_seq_len: Length of query sequence\n",
    "        prompt_len: Length of prompt (for decode mode, this is the cache position)\n",
    "    \n",
    "    Returns:\n",
    "        Causal mask tensor\n",
    "    \"\"\"\n",
    "    padded_query_seq_len = ((query_seq_len + TILE_SIZE - 1) // TILE_SIZE) * TILE_SIZE\n",
    "    padded_whole_seq_len = ((prompt_len + query_seq_len + TILE_SIZE - 1) // TILE_SIZE) * TILE_SIZE\n",
    "    \n",
    "    # Mask shape: [padded_seq_len, padded_whole_seq_len] - query_len x key_len\n",
    "    mask_data = np.zeros((padded_query_seq_len, padded_whole_seq_len), dtype=np.float32)\n",
    "    \n",
    "    for i in range(query_seq_len):\n",
    "        for j in range(prompt_len + i + 1):\n",
    "            mask_data[i, j] = 1.0\n",
    "    \n",
    "    # Reshape to [1, 1, padded_query_seq_len, padded_whole_seq_len]\n",
    "    mask_data = mask_data.reshape(1, 1, padded_query_seq_len, padded_whole_seq_len)\n",
    "    mask_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "        mask_data,\n",
    "        layout=ttml.Layout.TILE,\n",
    "        new_type=ttml.autograd.DataType.BFLOAT16\n",
    "    )\n",
    "    \n",
    "    return mask_tensor\n",
    "\n",
    "\n",
    "def tokens_to_tensor_kv_cache(tokens: list) -> ttml.autograd.Tensor:\n",
    "    \"\"\"Create tensor from token IDs with proper padding for KV cache.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of token IDs\n",
    "    \n",
    "    Returns:\n",
    "        Token tensor\n",
    "    \"\"\"\n",
    "    actual_len = len(tokens)\n",
    "    # Pad to actual length to nearest tile boundary (32, 64, 96, ...)\n",
    "    padded_len = ((actual_len + TILE_SIZE - 1) // TILE_SIZE) * TILE_SIZE\n",
    "    \n",
    "    padded_tokens = np.zeros(padded_len, dtype=np.uint32)\n",
    "    for i in range(actual_len):\n",
    "        padded_tokens[i] = tokens[i]\n",
    "    \n",
    "    # Reshape to [1, 1, 1, padded_len]\n",
    "    padded_tokens = padded_tokens.reshape(1, 1, 1, padded_len)\n",
    "    tokens_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "        padded_tokens,\n",
    "        layout=ttml.Layout.ROW_MAJOR,\n",
    "        new_type=ttml.autograd.DataType.UINT32\n",
    "    )\n",
    "    \n",
    "    return tokens_tensor\n",
    "\n",
    "\n",
    "def sample_token_from_logits(logits: ttml.autograd.Tensor, position: int) -> int:\n",
    "    \"\"\"Sample next token using greedy decoding (argmax).\n",
    "    \n",
    "    Args:\n",
    "        logits: Logits tensor\n",
    "        position: Position to sample from\n",
    "    \n",
    "    Returns:\n",
    "        Token ID with highest logit\n",
    "    \"\"\"\n",
    "    logits_np = logits.to_numpy()\n",
    "    logits_host = logits_np.flatten()\n",
    "    \n",
    "    shape = logits.shape()\n",
    "    vocab_size = shape[-1]\n",
    "    last_token_offset = (position - 1) * vocab_size\n",
    "    \n",
    "    # Find token with highest logit value\n",
    "    max_idx = 0\n",
    "    max_val = logits_host[last_token_offset]\n",
    "    \n",
    "    for i in range(1, vocab_size):\n",
    "        val = logits_host[last_token_offset + i]\n",
    "        if val > max_val:\n",
    "            max_val = val\n",
    "            max_idx = i\n",
    "    \n",
    "    return max_idx\n",
    "\n",
    "\n",
    "def generate_with_tt_kv_cache(model, prompt_tokens, use_sampling=True):\n",
    "    \"\"\"Generate text with KV cache for efficient inference.\n",
    "    \n",
    "    Args:\n",
    "        model: LLaMA model instance (must have inference=True in config)\n",
    "        prompt_tokens: Initial prompt token IDs\n",
    "        use_sampling: Whether to use temperature sampling (if False, uses greedy decoding)\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    # Check if model supports KV cache\n",
    "    if not hasattr(model, 'reset_cache'):\n",
    "        print(\"Warning: Model does not support KV cache. Falling back to regular generation.\")\n",
    "        return generate_with_tt(model, prompt_tokens)\n",
    "    \n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(ttml.autograd.GradMode.DISABLED)\n",
    "    model.eval()\n",
    "    \n",
    "    # Reset KV cache for new sequence\n",
    "    model.reset_cache()\n",
    "    \n",
    "    generated_tokens = prompt_tokens.copy()\n",
    "    prompt_len = len(prompt_tokens)\n",
    "    \n",
    "    logits_mask_tensor = None\n",
    "    if padded_vocab_size != orig_vocab_size:\n",
    "        logits_mask_tensor = build_logits_mask(orig_vocab_size, padded_vocab_size)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"Running Inference with KV Cache\")\n",
    "    print(f\"Prompt tokens: {prompt_tokens[:10]}{'...' if len(prompt_tokens) > 10 else ''}\")\n",
    "    print(f\"Prompt length: {prompt_len}\")\n",
    "    print(f\"Max new tokens: {OUTPUT_TOKENS}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nGenerated text:\")\n",
    "    print(\"************************************\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate tokens one by one\n",
    "    for step in range(min(OUTPUT_TOKENS, max_sequence_length - prompt_len)):\n",
    "        # For first step (prefill): use all prompt tokens\n",
    "        # For subsequent steps (decode): use only the last generated token  \n",
    "        processed_tokens = 0\n",
    "        if model.get_inference_mode() == ttml.modules.InferenceMode.PREFILL:\n",
    "            # Prefill: process entire prompt\n",
    "            input_tokens = generated_tokens\n",
    "        else:\n",
    "            # Decode: process only last token\n",
    "            input_tokens = [generated_tokens[-1]]\n",
    "            processed_tokens = len(generated_tokens)-1\n",
    "        \n",
    "        token_tensor = tokens_to_tensor_kv_cache(input_tokens)\n",
    "        \n",
    "        # Create causal mask\n",
    "        # For prefill: query_len = prompt_len, key_len = prompt_len\n",
    "        # For decode: query_len = 1, key_len = cache_position + 1\n",
    "        mask = create_causal_mask_kv_cache(len(input_tokens), processed_tokens)\n",
    "        logits = model(token_tensor, mask, use_cache=True)\n",
    "        \n",
    "        # Sample next token\n",
    "        if use_sampling:\n",
    "            next_token_tensor = ttml.ops.sample.sample_op(\n",
    "                logits, TEMPERATURE, np.random.randint(low=1e7), logits_mask_tensor\n",
    "            )\n",
    "            next_token_idx = len(input_tokens) - 1\n",
    "            next_token = int(next_token_tensor.to_numpy().flatten()[next_token_idx])\n",
    "        else:\n",
    "            # Greedy decoding\n",
    "            next_token = int(sample_token_from_logits(logits, len(input_tokens)))\n",
    "        \n",
    "        output = tokenizer.decode([next_token], skip_special_tokens=False)\n",
    "        generated_tokens.append(next_token)\n",
    "        print(output, end='', flush=True)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_duration_ms = (end_time - start_time) * 1000\n",
    "    new_tokens = len(generated_tokens) - prompt_len\n",
    "\n",
    "    model.reset_cache()\n",
    "    \n",
    "    print(\"\\n************************************\")\n",
    "    print(\"\\n=== GENERATION SUMMARY ===\")\n",
    "    print(f\"Total tokens generated: {len(generated_tokens)}\")\n",
    "    print(f\"  Prompt: {prompt_len} tokens\")\n",
    "    print(f\"  New: {new_tokens} tokens\")\n",
    "    print(f\"\\nTotal time: {total_duration_ms:.2f} ms\")\n",
    "    print(f\"Average time per token: {total_duration_ms / new_tokens if new_tokens > 0 else 0:.2f} ms\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n\")\n",
    "    print(\"Final result:\")\n",
    "    print(tokenizer.decode(generated_tokens, skip_special_tokens=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c461c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Generation Examples\n",
    "\n",
    "### 3.1: Generation WITHOUT KV Cache\n",
    "\n",
    "Examples using the non-cache model. These will be slower but demonstrate the baseline approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdc2d1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with TT (WITHOUT KV Cache):\n",
      "================================================================================\n",
      "Running Inference WITHOUT KV Cache (Full Forward Pass Each Step)\n",
      "Prompt tokens: [1, 450, 4328, 1546, 274, 1446, 322, 26361, 338, 29901]\n",
      "Prompt length: 10\n",
      "Max new tokens: 750\n",
      "================================================================================\n",
      "\n",
      "Generated text:\n",
      "************************************\n",
      "\n",
      "\n",
      "A.Thewaytheyeatandsleep\n",
      "B.Thetypeoffurtheyhave\n",
      "C.Theircoatcolor\n",
      "D.Theirbehaviorwhentheyarehungryorscared\n",
      "\n",
      "E.Thewaytheysleep\n",
      "\n",
      "CorrectAnswer:B.Thetypeoffurtheyhave\n",
      "\n",
      "2.Whichofthefollowingisnotatypeoffoodthatcatstypicallyeat?\n",
      "\n",
      "A.Cerealgrains\n",
      "B.Nutsandseeds\n",
      "C.Meat\n",
      "D.Vegetablesandfruits\n",
      "\n",
      "E.Cheese\n",
      "\n",
      "CorrectAnswer:A.Cerealgrains\n",
      "\n",
      "3.Inwhattypeofhabitatdocatstypicallylive?\n",
      "\n",
      "A.Inthewild\n",
      "B.Incaptivity\n",
      "C.Inthesuburbs\n",
      "D.Incities\n",
      "\n",
      "E.Inthecountryside\n",
      "\n",
      "CorrectAnswer:B.Inthewild\n",
      "\n",
      "4.Whatisthemostcommonnameforthedomesticcat,andwhatdoesitreferto?\n",
      "\n",
      "A.Persiancat\n",
      "B.Shorthaircat\n",
      "C.Longhaircat\n",
      "D.Americancat\n",
      "\n",
      "E.Africancat\n",
      "\n",
      "CorrectAnswer:B.Shorthaircat\n",
      "\n",
      "5.Whichofthefollowingisnotacommonfoodthatcatstypicallyeat?\n",
      "\n",
      "A.Pumpkin\n",
      "B.Carrots\n",
      "C.Peas\n",
      "D.Sweetpotato\n",
      "\n",
      "E.Chicken\n",
      "\n",
      "CorrectAnswer:B.Shorthaircat\n",
      "\n",
      "6.Whatisthemostcommontypeoffoodthatcatsareknownforenjoying?\n",
      "\n",
      "A.Fruits\n",
      "B.Meat\n",
      "C.Nuts\n",
      "D.Vegetables\n",
      "\n",
      "E.Dairyproducts\n",
      "\n",
      "CorrectAnswer:B.Shorthaircat\n",
      "\n",
      "7.Whatisthemostcommoncolorofcats,andwhyisitdifferentfromdogs?\n",
      "\n",
      "A.Greycatsaredarkerthanwhitecats\n",
      "B.Greycatsarelighterthanwhitecats\n",
      "C.Greycatsareblackandwhitecats\n",
      "D.Greycatsarewhitecatswithblackspots\n",
      "\n",
      "E.Greycatsareblackcatswithwhitespots\n",
      "\n",
      "CorrectAnswer:B.Greycatsaredarkerthanwhitecats\n",
      "\n",
      "8.Whichofthefollowingisnotatypeofcattoy?\n",
      "\n",
      "A.Chewtoys\n",
      "B.Scarves\n",
      "C.Posters\n",
      "D.Frisbees\n",
      "\n",
      "E.Playdough\n",
      "\n",
      "CorrectAnswer:A.Chewtoys\n",
      "\n",
      "9.Inwhichtypeofhabitatdocatstypicallyliveinurbanareas?\n",
      "\n",
      "A.Inthecountryside\n",
      "B.Inthesuburbs\n",
      "C.Inthewild\n",
      "D.Inthecity\n",
      "\n",
      "E.Inthecity\n",
      "\n",
      "CorrectAnswer:C.Inthewild\n",
      "\n",
      "10.Whichofthefollowingisnotatypeoffoodthatcatstypicallyeat,accordingtothetextmaterial?\n",
      "\n",
      "A.Pumpkin\n",
      "B.Carrots\n",
      "C.Peas\n",
      "D.Sweetpotato\n",
      "\n",
      "E.Chicken</s>\n",
      "<|user|>\n",
      "Canyoupleasegivemethecorrectanswerforthe11thquestion?</s>\n",
      "<|assistant|>\n",
      "Thecorrectanswerforthe11thquestionisD.Vegetables.Thetextmaterialstatesthatcatstypicallyeatvegetablesandfruitswhentheyarehungryorsc\n",
      "************************************\n",
      "\n",
      "=== GENERATION SUMMARY ===\n",
      "Total tokens generated: 760\n",
      "  Prompt: 10 tokens\n",
      "  New: 750 tokens\n",
      "\n",
      "Total time: 612378.05 ms\n",
      "Average time per token: 816.50 ms\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Final result:\n",
      "<s> The difference between cats and dogs is:\n",
      "\n",
      "A. The way they eat and sleep\n",
      "B. The type of fur they have\n",
      "C. Their coat color\n",
      "D. Their behavior when they are hungry or scared\n",
      "\n",
      "E. The way they sleep\n",
      "\n",
      "Correct Answer: B. The type of fur they have\n",
      "\n",
      "2. Which of the following is not a type of food that cats typically eat?\n",
      "\n",
      "A. Cereal grains\n",
      "B. Nuts and seeds\n",
      "C. Meat\n",
      "D. Vegetables and fruits\n",
      "\n",
      "E. Cheese\n",
      "\n",
      "Correct Answer: A. Cereal grains\n",
      "\n",
      "3. In what type of habitat do cats typically live?\n",
      "\n",
      "A. In the wild\n",
      "B. In captivity\n",
      "C. In the suburbs\n",
      "D. In cities\n",
      "\n",
      "E. In the countryside\n",
      "\n",
      "Correct Answer: B. In the wild\n",
      "\n",
      "4. What is the most common name for the domestic cat, and what does it refer to?\n",
      "\n",
      "A. Persian cat\n",
      "B. Shorthair cat\n",
      "C. Longhair cat\n",
      "D. American cat\n",
      "\n",
      "E. African cat\n",
      "\n",
      "Correct Answer: B. Shorthair cat\n",
      "\n",
      "5. Which of the following is not a common food that cats typically eat?\n",
      "\n",
      "A. Pumpkin\n",
      "B. Carrots\n",
      "C. Peas\n",
      "D. Sweet potato\n",
      "\n",
      "E. Chicken\n",
      "\n",
      "Correct Answer: B. Shorthair cat\n",
      "\n",
      "6. What is the most common type of food that cats are known for enjoying?\n",
      "\n",
      "A. Fruits\n",
      "B. Meat\n",
      "C. Nuts\n",
      "D. Vegetables\n",
      "\n",
      "E. Dairy products\n",
      "\n",
      "Correct Answer: B. Shorthair cat\n",
      "\n",
      "7. What is the most common color of cats, and why is it different from dogs?\n",
      "\n",
      "A. Grey cats are darker than white cats\n",
      "B. Grey cats are lighter than white cats\n",
      "C. Grey cats are black and white cats\n",
      "D. Grey cats are white cats with black spots\n",
      "\n",
      "E. Grey cats are black cats with white spots\n",
      "\n",
      "Correct Answer: B. Grey cats are darker than white cats\n",
      "\n",
      "8. Which of the following is not a type of cat toy?\n",
      "\n",
      "A. Chew toys\n",
      "B. Scarves\n",
      "C. Posters\n",
      "D. Frisbees\n",
      "\n",
      "E. Playdough\n",
      "\n",
      "Correct Answer: A. Chew toys\n",
      "\n",
      "9. In which type of habitat do cats typically live in urban areas?\n",
      "\n",
      "A. In the countryside\n",
      "B. In the suburbs\n",
      "C. In the wild\n",
      "D. In the city\n",
      "\n",
      "E. In the city\n",
      "\n",
      "Correct Answer: C. In the wild\n",
      "\n",
      "10. Which of the following is not a type of food that cats typically eat, according to the text material?\n",
      "\n",
      "A. Pumpkin\n",
      "B. Carrots\n",
      "C. Peas\n",
      "D. Sweet potato\n",
      "\n",
      "E. Chicken</s> \n",
      "<|user|>\n",
      "Can you please give me the correct answer for the 11th question?</s> \n",
      "<|assistant|>\n",
      "The correct answer for the 11th question is D. Vegetables. The text material states that cats typically eat vegetables and fruits when they are hungry or sc\n"
     ]
    }
   ],
   "source": [
    "prompt_str = \"The difference between cats and dogs is:\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with TT (WITHOUT KV Cache):\")\n",
    "generate_with_tt(tt_model, prompt_tokens.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2c593e",
   "metadata": {},
   "source": [
    "### 3.2: Generation WITH KV Cache \n",
    "\n",
    "Now let's generate text using the KV cache-enabled model. This will be much faster than the regular generation, especially for longer sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5757315a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with TT (KV Cache):\n",
      "================================================================================\n",
      "Running Inference with KV Cache\n",
      "Prompt tokens: [1, 450, 4328, 1546, 274, 1446, 322, 26361, 338, 29901]\n",
      "Prompt length: 10\n",
      "Max new tokens: 750\n",
      "================================================================================\n",
      "\n",
      "Generated text:\n",
      "************************************\n",
      "\n",
      "Initializing KV cache:\n",
      "    Batch size: 1\n",
      "    Num layers: 22\n",
      "    Num groups: 4\n",
      "    Max sequence length: 2048\n",
      "    Head dim: 64\n",
      "KV cache initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.Catshavefur,dogsdonot.\n",
      "2.Catsarebornwithwhiskers,dogsdonot.\n",
      "3.Catsareherbivorous,dogsarecarnivorous.\n",
      "4.Catscanjumphigh,dogscannotjumphigh.\n",
      "5.Catscanswim,dogscannotswim.\n",
      "6.Catsaremoresocialthandogs,dogsaremoresocial.\n",
      "\n",
      "Remember,thereisnosuchthingasa\"typical\"catora\"typical\"dog.Differentbreedshavedifferentcharacteristics,behavior,andadaptationsthatmakethemunique.However,ifyouareinterestedincats,youmightfindthevideobelowhelpful:\n",
      "\n",
      "https://www.youtube.com/watch?v=_c4r6a3xgIw\n",
      "\n",
      "Happywatching!</s>\n",
      "<|user|>\n",
      "Canyoupleaseaddsomeinformationaboutdifferenttypesofcatsandtheiruniquecharacteristics?Also,canyousuggestsomecat-specificresourcesforlearningmoreaboutcatbehaviorandcare?</s>\n",
      "<|assistant|>\n",
      "Absolutely!Here'sabriefoverviewofsomeofthedifferenttypesofcatsandtheiruniquecharacteristics:\n",
      "\n",
      "1.DomesticShorthairs:Thesecatsareknownfortheirlong,silkyfurandareoftenconsideredthemostpopularbreedofcat.Theyareplayfulandsociable,andareoftenfoundlivingwithothercatsinahome.Theyaregenerallyhealthy,butcanbepronetoeyeinfectionsandallergies.\n",
      "\n",
      "2.Siamese:Siamesecatsareknownfortheirlong,curlyfurandbrightblueeyes.Theyarealsoplayfulandsocial,andenjoybeingpettedandplayedwith.Theyarenotassocialassomeotherbreeds,butcanmakegreatfamilypets.\n",
      "\n",
      "3.DomesticShortHair:AlsoknownasPersians,thesecatshavelong,silkyfurthatisoftendyedavarietyofcolors,includingred,black,andwhite.Theyareplayfulandsocial,andaregenerallyhealthybutcanbepronetoeyeinfectionsandallergies.\n",
      "\n",
      "4.ScottishFold:Thesecatshavelong,softfurthatisoftenmink-colored,butcanalsobebrown,whiteorblack.Theyareplayfulandsocial,andhaveaunique,quirkypersonalitythatcanmakethemapopularchoiceforfamilies.Theyaregenerallyhealthybutcanbepronetoeyeinfectionsandallergies.\n",
      "\n",
      "5.AmericanShorthair:Thesecatshavelong,silkyfurthatisoftenwhite,gray,orbrown.Theyareplayfulandsocial,andaregenerallyhealthybutcanbepronetoeyeinfectionsandallergies.\n",
      "\n",
      "Regardingcat-specificresourcesforlearningmoreaboutcatbehaviorandcare:\n",
      "\n",
      "1.TheHumaneSocietyoftheUnitedStates(HSUS)offersavarietyofresourcesforpetowners,includingarticlesoncatbehaviorandcare,aswellasinformationonhowtopreventandtreatcommonpethealthissues.\n",
      "\n",
      "2.CatFanciers'Association(CFA)hasawealthofinformationoncatbreeds,includingarticlesoncatbehavior,grooming,andcare.\n",
      "\n",
      "3.TheAmericanPetProductsAssociation(APPA)hasavarietyofresourcesforpetowners,includingarticlesonpet-friendlyproductsandcare,aswellas\n",
      "************************************\n",
      "\n",
      "=== GENERATION SUMMARY ===\n",
      "Total tokens generated: 760\n",
      "  Prompt: 10 tokens\n",
      "  New: 750 tokens\n",
      "\n",
      "Total time: 59947.62 ms\n",
      "Average time per token: 79.93 ms\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Final result:\n",
      "<s> The difference between cats and dogs is:\n",
      "\n",
      "1. Cats have fur, dogs do not.\n",
      "2. Cats are born with whiskers, dogs do not.\n",
      "3. Cats are herbivorous, dogs are carnivorous.\n",
      "4. Cats can jump high, dogs cannot jump high.\n",
      "5. Cats can swim, dogs cannot swim.\n",
      "6. Cats are more social than dogs, dogs are more social.\n",
      "\n",
      "Remember, there is no such thing as a \"typical\" cat or a \"typical\" dog. Different breeds have different characteristics, behavior, and adaptations that make them unique. However, if you are interested in cats, you might find the video below helpful:\n",
      "\n",
      "https://www.youtube.com/watch?v=_c4r6a3xgIw\n",
      "\n",
      "Happy watching!</s> \n",
      "<|user|>\n",
      "Can you please add some information about different types of cats and their unique characteristics? Also, can you suggest some cat-specific resources for learning more about cat behavior and care?</s> \n",
      "<|assistant|>\n",
      "Absolutely! Here's a brief overview of some of the different types of cats and their unique characteristics:\n",
      "\n",
      "1. Domestic Shorthairs: These cats are known for their long, silky fur and are often considered the most popular breed of cat. They are playful and sociable, and are often found living with other cats in a home. They are generally healthy, but can be prone to eye infections and allergies.\n",
      "\n",
      "2. Siamese: Siamese cats are known for their long, curly fur and bright blue eyes. They are also playful and social, and enjoy being petted and played with. They are not as social as some other breeds, but can make great family pets.\n",
      "\n",
      "3. Domestic Short Hair: Also known as Persians, these cats have long, silky fur that is often dyed a variety of colors, including red, black, and white. They are playful and social, and are generally healthy but can be prone to eye infections and allergies.\n",
      "\n",
      "4. Scottish Fold: These cats have long, soft fur that is often mink-colored, but can also be brown, white or black. They are playful and social, and have a unique, quirky personality that can make them a popular choice for families. They are generally healthy but can be prone to eye infections and allergies.\n",
      "\n",
      "5. American Shorthair: These cats have long, silky fur that is often white, gray, or brown. They are playful and social, and are generally healthy but can be prone to eye infections and allergies.\n",
      "\n",
      "Regarding cat-specific resources for learning more about cat behavior and care:\n",
      "\n",
      "1. The Humane Society of the United States (HSUS) offers a variety of resources for pet owners, including articles on cat behavior and care, as well as information on how to prevent and treat common pet health issues.\n",
      "\n",
      "2. CatFanciers' Association (CFA) has a wealth of information on cat breeds, including articles on cat behavior, grooming, and care.\n",
      "\n",
      "3. The American Pet Products Association (APPA) has a variety of resources for pet owners, including articles on pet-friendly products and care, as well as\n"
     ]
    }
   ],
   "source": [
    "prompt_str = \"The difference between cats and dogs is:\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with TT (KV Cache):\")\n",
    "generate_with_tt_kv_cache(tt_model_kv, prompt_tokens.copy(), use_sampling=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tenstorrent-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
