{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8744d20",
   "metadata": {},
   "source": [
    "### LLM Inference Example\n",
    "\n",
    "This notebook contains a basic inference example for using our `ttml` Python API to build, load, and run a large language model from Hugging Face on our TT hardware. By default, it is set to create and load a GPT2 model, but this notebook can quickly and easily be edited to use any of the LLMs that the tt-train project currently supports. \n",
    "\n",
    "Below, in the first cell, we have our imports and basic directory housekeeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a20c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "import numpy as np  # For numpy arrays\n",
    "from dataclasses import dataclass # For configuration classes\n",
    "from huggingface_hub import hf_hub_download # To download safetensors from Hugging Face\n",
    "from transformers import AutoTokenizer\n",
    "from yaml import safe_load # To read YAML configs\n",
    "from pathlib import Path\n",
    "\n",
    "if 'TT_METAL_RUNTIME_ROOT' not in os.environ:\n",
    "    os.environ['TT_METAL_RUNTIME_ROOT'] = os.environ['TT_METAL_HOME']\n",
    "\n",
    "sys.path.append(f\"{os.environ['TT_METAL_HOME']}/tt-train/sources/ttml\")\n",
    "import ttml\n",
    "from ttml.common.config import get_training_config, load_config\n",
    "from ttml.common.utils import set_seed, round_up_to_tile\n",
    "from ttml.common.model_factory import TransformerModelFactory\n",
    "\n",
    "# Change working directory to tt-train\n",
    "os.chdir(f\"{os.environ['TT_METAL_HOME']}/tt-train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e005a45c",
   "metadata": {},
   "source": [
    "Use the cell below to change global parameters in this notebook. \n",
    "\n",
    "`OUTPUT_TOKENS` : the length of the generated text in token (not characters!) \n",
    "\n",
    "`WITH_SAMPLING` : enable or disable output token sampling (only used for PyTorch)\n",
    "\n",
    "`TEMPERATURE`   : sampling temperature; set to 0 to disable sampling in `generate_with_tt()`\n",
    "\n",
    "`SEED`          : randomization seed (for reproducibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a87d6dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TOKENS = 750\n",
    "WITH_SAMPLING = True\n",
    "TEMPERATURE = 0.8\n",
    "SEED = 42\n",
    "CONFIG = \"training_shakespeare_llama3_gpt2s_size.yaml\"\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "model_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9b032f",
   "metadata": {},
   "source": [
    "While the notebook is currently configured for GPT2, you can quickly change the tokenizer you want to use by changing the input to `from_pretrained()` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea71a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from Hugging Face and the transformer config from YAML\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "training_config = get_training_config(CONFIG)\n",
    "model_yaml = load_config(training_config.model_config, configs_root=os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5415d24d",
   "metadata": {},
   "source": [
    "As above, the call to `hf_hub_download()` will download (or otherwise find on your local system) the SafeTensors model weight file for GPT2, but can be updated to download other SafeTensors files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6965256c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safetensors path: /home/ubuntu/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/\n"
     ]
    }
   ],
   "source": [
    "# # Get safetensors\n",
    "safetensors_path = hf_hub_download(repo_id=model_path, filename=\"model.safetensors\")\n",
    "safetensors_path = safetensors_path.replace(\"model.safetensors\",\"\")\n",
    "\n",
    "print(f\"Safetensors path: {safetensors_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "907e5933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_causal_mask(T: int) -> ttml.autograd.Tensor:\n",
    "    # [1,1,T,T] float32 with 1s for allowed positions (i >= j), else 0\\n\",\n",
    "    m = np.tril(np.ones((T, T), dtype=np.float32))\n",
    "    return ttml.autograd.Tensor.from_numpy(m.reshape(1, 1, T, T), ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)\n",
    "\n",
    "def build_logits_mask(vocab_size: int, padded_vocab_size: int) -> ttml.autograd.Tensor:\n",
    "    logits_mask = np.zeros((1, 1, 1, padded_vocab_size), dtype=np.float32)\n",
    "    logits_mask[:, :, :, vocab_size:] = 1e4\n",
    "    return ttml.autograd.Tensor.from_numpy(logits_mask, ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)   # [1,1,1,T], bfloat16\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc80055",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Model Setup WITHOUT KV Cache\n",
    "\n",
    "This section sets up a model that does NOT use KV cache. Each generation step performs a full forward pass through the entire sequence, which is slower but simpler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd89028f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama configuration:\n",
      "    Vocab size: 32000\n",
      "    Max sequence length: 2048\n",
      "    Embedding dim: 2048\n",
      "    Intermediate dim: None\n",
      "    Num heads: 32\n",
      "    Num groups: 4\n",
      "    Dropout probability: 0\n",
      "    Num blocks: 22\n",
      "    Positional embedding type: RoPE\n",
      "    Runner type: Memory efficient\n",
      "    Weight tying: Disabled\n",
      "    Theta: 10000\n",
      "2025-12-04 15:48:22.736 | info     |             UMD | Starting topology discovery. (topology_discovery.cpp:69)\n",
      "2025-12-04 15:48:22.740 | info     |             UMD | Established firmware bundle version: 19.0.0 (topology_discovery.cpp:369)\n",
      "2025-12-04 15:48:22.740 | info     |             UMD | Established ETH FW version: 7.2.0 (topology_discovery_wormhole.cpp:324)\n",
      "2025-12-04 15:48:22.740 | info     |             UMD | Completed topology discovery. (topology_discovery.cpp:73)\n",
      "2025-12-04 15:48:22.741 | info     |          Device | Opening user mode device driver (tt_cluster.cpp:211)\n",
      "2025-12-04 15:48:22.741 | info     |             UMD | Starting topology discovery. (topology_discovery.cpp:69)\n",
      "2025-12-04 15:48:22.744 | info     |             UMD | Established firmware bundle version: 19.0.0 (topology_discovery.cpp:369)\n",
      "2025-12-04 15:48:22.744 | info     |             UMD | Established ETH FW version: 7.2.0 (topology_discovery_wormhole.cpp:324)\n",
      "2025-12-04 15:48:22.744 | info     |             UMD | Completed topology discovery. (topology_discovery.cpp:73)\n",
      "2025-12-04 15:48:22.744 | info     |             UMD | Starting topology discovery. (topology_discovery.cpp:69)\n",
      "2025-12-04 15:48:22.764 | info     |             UMD | Established firmware bundle version: 19.0.0 (topology_discovery.cpp:369)\n",
      "2025-12-04 15:48:22.765 | info     |             UMD | Established ETH FW version: 7.2.0 (topology_discovery_wormhole.cpp:324)\n",
      "2025-12-04 15:48:22.765 | info     |             UMD | Completed topology discovery. (topology_discovery.cpp:73)\n",
      "2025-12-04 15:48:22.765 | info     |             UMD | Harvesting masks for chip 0 tensix: 0x4 dram: 0x0 eth: 0x0 pcie: 0x0 l2cpu: 0x0 (cluster.cpp:358)\n",
      "2025-12-04 15:48:22.799 | warning  |             UMD | init_detect_tt_device_numanodes(): Could not determine NumaNodeSet for TT device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:564)\n",
      "2025-12-04 15:48:22.799 | warning  |             UMD | Could not find NumaNodeSet for TT Device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:173)\n",
      "2025-12-04 15:48:22.799 | warning  |             UMD | bind_area_memory_nodeset(): Unable to determine TT Device to NumaNode mapping for physical_device_id: 0. Skipping membind. (cpuset_lib.cpp:450)\n",
      "2025-12-04 15:48:22.799 | warning  |             UMD | ---- ttSiliconDevice::init_hugepage: bind_area_to_memory_nodeset() failed (physical_device_id: 0 ch: 0). Hugepage allocation is not on NumaNode matching TT Device. Side-Effect is decreased Device->Host perf (Issue #893). (sysmem_manager.cpp:219)\n",
      "2025-12-04 15:48:22.799 | info     |             UMD | Opening local chip ids/PCIe ids: {0}/[0] and remote chip ids {} (cluster.cpp:202)\n",
      "2025-12-04 15:48:22.800 | info     |             UMD | IOMMU: disabled (cluster.cpp:178)\n",
      "2025-12-04 15:48:22.800 | info     |             UMD | KMD version: 2.4.1 (cluster.cpp:181)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to allocate the TLB with size 1048576",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m tt_model_factory\u001b[38;5;241m.\u001b[39mtransformer_config\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m orig_vocab_size\n\u001b[1;32m      6\u001b[0m max_sequence_length \u001b[38;5;241m=\u001b[39m tt_model_factory\u001b[38;5;241m.\u001b[39mtransformer_config\u001b[38;5;241m.\u001b[39mmax_sequence_length\n\u001b[0;32m----> 8\u001b[0m tt_model \u001b[38;5;241m=\u001b[39m \u001b[43mtt_model_factory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m tt_model\u001b[38;5;241m.\u001b[39mload_from_safetensors(safetensors_path)\n\u001b[1;32m     10\u001b[0m tt_model\n",
      "File \u001b[0;32m~/tt-metal/tt-train/sources/ttml/ttml/common/model_factory.py:166\u001b[0m, in \u001b[0;36mTransformerModelFactory.create_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_gpt2()\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_llama\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/tt-metal/tt-train/sources/ttml/ttml/common/model_factory.py:152\u001b[0m, in \u001b[0;36mTransformerModelFactory._create_llama\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_config\u001b[38;5;241m.\u001b[39menable_tp:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ttml\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mllama\u001b[38;5;241m.\u001b[39mcreate_llama_model(lcfg)\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mttml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_llama_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to allocate the TLB with size 1048576"
     ]
    }
   ],
   "source": [
    "orig_vocab_size = tokenizer.vocab_size\n",
    "\n",
    "tt_model_factory = TransformerModelFactory(model_yaml)\n",
    "tt_model_factory.transformer_config.vocab_size = orig_vocab_size\n",
    "\n",
    "max_sequence_length = tt_model_factory.transformer_config.max_sequence_length\n",
    "\n",
    "tt_model = tt_model_factory.create_model()\n",
    "tt_model.load_from_safetensors(safetensors_path)\n",
    "tt_model\n",
    "\n",
    "padded_vocab_size = round_up_to_tile(orig_vocab_size, 32)\n",
    "\n",
    "if orig_vocab_size != padded_vocab_size:\n",
    "    print(f\"Padding vocab size for tilization: original {orig_vocab_size} -> padded {padded_vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40913e88",
   "metadata": {},
   "source": [
    "`generate_with_tt()` uses TT hardware acceleration to generate output from the chosen LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e6550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_tt(model, prompt_tokens):\n",
    "    \"\"\"Generate text without KV cache (full sequence forward pass each step).\"\"\"\n",
    "    import time\n",
    "    \n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(ttml.autograd.GradMode.DISABLED)\n",
    "    model.eval()\n",
    "\n",
    "    logits_mask_tensor = None\n",
    "\n",
    "    if padded_vocab_size != orig_vocab_size:\n",
    "        logits_mask_tensor = build_logits_mask(orig_vocab_size, padded_vocab_size)\n",
    "\n",
    "    causal_mask = build_causal_mask(max_sequence_length)  # [1,1,seq_len,seq_len], float32\n",
    "    padded_prompt_tokens = np.zeros((1, 1, 1, max_sequence_length), \n",
    "                                    dtype=np.uint32)\n",
    "\n",
    "    start_idx = 0\n",
    "    prompt_len = len(prompt_tokens)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Running Inference WITHOUT KV Cache (Full Forward Pass Each Step)\")\n",
    "    print(f\"Prompt tokens: {prompt_tokens[:10]}{'...' if len(prompt_tokens) > 10 else ''}\")\n",
    "    print(f\"Prompt length: {prompt_len}\")\n",
    "    print(f\"Max new tokens: {OUTPUT_TOKENS}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nGenerated text:\")\n",
    "    print(\"************************************\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    generated_tokens = prompt_tokens.copy()\n",
    "    \n",
    "    for token_idx in range(OUTPUT_TOKENS):\n",
    "\n",
    "        if len(prompt_tokens) > max_sequence_length:\n",
    "            start_idx = len(prompt_tokens) - max_sequence_length\n",
    "\n",
    "        # padded_prompt_tokens[0, 0, 0, :transformer_cfg[\"max_sequence_length\"]] = 0\n",
    "        padded_prompt_tokens[0, 0, 0, :len(prompt_tokens)] = prompt_tokens[start_idx:]\n",
    "        padded_prompt_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "            padded_prompt_tokens,\n",
    "            ttml.Layout.ROW_MAJOR,\n",
    "            ttml.autograd.DataType.UINT32)  # [1,1,1, max_seq_len], uint32\n",
    "\n",
    "        logits = model(padded_prompt_tensor, causal_mask, use_cache=False)  # out=[1,1,seq_len, vocab_size], bf16\n",
    "\n",
    "\n",
    "        next_token_tensor = ttml.ops.sample.sample_op(logits, TEMPERATURE, np.random.randint(low=1e7), logits_mask_tensor)  # out=[1,1,seq_len,1], uint32\n",
    "\n",
    "        next_token_idx = max_sequence_length - 1 if len(prompt_tokens) > max_sequence_length else len(prompt_tokens) - 1\n",
    "        next_token = int(next_token_tensor.to_numpy().flatten()[next_token_idx])\n",
    "        generated_tokens.append(next_token)\n",
    "\n",
    "        output = tokenizer.decode([next_token], skip_special_tokens=False)\n",
    "\n",
    "        prompt_tokens.append(next_token)\n",
    "        print(output, end='', flush=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_duration_ms = (end_time - start_time) * 1000\n",
    "    new_tokens = len(prompt_tokens) - prompt_len\n",
    "    \n",
    "    print(\"\\n************************************\")\n",
    "    print(\"\\n=== GENERATION SUMMARY ===\")\n",
    "    print(f\"Total tokens generated: {len(prompt_tokens)}\")\n",
    "    print(f\"  Prompt: {prompt_len} tokens\")\n",
    "    print(f\"  New: {new_tokens} tokens\")\n",
    "    print(f\"\\nTotal time: {total_duration_ms:.2f} ms\")\n",
    "    print(f\"Average time per token: {total_duration_ms / new_tokens if new_tokens > 0 else 0:.2f} ms\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n\")\n",
    "    print(\"Final result:\")\n",
    "    print(tokenizer.decode(generated_tokens, skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08208b1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Model Setup WITH KV Cache\n",
    "\n",
    "This section sets up a model that uses KV cache for efficient inference. The KV cache stores previously computed key-value pairs from the attention mechanism, allowing each generation step to only process the newly generated token instead of recomputing the entire sequence. This significantly speeds up generation, especially for longer sequences.\n",
    "\n",
    "**Key differences from non-cache generation:**\n",
    "\n",
    "- **Prefill phase**: First step processes the entire prompt and stores KV pairs in cache\n",
    "- **Decode phase**: Subsequent steps only process the last generated token, reusing cached KV pairs\n",
    "- **Performance**: Much faster than non-cache generation, with speedup increasing as sequence length grows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08c1561",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_vocab_size = tokenizer.vocab_size\n",
    "\n",
    "tt_model_factory = TransformerModelFactory(model_yaml)\n",
    "tt_model_factory.transformer_config.vocab_size = orig_vocab_size\n",
    "\n",
    "max_sequence_length = tt_model_factory.transformer_config.max_sequence_length\n",
    "\n",
    "tt_model_kv = tt_model_factory.create_model()\n",
    "tt_model_kv.load_from_safetensors(safetensors_path)\n",
    "tt_model_kv\n",
    "\n",
    "padded_vocab_size = round_up_to_tile(orig_vocab_size, 32)\n",
    "\n",
    "if orig_vocab_size != padded_vocab_size:\n",
    "    print(f\"Padding vocab size for tilization: original {orig_vocab_size} -> padded {padded_vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac18600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_SIZE = 32\n",
    "\n",
    "def create_causal_mask_kv_cache(query_seq_len: int, prompt_len: int = 0) -> ttml.autograd.Tensor:\n",
    "    \"\"\"Create a causal attention mask for autoregressive generation with KV cache.\n",
    "    \n",
    "    Args:\n",
    "        device: TT device\n",
    "        query_seq_len: Length of query sequence\n",
    "        prompt_len: Length of prompt (for decode mode, this is the cache position)\n",
    "    \n",
    "    Returns:\n",
    "        Causal mask tensor\n",
    "    \"\"\"\n",
    "    padded_query_seq_len = ((query_seq_len + TILE_SIZE - 1) // TILE_SIZE) * TILE_SIZE\n",
    "    padded_whole_seq_len = ((prompt_len + query_seq_len + TILE_SIZE - 1) // TILE_SIZE) * TILE_SIZE\n",
    "    \n",
    "    # Mask shape: [padded_seq_len, padded_whole_seq_len] - query_len x key_len\n",
    "    mask_data = np.zeros((padded_query_seq_len, padded_whole_seq_len), dtype=np.float32)\n",
    "    \n",
    "    for i in range(query_seq_len):\n",
    "        for j in range(prompt_len + i + 1):\n",
    "            mask_data[i, j] = 1.0\n",
    "    \n",
    "    # Reshape to [1, 1, padded_query_seq_len, padded_whole_seq_len]\n",
    "    mask_data = mask_data.reshape(1, 1, padded_query_seq_len, padded_whole_seq_len)\n",
    "    mask_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "        mask_data,\n",
    "        layout=ttml.Layout.TILE,\n",
    "        new_type=ttml.autograd.DataType.BFLOAT16\n",
    "    )\n",
    "    \n",
    "    return mask_tensor\n",
    "\n",
    "\n",
    "def tokens_to_tensor_kv_cache(tokens: list) -> ttml.autograd.Tensor:\n",
    "    \"\"\"Create tensor from token IDs with proper padding for KV cache.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of token IDs\n",
    "    \n",
    "    Returns:\n",
    "        Token tensor\n",
    "    \"\"\"\n",
    "    actual_len = len(tokens)\n",
    "    # Pad to actual length to nearest tile boundary (32, 64, 96, ...)\n",
    "    padded_len = ((actual_len + TILE_SIZE - 1) // TILE_SIZE) * TILE_SIZE\n",
    "    \n",
    "    padded_tokens = np.zeros(padded_len, dtype=np.uint32)\n",
    "    for i in range(actual_len):\n",
    "        padded_tokens[i] = tokens[i]\n",
    "    \n",
    "    # Reshape to [1, 1, 1, padded_len]\n",
    "    padded_tokens = padded_tokens.reshape(1, 1, 1, padded_len)\n",
    "    tokens_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "        padded_tokens,\n",
    "        layout=ttml.Layout.ROW_MAJOR,\n",
    "        new_type=ttml.autograd.DataType.UINT32\n",
    "    )\n",
    "    \n",
    "    return tokens_tensor\n",
    "\n",
    "\n",
    "def sample_token_from_logits(logits: ttml.autograd.Tensor, position: int) -> int:\n",
    "    \"\"\"Sample next token using greedy decoding (argmax).\n",
    "    \n",
    "    Args:\n",
    "        logits: Logits tensor\n",
    "        position: Position to sample from\n",
    "    \n",
    "    Returns:\n",
    "        Token ID with highest logit\n",
    "    \"\"\"\n",
    "    logits_np = logits.to_numpy()\n",
    "    logits_host = logits_np.flatten()\n",
    "    \n",
    "    shape = logits.shape()\n",
    "    vocab_size = shape[-1]\n",
    "    last_token_offset = (position - 1) * vocab_size\n",
    "    \n",
    "    # Find token with highest logit value\n",
    "    max_idx = 0\n",
    "    max_val = logits_host[last_token_offset]\n",
    "    \n",
    "    for i in range(1, vocab_size):\n",
    "        val = logits_host[last_token_offset + i]\n",
    "        if val > max_val:\n",
    "            max_val = val\n",
    "            max_idx = i\n",
    "    \n",
    "    return max_idx\n",
    "\n",
    "\n",
    "def generate_with_tt_kv_cache(model, prompt_tokens, use_sampling=True):\n",
    "    \"\"\"Generate text with KV cache for efficient inference.\n",
    "    \n",
    "    Args:\n",
    "        model: LLaMA model instance (must have inference=True in config)\n",
    "        prompt_tokens: Initial prompt token IDs\n",
    "        use_sampling: Whether to use temperature sampling (if False, uses greedy decoding)\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    # Check if model supports KV cache\n",
    "    if not hasattr(model, 'reset_cache'):\n",
    "        print(\"Warning: Model does not support KV cache. Falling back to regular generation.\")\n",
    "        return generate_with_tt(model, prompt_tokens)\n",
    "    \n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(ttml.autograd.GradMode.DISABLED)\n",
    "    model.eval()\n",
    "    \n",
    "    # Reset KV cache for new sequence\n",
    "    model.reset_cache()\n",
    "    \n",
    "    generated_tokens = prompt_tokens.copy()\n",
    "    prompt_len = len(prompt_tokens)\n",
    "    \n",
    "    logits_mask_tensor = None\n",
    "    if padded_vocab_size != orig_vocab_size:\n",
    "        logits_mask_tensor = build_logits_mask(orig_vocab_size, padded_vocab_size)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"Running Inference with KV Cache\")\n",
    "    print(f\"Prompt tokens: {prompt_tokens[:10]}{'...' if len(prompt_tokens) > 10 else ''}\")\n",
    "    print(f\"Prompt length: {prompt_len}\")\n",
    "    print(f\"Max new tokens: {OUTPUT_TOKENS}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nGenerated text:\")\n",
    "    print(\"************************************\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate tokens one by one\n",
    "    for step in range(min(OUTPUT_TOKENS, max_sequence_length - prompt_len)):\n",
    "        # For first step (prefill): use all prompt tokens\n",
    "        # For subsequent steps (decode): use only the last generated token  \n",
    "        prompt_len = 0\n",
    "        if model.get_inference_mode() == ttml.modules.InferenceMode.DECODE:\n",
    "            # Prefill: process entire prompt\n",
    "            input_tokens = generated_tokens\n",
    "        else:\n",
    "            # Decode: process only last token\n",
    "            input_tokens = [generated_tokens[-1]]\n",
    "            prompt_len = len(generated_tokens)-1\n",
    "        \n",
    "        token_tensor = tokens_to_tensor_kv_cache(input_tokens)\n",
    "        \n",
    "        # Create causal mask\n",
    "        # For prefill: query_len = prompt_len, key_len = prompt_len\n",
    "        # For decode: query_len = 1, key_len = cache_position + 1\n",
    "        mask = create_causal_mask_kv_cache(len(input_tokens), prompt_len)\n",
    "        logits = model(token_tensor, mask, use_cache=True)\n",
    "        \n",
    "        # Sample next token\n",
    "        if use_sampling:\n",
    "            next_token_tensor = ttml.ops.sample.sample_op(\n",
    "                logits, TEMPERATURE, np.random.randint(low=1e7), logits_mask_tensor\n",
    "            )\n",
    "            next_token_idx = len(input_tokens) - 1\n",
    "            next_token = int(next_token_tensor.to_numpy().flatten()[next_token_idx])\n",
    "        else:\n",
    "            # Greedy decoding\n",
    "            next_token = int(sample_token_from_logits(logits, len(input_tokens)))\n",
    "        \n",
    "        output = tokenizer.decode([next_token], skip_special_tokens=False)\n",
    "        generated_tokens.append(next_token)\n",
    "        print(output, end='', flush=True)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_duration_ms = (end_time - start_time) * 1000\n",
    "    new_tokens = len(generated_tokens) - prompt_len\n",
    "\n",
    "    model.reset_cache()\n",
    "    \n",
    "    print(\"\\n************************************\")\n",
    "    print(\"\\n=== GENERATION SUMMARY ===\")\n",
    "    print(f\"Total tokens generated: {len(generated_tokens)}\")\n",
    "    print(f\"  Prompt: {prompt_len} tokens\")\n",
    "    print(f\"  New: {new_tokens} tokens\")\n",
    "    print(f\"\\nTotal time: {total_duration_ms:.2f} ms\")\n",
    "    print(f\"Average time per token: {total_duration_ms / new_tokens if new_tokens > 0 else 0:.2f} ms\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n\")\n",
    "    print(\"Final result:\")\n",
    "    print(tokenizer.decode(generated_tokens, skip_special_tokens=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c461c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Generation Examples\n",
    "\n",
    "### 3.1: Generation WITHOUT KV Cache\n",
    "\n",
    "Examples using the non-cache model. These will be slower but demonstrate the baseline approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc2d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"The difference between cats and dogs is:\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with TT (WITHOUT KV Cache):\")\n",
    "generate_with_tt(tt_model, prompt_tokens.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2c593e",
   "metadata": {},
   "source": [
    "### 3.2: Generation WITH KV Cache \n",
    "\n",
    "Now let's generate text using the KV cache-enabled model. This will be much faster than the regular generation, especially for longer sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5757315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"The difference between cats and dogs is:\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with TT (KV Cache):\")\n",
    "generate_with_tt_kv_cache(tt_model_kv, prompt_tokens.copy(), use_sampling=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tenstorrent-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
