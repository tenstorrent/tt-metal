{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a20c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tokenizers safetensors\n",
    "\n",
    "import os, sys, math, random, textwrap\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import GPT2Tokenizer\n",
    "from yaml import safe_load, Loader\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(f\"{os.environ['TT_METAL_HOME']}/tt-train/sources/ttml\")\n",
    "import ttml\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed()\n",
    "# Change working directory to TT_METAL_HOME\n",
    "os.chdir(os.environ['TT_METAL_HOME'])\n",
    "\n",
    "OUTPUT_TOKENS = 256\n",
    "\n",
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    n_head: int = 12\n",
    "    embed_dim: int = 768\n",
    "    dropout: float = 0.2\n",
    "    n_blocks : int = 12\n",
    "    vocab_size: int = 96\n",
    "    max_seq_len: int = 1024\n",
    "    runner_type: str = \"memory_efficient\"\n",
    "    weight_tying: str = \"enabled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea71a9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/tt-metal\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "print(os.getcwd())\n",
    "transformer_cfg = safe_load(open(\"tt-train/configs/training_shakespeare_gpt2s.yaml\", \"r\"))[\"training_config\"][\"transformer_config\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6965256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get safetensors\n",
    "local_path = hf_hub_download(repo_id=\"gpt2\", filename=\"model.safetensors\")\n",
    "local_path = local_path.replace(\"model.safetensors\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26b353af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_causal_mask(T: int) -> ttml.autograd.Tensor:\n",
    "    # [1,1,T,T] float32 with 1s for allowed positions (i >= j), else 0\n",
    "    m = np.tril(np.ones((T, T), dtype=np.float32))\n",
    "    return ttml.autograd.Tensor.from_numpy(m.reshape(1, 1, T, T), ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)\n",
    "\n",
    "def build_logits_mask(vocab_size: int, padded_vocab_size: int) -> ttml.autograd.Tensor:\n",
    "    logits_mask = np.zeros((1, 1, 1, padded_vocab_size), dtype=np.float32)\n",
    "    logits_mask[:, :, :, vocab_size:] = 1e4\n",
    "    return ttml.autograd.Tensor.from_numpy(logits_mask, ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)   # [1,1,1,T], float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd89028f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: vocab size 50257 is not multiple of 32, padding for tilizing.\n",
      "Transformer configuration:\n",
      "    Vocab size: 50272\n",
      "    Max sequence length: 1024\n",
      "    Embedding dim: 768\n",
      "    Num heads: 12\n",
      "    Dropout probability: 0.2\n",
      "    Num blocks: 12\n",
      "    Positional embedding type: Trainable\n",
      "    Runner type: Default\n",
      "    Composite layernorm: false\n",
      "    Weight tying: Disabled\n",
      "Loading model from: /home/ubuntu/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors\n",
      "parameter name: transformer/gpt_block_9/mlp/fc2/bias\n",
      "parameter name: transformer/gpt_block_9/mlp/fc1/weight\n",
      "parameter name: transformer/gpt_block_9/attention/qkv_linear/bias\n",
      "parameter name: transformer/gpt_block_9/attention/out_linear/bias\n",
      "parameter name: transformer/gpt_block_8/mlp/fc2/weight\n",
      "parameter name: transformer/gpt_block_8/mlp/fc1/weight\n",
      "parameter name: transformer/gpt_block_8/attention/qkv_linear/bias\n",
      "parameter name: transformer/gpt_block_7/mlp/fc2/weight\n",
      "parameter name: transformer/gpt_block_9/attention/out_linear/weight\n",
      "parameter name: transformer/gpt_block_7/mlp/fc2/bias\n",
      "parameter name: transformer/gpt_block_7/mlp/fc1/weight\n",
      "parameter name: transformer/gpt_block_7/attention/qkv_linear/weight\n",
      "parameter name: transformer/gpt_block_0/ln1/gamma\n",
      "parameter name: transformer/gpt_block_6/attention/out_linear/bias\n",
      "parameter name: transformer/gpt_block_0/attention/qkv_linear/bias\n",
      "parameter name: transformer/gpt_block_4/ln1/beta\n",
      "parameter name: transformer/gpt_block_8/mlp/fc2/bias\n",
      "parameter name: transformer/gpt_block_0/attention/out_linear/weight\n",
      "parameter name: transformer/gpt_block_8/ln2/beta\n",
      "parameter name: transformer/gpt_block_9/ln1/beta\n",
      "parameter name: transformer/gpt_block_5/ln1/gamma\n",
      "parameter name: transformer/gpt_block_11/ln2/beta\n",
      "parameter name: transformer/gpt_block_3/mlp/fc2/bias\n",
      "parameter name: transformer/gpt_block_7/ln2/beta\n",
      "parameter name: transformer/gpt_block_11/attention/qkv_linear/weight\n",
      "parameter name: transformer/gpt_block_0/mlp/fc1/weight\n",
      "parameter name: transformer/gpt_block_7/ln1/beta\n",
      "parameter name: transformer/gpt_block_1/mlp/fc1/weight\n",
      "parameter name: transformer/gpt_block_2/mlp/fc2/bias\n",
      "parameter name: transformer/gpt_block_6/ln1/gamma\n",
      "parameter name: transformer/gpt_block_5/attention/qkv_linear/weight\n",
      "parameter name: transformer/gpt_block_7/mlp/fc1/bias\n",
      "parameter name: transformer/gpt_block_6/ln1/beta\n",
      "parameter name: transformer/gpt_block_5/ln2/gamma\n",
      "parameter name: transformer/gpt_block_0/attention/qkv_linear/weight\n",
      "parameter name: transformer/gpt_block_4/ln2/gamma\n",
      "parameter name: transformer/gpt_block_0/ln2/beta\n",
      "parameter name: transformer/gpt_block_8/ln1/gamma\n",
      "parameter name: transformer/gpt_block_6/mlp/fc1/bias\n",
      "parameter name: transformer/gpt_block_4/ln1/gamma\n",
      "parameter name: transformer/gpt_block_5/ln1/beta\n",
      "parameter name: transformer/gpt_block_1/ln2/beta\n",
      "parameter name: transformer/gpt_block_7/ln1/gamma\n",
      "parameter name: transformer/gpt_block_11/mlp/fc2/bias\n",
      "parameter name: transformer/gpt_block_1/ln2/gamma\n",
      "parameter name: transformer/gpt_block_0/attention/out_linear/bias\n",
      "parameter name: transformer/gpt_block_0/mlp/fc1/bias\n",
      "parameter name: transformer/ln_fc/beta\n",
      "parameter name: transformer/gpt_block_1/ln1/beta\n",
      "parameter name: transformer/gpt_block_7/ln2/gamma\n",
      "parameter name: transformer/gpt_block_3/attention/qkv_linear/bias\n",
      "parameter name: transformer/tok_emb/weight\n",
      "parameter name: transformer/gpt_block_10/mlp/fc2/weight\n",
      "parameter name: transformer/gpt_block_4/ln2/beta\n",
      "parameter name: transformer/gpt_block_2/ln2/beta\n",
      "parameter name: transformer/gpt_block_8/ln2/gamma\n",
      "parameter name: transformer/gpt_block_3/ln1/gamma\n",
      "parameter name: transformer/gpt_block_2/attention/qkv_linear/bias\n",
      "parameter name: transformer/gpt_block_3/attention/out_linear/bias\n",
      "parameter name: transformer/gpt_block_4/mlp/fc2/bias\n",
      "parameter name: transformer/gpt_block_10/ln2/gamma\n",
      "parameter name: transformer/pos_emb/weight\n",
      "parameter name: transformer/gpt_block_5/mlp/fc2/bias\n",
      "parameter name: transformer/gpt_block_11/ln1/beta\n",
      "parameter name: transformer/gpt_block_9/attention/qkv_linear/weight\n",
      "parameter name: transformer/gpt_block_0/ln2/gamma\n",
      "parameter name: transformer/gpt_block_11/ln1/gamma\n",
      "parameter name: transformer/gpt_block_1/attention/out_linear/weight\n",
      "parameter name: transformer/gpt_block_5/mlp/fc2/weight\n",
      "parameter name: transformer/gpt_block_9/ln2/gamma\n",
      "parameter name: transformer/gpt_block_10/ln1/gamma\n",
      "parameter name: transformer/gpt_block_4/attention/out_linear/weight\n",
      "parameter name: transformer/gpt_block_9/ln1/gamma\n",
      "parameter name: transformer/gpt_block_0/mlp/fc2/weight\n",
      "parameter name: transformer/gpt_block_5/attention/out_linear/weight\n",
      "parameter name: transformer/gpt_block_10/mlp/fc1/bias\n",
      "parameter name: transformer/gpt_block_11/mlp/fc1/bias\n",
      "parameter name: transformer/gpt_block_4/mlp/fc1/weight\n",
      "parameter name: transformer/gpt_block_2/ln1/gamma\n",
      "parameter name: transformer/fc/weight\n",
      "parameter name: transformer/gpt_block_11/ln2/gamma\n",
      "parameter name: transformer/ln_fc/gamma\n",
      "parameter name: transformer/gpt_block_8/attention/out_linear/weight\n",
      "parameter name: transformer/gpt_block_9/ln2/beta\n",
      "parameter name: transformer/gpt_block_6/attention/out_linear/weight\n",
      "parameter name: transformer/gpt_block_6/ln2/gamma\n",
      "parameter name: transformer/gpt_block_4/attention/qkv_linear/bias\n",
      "parameter name: transformer/gpt_block_10/ln2/beta\n",
      "parameter name: transformer/gpt_block_6/mlp/fc1/weight\n",
      "parameter name: transformer/gpt_block_2/ln2/gamma\n",
      "parameter name: transformer/gpt_block_1/ln1/gamma\n",
      "parameter name: transformer/gpt_block_0/mlp/fc2/bias\n",
      "parameter name: transformer/gpt_block_1/mlp/fc2/bias\n",
      "parameter name: transformer/gpt_block_3/ln2/gamma\n",
      "parameter name: transformer/gpt_block_9/mlp/fc1/bias\n",
      "parameter name: transformer/gpt_block_3/ln1/beta\n",
      "parameter name: transformer/gpt_block_1/attention/out_linear/bias\n",
      "parameter name: transformer/gpt_block_2/mlp/fc2/weight\n",
      "parameter name: transformer/gpt_block_1/attention/qkv_linear/weight\n",
      "parameter name: transformer/gpt_block_1/mlp/fc1/bias\n",
      "parameter name: transformer/gpt_block_3/attention/out_linear/weight\n",
      "parameter name: transformer/gpt_block_4/attention/qkv_linear/weight\n",
      "parameter name: transformer/gpt_block_8"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_ttml.models.gpt2.GPT2Transformer at 0x7f8edc3a0030>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/attention/out_linear/bias\n",
      "parameter name: transformer/gpt_block_11/mlp/fc1/weight\n",
      "parameter name: transformer/gpt_block_1/mlp/fc2/weight\n",
      "parameter name: transformer/gpt_block_2/mlp/fc1/weight\n",
      "parameter name: transformer/gpt_block_1/attention/qkv_linear/bias\n",
      "parameter name: transformer/gpt_block_10/mlp/fc1/weight\n",
      "parameter name: transformer/gpt_block_10/attention/out_linear/bias\n",
      "parameter name: transformer/gpt_block_11/attention/out_linear/bias\n",
      "parameter name: transformer/gpt_block_8/attention/qkv_linear/weight\n",
      "parameter name: transformer/gpt_block_2/ln1/beta\n",
      "parameter name: transformer/gpt_block_10/attention/out_linear/weight\n",
      "parameter name: transformer/gpt_block_5/ln2/beta\n",
      "parameter name: transformer/gpt_block_11/mlp/fc2/weight\n",
      "parameter name: transformer/gpt_block_10/attention/qkv_linear/bias\n",
      "parameter name: transformer/gpt_block_10/mlp/fc2/bias\n",
      "parameter name: transformer/gpt_block_4/mlp/fc1/bias\n",
      "parameter name: transformer/gpt_block_11/attention/out_linear/weight\n",
      "parameter name: transformer/gpt_block_3/mlp/fc2/weight\n",
      "parameter name: transformer/gpt_block_11/attention/qkv_linear/bias\n",
      "parameter name: transformer/gpt_block_5/mlp/fc1/weight\n",
      "parameter name: transformer/gpt_block_2/attention/out_linear/bias\n",
      "parameter name: transformer/gpt_block_2/attention/out_linear/weight\n",
      "parameter name: transformer/gpt_block_6/ln2/beta\n",
      "parameter name: transformer/gpt_block_2/attention/qkv_linear/weight\n",
      "parameter name: transformer/gpt_block_0/ln1/beta\n",
      "parameter name: transformer/gpt_block_10/attention/qkv_linear/weight\n",
      "parameter name: transformer/gpt_block_2/mlp/fc1/bias\n",
      "parameter name: transformer/gpt_block_3/attention/qkv_linear/weight\n",
      "parameter name: transformer/gpt_block_3/mlp/fc1/bias\n",
      "parameter name: transformer/gpt_block_8/ln1/beta\n",
      "parameter name: transformer/gpt_block_3/mlp/fc1/weight\n",
      "parameter name: transformer/gpt_block_6/attention/qkv_linear/bias\n",
      "parameter name: transformer/gpt_block_4/attention/out_linear/bias\n",
      "parameter name: transformer/gpt_block_4/mlp/fc2/weight\n",
      "parameter name: transformer/gpt_block_9/mlp/fc2/weight\n",
      "parameter name: transformer/gpt_block_10/ln1/beta\n",
      "parameter name: transformer/gpt_block_5/attention/out_linear/bias\n",
      "parameter name: transformer/gpt_block_5/attention/qkv_linear/bias\n",
      "parameter name: transformer/gpt_block_5/mlp/fc1/bias\n",
      "parameter name: transformer/gpt_block_6/attention/qkv_linear/weight\n",
      "parameter name: transformer/gpt_block_6/mlp/fc2/bias\n",
      "parameter name: transformer/gpt_block_8/mlp/fc1/bias\n",
      "parameter name: transformer/gpt_block_7/attention/qkv_linear/bias\n",
      "parameter name: transformer/gpt_block_3/ln2/beta\n",
      "parameter name: transformer/gpt_block_6/mlp/fc2/weight\n",
      "parameter name: transformer/gpt_block_7/attention/out_linear/bias\n",
      "parameter name: transformer/gpt_block_7/attention/out_linear/weight\n",
      "Loading tensor: h.0.attn.bias, shape:Shape([1, 1, 1024, 1024]), format: F32\n",
      "Loading tensor: h.0.attn.c_attn.bias, shape:Shape([2304]), format: F32\n",
      " Parameter transformer/gpt_block_0/attention/qkv_linear/bias, shape: Shape([1, 1, 1, 2304])\n",
      "Loading tensor: h.0.attn.c_attn.weight, shape:Shape([768, 2304]), format: F32\n",
      "Transposing!\n",
      " Parameter transformer/gpt_block_0/attention/qkv_linear/weight, shape: Shape([1, 1, 2304, 768])\n",
      "Loading tensor: h.0.attn.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_0/attention/out_linear/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.0.attn.c_proj.weight, shape:Shape([768, 768]), format: F32\n",
      "Transposing!\n",
      " Parameter transformer/gpt_block_0/attention/out_linear/weight, shape: Shape([1, 1, 768, 768])\n",
      "Loading tensor: h.0.ln_1.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_0/ln1/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.0.ln_1.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_0/ln1/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.0.ln_2.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_0/ln2/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.0.ln_2.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_0/ln2/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.0.mlp.c_fc.bias, shape:Shape([3072]), format: F32\n",
      " Parameter transformer/gpt_block_0/mlp/fc1/bias, shape: Shape([1, 1, 1, 3072])\n",
      "Loading tensor: h.0.mlp.c_fc.weight, shape:Shape([768, 3072]), format: F32\n",
      " Parameter transformer/gpt_block_0/mlp/fc1/weight, shape: Shape([1, 1, 3072, 768])\n",
      "Transposing!\n",
      "Loading tensor: h.0.mlp.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_0/mlp/fc2/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.0.mlp.c_proj.weight, shape:Shape([3072, 768]), format: F32\n",
      " Parameter transformer/gpt_block_0/mlp/fc2/weight, shape: Shape([1, 1, 768, 3072])\n",
      "Transposing!\n",
      "Loading tensor: h.1.attn.bias, shape:Shape([1, 1, 1024, 1024]), format: F32\n",
      "Loading tensor: h.1.attn.c_attn.bias, shape:Shape([2304]), format: F32\n",
      " Parameter transformer/gpt_block_1/attention/qkv_linear/bias, shape: Shape([1, 1, 1, 2304])\n",
      "Loading tensor: h.1.attn.c_attn.weight, shape:Shape([768, 2304]), format: F32\n",
      "Transposing!\n",
      " Parameter transformer/gpt_block_1/attention/qkv_linear/weight, shape: Shape([1, 1, 2304, 768])\n",
      "Loading tensor: h.1.attn.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_1/attention/out_linear/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.1.attn.c_proj.weight, shape:Shape([768, 768]), format: F32\n",
      "Transposing!\n",
      " Parameter transformer/gpt_block_1/attention/out_linear/weight, shape: Shape([1, 1, 768, 768])\n",
      "Loading tensor: h.1.ln_1.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_1/ln1/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.1.ln_1.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_1/ln1/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.1.ln_2.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_1/ln2/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.1.ln_2.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_1/ln2/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.1.mlp.c_fc.bias, shape:Shape([3072]), format: F32\n",
      " Parameter transformer/gpt_block_1/mlp/fc1/bias, shape: Shape([1, 1, 1, 3072])\n",
      "Loading tensor: h.1.mlp.c_fc.weight, shape:Shape([768, 3072]), format: F32\n",
      " Parameter transformer/gpt_block_1/mlp/fc1/weight, shape: Shape([1, 1, 3072, 768])\n",
      "Transposing!\n",
      "Loading tensor: h.1.mlp.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_1/mlp/fc2/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.1.mlp.c_proj.weight, shape:Shape([3072, 768]), format: F32\n",
      " Parameter transformer/gpt_block_1/mlp/fc2/weight, shape: Shape([1, 1, 768, 3072])\n",
      "Transposing!\n",
      "Loading tensor: h.10.attn.bias, shape:Shape([1, 1, 1024, 1024]), format: F32\n",
      "Loading tensor: h.10.attn.c_attn.bias, shape:Shape([2304]), format: F32\n",
      " Parameter transformer/gpt_block_10/attention/qkv_linear/bias, shape: Shape([1, 1, 1, 2304])\n",
      "Loading tensor: h.10.attn.c_attn.weight, shape:Shape([768, 2304]), format: F32\n",
      "Transposing!\n",
      " Parameter transformer/gpt_block_10/attention/qkv_linear/weight, shape: Shape([1, 1, 2304, 768])\n",
      "Loading tensor: h.10.attn.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_10/attention/out_linear/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.10.attn.c_proj.weight, shape:Shape([768, 768]), format: F32\n",
      "Transposing!\n",
      " Parameter transformer/gpt_block_10/attention/out_linear/weight, shape: Shape([1, 1, 768, 768])\n",
      "Loading tensor: h.10.ln_1.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_10/ln1/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.10.ln_1.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_10/ln1/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.10.ln_2.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_10/ln2/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.10.ln_2.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_10/ln2/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.10.mlp.c_fc.bias, shape:Shape([3072]), format: F32\n",
      " Parameter transformer/gpt_block_10/mlp/fc1/bias, shape: Shape([1, 1, 1, 3072])\n",
      "Loading tensor: h.10.mlp.c_fc.weight, shape:Shape([768, 3072]), format: F32\n",
      " Parameter transformer/gpt_block_10/mlp/fc1/weight, shape: Shape([1, 1, 3072, 768])\n",
      "Transposing!\n",
      "Loading tensor: h.10.mlp.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_10/mlp/fc2/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.10.mlp.c_proj.weight, shape:Shape([3072, 768]), format: F32\n",
      " Parameter transformer/gpt_block_10/mlp/fc2/weight, shape: Shape([1, 1, 768, 3072])\n",
      "Transposing!\n",
      "Loading tensor: h.11.attn.bias, shape:Shape([1, 1, 1024, 1024]), format: F32\n",
      "Loading tensor: h.11.attn.c_attn.bias, shape:Shape([2304]), format: F32\n",
      " Parameter transformer/gpt_block_11/attention/qkv_linear/bias, shape: Shape([1, 1, 1, 2304])\n",
      "Loading tensor: h.11.attn.c_attn.weight, shape:Shape([768, 2304]), format: F32\n",
      "Transposing!\n",
      " Parameter transformer/gpt_block_11/attention/qkv_linear/weight, shape: Shape([1, 1, 2304, 768])\n",
      "Loading tensor: h.11.attn.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_11/attention/out_linear/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.11.attn.c_proj.weight, shape:Shape([768, 768]), format: F32\n",
      "Transposing!\n",
      " Parameter transformer/gpt_block_11/attention/out_linear/weight, shape: Shape([1, 1, 768, 768])\n",
      "Loading tensor: h.11.ln_1.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_11/ln1/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.11.ln_1.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_11/ln1/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.11.ln_2.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_11/ln2/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.11.ln_2.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_11/ln2/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.11.mlp.c_fc.bias, shape:Shape([3072]), format: F32\n",
      " Parameter transformer/gpt_block_11/mlp/fc1/bias, shape: Shape([1, 1, 1, 3072])\n",
      "Loading tensor: h.11.mlp.c_fc.weight, shape:Shape([768, 3072]), format: F32\n",
      " Parameter transformer/gpt_block_11/mlp/fc1/weight, shape: Shape([1, 1, 3072, 768])\n",
      "Transposing!\n",
      "Loading tensor: h.11.mlp.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_11/mlp/fc2/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.11.mlp.c_proj.weight, shape:Shape([3072, 768]), format: F32\n",
      " Parameter transformer/gpt_block_11/mlp/fc2/weight, shape: Shape([1, 1, 768, 3072])\n",
      "Transposing!\n",
      "Loading tensor: h.2.attn.bias, shape:Shape([1, 1, 1024, 1024]), format: F32\n",
      "Loading tensor: h.2.attn.c_attn.bias, shape:Shape([2304]), format: F32\n",
      " Parameter transformer/gpt_block_2/attention/qkv_linear/bias, shape: Shape([1, 1, 1, 2304])\n",
      "Loading tensor: h.2.attn.c_attn.weight, shape:Shape([768, 2304]), format: F32\n",
      "Transposing!\n",
      " Parameter transformer/gpt_block_2/attention/qkv_linear/weight, shape: Shape([1, 1, 2304, 768])\n",
      "Loading tensor: h.2.attn.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_2/attention/out_linear/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.2.attn.c_proj.weight, shape:Shape([768, 768]), format: F32\n",
      "Transposing!\n",
      " Parameter transformer/gpt_block_2/attention/out_linear/weight, shape: Shape([1, 1, 768, 768])\n",
      "Loading tensor: h.2.ln_1.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_2/ln1/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.2.ln_1.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_2/ln1/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.2.ln_2.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_2/ln2/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.2.ln_2.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_2/ln2/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.2.mlp.c_fc.bias, shape:Shape([3072]), format: F32\n",
      " Parameter transformer/gpt_block_2/mlp/fc1/bias, shape: Shape([1, 1, 1, 3072])\n",
      "Loading tensor: h.2.mlp.c_fc.weight, shape:Shape([768, 3072]), format: F32\n",
      " Parameter transformer/gpt_block_2/mlp/fc1/weight, shape: Shape([1, 1, 3072, 768])\n",
      "Transposing!\n",
      "Loading tensor: h.2.mlp.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_2/mlp/fc2/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.2.mlp.c_proj.weight, shape:Shape([3072, 768]), format: F32\n",
      " Parameter transformer/gpt_block_2/mlp/fc2/weight, shape: Shape([1, 1, 768, 3072])\n",
      "Transposing!\n",
      "Loading tensor: h.3.attn.bias, shape:Shape([1, 1, 1024, 1024]), format: F32\n",
      "Loading tensor: h.3.attn.c_attn.bias, shape:Shape([2304]), format: F32\n",
      " Parameter transformer/gpt_block_3/attention/qkv_linear/bias, shape: Shape([1, 1, 1, 2304])\n",
      "Loading tensor: h.3.attn.c_attn.weight, shape:Shape([768, 2304]), format: F32\n",
      "Transposing!\n",
      " Parameter transformer/gpt_block_3/attention/qkv_linear/weight, shape: Shape([1, 1, 2304, 768])\n",
      "Loading tensor: h.3.attn.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_3/attention/out_linear/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.3.attn.c_proj.weight, shape:Shape([768, 768]), format: F32\n",
      "Transposing!\n",
      " Parameter transformer/gpt_block_3/attention/out_linear/weight, shape: Shape([1, 1, 768, 768])\n",
      "Loading tensor: h.3.ln_1.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_3/ln1/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.3.ln_1.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_3/ln1/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.3.ln_2.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_3/ln2/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.3.ln_2.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_3/ln2/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.3.mlp.c_fc.bias, shape:Shape([3072]), format: F32\n",
      " Parameter transformer/gpt_block_3/mlp/fc1/bias, shape: Shape([1, 1, 1, 3072])\n",
      "Loading tensor: h.3.mlp.c_fc.weight, shape:Shape([768, 3072]), format: F32\n",
      " Parameter transformer/gpt_block_3/mlp/fc1/weight, shape: Shape([1, 1, 3072, 768])\n",
      "Transposing!\n",
      "Loading tensor: h.3.mlp.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_3/mlp/fc2/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.3.mlp.c_proj.weight, shape:Shape([3072, 768]), format: F32\n",
      " Parameter transformer/gpt_block_3/mlp/fc2/weight, shape: Shape([1, 1, 768, 3072])\n",
      "Transposing!\n",
      "Loading tensor: h.4.attn.bias, shape:Shape([1, 1, 1024, 1024]), format: F32\n",
      "Loading tensor: h.4.attn.c_attn.bias, shape:Shape([2304]), format: F32\n",
      " Parameter transformer/gpt_block_4/attention/qkv_linear/bias, shape: Shape([1, 1, 1, 2304])\n",
      "Loading tensor: h.4.attn.c_attn.weight, shape:Shape([768, 2304]), format: F32\n",
      "Transposing!\n",
      " Parameter transformer/gpt_block_4/attention/qkv_linear/weight, shape: Shape([1, 1, 2304, 768])\n",
      "Loading tensor: h.4.attn.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_4/attention/out_linear/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.4.attn.c_proj.weight, shape:Shape([768, 768]), format: F32\n",
      "Transposing!\n",
      " Parameter transformer/gpt_block_4/attention/out_linear/weight, shape: Shape([1, 1, 768, 768])\n",
      "Loading tensor: h.4.ln_1.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_4/ln1/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.4.ln_1.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_4/ln1/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.4.ln_2.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_4/ln2/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.4.ln_2.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_4/ln2/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.4.mlp.c_fc.bias, shape:Shape([3072]), format: F32\n",
      " Parameter transformer/gpt_block_4/mlp/fc1/bias, shape: Shape([1, 1, 1, 3072])\n",
      "Loading tensor: h.4.mlp.c_fc.weight, shape:Shape([768, 3072]), format: F32\n",
      " Parameter transformer/gpt_block_4/mlp/fc1/weight, shape: Shape([1, 1, 3072, 768])\n",
      "Transposing!\n",
      "Loading tensor: h.4.mlp.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_4/mlp/fc2/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.4.mlp.c_proj.weight, shape:Shape([3072, 768]), format: F32\n",
      " Parameter transformer/gpt_block_4/mlp/fc2/weight, shape: Shape([1, 1, 768, 3072])\n",
      "Transposing!\n",
      "Loading tensor: h.5.attn.bias, shape:Shape([1, 1, 1024, 1024]), format: F32\n",
      "Loading tensor: h.5.attn.c_attn.bias, shape:Shape([2304]), format: F32\n",
      " Parameter transformer/gpt_block_5/attention/qkv_linear/bias, shape: Shape([1, 1, 1, 2304])\n",
      "Loading tensor: h.5.attn.c_attn.weight, shape:Shape([768, 2304]), format: F32\n",
      "Transposing!\n",
      " Parameter transformer/gpt_block_5/attention/qkv_linear/weight, shape: Shape([1, 1, 2304, 768])\n",
      "Loading tensor: h.5.attn.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_5/attention/out_linear/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.5.attn.c_proj.weight, shape:Shape([768, 768]), format: F32\n",
      "Transposing!\n",
      " Parameter transformer/gpt_block_5/attention/out_linear/weight, shape: Shape([1, 1, 768, 768])\n",
      "Loading tensor: h.5.ln_1.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_5/ln1/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.5.ln_1.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_5/ln1/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.5.ln_2.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_5/ln2/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.5.ln_2.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_5/ln2/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.5.mlp.c_fc.bias, shape:Shape([3072]), format: F32\n",
      " Parameter transformer/gpt_block_5/mlp/fc1/bias, shape: Shape([1, 1, 1, 3072])\n",
      "Loading tensor: h.5.mlp.c_fc.weight, shape:Shape([768, 3072]), format: F32\n",
      " Parameter transformer/gpt_block_5/mlp/fc1/weight, shape: Shape([1, 1, 3072, 768])\n",
      "Transposing!\n",
      "Loading tensor: h.5.mlp.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_5/mlp/fc2/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.5.mlp.c_proj.weight, shape:Shape([3072, 768]), format: F32\n",
      " Parameter transformer/gpt_block_5/mlp/fc2/weight, shape: Shape([1, 1, 768, 3072])\n",
      "Transposing!\n",
      "Loading tensor: h.6.attn.bias, shape:Shape([1, 1, 1024, 1024]), format: F32\n",
      "Loading tensor: h.6.attn.c_attn.bias, shape:Shape([2304]), format: F32\n",
      " Parameter transformer/gpt_block_6/attention/qkv_linear/bias, shape: Shape([1, 1, 1, 2304])\n",
      "Loading tensor: h.6.attn.c_attn.weight, shape:Shape([768, 2304]), format: F32\n",
      "Transposing!\n",
      " Parameter transformer/gpt_block_6/attention/qkv_linear/weight, shape: Shape([1, 1, 2304, 768])\n",
      "Loading tensor: h.6.attn.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_6/attention/out_linear/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.6.attn.c_proj.weight, shape:Shape([768, 768]), format: F32\n",
      "Transposing!\n",
      " Parameter transformer/gpt_block_6/attention/out_linear/weight, shape: Shape([1, 1, 768, 768])\n",
      "Loading tensor: h.6.ln_1.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_6/ln1/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.6.ln_1.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_6/ln1/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.6.ln_2.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_6/ln2/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.6.ln_2.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_6/ln2/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.6.mlp.c_fc.bias, shape:Shape([3072]), format: F32\n",
      " Parameter transformer/gpt_block_6/mlp/fc1/bias, shape: Shape([1, 1, 1, 3072])\n",
      "Loading tensor: h.6.mlp.c_fc.weight, shape:Shape([768, 3072]), format: F32\n",
      " Parameter transformer/gpt_block_6/mlp/fc1/weight, shape: Shape([1, 1, 3072, 768])\n",
      "Transposing!\n",
      "Loading tensor: h.6.mlp.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_6/mlp/fc2/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.6.mlp.c_proj.weight, shape:Shape([3072, 768]), format: F32\n",
      " Parameter transformer/gpt_block_6/mlp/fc2/weight, shape: Shape([1, 1, 768, 3072])\n",
      "Transposing!\n",
      "Loading tensor: h.7.attn.bias, shape:Shape([1, 1, 1024, 1024]), format: F32\n",
      "Loading tensor: h.7.attn.c_attn.bias, shape:Shape([2304]), format: F32\n",
      " Parameter transformer/gpt_block_7/attention/qkv_linear/bias, shape: Shape([1, 1, 1, 2304])\n",
      "Loading tensor: h.7.attn.c_attn.weight, shape:Shape([768, 2304]), format: F32\n",
      "Transposing!\n",
      " Parameter transformer/gpt_block_7/attention/qkv_linear/weight, shape: Shape([1, 1, 2304, 768])\n",
      "Loading tensor: h.7.attn.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_7/attention/out_linear/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.7.attn.c_proj.weight, shape:Shape([768, 768]), format: F32\n",
      "Transposing!\n",
      " Parameter transformer/gpt_block_7/attention/out_linear/weight, shape: Shape([1, 1, 768, 768])\n",
      "Loading tensor: h.7.ln_1.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_7/ln1/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.7.ln_1.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_7/ln1/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.7.ln_2.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_7/ln2/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.7.ln_2.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_7/ln2/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.7.mlp.c_fc.bias, shape:Shape([3072]), format: F32\n",
      " Parameter transformer/gpt_block_7/mlp/fc1/bias, shape: Shape([1, 1, 1, 3072])\n",
      "Loading tensor: h.7.mlp.c_fc.weight, shape:Shape([768, 3072]), format: F32\n",
      " Parameter transformer/gpt_block_7/mlp/fc1/weight, shape: Shape([1, 1, 3072, 768])\n",
      "Transposing!\n",
      "Loading tensor: h.7.mlp.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_7/mlp/fc2/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.7.mlp.c_proj.weight, shape:Shape([3072, 768]), format: F32\n",
      " Parameter transformer/gpt_block_7/mlp/fc2/weight, shape: Shape([1, 1, 768, 3072])\n",
      "Transposing!\n",
      "Loading tensor: h.8.attn.bias, shape:Shape([1, 1, 1024, 1024]), format: F32\n",
      "Loading tensor: h.8.attn.c_attn.bias, shape:Shape([2304]), format: F32\n",
      " Parameter transformer/gpt_block_8/attention/qkv_linear/bias, shape: Shape([1, 1, 1, 2304])\n",
      "Loading tensor: h.8.attn.c_attn.weight, shape:Shape([768, 2304]), format: F32\n",
      "Transposing!\n",
      " Parameter transformer/gpt_block_8/attention/qkv_linear/weight, shape: Shape([1, 1, 2304, 768])\n",
      "Loading tensor: h.8.attn.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_8/attention/out_linear/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.8.attn.c_proj.weight, shape:Shape([768, 768]), format: F32\n",
      "Transposing!\n",
      " Parameter transformer/gpt_block_8/attention/out_linear/weight, shape: Shape([1, 1, 768, 768])\n",
      "Loading tensor: h.8.ln_1.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_8/ln1/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.8.ln_1.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_8/ln1/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.8.ln_2.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_8/ln2/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.8.ln_2.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_8/ln2/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.8.mlp.c_fc.bias, shape:Shape([3072]), format: F32\n",
      " Parameter transformer/gpt_block_8/mlp/fc1/bias, shape: Shape([1, 1, 1, 3072])\n",
      "Loading tensor: h.8.mlp.c_fc.weight, shape:Shape([768, 3072]), format: F32\n",
      " Parameter transformer/gpt_block_8/mlp/fc1/weight, shape: Shape([1, 1, 3072, 768])\n",
      "Transposing!\n",
      "Loading tensor: h.8.mlp.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_8/mlp/fc2/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.8.mlp.c_proj.weight, shape:Shape([3072, 768]), format: F32\n",
      " Parameter transformer/gpt_block_8/mlp/fc2/weight, shape: Shape([1, 1, 768, 3072])\n",
      "Transposing!\n",
      "Loading tensor: h.9.attn.bias, shape:Shape([1, 1, 1024, 1024]), format: F32\n",
      "Loading tensor: h.9.attn.c_attn.bias, shape:Shape([2304]), format: F32\n",
      " Parameter transformer/gpt_block_9/attention/qkv_linear/bias, shape: Shape([1, 1, 1, 2304])\n",
      "Loading tensor: h.9.attn.c_attn.weight, shape:Shape([768, 2304]), format: F32\n",
      "Transposing!\n",
      " Parameter transformer/gpt_block_9/attention/qkv_linear/weight, shape: Shape([1, 1, 2304, 768])\n",
      "Loading tensor: h.9.attn.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_9/attention/out_linear/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.9.attn.c_proj.weight, shape:Shape([768, 768]), format: F32\n",
      "Transposing!\n",
      " Parameter transformer/gpt_block_9/attention/out_linear/weight, shape: Shape([1, 1, 768, 768])\n",
      "Loading tensor: h.9.ln_1.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_9/ln1/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.9.ln_1.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_9/ln1/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.9.ln_2.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_9/ln2/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.9.ln_2.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_9/ln2/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.9.mlp.c_fc.bias, shape:Shape([3072]), format: F32\n",
      " Parameter transformer/gpt_block_9/mlp/fc1/bias, shape: Shape([1, 1, 1, 3072])\n",
      "Loading tensor: h.9.mlp.c_fc.weight, shape:Shape([768, 3072]), format: F32\n",
      " Parameter transformer/gpt_block_9/mlp/fc1/weight, shape: Shape([1, 1, 3072, 768])\n",
      "Transposing!\n",
      "Loading tensor: h.9.mlp.c_proj.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/gpt_block_9/mlp/fc2/bias, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: h.9.mlp.c_proj.weight, shape:Shape([3072, 768]), format: F32\n",
      " Parameter transformer/gpt_block_9/mlp/fc2/weight, shape: Shape([1, 1, 768, 3072])\n",
      "Transposing!\n",
      "Loading tensor: ln_f.bias, shape:Shape([768]), format: F32\n",
      " Parameter transformer/ln_fc/beta, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: ln_f.weight, shape:Shape([768]), format: F32\n",
      " Parameter transformer/ln_fc/gamma, shape: Shape([1, 1, 1, 768])\n",
      "Loading tensor: wpe.weight, shape:Shape([1024, 768]), format: F32\n",
      " Parameter transformer/pos_emb/weight, shape: Shape([1, 1, 1024, 768])\n",
      "Loading tensor: wte.weight, shape:Shape([50257, 768]), format: F32\n",
      " Parameter transformer/fc/weight, shape: Shape([1, 1, 50272, 768])\n",
      "Original shape 50257, 768Transformed shape 50272, 768\n"
     ]
    }
   ],
   "source": [
    "def create_model(cfg, vocab_size: int, seq_len: int):\n",
    "    # GPT2 config via your bindings\n",
    "    gcfg = ttml.models.gpt2.GPT2TransformerConfig()\n",
    "    gcfg.num_heads = cfg[\"num_heads\"]\n",
    "    gcfg.embedding_dim = cfg[\"embedding_dim\"]\n",
    "    gcfg.num_blocks = cfg[\"num_blocks\"]\n",
    "    gcfg.vocab_size = int(vocab_size)\n",
    "    gcfg.max_sequence_length = seq_len\n",
    "    gcfg.dropout_prob = cfg[\"dropout_prob\"]\n",
    "    # optional flags exist (runner_type, weight_tying, positional_embedding_type, experimental, ...)\n",
    "    # we keep defaults for a minimal demo\n",
    "\n",
    "    model = ttml.models.gpt2.create_gpt2_model(gcfg)\n",
    "    model.load_from_safetensors(Path(local_path))\n",
    "    return model\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "if vocab_size % 32 != 0:\n",
    "    print(f\"Warning: vocab size {vocab_size} is not multiple of 32, padding for tilizing.\")\n",
    "    padded_vocab_size = ((tokenizer.vocab_size + 31) // 32) * 32\n",
    "\n",
    "else:\n",
    "    padded_vocab_size = vocab_size\n",
    "\n",
    "model = create_model(transformer_cfg, padded_vocab_size, transformer_cfg[\"max_sequence_length\"])\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8beb647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ( ( ( ( ( ( ( ( ( ( ( ( ( the ( ( \" ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n",
      " ( ( ( ( ( and ( ( and (\n",
      " 35 and\n",
      " F the and and ( 35 ( 34 34 35 F 35\n",
      " 35\n",
      " ( 35<|endoftext|> 35 ( 35 34 35 36 39 54<|endoftext|> 35 45 35 35 35<|endoftext|> 35\n",
      " 35 35 I 35 ( 35\n",
      " 35<|endoftext|> 35 I 35 35 34 35 I 35\n",
      " 35 35 35 35 I<|endoftext|> 35<|endoftext|><|endoftext|> I 35 35 35\n",
      " 35<|endoftext|> 31 35 I 35 35 35 45 I 35 35 36 35\n",
      " 35<|endoftext|> 35<|endoftext|> 36 A 31 I<|endoftext|> 31 35 F 35 35 35 ( 35\n",
      " I A 35 I<|endoftext|>\n",
      "<|endoftext|><|endoftext|>\n",
      "\n",
      " 31 35 I 35 The<|endoftext|> (<|endoftext|> 45 36 35<|endoftext|> 35\n",
      " 36 18 36 A The<|endoftext|> 34\n",
      " 35 I\n",
      " 35 35 35<|endoftext|> 34\n",
      " 35<|endoftext|> I 45<|endoftext|> 36 35 45 The 35 55\n",
      " 35<|endoftext|> 36<|endoftext|> 35 35 I<|endoftext|> 36\n",
      "<|endoftext|><|endoftext|> ( 35 I H<|endoftext|>\n",
      " 34 I<|endoftext|> A I 35<|endoftext|><|endoftext|> 35<|endoftext|> F<|endoftext|>\n",
      "\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> I 35 I<|endoftext|> I 36<|endoftext|><|endoftext|> I I 36 ( I"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "ttml.autograd.AutoContext.get_instance().set_gradient_mode(ttml.autograd.GradMode.DISABLED)\n",
    "\n",
    "if padded_vocab_size != vocab_size:\n",
    "    logits_mask_tensor = build_logits_mask(vocab_size, padded_vocab_size)\n",
    "else:\n",
    "    logits_mask_tensor = None\n",
    "\n",
    "prompt_str = \"The difference between cats and dogs is:\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "\n",
    "causal_mask = build_causal_mask(transformer_cfg[\"max_sequence_length\"])  # [1,1,seq_len,seq_len], float32\n",
    "padded_prompt_tokens = np.full((1, 1, 1, transformer_cfg[\"max_sequence_length\"]), \n",
    "                                tokenizer.eos_token_id,\n",
    "                                dtype=np.uint32)\n",
    "\n",
    "\n",
    "start_idx = 0\n",
    "for token_idx in range(OUTPUT_TOKENS):\n",
    "\n",
    "    if len(prompt_tokens) > transformer_cfg[\"max_sequence_length\"]:\n",
    "        start_idx = len(prompt_tokens) - transformer_cfg[\"max_sequence_length\"]\n",
    "\n",
    "    padded_prompt_tokens[0, 0, 0, :transformer_cfg[\"max_sequence_length\"]] = tokenizer.eos_token_id\n",
    "    padded_prompt_tokens[0, 0, 0, start_idx:start_idx + len(prompt_tokens)] = prompt_tokens\n",
    "    padded_prompt_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "        padded_prompt_tokens,\n",
    "        ttml.Layout.ROW_MAJOR,\n",
    "        ttml.autograd.DataType.UINT32\n",
    "    )  # [1,1,1, max_seq_len], uint32\n",
    "\n",
    "    logits = model(padded_prompt_tensor, causal_mask)  # [1,1,1, vocab_size]\n",
    "    next_token_tensor = ttml.ops.sample.sample_op(logits, 1.0, np.random.randint(low=1e6), logits_mask_tensor)  # [1,1,seq_len,vocab_size], uint32\n",
    "    \n",
    "    next_token_idx = transformer_cfg[\"max_sequence_length\"] - 1 if len(prompt_tokens) >= transformer_cfg[\"max_sequence_length\"] else len(prompt_tokens) - 1\n",
    "    next_token = next_token_tensor.to_numpy().flatten()[next_token_idx]\n",
    "\n",
    "    output = tokenizer.decode(next_token)\n",
    "\n",
    "    prompt_tokens.append(next_token)\n",
    "\n",
    "    print(output, end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
