{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f7bc745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttml\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6545c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TOKENS = 100\n",
    "WITH_SAMPLING = True\n",
    "TEMPERATURE = 0.0\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41f3ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\" \n",
    "CONFIG = \"training_shakespeare_llama3_2_1B_fixed.yaml\" # now working fine (with proper shape for tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e1948c",
   "metadata": {},
   "source": [
    "model_id =  \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "CONFIG = \"training_shakespeare_tinyllama.yaml\" # working fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c120696",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "CONFIG = \"training_shakespeare_llama3_2_3B.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a19149b",
   "metadata": {},
   "source": [
    "model_id =  \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "CONFIG = \"training_shakespeare_llama3_8B_tp.yaml\" # OOM on 12 GB, sucesfully loaded weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b615d9",
   "metadata": {},
   "source": [
    "model_id = \"Qwen/Qwen3-0.6B\" \n",
    "CONFIG = \"training_shakespeare_qwen3_0_6B.yaml\" # working, not 1-1 as llama, but speak something nongibberish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415a0c09",
   "metadata": {},
   "source": [
    "model_id = \"Qwen/Qwen3-1.7B\" \n",
    "CONFIG = \"training_shakespeare_qwen3_1_7B.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13665422",
   "metadata": {},
   "source": [
    "model_id = \"Qwen/Qwen3-4B\"\n",
    "CONFIG = \"training_shakespeare_qwen3_4B.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df6d6bf9-a010-4206-b004-807ee500d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "import numpy as np  # For numpy arrays\n",
    "from dataclasses import dataclass # For configuration classes\n",
    "from huggingface_hub import hf_hub_download # To download safetensors from Hugging Face\n",
    "from transformers import AutoTokenizer\n",
    "from yaml import safe_load # To read YAML configs\n",
    "from pathlib import Path\n",
    "\n",
    "import ttml\n",
    "from ttml.common.config import get_config, TransformerConfig\n",
    "from ttml.common.utils import set_seed, round_up_to_tile\n",
    "from ttml.common.model_factory import TransformerModelFactory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55071c7a-74fe-4f56-8f33-e45ffe699c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from Hugging Face and the transformer config from YAML\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "transformer_config = TransformerConfig(get_config(CONFIG).get(\"training_config\", {}).get(\"transformer_config\",{}))\n",
    "yaml_config = get_config(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e7883f6-f8e6-436b-8908-62c8a26e1e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "safetensors_path = hf_hub_download(repo_id=model_id, filename=\"config.json\")\n",
    "safetensors_path = safetensors_path.replace(\"config.json\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f77e69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03115b5580134d9e8ce7e1ad3da87f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "torch.manual_seed(SEED)\n",
    "torch_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f405407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128000, 128256, 128256)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size, torch_model.state_dict()['model.embed_tokens.weight'].shape[0], torch_model.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50d8ce12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torch_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf74d9ac-7afd-4c6f-887e-b793c6e44c18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128256\n"
     ]
    }
   ],
   "source": [
    "orig_vocab_size = torch_model.vocab_size\n",
    "print(orig_vocab_size)\n",
    "tt_model_factory = TransformerModelFactory(yaml_config)\n",
    "tt_model_factory.transformer_config.vocab_size = orig_vocab_size\n",
    "\n",
    "max_sequence_length = tt_model_factory.transformer_config.max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56dff343",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttml.autograd.AutoContext.get_instance().set_init_mode(ttml.autograd.InitMode.DISABLED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff6ac0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama configuration:\n",
      "    Vocab size: 128256\n",
      "    Max sequence length: 2048\n",
      "    Embedding dim: 3072\n",
      "    Intermediate dim: 8192\n",
      "    Num heads: 24\n",
      "    Num groups: 8\n",
      "    Dropout probability: 0\n",
      "    Num blocks: 28\n",
      "    Positional embedding type: RoPE\n",
      "    Runner type: Memory efficient\n",
      "    Weight tying: Enabled\n",
      "    Theta: 500000\n",
      "2025-11-27 23:33:06.159 | info     |             UMD | Established cluster ETH FW version: 6.14.0 (topology_discovery_wormhole.cpp:359)\n",
      "2025-11-27 23:33:06.163 | info     |          Device | Opening user mode device driver (tt_cluster.cpp:209)\n",
      "2025-11-27 23:33:06.183 | info     |             UMD | Established cluster ETH FW version: 6.14.0 (topology_discovery_wormhole.cpp:359)\n",
      "2025-11-27 23:33:06.222 | info     |             UMD | Established cluster ETH FW version: 6.14.0 (topology_discovery_wormhole.cpp:359)\n",
      "2025-11-27 23:33:06.226 | info     |             UMD | Harvesting mask for chip 0 is 0x1 (NOC0: 0x1, simulated harvesting mask: 0x0). (cluster.cpp:413)\n",
      "2025-11-27 23:33:06.262 | warning  |             UMD | init_detect_tt_device_numanodes(): Could not determine NumaNodeSet for TT device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:564)\n",
      "2025-11-27 23:33:06.262 | warning  |             UMD | Could not find NumaNodeSet for TT Device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:173)\n",
      "2025-11-27 23:33:06.262 | warning  |             UMD | bind_area_memory_nodeset(): Unable to determine TT Device to NumaNode mapping for physical_device_id: 0. Skipping membind. (cpuset_lib.cpp:450)\n",
      "2025-11-27 23:33:06.262 | warning  |             UMD | ---- ttSiliconDevice::init_hugepage: bind_area_to_memory_nodeset() failed (physical_device_id: 0 ch: 0). Hugepage allocation is not on NumaNode matching TT Device. Side-Effect is decreased Device->Host perf (Issue #893). (sysmem_manager.cpp:212)\n",
      "2025-11-27 23:33:06.263 | info     |             UMD | Opening local chip ids/PCIe ids: {0}/[0] and remote chip ids {} (cluster.cpp:257)\n",
      "2025-11-27 23:33:06.263 | info     |             UMD | All devices in cluster running firmware version: 80.17.0 (cluster.cpp:235)\n",
      "2025-11-27 23:33:06.263 | info     |             UMD | IOMMU: disabled (cluster.cpp:177)\n",
      "2025-11-27 23:33:06.263 | info     |             UMD | KMD version: 2.0.0 (cluster.cpp:180)\n",
      "2025-11-27 23:33:06.266 | info     |             UMD | Pinning pages for Hugepage: virtual address 0x7f1e80000000 and size 0x40000000 pinned to physical address 0x200000000 (pci_device.cpp:536)\n",
      "2025-11-27 23:33:07.556 | info     |           Metal | Profiler started on device 0 (device_pool.cpp:203)\n",
      "Model created: 7.940993309020996\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "tt_model = tt_model_factory.create_model()\n",
    "print(f\"Model created: {time() - start_time}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfb5a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b38196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llama 3b\n",
    "#Model created: 238.99020171165466\n",
    "#Model loaded: 47.06614542007446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b490f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llama 3b\n",
    "#Model created: 7.940993309020996\n",
    "#Model loaded: 58.52051877975464"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb62dae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 58.52051877975464\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "tt_model.load_from_safetensors(safetensors_path)\n",
    "print(f\"Model loaded: {time() - start_time}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bdc85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_vocab_size = round_up_to_tile(orig_vocab_size, 32)\n",
    "if orig_vocab_size != padded_vocab_size:\n",
    "    print(f\"Padding vocab size for tilization: original {orig_vocab_size} -> padded {padded_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb047274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3cf900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4d4f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_causal_mask(T: int) -> ttml.autograd.Tensor:\n",
    "    # [1,1,T,T] float32 with 1s for allowed positions (i >= j), else 0\\n\",\n",
    "    m = np.tril(np.ones((T, T), dtype=np.float32))\n",
    "    return ttml.autograd.Tensor.from_numpy(m.reshape(1, 1, T, T), ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)\n",
    "\n",
    "def build_logits_mask(vocab_size: int, padded_vocab_size: int) -> ttml.autograd.Tensor:\n",
    "    logits_mask = np.zeros((1, 1, 1, padded_vocab_size), dtype=np.float32)\n",
    "    logits_mask[:, :, :, vocab_size:] = 1e4\n",
    "    return ttml.autograd.Tensor.from_numpy(logits_mask, ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)   # [1,1,1,T], bfloat16\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40913e88",
   "metadata": {},
   "source": [
    "`generate_with_tt()` uses TT hardware acceleration to generate output from the chosen LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18e6550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_tt(model, prompt_tokens):\n",
    "    import time\n",
    "    \n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(ttml.autograd.GradMode.DISABLED)\n",
    "    model.eval()\n",
    "\n",
    "    logits_mask_tensor = None\n",
    "\n",
    "    if padded_vocab_size != orig_vocab_size:\n",
    "        logits_mask_tensor = build_logits_mask(orig_vocab_size, padded_vocab_size)\n",
    "\n",
    "    causal_mask = build_causal_mask(max_sequence_length)  # [1,1,seq_len,seq_len], float32\n",
    "    padded_prompt_tokens = np.zeros((1, 1, 1, max_sequence_length), \n",
    "                                    dtype=np.uint32)\n",
    "\n",
    "    start_idx = 0\n",
    "\n",
    "    print(\"************************************\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for token_idx in range(OUTPUT_TOKENS):\n",
    "\n",
    "        if len(prompt_tokens) > max_sequence_length:\n",
    "            start_idx = len(prompt_tokens) - max_sequence_length\n",
    "\n",
    "        # padded_prompt_tokens[0, 0, 0, :transformer_cfg[\"max_sequence_length\"]] = 0\n",
    "        padded_prompt_tokens[0, 0, 0, :len(prompt_tokens)] = prompt_tokens[start_idx:]\n",
    "        padded_prompt_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "            padded_prompt_tokens,\n",
    "            ttml.Layout.ROW_MAJOR,\n",
    "            ttml.autograd.DataType.UINT32)  # [1,1,1, max_seq_len], uint32\n",
    "\n",
    "        logits = model(padded_prompt_tensor, causal_mask)  # out=[1,1,seq_len, vocab_size], bf16\n",
    "\n",
    "\n",
    "        next_token_tensor = ttml.ops.sample.sample_op(logits, TEMPERATURE, np.random.randint(low=1e7), logits_mask_tensor)  # out=[1,1,seq_len,1], uint32\n",
    "\n",
    "        next_token_idx = max_sequence_length - 1 if len(prompt_tokens) > max_sequence_length else len(prompt_tokens) - 1\n",
    "        next_token = next_token_tensor.to_numpy().flatten()[next_token_idx]\n",
    "\n",
    "        output = tokenizer.decode(next_token)\n",
    "\n",
    "        prompt_tokens.append(next_token)\n",
    "        print(output, end='', flush=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = OUTPUT_TOKENS / elapsed_time\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {OUTPUT_TOKENS} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7824c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_pytorch(torch_model, prompt_tokens):\n",
    "    import time\n",
    "    import torch.nn.functional as F\n",
    "    from transformers import DynamicCache\n",
    "    \n",
    "    torch_model.eval()\n",
    "    \n",
    "    print(\"************************************\")\n",
    "    # Convert list to tensor and add batch dimension\n",
    "    if isinstance(prompt_tokens, list):\n",
    "        prompt_tokens = torch.tensor([prompt_tokens])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize KV cache using the new DynamicCache API\n",
    "    past_key_values = DynamicCache()\n",
    "    input_ids = prompt_tokens\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(OUTPUT_TOKENS):\n",
    "            # Get model outputs with KV cache\n",
    "            outputs = torch_model(\n",
    "                input_ids=input_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            past_key_values = outputs.past_key_values\n",
    "            \n",
    "            # Get logits for the last token\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            \n",
    "            # Apply temperature and sample\n",
    "            if WITH_SAMPLING and TEMPERATURE > 0:\n",
    "                next_token_logits = next_token_logits / TEMPERATURE\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                # Greedy sampling\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            \n",
    "            # Decode and print the token\n",
    "            output = tokenizer.decode(next_token[0])\n",
    "            print(output, end='', flush=True)\n",
    "            \n",
    "            # For next iteration, only pass the new token (KV cache handles the rest)\n",
    "            input_ids = next_token\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = OUTPUT_TOKENS / elapsed_time\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {OUTPUT_TOKENS} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b17b326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_pytorch_batch(torch_model, prompt_tokens):\n",
    "    \"\"\"Old version: non-streaming batch generation using torch_model.generate()\"\"\"\n",
    "    import time\n",
    "    \n",
    "    torch_model.eval()\n",
    "    \n",
    "    print(\"************************************\")\n",
    "    # Convert list to tensor and add batch dimension\n",
    "    if isinstance(prompt_tokens, list):\n",
    "        prompt_tokens = torch.tensor([prompt_tokens])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = torch_model.generate(\n",
    "            prompt_tokens,\n",
    "            max_new_tokens=OUTPUT_TOKENS,\n",
    "            do_sample=WITH_SAMPLING,  # Enable sampling\n",
    "            temperature=TEMPERATURE,   # Temperature for sampling\n",
    "            num_beams=1  # Use multinomial sampling (standard sampling)\n",
    "        )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = OUTPUT_TOKENS / elapsed_time\n",
    "    \n",
    "    generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    for t in generated_text:\n",
    "        print(t)\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {OUTPUT_TOKENS} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4813faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with torch model:\n",
      "************************************\n",
      " - How to optimize?\n",
      "PyTorch is a powerful deep learning framework that can be used for a wide range of tasks, including image and speech recognition, natural language processing, and more. However, PyTorch can be computationally expensive, especially when working with large datasets or complex models. Here are some tips to optimize PyTorch code for CPU generation:\n",
      "\n",
      "1.  **Use the `torch.no_grad()` context manager**: This context manager tells PyTorch not to record gradients during the\n",
      "************************************\n",
      "Generated 100 tokens in 45.69 seconds\n",
      "Performance: 2.19 tokens/second\n",
      "************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_str = \"Generating with pytorch(CPU, might be slow)\"\n",
    "\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with torch model:\")\n",
    "generate_with_pytorch(torch_model, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a42273b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with TT:\n",
      "************************************\n",
      " - How to improve performance?\n",
      "PyTorch is a popular deep learning framework that can be used for both research and production. However, it can be slow for large models and"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m prompt_tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt_str)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating with TT:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mgenerate_with_tt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtt_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 39\u001b[0m, in \u001b[0;36mgenerate_with_tt\u001b[0;34m(model, prompt_tokens)\u001b[0m\n\u001b[1;32m     36\u001b[0m next_token_tensor \u001b[38;5;241m=\u001b[39m ttml\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39msample\u001b[38;5;241m.\u001b[39msample_op(logits, TEMPERATURE, np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e7\u001b[39m), logits_mask_tensor)  \u001b[38;5;66;03m# out=[1,1,seq_len,1], uint32\u001b[39;00m\n\u001b[1;32m     38\u001b[0m next_token_idx \u001b[38;5;241m=\u001b[39m max_sequence_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt_tokens) \u001b[38;5;241m>\u001b[39m max_sequence_length \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt_tokens) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 39\u001b[0m next_token \u001b[38;5;241m=\u001b[39m \u001b[43mnext_token_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()[next_token_idx]\n\u001b[1;32m     41\u001b[0m output \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(next_token)\n\u001b[1;32m     43\u001b[0m prompt_tokens\u001b[38;5;241m.\u001b[39mappend(next_token)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prompt_str = \"Generating with pytorch(CPU, might be slow)\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with TT:\")\n",
    "generate_with_tt(tt_model, prompt_tokens.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e485d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdb0ead4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1307019640.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[19], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    PyTorch is a powerful deep learning framework, but it can be slow for generating data, especially when using CPU. Here are some suggestions to speed up the process:\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    " - How to speed it up?\n",
    "\n",
    "PyTorch is a powerful deep learning framework, but it can be slow for generating data, especially when using CPU. Here are some suggestions to speed up the process:\n",
    "\n",
    "1.  **Use a GPU**: PyTorch supports GPU acceleration, which can significantly speed up the process. You can use a GPU with a CUDA-enabled NVIDIA GPU or a Tesla V100 GPU.\n",
    "2.  **Use a faster backend**: PyTorch has a few different backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59adb313",
   "metadata": {},
   "outputs": [],
   "source": [
    " - How to speed it up?\n",
    "\n",
    "PyTorch is a powerful deep learning framework, but it can be slow for generating data, especially when using CPU. Here are some tips to speed up the process:\n",
    "\n",
    "1.  **Use a GPU**: PyTorch supports GPU acceleration, which can significantly speed up the process. You can use a GPU with a CUDA-enabled NVIDIA GPU or a Tesla V100 GPU.\n",
    "2.  **Use a faster data type**: PyTorch uses `float32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12548c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c891bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706f1484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7d5111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5863214e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ac2db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b811811",
   "metadata": {},
   "source": [
    "def generate_with_pytorch_no_cache(torch_model, prompt_tokens):\n",
    "    import time\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    torch_model.eval()\n",
    "    \n",
    "    print(\"************************************\")\n",
    "    # Convert list to tensor and add batch dimension\n",
    "    if isinstance(prompt_tokens, list):\n",
    "        input_ids = torch.tensor([prompt_tokens])\n",
    "    else:\n",
    "        input_ids = prompt_tokens\n",
    "    \n",
    "    start_time = time.time()\n",
    "    tokens = 10  # this is very slow\n",
    "    with torch.no_grad():\n",
    "        for i in range(tokens):\n",
    "            # Process the entire sequence every time (no KV cache)\n",
    "            outputs = torch_model(\n",
    "                input_ids=input_ids,\n",
    "                use_cache=False\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Get logits for the last token\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            \n",
    "            # Apply temperature and sample\n",
    "            if WITH_SAMPLING and TEMPERATURE > 0:\n",
    "                next_token_logits = next_token_logits / TEMPERATURE\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                # Greedy sampling\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            \n",
    "            # Decode and print the token\n",
    "            output = tokenizer.decode(next_token[0])\n",
    "            print(output, end='', flush=True)\n",
    "            \n",
    "            # Append the new token to the full sequence for next iteration\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = tokens / elapsed_time\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {tokens} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c4b440",
   "metadata": {},
   "source": [
    "prompt_str = \"Generating with PyTorch, (should be slow, without kv-caching)\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with PyTorch:\")\n",
    "generate_with_pytorch_no_cache(torch_model, prompt_tokens.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8521eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = torch_model.state_dict()\n",
    "for s in sd:\n",
    "    print(s, sd[s].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e868214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = tt_model.parameters()\n",
    "for s in k:\n",
    "    print(s, k[s].shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aa5542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e9744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def apply_rope_permutation(w, num_heads):\n",
    "    \"\"\"\n",
    "    Apply RoPE row permutation to match TT's q_proj weight layout.\n",
    "    TT applies unpermute_proj_rows during loading which interleaves rows within each head.\n",
    "    \"\"\"\n",
    "    rows, cols = w.shape\n",
    "    head_dim = rows // num_heads\n",
    "    \n",
    "    out = np.zeros_like(w)\n",
    "    for h in range(num_heads):\n",
    "        head_start = h * head_dim\n",
    "        half = head_dim // 2\n",
    "        \n",
    "        # Interleave: [0..half-1, half..head_dim-1] → [0, half, 1, half+1, ..., half-1, head_dim-1]\n",
    "        for i in range(half):\n",
    "            out[head_start + 2*i] = w[head_start + i]\n",
    "            out[head_start + 2*i + 1] = w[head_start + half + i]\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def compare_weights_llama(torch_model, tt_model):\n",
    "    \"\"\"\n",
    "    Compare weights between PyTorch model and TT-Metal model.\n",
    "    \n",
    "    Args:\n",
    "        torch_model: PyTorch model (with .state_dict())\n",
    "        tt_model: TT-Metal model (with .parameters())\n",
    "    \"\"\"\n",
    "    \n",
    "    torch_sd = torch_model.state_dict()\n",
    "    tt_params = tt_model.parameters()\n",
    "    \n",
    "    # Get num_heads from torch model config\n",
    "    num_heads = torch_model.config.num_attention_heads\n",
    "    \n",
    "    # Detect weight tying configuration from TT model\n",
    "    has_tok_emb = 'llama/tok_emb/weight' in tt_params\n",
    "    has_fc = 'llama/fc/weight' in tt_params\n",
    "    weight_tying_enabled = not has_tok_emb  # If tok_emb doesn't exist, weight tying is enabled\n",
    "    \n",
    "    # Mapping from PyTorch parameter names to TT-Metal parameter names\n",
    "    pytorch_to_tt_mapping = {\n",
    "        # Final layer norm\n",
    "        'model.norm.weight': 'llama/ln_fc/gamma',\n",
    "    }\n",
    "    \n",
    "    # Add embedding mappings based on weight tying configuration\n",
    "    if weight_tying_enabled:\n",
    "        # Weight tying enabled: both embed_tokens and lm_head use fc/weight\n",
    "        pytorch_to_tt_mapping['model.embed_tokens.weight'] = 'llama/fc/weight'\n",
    "        # lm_head.weight should also map to fc/weight, but we'll skip it to avoid duplicate checks\n",
    "    else:\n",
    "        # Weight tying disabled: separate tok_emb and fc\n",
    "        pytorch_to_tt_mapping['model.embed_tokens.weight'] = 'llama/tok_emb/weight'\n",
    "        pytorch_to_tt_mapping['lm_head.weight'] = 'llama/fc/weight'\n",
    "    \n",
    "    # Add layer-specific mappings\n",
    "    for i in range(50):  # Support up to 50 layers\n",
    "        layer_prefix_pt = f'model.layers.{i}'\n",
    "        layer_prefix_tt = f'llama/llama_block_{i}'\n",
    "        \n",
    "        pytorch_to_tt_mapping.update({\n",
    "            f'{layer_prefix_pt}.input_layernorm.weight': f'{layer_prefix_tt}/attention_norm/gamma',\n",
    "            f'{layer_prefix_pt}.post_attention_layernorm.weight': f'{layer_prefix_tt}/mlp_norm/gamma',\n",
    "            f'{layer_prefix_pt}.self_attn.q_proj.weight': f'{layer_prefix_tt}/attention/q_linear/weight',\n",
    "            f'{layer_prefix_pt}.self_attn.o_proj.weight': f'{layer_prefix_tt}/attention/out_linear/weight',\n",
    "            # k_proj and v_proj are combined into kv_linear in TT\n",
    "            f'{layer_prefix_pt}.mlp.gate_proj.weight': f'{layer_prefix_tt}/mlp/w1/weight',\n",
    "            f'{layer_prefix_pt}.mlp.up_proj.weight': f'{layer_prefix_tt}/mlp/w3/weight',\n",
    "            f'{layer_prefix_pt}.mlp.down_proj.weight': f'{layer_prefix_tt}/mlp/w2/weight',\n",
    "        })\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"WEIGHT COMPARISON: PyTorch vs TT-Metal\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Note: Detected num_heads={num_heads} for RoPE permutation\")\n",
    "    print(f\"Note: Weight tying {'ENABLED' if weight_tying_enabled else 'DISABLED'}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    mismatches = []\n",
    "    matches = []\n",
    "    \n",
    "    for pt_name in torch_sd.keys():\n",
    "        if 'bias' in pt_name:\n",
    "            continue  # Skip bias parameters\n",
    "            \n",
    "        pt_tensor = torch_sd[pt_name]\n",
    "        pt_shape = tuple(pt_tensor.shape)\n",
    "        \n",
    "        # Handle k_proj and v_proj specially (they're combined in TT)\n",
    "        if '.self_attn.k_proj.weight' in pt_name or '.self_attn.v_proj.weight' in pt_name:\n",
    "            layer_idx = pt_name.split('.')[2]\n",
    "            tt_name = f'llama/llama_block_{layer_idx}/attention/kv_linear/weight'\n",
    "            \n",
    "            if tt_name in tt_params:\n",
    "                tt_tensor_np = tt_params[tt_name].to_numpy()\n",
    "                tt_shape = tt_tensor_np.shape\n",
    "                \n",
    "                # k_proj and v_proj are concatenated in kv_linear\n",
    "                # Expected: k_proj [512, 2048] + v_proj [512, 2048] = kv_linear [1, 1, 1024, 2048]\n",
    "                if '.self_attn.k_proj.weight' in pt_name:\n",
    "                    print(f\"\\n{pt_name}\")\n",
    "                    print(f\"  PyTorch: {pt_shape}\")\n",
    "                    print(f\"  TT (kv combined): {tt_shape}\")\n",
    "                    print(f\"  Status: K and V are combined in TT as kv_linear\")\n",
    "            continue\n",
    "        \n",
    "        # Get corresponding TT parameter name\n",
    "        tt_name = pytorch_to_tt_mapping.get(pt_name)\n",
    "        if not tt_name:\n",
    "            continue\n",
    "            \n",
    "        if tt_name not in tt_params:\n",
    "            print(f\"\\n❌ MISSING: {pt_name} -> {tt_name}\")\n",
    "            print(f\"   PyTorch shape: {pt_shape}\")\n",
    "            mismatches.append((pt_name, \"MISSING IN TT\"))\n",
    "            continue\n",
    "        \n",
    "        # Get TT tensor\n",
    "        tt_tensor_np = tt_params[tt_name].to_numpy()\n",
    "        tt_shape = tt_tensor_np.shape\n",
    "        \n",
    "        # Remove batch dimensions [1, 1, ...] from TT tensor\n",
    "        tt_shape_no_batch = tt_shape[2:] if len(tt_shape) == 4 else tt_shape\n",
    "        \n",
    "        # Compare shapes\n",
    "        pt_numpy = pt_tensor.cpu().float().numpy()  # Convert to float32 for numpy compatibility\n",
    "        \n",
    "        # Special handling for q_proj: TT applies RoPE row permutation during loading\n",
    "        is_q_proj = '.self_attn.q_proj.weight' in pt_name\n",
    "        if is_q_proj and len(pt_shape) == 2 and pt_shape[0] % num_heads == 0:\n",
    "            pt_numpy = apply_rope_permutation(pt_numpy, num_heads)\n",
    "        \n",
    "        # For layer norms: PT (N,) vs TT (1, N) - both are fine, just broadcasting\n",
    "        # Check if PT is 1D and TT has leading 1s that can be squeezed\n",
    "        if len(pt_shape) == 1 and len(tt_shape_no_batch) == 2 and tt_shape_no_batch[0] == 1:\n",
    "            tt_shape_no_batch = (tt_shape_no_batch[1],)  # Squeeze leading 1\n",
    "        \n",
    "        # Check if shapes match (with or without transpose)\n",
    "        shape_match = (pt_shape == tt_shape_no_batch) or (pt_shape == tt_shape_no_batch[::-1])\n",
    "        \n",
    "        if shape_match:\n",
    "            # Check actual values\n",
    "            # Reshape TT data to match PT shape (handle batch dims and potential squeezing)\n",
    "            if len(tt_shape) == 4:\n",
    "                tt_data = tt_tensor_np.reshape(tt_shape[2:])  # Remove [1,1,...] batch dims\n",
    "            else:\n",
    "                tt_data = tt_tensor_np.reshape(tt_shape)\n",
    "            \n",
    "            # Squeeze if needed for 1D comparisons\n",
    "            tt_data = tt_data.squeeze()\n",
    "            pt_numpy_squeezed = pt_numpy.squeeze()\n",
    "            \n",
    "            # Handle transpose if needed\n",
    "            if pt_numpy_squeezed.shape != tt_data.shape and len(tt_data.shape) == 2:\n",
    "                tt_data = tt_data.T\n",
    "            \n",
    "            diff = np.abs(pt_numpy_squeezed - tt_data).max()\n",
    "            rel_diff = diff / (np.abs(pt_numpy_squeezed).max() + 1e-8)\n",
    "            \n",
    "            status = \"✓\" if diff < 1e-3 else \"⚠\"\n",
    "            note = \" (after RoPE permutation)\" if is_q_proj else \"\"\n",
    "            print(f\"\\n{status} {pt_name}{note}\")\n",
    "            print(f\"  PyTorch: {pt_shape}\")\n",
    "            print(f\"  TT:      {tt_shape} -> {tt_shape_no_batch}\")\n",
    "            print(f\"  Max diff: {diff:.6f}, Rel diff: {rel_diff:.6f}\")\n",
    "            \n",
    "            if diff < 1e-3:\n",
    "                matches.append(pt_name)\n",
    "            else:\n",
    "                mismatches.append((pt_name, f\"VALUE_DIFF={diff:.6f}\"))\n",
    "        else:\n",
    "            print(f\"\\n❌ SHAPE MISMATCH: {pt_name}\")\n",
    "            print(f\"  PyTorch: {pt_shape}\")\n",
    "            print(f\"  TT:      {tt_shape} -> {tt_shape_no_batch}\")\n",
    "            mismatches.append((pt_name, f\"SHAPE: PT={pt_shape} vs TT={tt_shape_no_batch}\"))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"SUMMARY: {len(matches)} matches, {len(mismatches)} mismatches\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if mismatches:\n",
    "        print(\"\\n❌ MISMATCHES:\")\n",
    "        for name, issue in mismatches:\n",
    "            print(f\"  - {name}: {issue}\")\n",
    "    \n",
    "    return matches, mismatches\n",
    "\n",
    "\n",
    "# Usage example (commented out):\n",
    "matches, mismatches = compare_weights_llama(torch_model, tt_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7af5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Qwen3 Weight Comparison Script\n",
    "\n",
    "Compares weights between PyTorch Qwen3 model and TT-Metal Qwen3 model.\n",
    "\n",
    "Key Qwen3 features:\n",
    "- Explicit head_dim (128 for 0.6B model)\n",
    "- Q/K normalization for numerical stability (CRITICAL!)\n",
    "- Attention dimension != embedding dimension\n",
    "- Weight tying enabled by default\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def apply_rope_permutation(w, num_heads):\n",
    "    \"\"\"\n",
    "    Apply RoPE row permutation to match TT's q_proj/k_proj weight layout.\n",
    "    TT applies unpermute_proj_rows during loading which interleaves rows within each head.\n",
    "    \n",
    "    This reorders: [0..D/2-1, D/2..D-1] → [0, D/2, 1, D/2+1, ..., D/2-1, D-1]\n",
    "    \"\"\"\n",
    "    rows, cols = w.shape\n",
    "    head_dim = rows // num_heads\n",
    "    \n",
    "    if head_dim % 2 != 0:\n",
    "        raise ValueError(f\"Head dimension {head_dim} must be even for RoPE permutation\")\n",
    "    \n",
    "    out = np.zeros_like(w)\n",
    "    for h in range(num_heads):\n",
    "        head_start = h * head_dim\n",
    "        half = head_dim // 2\n",
    "        \n",
    "        # Interleave: first half and second half of each head\n",
    "        for i in range(half):\n",
    "            out[head_start + 2*i] = w[head_start + i]\n",
    "            out[head_start + 2*i + 1] = w[head_start + half + i]\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def compare_qwen3_weights(torch_model, tt_model, verbose=True):\n",
    "    \"\"\"\n",
    "    Compare weights between PyTorch Qwen3 model and TT-Metal Qwen3 model.\n",
    "    \n",
    "    Qwen3-specific features:\n",
    "    - Q projection: [2048, 1024] (projects UP to attention space)\n",
    "    - K projection: [1024, 1024] (num_kv_heads * head_dim)\n",
    "    - V projection: [1024, 1024] (num_kv_heads * head_dim)\n",
    "    - O projection: [1024, 2048] (projects DOWN to embedding space)\n",
    "    - Q/K normalization: RMSNorm on head_dim (128) - CRITICAL!\n",
    "    \n",
    "    Args:\n",
    "        torch_model: PyTorch Qwen3 model (with .state_dict())\n",
    "        tt_model: TT-Metal Qwen3 model (with .parameters())\n",
    "        verbose: Print detailed comparison (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (matches, mismatches, missing_in_tt)\n",
    "    \"\"\"\n",
    "    \n",
    "    torch_sd = torch_model.state_dict()\n",
    "    tt_params = tt_model.parameters()\n",
    "    \n",
    "    # Get configuration from torch model\n",
    "    num_heads = torch_model.config.num_attention_heads\n",
    "    num_kv_heads = torch_model.config.num_key_value_heads\n",
    "    head_dim = getattr(torch_model.config, 'head_dim', 128)\n",
    "    hidden_size = torch_model.config.hidden_size\n",
    "    \n",
    "    # Detect weight tying configuration from TT model\n",
    "    has_tok_emb = 'qwen3/tok_emb/weight' in tt_params\n",
    "    has_fc = 'qwen3/fc/weight' in tt_params\n",
    "    weight_tying_enabled = not has_tok_emb  # If tok_emb doesn't exist, weight tying is enabled\n",
    "    \n",
    "    # Mapping from PyTorch parameter names to TT-Metal parameter names\n",
    "    pytorch_to_tt_mapping = {\n",
    "        # Final layer norm\n",
    "        'model.norm.weight': 'qwen3/ln_fc/gamma',\n",
    "    }\n",
    "    \n",
    "    # Add embedding mappings based on weight tying configuration\n",
    "    if weight_tying_enabled:\n",
    "        pytorch_to_tt_mapping['model.embed_tokens.weight'] = 'qwen3/fc/weight'\n",
    "        pytorch_to_tt_mapping['lm_head.weight'] = 'qwen3/fc/weight'  # Both tied to fc/weight\n",
    "    else:\n",
    "        pytorch_to_tt_mapping['model.embed_tokens.weight'] = 'qwen3/tok_emb/weight'\n",
    "        pytorch_to_tt_mapping['lm_head.weight'] = 'qwen3/fc/weight'\n",
    "    \n",
    "    # Add layer-specific mappings\n",
    "    for i in range(50):  # Support up to 50 layers\n",
    "        layer_prefix_pt = f'model.layers.{i}'\n",
    "        layer_prefix_tt = f'qwen3/qwen3_block_{i}'\n",
    "        \n",
    "        pytorch_to_tt_mapping.update({\n",
    "            # Layer norms\n",
    "            f'{layer_prefix_pt}.input_layernorm.weight': f'{layer_prefix_tt}/attention_norm/gamma',\n",
    "            f'{layer_prefix_pt}.post_attention_layernorm.weight': f'{layer_prefix_tt}/mlp_norm/gamma',\n",
    "            \n",
    "            # Attention projections - SEPARATE Q, K, V (not combined!)\n",
    "            f'{layer_prefix_pt}.self_attn.q_proj.weight': f'{layer_prefix_tt}/attention/q_linear/weight',\n",
    "            f'{layer_prefix_pt}.self_attn.k_proj.weight': f'{layer_prefix_tt}/attention/k_linear/weight',\n",
    "            f'{layer_prefix_pt}.self_attn.v_proj.weight': f'{layer_prefix_tt}/attention/v_linear/weight',\n",
    "            f'{layer_prefix_pt}.self_attn.o_proj.weight': f'{layer_prefix_tt}/attention/out_linear/weight',\n",
    "            \n",
    "            # Q/K norms - CRITICAL for Qwen3 numerical stability!\n",
    "            f'{layer_prefix_pt}.self_attn.q_norm.weight': f'{layer_prefix_tt}/attention/q_norm/gamma',\n",
    "            f'{layer_prefix_pt}.self_attn.k_norm.weight': f'{layer_prefix_tt}/attention/k_norm/gamma',\n",
    "            \n",
    "            # MLP projections\n",
    "            f'{layer_prefix_pt}.mlp.gate_proj.weight': f'{layer_prefix_tt}/mlp/w1/weight',\n",
    "            f'{layer_prefix_pt}.mlp.up_proj.weight': f'{layer_prefix_tt}/mlp/w3/weight',\n",
    "            f'{layer_prefix_pt}.mlp.down_proj.weight': f'{layer_prefix_tt}/mlp/w2/weight',\n",
    "        })\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"WEIGHT COMPARISON: PyTorch Qwen3 vs TT-Metal Qwen3\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Model Configuration:\")\n",
    "        print(f\"  - num_attention_heads: {num_heads}\")\n",
    "        print(f\"  - num_key_value_heads: {num_kv_heads}\")\n",
    "        print(f\"  - head_dim: {head_dim}\")\n",
    "        print(f\"  - hidden_size: {hidden_size}\")\n",
    "        print(f\"  - attention_output_dim: {num_heads * head_dim}\")\n",
    "        print(f\"  - weight_tying: {'ENABLED' if weight_tying_enabled else 'DISABLED'}\")\n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    mismatches = []\n",
    "    matches = []\n",
    "    missing_in_tt = []\n",
    "    \n",
    "    for pt_name in torch_sd.keys():\n",
    "        if 'bias' in pt_name:\n",
    "            continue  # Skip bias parameters (Qwen3 has no biases)\n",
    "            \n",
    "        pt_tensor = torch_sd[pt_name]\n",
    "        pt_shape = tuple(pt_tensor.shape)\n",
    "        \n",
    "        # Special handling for k_proj: apply RoPE permutation\n",
    "        if '.self_attn.k_proj.weight' in pt_name:\n",
    "            layer_idx = pt_name.split('.')[2]\n",
    "            tt_name = f'qwen3/qwen3_block_{layer_idx}/attention/k_linear/weight'\n",
    "            \n",
    "            if tt_name in tt_params:\n",
    "                pt_numpy = pt_tensor.cpu().float().numpy()\n",
    "                \n",
    "                # Apply RoPE permutation to K projection\n",
    "                pt_numpy_permuted = apply_rope_permutation(pt_numpy, num_kv_heads)\n",
    "                \n",
    "                tt_tensor_np = tt_params[tt_name].to_numpy()\n",
    "                tt_shape = tt_tensor_np.shape\n",
    "                tt_data = tt_tensor_np.reshape(tt_shape[2:]) if len(tt_shape) == 4 else tt_tensor_np\n",
    "                \n",
    "                # Handle transpose if needed\n",
    "                if pt_numpy_permuted.shape != tt_data.shape:\n",
    "                    tt_data = tt_data.T\n",
    "                \n",
    "                diff = np.abs(pt_numpy_permuted - tt_data).max()\n",
    "                rel_diff = diff / (np.abs(pt_numpy_permuted).max() + 1e-8)\n",
    "                \n",
    "                status = \"✓\" if diff < 1e-4 else \"⚠\"\n",
    "                if verbose:\n",
    "                    print(f\"\\n{status} {pt_name} (after RoPE permutation)\")\n",
    "                    print(f\"  PyTorch: {pt_shape}\")\n",
    "                    print(f\"  TT:      {tt_shape}\")\n",
    "                    print(f\"  Max diff: {diff:.6f}, Rel diff: {rel_diff:.6f}\")\n",
    "                \n",
    "                if diff < 1e-3:\n",
    "                    matches.append(pt_name)\n",
    "                else:\n",
    "                    mismatches.append((pt_name, f\"K_DIFF={diff:.6f}\"))\n",
    "            else:\n",
    "                missing_in_tt.append((pt_name, tt_name))\n",
    "            continue\n",
    "        \n",
    "        # Get corresponding TT parameter name\n",
    "        tt_name = pytorch_to_tt_mapping.get(pt_name)\n",
    "        if not tt_name:\n",
    "            if verbose:\n",
    "                print(f\"\\n⊗ NOT MAPPED: {pt_name}\")\n",
    "                print(f\"  PyTorch: {pt_shape}\")\n",
    "                print(f\"  Status: No TT equivalent defined in mapping\")\n",
    "            missing_in_tt.append((pt_name, \"NOT_MAPPED\"))\n",
    "            continue\n",
    "            \n",
    "        if tt_name not in tt_params:\n",
    "            if verbose:\n",
    "                print(f\"\\n❌ MISSING IN TT: {pt_name} -> {tt_name}\")\n",
    "                print(f\"   PyTorch shape: {pt_shape}\")\n",
    "            missing_in_tt.append((pt_name, tt_name))\n",
    "            continue\n",
    "        \n",
    "        # Get TT tensor\n",
    "        tt_tensor_np = tt_params[tt_name].to_numpy()\n",
    "        tt_shape = tt_tensor_np.shape\n",
    "        \n",
    "        # Remove batch dimensions [1, 1, ...] from TT tensor\n",
    "        tt_shape_no_batch = tt_shape[2:] if len(tt_shape) == 4 else tt_shape\n",
    "        \n",
    "        # Compare shapes\n",
    "        pt_numpy = pt_tensor.cpu().float().numpy()\n",
    "        \n",
    "        # Check what type of parameter this is\n",
    "        is_q_proj = '.self_attn.q_proj.weight' in pt_name\n",
    "        is_q_norm = '.self_attn.q_norm.weight' in pt_name\n",
    "        is_k_norm = '.self_attn.k_norm.weight' in pt_name\n",
    "        is_v_proj = '.self_attn.v_proj.weight' in pt_name\n",
    "        \n",
    "        # Apply RoPE permutation for Q projection\n",
    "        if is_q_proj:\n",
    "            if len(pt_shape) == 2 and pt_shape[0] == num_heads * head_dim:\n",
    "                pt_numpy = apply_rope_permutation(pt_numpy, num_heads)\n",
    "        \n",
    "        # For layer norms: PT (N,) vs TT (1, N) or (1, 1, 1, N) - handle squeezing\n",
    "        if len(pt_shape) == 1:\n",
    "            if len(tt_shape_no_batch) == 2 and tt_shape_no_batch[0] == 1:\n",
    "                tt_shape_no_batch = (tt_shape_no_batch[1],)\n",
    "            elif len(tt_shape_no_batch) == 1:\n",
    "                pass  # Already 1D\n",
    "        \n",
    "        # Check if shapes match (with or without transpose)\n",
    "        shape_match = (pt_shape == tt_shape_no_batch) or (pt_shape == tt_shape_no_batch[::-1])\n",
    "        \n",
    "        if shape_match:\n",
    "            # Check actual values\n",
    "            if len(tt_shape) == 4:\n",
    "                tt_data = tt_tensor_np.reshape(tt_shape[2:])\n",
    "            else:\n",
    "                tt_data = tt_tensor_np.reshape(tt_shape)\n",
    "            \n",
    "            tt_data = tt_data.squeeze()\n",
    "            pt_numpy_squeezed = pt_numpy.squeeze()\n",
    "            \n",
    "            # Handle transpose if needed\n",
    "            if pt_numpy_squeezed.shape != tt_data.shape and len(tt_data.shape) == 2:\n",
    "                tt_data = tt_data.T\n",
    "            \n",
    "            diff = np.abs(pt_numpy_squeezed - tt_data).max()\n",
    "            rel_diff = diff / (np.abs(pt_numpy_squeezed).max() + 1e-8)\n",
    "            \n",
    "            status = \"✓\" if diff < 1e-3 else \"⚠\"\n",
    "            note = \"\"\n",
    "            if is_q_proj:\n",
    "                note = \" (after RoPE permutation)\"\n",
    "            elif is_q_norm:\n",
    "                note = \" [CRITICAL Q/K NORM]\"\n",
    "            elif is_k_norm:\n",
    "                note = \" [CRITICAL Q/K NORM]\"\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\n{status} {pt_name}{note}\")\n",
    "                print(f\"  PyTorch: {pt_shape}\")\n",
    "                print(f\"  TT:      {tt_shape} -> {tt_shape_no_batch}\")\n",
    "                print(f\"  Max diff: {diff:.6f}, Rel diff: {rel_diff:.6f}\")\n",
    "            \n",
    "            if diff < 1e-3:\n",
    "                matches.append(pt_name)\n",
    "            else:\n",
    "                mismatches.append((pt_name, f\"VALUE_DIFF={diff:.6f}\"))\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"\\n❌ SHAPE MISMATCH: {pt_name}\")\n",
    "                print(f\"  PyTorch: {pt_shape}\")\n",
    "                print(f\"  TT:      {tt_shape} -> {tt_shape_no_batch}\")\n",
    "            mismatches.append((pt_name, f\"SHAPE: PT={pt_shape} vs TT={tt_shape_no_batch}\"))\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"SUMMARY: {len(matches)} matches, {len(mismatches)} mismatches, {len(missing_in_tt)} missing in TT\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        if missing_in_tt:\n",
    "            print(f\"\\n❌ MISSING IN TT ({len(missing_in_tt)}):\")\n",
    "            for pt_name, tt_name in missing_in_tt:\n",
    "                print(f\"  - {pt_name} -> {tt_name}\")\n",
    "        \n",
    "        if mismatches:\n",
    "            print(\"\\n❌ MISMATCHES:\")\n",
    "            for name, issue in mismatches:\n",
    "                print(f\"  - {name}: {issue}\")\n",
    "        \n",
    "        if len(mismatches) == 0 and len(missing_in_tt) == 0:\n",
    "            print(\"\\n🎉 ALL WEIGHTS MATCH PERFECTLY!\")\n",
    "            print(f\"✅ {len(matches)} parameters validated\")\n",
    "            print(\"✅ Q/K normalization layers loaded correctly\")\n",
    "            print(\"✅ Qwen3 model is production-ready!\")\n",
    "    \n",
    "    return matches, mismatches, missing_in_tt\n",
    "\n",
    "\n",
    "matches, mismatches, missing = compare_qwen3_weights(torch_model, tt_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525920ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(torch_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c006c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1274ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d4984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade8881f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ccd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "import sys\n",
    "\n",
    "def convert_notebook_to_py(notebook_path, output_path):\n",
    "    \n",
    "    with open(notebook_path) as ff:\n",
    "        nb_in = nbformat.read(ff, nbformat.NO_CONVERT)\n",
    "    \n",
    "    source = \"\"\n",
    "    for cell in nb_in['cells']:\n",
    "        if cell['cell_type'] == 'code':\n",
    "            # cell['source'] can be a string or a list of strings\n",
    "            cell_source = cell['source']\n",
    "            \n",
    "            # Convert to list of lines if it's a string\n",
    "            if isinstance(cell_source, str):\n",
    "                lines = cell_source.split('\\n')\n",
    "            else:\n",
    "                # It's already a list, but may contain newlines within elements\n",
    "                lines = []\n",
    "                for item in cell_source:\n",
    "                    lines.extend(item.split('\\n'))\n",
    "            \n",
    "            # Filter out lines starting with '%' (magic commands)\n",
    "            filtered_lines = [line for line in lines if not line.strip().startswith('%')]\n",
    "            \n",
    "            # Join the lines back together\n",
    "            cell_code = '\\n'.join(filtered_lines)\n",
    "            \n",
    "            # Add to source with a newline separator\n",
    "            if cell_code.strip():  # Only add non-empty cells\n",
    "                source = source + '\\n' + cell_code\n",
    "    \n",
    "    # Write to output file\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(source)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0893d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_notebook_to_py('llm_inference_new.ipynb', 'llm_inference_new.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e26ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f868e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71842a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
