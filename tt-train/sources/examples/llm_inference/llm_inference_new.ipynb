{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f838d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TT_METAL_HOME=/home/ubuntu/tt-metal\n",
      "env: TT_METAL_RUNTIME_ROOT=/home/ubuntu/tt-metal\n"
     ]
    }
   ],
   "source": [
    "%env TT_METAL_HOME=/home/ubuntu/tt-metal\n",
    "%env TT_METAL_RUNTIME_ROOT=/home/ubuntu/tt-metal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb31718",
   "metadata": {},
   "source": [
    "%env TT_LOGGER_LEVEL=debug\n",
    "%env TT_LOGGER_TYPES=Op\n",
    "%env TTNN_ENABLE_LOGGING=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f7bc745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-14 20:54:35.903 | DEBUG    | ttnn:<module>:77 - Initial ttnn.CONFIG:\n",
      "Config{cache_path=/home/ubuntu/.cache/ttnn,model_cache_path=/home/ubuntu/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=true,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_should_raise_exception=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n"
     ]
    }
   ],
   "source": [
    "import ttml\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6545c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TOKENS = 100\n",
    "WITH_SAMPLING = True\n",
    "TEMPERATURE = 0.0\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd831b0",
   "metadata": {},
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\" \n",
    "CONFIG = \"training_shakespeare_llama3_2_1B_fixed.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e1948c",
   "metadata": {},
   "source": [
    "model_id =  \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "CONFIG = \"training_shakespeare_tinyllama.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def0e6e",
   "metadata": {},
   "source": [
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "CONFIG = \"training_shakespeare_llama3_2_3B.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a19149b",
   "metadata": {},
   "source": [
    "model_id =  \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "CONFIG = \"training_shakespeare_llama3_8B_tp.yaml\" # OOM on 12 GB, sucesfully loaded weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45048e53",
   "metadata": {},
   "source": [
    "model_id = \"Qwen/Qwen3-0.6B\" \n",
    "CONFIG = \"training_shakespeare_qwen3_0_6B.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abaa683",
   "metadata": {},
   "source": [
    "model_id = \"Qwen/Qwen3-1.7B\" \n",
    "CONFIG = \"training_shakespeare_qwen3_1_7B.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d045055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen3-4B\"\n",
    "CONFIG = \"training_shakespeare_qwen3_4B.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df6d6bf9-a010-4206-b004-807ee500d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "import numpy as np  # For numpy arrays\n",
    "from dataclasses import dataclass # For configuration classes\n",
    "from huggingface_hub import hf_hub_download # To download safetensors from Hugging Face\n",
    "from transformers import AutoTokenizer\n",
    "from yaml import safe_load # To read YAML configs\n",
    "from pathlib import Path\n",
    "\n",
    "import ttml\n",
    "from ttml.common.config import get_training_config, load_config, TransformerConfig\n",
    "from ttml.common.utils import set_seed, round_up_to_tile\n",
    "from ttml.common.model_factory import TransformerModelFactory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2372b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6991716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ttml.common.config.TrainingConfig at 0x7f9d4172a8f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_training_config(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55071c7a-74fe-4f56-8f33-e45ffe699c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from Hugging Face and the transformer config from YAML\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "training_config = get_training_config(CONFIG)\n",
    "model_yaml = load_config(training_config.model_config, configs_root=os.getcwd() + '/../../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e7883f6-f8e6-436b-8908-62c8a26e1e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "safetensors_path = hf_hub_download(repo_id=model_id, filename=\"config.json\")\n",
    "safetensors_path = safetensors_path.replace(\"config.json\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f77e69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a70f692ab3d47cbb8e7d87780f3cf05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "torch.manual_seed(SEED)\n",
    "torch_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f405407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151643, 151936, 151936)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size, torch_model.state_dict()['model.embed_tokens.weight'].shape[0], torch_model.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50d8ce12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torch_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf74d9ac-7afd-4c6f-887e-b793c6e44c18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151936\n"
     ]
    }
   ],
   "source": [
    "orig_vocab_size = torch_model.vocab_size\n",
    "print(orig_vocab_size)\n",
    "tt_model_factory = TransformerModelFactory(model_yaml)\n",
    "tt_model_factory.transformer_config.vocab_size = orig_vocab_size\n",
    "\n",
    "max_sequence_length = tt_model_factory.transformer_config.max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56dff343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ttml.autograd.AutoContext.get_instance().set_init_mode(ttml.autograd.InitMode.DISABLED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ad0b618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transformer_config': {'model_type': 'qwen3',\n",
       "  'num_heads': 32,\n",
       "  'num_groups': 8,\n",
       "  'embedding_dim': 2560,\n",
       "  'head_dim': 128,\n",
       "  'intermediate_dim': 9728,\n",
       "  'dropout_prob': 0.0,\n",
       "  'num_blocks': 36,\n",
       "  'weight_tying': 'enabled',\n",
       "  'vocab_size': 151936,\n",
       "  'max_sequence_length': 2048,\n",
       "  'runner_type': 'memory_efficient',\n",
       "  'theta': 1000000.0,\n",
       "  'rms_norm_eps': 1e-06}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "460b4c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttml.autograd.AutoContext.get_instance().set_init_mode(ttml.autograd.InitMode.DISABLED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff6ac0b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3 configuration:\n",
      "    Vocab size: 151936\n",
      "    Max sequence length: 2048\n",
      "    Embedding dim (hidden_size): 2560\n",
      "    Head dim: 128\n",
      "    Attention output dim: 4096\n",
      "    Intermediate dim: 9728\n",
      "    Num heads: 32\n",
      "    Num groups (KV heads): 8\n",
      "    Dropout probability: 0\n",
      "    Num blocks: 36\n",
      "    Positional embedding type: RoPE\n",
      "    Runner type: Memory efficient\n",
      "    Weight tying: Enabled\n",
      "    Theta: 1000000\n",
      "    RMSNorm epsilon: 1e-06\n",
      "2026-01-14 20:54:51.332 | info     |             UMD | Established firmware bundle version: 19.4.0 (topology_discovery.cpp:369)\n",
      "2026-01-14 20:54:51.332 | info     |             UMD | Firmware bundle version 19.4.0 on the system is newer than the latest fully tested version 19.1.0 for wormhole_b0 architecture. Newest features may not be supported. (topology_discovery.cpp:395)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (9, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (9, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (1, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (1, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (8, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (8, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (2, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (2, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (7, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (7, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (3, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (3, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (6, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (6, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (4, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (4, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (9, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (9, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (1, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (1, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (8, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (8, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (2, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (2, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (7, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (7, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (3, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (3, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (6, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (6, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (4, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.332 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (4, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.333 | info     |          Device | Opening user mode device driver (tt_cluster.cpp:223)\n",
      "2026-01-14 20:54:51.337 | info     |             UMD | Established firmware bundle version: 19.4.0 (topology_discovery.cpp:369)\n",
      "2026-01-14 20:54:51.337 | info     |             UMD | Firmware bundle version 19.4.0 on the system is newer than the latest fully tested version 19.1.0 for wormhole_b0 architecture. Newest features may not be supported. (topology_discovery.cpp:395)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (9, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (9, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (1, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (1, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (8, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (8, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (2, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (2, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (7, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (7, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (3, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (3, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (6, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (6, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (4, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (4, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (9, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (9, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (1, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (1, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (8, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (8, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (2, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (2, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (7, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (7, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (3, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (3, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (6, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (6, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (4, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.337 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (4, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.357 | info     |             UMD | Established firmware bundle version: 19.4.0 (topology_discovery.cpp:369)\n",
      "2026-01-14 20:54:51.357 | info     |             UMD | Firmware bundle version 19.4.0 on the system is newer than the latest fully tested version 19.1.0 for wormhole_b0 architecture. Newest features may not be supported. (topology_discovery.cpp:395)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (9, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (9, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (1, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (1, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (8, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (8, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (2, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (2, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (7, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (7, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (3, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (3, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (6, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (6, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (4, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (4, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (9, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (9, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (1, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (1, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (8, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (8, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (2, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (2, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (7, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (7, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (3, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (3, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (6, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (6, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (4, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-14 20:54:51.357 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (4, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-14 20:54:51.358 | info     |             UMD | Harvesting masks for chip 0 tensix: 0x1 dram: 0x0 eth: 0x0 pcie: 0x0 l2cpu: 0x0 (cluster.cpp:360)\n",
      "2026-01-14 20:54:51.394 | warning  |             UMD | init_detect_tt_device_numanodes(): Could not determine NumaNodeSet for TT device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:564)\n",
      "2026-01-14 20:54:51.394 | warning  |             UMD | Could not find NumaNodeSet for TT Device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:173)\n",
      "2026-01-14 20:54:51.394 | warning  |             UMD | bind_area_memory_nodeset(): Unable to determine TT Device to NumaNode mapping for physical_device_id: 0. Skipping membind. (cpuset_lib.cpp:450)\n",
      "2026-01-14 20:54:51.394 | warning  |             UMD | ---- ttSiliconDevice::init_hugepage: bind_area_to_memory_nodeset() failed (physical_device_id: 0 ch: 0). Hugepage allocation is not on NumaNode matching TT Device. Side-Effect is decreased Device->Host perf (Issue #893). (sysmem_manager.cpp:217)\n",
      "2026-01-14 20:54:51.394 | info     |             UMD | Opening local chip ids/PCIe ids: {0}/[0] and remote chip ids {} (cluster.cpp:204)\n",
      "2026-01-14 20:54:51.394 | info     |             UMD | IOMMU: disabled (cluster.cpp:180)\n",
      "2026-01-14 20:54:51.394 | info     |             UMD | KMD version: 2.6.1 (cluster.cpp:183)\n",
      "2026-01-14 20:54:51.397 | info     |             UMD | Mapped hugepage 0x200000000 to NOC address 0x800000000 (sysmem_manager.cpp:245)\n",
      "2026-01-14 20:54:51.420 | info     |     Distributed | Using auto discovery to generate mesh graph. (metal_context.cpp:827)\n",
      "2026-01-14 20:54:51.420 | info     |     Distributed | Constructing control plane using auto-discovery (no mesh graph descriptor). (metal_context.cpp:804)\n",
      "2026-01-14 20:54:51.420 | info     |          Fabric | TopologyMapper mapping start (mesh=0): n_log=1, n_phys=1, log_deg_hist={0:1}, phys_deg_hist={0:1} (topology_mapper_utils.cpp:171)\n",
      "2026-01-14 20:54:51.420 | info     |          Fabric | TopologyMapper mapping start (mesh=0): n_log=1, n_phys=1, log_deg_hist={0:1}, phys_deg_hist={0:1} (topology_mapper_utils.cpp:171)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created: 3.184882879257202\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "tt_model = tt_model_factory.create_model()\n",
    "print(f\"Model created: {time() - start_time}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfb5a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b38196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llama 3b\n",
    "#Model created: 238.99020171165466\n",
    "#Model loaded: 47.06614542007446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68b490f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llama 3b\n",
    "#Model created: 7.940993309020996\n",
    "#Model loaded: 58.52051877975464"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb62dae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 66.52387261390686\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "tt_model.load_from_safetensors(safetensors_path)\n",
    "print(f\"Model loaded: {time() - start_time}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bdc85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_vocab_size = round_up_to_tile(orig_vocab_size, 32)\n",
    "if orig_vocab_size != padded_vocab_size:\n",
    "    print(f\"Padding vocab size for tilization: original {orig_vocab_size} -> padded {padded_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb047274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3cf900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4d4f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_causal_mask(T: int) -> ttml.autograd.Tensor:\n",
    "    # [1,1,T,T] float32 with 1s for allowed positions (i >= j), else 0\\n\",\n",
    "    m = np.tril(np.ones((T, T), dtype=np.float32))\n",
    "    return ttml.autograd.Tensor.from_numpy(m.reshape(1, 1, T, T), ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)\n",
    "\n",
    "def build_logits_mask(vocab_size: int, padded_vocab_size: int) -> ttml.autograd.Tensor:\n",
    "    logits_mask = np.zeros((1, 1, 1, padded_vocab_size), dtype=np.float32)\n",
    "    logits_mask[:, :, :, vocab_size:] = 1e4\n",
    "    return ttml.autograd.Tensor.from_numpy(logits_mask, ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)   # [1,1,1,T], bfloat16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77f8e7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_SIZE = 32\n",
    "\n",
    "def round_up(value: int) -> int:\n",
    "    return ((value + TILE_SIZE - 1) // TILE_SIZE) * TILE_SIZE\n",
    "\n",
    "def create_causal_mask_kv(query_len: int, prompt_len: int = 0) -> ttml.autograd.Tensor:\n",
    "    whole_len = prompt_len + query_len\n",
    "    padded_q = round_up(query_len)\n",
    "    padded_w = round_up(whole_len)\n",
    "    mask = np.zeros((padded_q, padded_w), dtype=np.float32)\n",
    "    for i in range(query_len):\n",
    "        for j in range(prompt_len + i + 1):\n",
    "            mask[i, j] = 1.0\n",
    "    return ttml.autograd.Tensor.from_numpy(mask.reshape(1, 1, padded_q, padded_w), ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)\n",
    "\n",
    "def tokens_to_tensor_kv(tokens: list) -> ttml.autograd.Tensor:\n",
    "    padded_len = round_up(len(tokens))\n",
    "    padded = np.zeros(padded_len, dtype=np.uint32)\n",
    "    padded[:len(tokens)] = tokens\n",
    "    return ttml.autograd.Tensor.from_numpy(padded.reshape(1, 1, 1, padded_len), ttml.Layout.ROW_MAJOR, ttml.autograd.DataType.UINT32)\n",
    "\n",
    "def generate_with_tt_kv_cache(model, prompt_tokens, transformer_config):\n",
    "    import time\n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(ttml.autograd.GradMode.DISABLED)\n",
    "    model.eval()\n",
    "\n",
    "    logits_mask_tensor = build_logits_mask(orig_vocab_size, padded_vocab_size) if padded_vocab_size != orig_vocab_size else None\n",
    "\n",
    "    head_dim = getattr(transformer_config, 'head_dim', None) or (transformer_config.embedding_dim // transformer_config.num_heads)\n",
    "    kv_cache = ttml.models.KvCache(\n",
    "        transformer_config.num_blocks, 1, transformer_config.num_groups,\n",
    "        transformer_config.max_sequence_length, head_dim\n",
    "    )\n",
    "    kv_cache.reset()\n",
    "\n",
    "    generated = prompt_tokens.copy()\n",
    "    print(\"************************************\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for step in tqdm(range(min(OUTPUT_TOKENS, transformer_config.max_sequence_length - len(prompt_tokens)))):\n",
    "        if kv_cache.get_cache_position() == 0:\n",
    "            input_tokens = generated\n",
    "            processed = 0\n",
    "        else:\n",
    "            input_tokens = [generated[-1]]\n",
    "            processed = len(generated) - 1\n",
    "\n",
    "        token_tensor = tokens_to_tensor_kv(input_tokens)\n",
    "        mask = create_causal_mask_kv(len(input_tokens), processed)\n",
    "        logits = model(token_tensor, mask, kv_cache=kv_cache, new_tokens=len(input_tokens))\n",
    "\n",
    "        next_token_tensor = ttml.ops.sample.sample_op(logits, TEMPERATURE, np.random.randint(low=1e7), logits_mask_tensor)\n",
    "        next_token = int(next_token_tensor.to_numpy().flatten()[len(input_tokens) - 1])\n",
    "        generated.append(next_token)\n",
    "        print(tokenizer.decode([next_token]), end='', flush=True)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    tokens_per_second = OUTPUT_TOKENS / elapsed_time\n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {OUTPUT_TOKENS} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40913e88",
   "metadata": {},
   "source": [
    "`generate_with_tt()` uses TT hardware acceleration to generate output from the chosen LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18e6550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_tt(model, prompt_tokens):\n",
    "    import time\n",
    "    \n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(ttml.autograd.GradMode.DISABLED)\n",
    "    model.eval()\n",
    "\n",
    "    logits_mask_tensor = None\n",
    "\n",
    "    if padded_vocab_size != orig_vocab_size:\n",
    "        logits_mask_tensor = build_logits_mask(orig_vocab_size, padded_vocab_size)\n",
    "\n",
    "    \n",
    "    padded_prompt_tokens = np.zeros((1, 1, 1, max_sequence_length), \n",
    "                                    dtype=np.uint32)\n",
    "\n",
    "    start_idx = 0\n",
    "\n",
    "    print(\"************************************\")\n",
    "    start_time = time.time()\n",
    "    causal_mask = build_causal_mask(max_sequence_length)  # [1,1,seq_len,seq_len], float32\n",
    "    \n",
    "    for token_idx in tqdm(range(OUTPUT_TOKENS)):\n",
    "\n",
    "        if len(prompt_tokens) > max_sequence_length:\n",
    "            start_idx = len(prompt_tokens) - max_sequence_length\n",
    "\n",
    "        padded_prompt_tokens[0, 0, 0, :len(prompt_tokens)] = prompt_tokens[start_idx:]\n",
    "        padded_prompt_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "            padded_prompt_tokens,\n",
    "            ttml.Layout.ROW_MAJOR,\n",
    "            ttml.autograd.DataType.UINT32)  # [1,1,1, max_seq_len], uint32\n",
    "        \n",
    "        #causal_mask = build_causal_mask(len(prompt_tokens))  # [1,1,seq_len,seq_len], float32\n",
    "        \n",
    "        logits = model(padded_prompt_tensor, causal_mask)  # out=[1,1,seq_len, vocab_size], bf16\n",
    "\n",
    "\n",
    "        next_token_tensor = ttml.ops.sample.sample_op(logits, TEMPERATURE, np.random.randint(low=1e7), logits_mask_tensor)  # out=[1,1,seq_len,1], uint32\n",
    "\n",
    "        next_token_idx = max_sequence_length - 1 if len(prompt_tokens) > max_sequence_length else len(prompt_tokens) - 1\n",
    "        next_token = next_token_tensor.to_numpy().flatten()[next_token_idx]\n",
    "\n",
    "        output = tokenizer.decode(next_token)\n",
    "\n",
    "        prompt_tokens.append(next_token)\n",
    "        print(output, end='', flush=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = OUTPUT_TOKENS / elapsed_time\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {OUTPUT_TOKENS} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7824c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_pytorch(torch_model, prompt_tokens):\n",
    "    import time\n",
    "    import torch.nn.functional as F\n",
    "    from transformers import DynamicCache\n",
    "    \n",
    "    torch_model.eval()\n",
    "    \n",
    "    print(\"************************************\")\n",
    "    # Convert list to tensor and add batch dimension\n",
    "    if isinstance(prompt_tokens, list):\n",
    "        prompt_tokens = torch.tensor([prompt_tokens])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize KV cache using the new DynamicCache API\n",
    "    past_key_values = DynamicCache()\n",
    "    input_ids = prompt_tokens\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(OUTPUT_TOKENS)):\n",
    "            # Get model outputs with KV cache\n",
    "            outputs = torch_model(\n",
    "                input_ids=input_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            past_key_values = outputs.past_key_values\n",
    "            \n",
    "            # Get logits for the last token\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            \n",
    "            # Apply temperature and sample\n",
    "            if WITH_SAMPLING and TEMPERATURE > 0:\n",
    "                next_token_logits = next_token_logits / TEMPERATURE\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                # Greedy sampling\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            \n",
    "            # Decode and print the token\n",
    "            output = tokenizer.decode(next_token[0])\n",
    "            print(output, end='', flush=True)\n",
    "            \n",
    "            # For next iteration, only pass the new token (KV cache handles the rest)\n",
    "            input_ids = next_token\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = OUTPUT_TOKENS / elapsed_time\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {OUTPUT_TOKENS} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4813faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with torch:\n",
      "************************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7fcad57e234935bf9155e4b7653fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A Comparison of Training Speeds\n",
      "\n",
      "In this article, we will compare the training speeds of two different deep learning frameworks: PyTorch and ttML. We will focus on the training speed of a simple neural network on a dataset. We will also compare the performance of the two frameworks in terms of training time, memory usage, and computational efficiency.\n",
      "\n",
      "First, we will define the problem. We will use a simple neural network to classify images from the MNIST dataset. The MNIST dataset consists of\n",
      "************************************\n",
      "Generated 100 tokens in 40.45 seconds\n",
      "Performance: 2.47 tokens/second\n",
      "************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_str = \"Generating with pyTorch vs ttML:\"\n",
    "\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with torch:\")\n",
    "generate_with_pytorch(torch_model, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59d89257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with TTML (KV Cache):\n",
      "************************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7cbcaa476740bdbfc28ffb13a323f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A Comparison of Training Speeds\n",
      "\n",
      "In this article, we will compare the training speeds of two different deep learning frameworks: PyTorch and ttML. We will focus on the training speed of a simple neural network on a dataset. We will also compare the training speed of the same model in both frameworks and see which one is faster.\n",
      "\n",
      "First, we need to understand what PyTorch and ttML are. PyTorch is a deep learning framework that is known for its flexibility and ease of\n",
      "************************************\n",
      "Generated 100 tokens in 12.47 seconds\n",
      "Performance: 8.02 tokens/second\n",
      "************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_str = \"Generating with pyTorch vs ttML:\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with TTML (KV Cache):\")\n",
    "generate_with_tt_kv_cache(tt_model, prompt_tokens, tt_model_factory.transformer_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a42273b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with TT (NO KV Cache, just max_tokens every time):\n",
      "************************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853d0ef33b7a4a31a528a17ec2500188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A Comparative Analysis\n",
      "\n",
      "In the realm of machine learning, two prominent frameworks, PyTorch and ttML, have emerged as powerful tools for model development. This analysis delves into the"
     ]
    }
   ],
   "source": [
    "prompt_str = \"Generating with pyTorch vs ttML:\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with TT (No KV Cache, just max_tokens every time):\")\n",
    "generate_with_tt(tt_model, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d334cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
