{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8744d20",
   "metadata": {},
   "source": [
    "### LLM Inference Example\n",
    "\n",
    "This notebook contains a basic inference example for using our `ttml` Python API to build, load, and run a large language model from Hugging Face on our TT hardware. By default, it is set to create and load a GPT2 model, but this notebook can quickly and easily be edited to use any of the LLMs that the tt-train project currently supports. \n",
    "\n",
    "Below, in the first cell, we have our imports and basic directory housekeeping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e005a45c",
   "metadata": {},
   "source": [
    "Use the cell below to change global parameters in this notebook. \n",
    "\n",
    "`OUTPUT_TOKENS` : the length of the generated text in token (not characters!) \n",
    "\n",
    "`WITH_SAMPLING` : enable or disable output token sampling (only used for PyTorch)\n",
    "\n",
    "`TEMPERATURE`   : sampling temperature; set to 0 to disable sampling in `generate_with_tt()`\n",
    "\n",
    "`SEED`          : randomization seed (for reproducibility)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a314fa7",
   "metadata": {},
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\" \n",
    "CONFIG = \"training_shakespeare_llama3_2_1B_fixed.yaml\" # now working fine (with proper shape for tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e1948c",
   "metadata": {},
   "source": [
    "model_id =  \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "CONFIG = \"training_shakespeare_tinyllama.yaml\" # working fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5bed3fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id =  \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "CONFIG = \"training_shakespeare_llama3_8B_tp.yaml\" # OOM on 12 GB, sucesfully loaded weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6545c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TOKENS = 100\n",
    "WITH_SAMPLING = True\n",
    "TEMPERATURE = 0.8\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a090295",
   "metadata": {},
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df6d6bf9-a010-4206-b004-807ee500d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "import numpy as np  # For numpy arrays\n",
    "from dataclasses import dataclass # For configuration classes\n",
    "from huggingface_hub import hf_hub_download # To download safetensors from Hugging Face\n",
    "from transformers import AutoTokenizer\n",
    "from yaml import safe_load # To read YAML configs\n",
    "from pathlib import Path\n",
    "\n",
    "import ttml\n",
    "from ttml.common.config import get_config, TransformerConfig\n",
    "from ttml.common.utils import set_seed, round_up_to_tile\n",
    "from ttml.common.model_factory import TransformerModelFactory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55071c7a-74fe-4f56-8f33-e45ffe699c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1950a4e07c7749acbd3a352c70f83ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d762f7aeb9474165bb9ebdab5780a13e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4a9c083e8e45edbcec2020d31039e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer from Hugging Face and the transformer config from YAML\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "transformer_config = TransformerConfig(get_config(CONFIG).get(\"training_config\", {}).get(\"transformer_config\",{}))\n",
    "yaml_config = get_config(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af21d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e7883f6-f8e6-436b-8908-62c8a26e1e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "safetensors_path = hf_hub_download(repo_id=model_id, filename=\"config.json\")\n",
    "safetensors_path = safetensors_path.replace(\"config.json\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f77e69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676b79ede0444a0ca424e89bcd41a60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4130d45b7f84462a94902998daf8b5f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817a42076c91474b9a3dc61ca159568a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1bc65a407c41959aba7d711ddad408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101660c9751e4b09a63e12665ee20bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076ba7199aa74fb6bec48bfc1bcb6cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ae798ea24c4d52944651aeaa8b7200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f50a18ff214551872e7fe9c6bcb96f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fe98a260d64c4da289473a36e69f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "torch.manual_seed(SEED)\n",
    "torch_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f405407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128000, 128256, 128256)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size, torch_model.state_dict()['model.embed_tokens.weight'].shape[0], torch_model.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50d8ce12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torch_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18d627a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_size = tokenizer.vocab_size + len(tokenizer.added_tokens_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf74d9ac-7afd-4c6f-887e-b793c6e44c18",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128256\n",
      "t with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.8.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.8.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 8\n",
      "Loading tensor: model.layers.9.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.9.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.9.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.9.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.9.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.9.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.9.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.9.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.9.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_9/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 9\n",
      "Loading tensor: model.norm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/ln_fc/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "All 130 parameters were successfully loaded and used.\n",
      "Llama configuration:\n",
      "    Vocab size: 128256\n",
      "    Max sequence length: 8192\n",
      "    Embedding dim: 4096\n",
      "    Intermediate dim: 14336\n",
      "    Num heads: 32\n",
      "    Num groups: 8\n",
      "    Dropout probability: 0\n",
      "    Num blocks: 32\n",
      "    Positional embedding type: RoPE\n",
      "    Runner type: Memory efficient\n",
      "    Weight tying: Disabled\n",
      "    Theta: 500000\n",
      "    RoPE scaling enabled:\n",
      "        Scaling factor: 8\n",
      "        Original context length: 8192\n",
      "        High freq factor: 4\n",
      "        Low freq factor: 1\n",
      "2025-11-06 23:41:54.386 | critical |          Always | Out of Memory: Not enough space to allocate 117440512 B DRAM buffer across 12 banks, where each bank needs to store 9787392 B, but bank size is only 1071181792 B (assert.hpp:103)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "TT_FATAL @ /home/ubuntu/tt-metal/tt_metal/impl/allocator/bank_manager.cpp:431: address.has_value()\ninfo:\nOut of Memory: Not enough space to allocate 117440512 B DRAM buffer across 12 banks, where each bank needs to store 9787392 B, but bank size is only 1071181792 B\nbacktrace:\n --- /home/ubuntu/tt-metal/build/lib/libtt_metal.so(+0x524cdd) [0x7f754d565cdd]\n --- tt::tt_metal::BankManager::allocate_buffer(unsigned long, unsigned long, bool, CoreRangeSet const&, std::optional<unsigned int>, ttsl::StrongType<unsigned int, tt::tt_metal::AllocatorIDTag>)\n --- tt::tt_metal::Allocator::allocate_buffer(tt::tt_metal::Buffer*)\n --- tt::tt_metal::Buffer::allocate_impl()\n --- tt::tt_metal::Buffer::create(tt::tt_metal::IDevice*, unsigned long, unsigned long, tt::tt_metal::BufferType, tt::tt_metal::BufferShardingArgs const&, std::optional<bool>, std::optional<ttsl::StrongType<unsigned char, tt::tt_metal::SubDeviceIdTag> >)\n --- tt::tt_metal::distributed::MeshBuffer::create(std::variant<tt::tt_metal::distributed::ReplicatedBufferConfig, tt::tt_metal::distributed::ShardedBufferConfig> const&, tt::tt_metal::distributed::DeviceLocalBufferConfig const&, tt::tt_metal::distributed::MeshDevice*, std::optional<unsigned long>)\n --- tt::tt_metal::tensor_impl::allocate_device_buffer(tt::tt_metal::distributed::MeshDevice*, tt::tt_metal::TensorSpec const&)\n --- tt::tt_metal::allocate_tensor_on_device(tt::tt_metal::TensorSpec const&, tt::tt_metal::distributed::MeshDevice*)\n --- tt::tt_metal::create_device_tensor(tt::tt_metal::TensorSpec const&, tt::tt_metal::IDevice*)\n --- tt::tt_metal::operation::program_output_helper<ttnn::operations::data_movement::TilizeWithValPadding, has_create_program<ttnn::operations::data_movement::TilizeWithValPadding>::value>::type tt::tt_metal::operation::default_create_output_tensors<ttnn::operations::data_movement::TilizeWithValPadding>(ttnn::operations::data_movement::TilizeWithValPadding const&, std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > const&, std::vector<std::optional<tt::tt_metal::Tensor>, std::allocator<std::optional<tt::tt_metal::Tensor> > > const&)\n --- /home/ubuntu/tt-metal/build/lib/_ttnncpp.so(+0x9db6ce) [0x7f753b5f66ce]\n --- tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >::create_output_tensors(tt::tt_metal::operation::DeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > > const&, tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >::tensor_args_t const&)\n --- tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >::tensor_return_value_t ttnn::device_operation::detail::launch_on_device<tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > > >(tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >::operation_attributes_t const&, tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >::tensor_args_t const&)\n --- tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >::tensor_return_value_t ttnn::device_operation::detail::invoke<tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > > >(tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >::operation_attributes_t const&, tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >::tensor_args_t const&)\n --- /home/ubuntu/tt-metal/build/lib/_ttnncpp.so(+0x6e2c5f) [0x7f753b2fdc5f]\n --- std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > tt::tt_metal::operation::run<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >(tt::tt_metal::operation::DeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >&&, std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > const&, std::vector<std::optional<tt::tt_metal::Tensor const>, std::allocator<std::optional<tt::tt_metal::Tensor const> > > const&, std::vector<std::optional<tt::tt_metal::Tensor>, std::allocator<std::optional<tt::tt_metal::Tensor> > > const&)\n --- /home/ubuntu/tt-metal/build/lib/_ttnncpp.so(+0x9c2736) [0x7f753b5dd736]\n --- /home/ubuntu/tt-metal/build/lib/_ttnncpp.so(+0x9776bb) [0x7f753b5926bb]\n --- ttnn::operations::data_movement::ExecuteTilizeWithValPadding::invoke(tt::tt_metal::Tensor const&, tt::tt_metal::Shape const&, std::variant<unsigned int, float>, std::optional<tt::tt_metal::MemoryConfig> const&, std::optional<tt::tt_metal::DataType>, bool)\n --- ttnn::operations::data_movement::ExecuteTilizeWithZeroPadding::invoke(tt::tt_metal::Tensor const&, std::optional<tt::tt_metal::MemoryConfig> const&, std::optional<tt::tt_metal::DataType>, bool)\n --- /home/ubuntu/tt-metal/python_env/lib/python3.10/site-packages/ttml/_ttml.cpython-310-x86_64-linux-gnu.so(_ZNK4ttnn10decorators22registered_operation_tIXtlN7reflect6v1_2_512fixed_stringIcLm30EEEtlA31_cLc116ELc116ELc110ELc110ELc58ELc58ELc116ELc105ELc108ELc105ELc122ELc101ELc95ELc119ELc105ELc116ELc104ELc95ELc122ELc101ELc114ELc111ELc95ELc112ELc97ELc100ELc100ELc105ELc110ELc103EEEENS_10operations13data_movement28ExecuteTilizeWithZeroPaddingEE16invoke_compositeIJRN2tt8tt_metal6TensorERNSD_12MemoryConfigERKSt9nullopt_tbEEEDaDpOT_+0x41) [0x7f75401ccc61]\n --- /home/ubuntu/tt-metal/python_env/lib/python3.10/site-packages/ttml/_ttml.cpython-310-x86_64-linux-gnu.so(_ZNK4ttnn10decorators22registered_operation_tIXtlN7reflect6v1_2_512fixed_stringIcLm30EEEtlA31_cLc116ELc116ELc110ELc110ELc58ELc58ELc116ELc105ELc108ELc105ELc122ELc101ELc95ELc119ELc105ELc116ELc104ELc95ELc122ELc101ELc114ELc111ELc95ELc112ELc97ELc100ELc100ELc105ELc110ELc103EEEENS_10operations13data_movement28ExecuteTilizeWithZeroPaddingEE13traced_invokeIJRN2tt8tt_metal6TensorERNSD_12MemoryConfigERKSt9nullopt_tbEEEDaDpOT_+0x77) [0x7f75401cc8f7]\n --- tt::tt_metal::Tensor ttml::core::from_vector<float, (tt::tt_metal::DataType)0>(std::vector<float, std::allocator<float> > const&, tt::tt_metal::Shape const&, tt::tt_metal::distributed::MeshDevice*, tt::tt_metal::Layout, ttnn::distributed::TensorToMesh const*)\n --- ttml::init::uniform_init(std::shared_ptr<ttml::autograd::Tensor>&, tt::tt_metal::Shape const&, ttml::init::UniformRange)\n --- ttml::modules::LinearLayer::LinearLayer(unsigned int, unsigned int, bool)\n --- ttml::modules::LlamaMLP::LlamaMLP(unsigned int, std::optional<unsigned int>, float)\n --- ttml::modules::LlamaBlock::LlamaBlock(unsigned int, unsigned int, unsigned int, ttml::ops::RotaryEmbeddingParams const&, float, std::optional<unsigned int>)\n --- ttml::models::llama::Llama::Llama(ttml::models::llama::LlamaConfig const&)\n --- ttml::models::llama::create(ttml::models::llama::LlamaConfig const&)\n --- /home/ubuntu/tt-metal/python_env/lib/python3.10/site-packages/ttml/_ttml.cpython-310-x86_64-linux-gnu.so(+0x1003e3) [0x7f754016c3e3]\n --- /home/ubuntu/tt-metal/python_env/lib/python3.10/site-packages/ttml/_ttml.cpython-310-x86_64-linux-gnu.so(+0x13ed13) [0x7f75401aad13]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x5603) [0x556094d39763]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x556094d4a1bc]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x556094d34967]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x556094d4a1bc]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x556094d34967]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x25a566) [0x556094e19566]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(PyEval_EvalCode+0x86) [0x556094e19436]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x25fb4d) [0x556094e1eb4d]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x18b419) [0x556094d4a419]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x6c0) [0x556094d34820]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7270) [0x556094d66270]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x278e) [0x556094d368ee]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7270) [0x556094d66270]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x278e) [0x556094d368ee]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7270) [0x556094d66270]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x277a5f) [0x556094e36a5f]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x195eab) [0x556094d54eab]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x556094d34967]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x556094d4a1bc]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x6c0) [0x556094d34820]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x556094d4a1bc]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x556094d34967]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x198311) [0x556094d57311]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(PyObject_Call+0x122) [0x556094d57fb2]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x2a8e) [0x556094d36bee]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x198311) [0x556094d57311]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x1990) [0x556094d35af0]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7270) [0x556094d66270]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x278e) [0x556094d368ee]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7270) [0x556094d66270]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x278e) [0x556094d368ee]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7270) [0x556094d66270]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x278e) [0x556094d368ee]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7270) [0x556094d66270]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x278e) [0x556094d368ee]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7270) [0x556094d66270]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x278e) [0x556094d368ee]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7270) [0x556094d66270]\n --- /usr/lib/python3.10/lib-dynload/_asyncio.cpython-310-x86_64-linux-gnu.so(+0x928e) [0x7f75cb3a528e]\n --- /usr/lib/python3.10/lib-dynload/_asyncio.cpython-310-x86_64-linux-gnu.so(+0xa49b) [0x7f75cb3a649b]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x18a384) [0x556094d49384]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x25bc65) [0x556094e1ac65]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x2c895a) [0x556094e8795a]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x17e50f) [0x556094d3d50f]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x65c3) [0x556094d3a723]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x556094d4a1bc]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x556094d34967]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x556094d4a1bc]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x556094d34967]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x556094d4a1bc]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x556094d34967]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x556094d4a1bc]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x556094d34967]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x556094d4a1bc]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x556094d34967]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x198311) [0x556094d57311]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x5603) [0x556094d39763]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x25a566) [0x556094e19566]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(PyEval_EvalCode+0x86) [0x556094e19436]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x25fb4d) [0x556094e1eb4d]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x18b419) [0x556094d4a419]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x6c0) [0x556094d34820]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x556094d4a1bc]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x6c0) [0x556094d34820]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x556094d4a1bc]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x27545d) [0x556094e3445d]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m tt_model_factory\u001b[38;5;241m.\u001b[39mtransformer_config\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m orig_vocab_size\n\u001b[1;32m      6\u001b[0m max_sequence_length \u001b[38;5;241m=\u001b[39m tt_model_factory\u001b[38;5;241m.\u001b[39mtransformer_config\u001b[38;5;241m.\u001b[39mmax_sequence_length\n\u001b[0;32m----> 8\u001b[0m tt_model \u001b[38;5;241m=\u001b[39m \u001b[43mtt_model_factory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m tt_model\u001b[38;5;241m.\u001b[39mload_from_safetensors(safetensors_path)\n\u001b[1;32m     11\u001b[0m padded_vocab_size \u001b[38;5;241m=\u001b[39m round_up_to_tile(orig_vocab_size, \u001b[38;5;241m32\u001b[39m)\n",
      "File \u001b[0;32m~/tt-metal/tt-train/sources/ttml/ttml/common/model_factory.py:169\u001b[0m, in \u001b[0;36mTransformerModelFactory.create_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_gpt2()\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_llama\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/tt-metal/tt-train/sources/ttml/ttml/common/model_factory.py:155\u001b[0m, in \u001b[0;36mTransformerModelFactory._create_llama\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_config\u001b[38;5;241m.\u001b[39menable_tp:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ttml\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mllama\u001b[38;5;241m.\u001b[39mcreate_llama_model(lcfg)\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mttml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_llama_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: TT_FATAL @ /home/ubuntu/tt-metal/tt_metal/impl/allocator/bank_manager.cpp:431: address.has_value()\ninfo:\nOut of Memory: Not enough space to allocate 117440512 B DRAM buffer across 12 banks, where each bank needs to store 9787392 B, but bank size is only 1071181792 B\nbacktrace:\n --- /home/ubuntu/tt-metal/build/lib/libtt_metal.so(+0x524cdd) [0x7f754d565cdd]\n --- tt::tt_metal::BankManager::allocate_buffer(unsigned long, unsigned long, bool, CoreRangeSet const&, std::optional<unsigned int>, ttsl::StrongType<unsigned int, tt::tt_metal::AllocatorIDTag>)\n --- tt::tt_metal::Allocator::allocate_buffer(tt::tt_metal::Buffer*)\n --- tt::tt_metal::Buffer::allocate_impl()\n --- tt::tt_metal::Buffer::create(tt::tt_metal::IDevice*, unsigned long, unsigned long, tt::tt_metal::BufferType, tt::tt_metal::BufferShardingArgs const&, std::optional<bool>, std::optional<ttsl::StrongType<unsigned char, tt::tt_metal::SubDeviceIdTag> >)\n --- tt::tt_metal::distributed::MeshBuffer::create(std::variant<tt::tt_metal::distributed::ReplicatedBufferConfig, tt::tt_metal::distributed::ShardedBufferConfig> const&, tt::tt_metal::distributed::DeviceLocalBufferConfig const&, tt::tt_metal::distributed::MeshDevice*, std::optional<unsigned long>)\n --- tt::tt_metal::tensor_impl::allocate_device_buffer(tt::tt_metal::distributed::MeshDevice*, tt::tt_metal::TensorSpec const&)\n --- tt::tt_metal::allocate_tensor_on_device(tt::tt_metal::TensorSpec const&, tt::tt_metal::distributed::MeshDevice*)\n --- tt::tt_metal::create_device_tensor(tt::tt_metal::TensorSpec const&, tt::tt_metal::IDevice*)\n --- tt::tt_metal::operation::program_output_helper<ttnn::operations::data_movement::TilizeWithValPadding, has_create_program<ttnn::operations::data_movement::TilizeWithValPadding>::value>::type tt::tt_metal::operation::default_create_output_tensors<ttnn::operations::data_movement::TilizeWithValPadding>(ttnn::operations::data_movement::TilizeWithValPadding const&, std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > const&, std::vector<std::optional<tt::tt_metal::Tensor>, std::allocator<std::optional<tt::tt_metal::Tensor> > > const&)\n --- /home/ubuntu/tt-metal/build/lib/_ttnncpp.so(+0x9db6ce) [0x7f753b5f66ce]\n --- tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >::create_output_tensors(tt::tt_metal::operation::DeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > > const&, tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >::tensor_args_t const&)\n --- tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >::tensor_return_value_t ttnn::device_operation::detail::launch_on_device<tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > > >(tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >::operation_attributes_t const&, tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >::tensor_args_t const&)\n --- tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >::tensor_return_value_t ttnn::device_operation::detail::invoke<tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > > >(tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >::operation_attributes_t const&, tt::tt_metal::operation::OldInfraDeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >::tensor_args_t const&)\n --- /home/ubuntu/tt-metal/build/lib/_ttnncpp.so(+0x6e2c5f) [0x7f753b2fdc5f]\n --- std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > tt::tt_metal::operation::run<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >(tt::tt_metal::operation::DeviceOperation<std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > >&&, std::vector<tt::tt_metal::Tensor, std::allocator<tt::tt_metal::Tensor> > const&, std::vector<std::optional<tt::tt_metal::Tensor const>, std::allocator<std::optional<tt::tt_metal::Tensor const> > > const&, std::vector<std::optional<tt::tt_metal::Tensor>, std::allocator<std::optional<tt::tt_metal::Tensor> > > const&)\n --- /home/ubuntu/tt-metal/build/lib/_ttnncpp.so(+0x9c2736) [0x7f753b5dd736]\n --- /home/ubuntu/tt-metal/build/lib/_ttnncpp.so(+0x9776bb) [0x7f753b5926bb]\n --- ttnn::operations::data_movement::ExecuteTilizeWithValPadding::invoke(tt::tt_metal::Tensor const&, tt::tt_metal::Shape const&, std::variant<unsigned int, float>, std::optional<tt::tt_metal::MemoryConfig> const&, std::optional<tt::tt_metal::DataType>, bool)\n --- ttnn::operations::data_movement::ExecuteTilizeWithZeroPadding::invoke(tt::tt_metal::Tensor const&, std::optional<tt::tt_metal::MemoryConfig> const&, std::optional<tt::tt_metal::DataType>, bool)\n --- /home/ubuntu/tt-metal/python_env/lib/python3.10/site-packages/ttml/_ttml.cpython-310-x86_64-linux-gnu.so(_ZNK4ttnn10decorators22registered_operation_tIXtlN7reflect6v1_2_512fixed_stringIcLm30EEEtlA31_cLc116ELc116ELc110ELc110ELc58ELc58ELc116ELc105ELc108ELc105ELc122ELc101ELc95ELc119ELc105ELc116ELc104ELc95ELc122ELc101ELc114ELc111ELc95ELc112ELc97ELc100ELc100ELc105ELc110ELc103EEEENS_10operations13data_movement28ExecuteTilizeWithZeroPaddingEE16invoke_compositeIJRN2tt8tt_metal6TensorERNSD_12MemoryConfigERKSt9nullopt_tbEEEDaDpOT_+0x41) [0x7f75401ccc61]\n --- /home/ubuntu/tt-metal/python_env/lib/python3.10/site-packages/ttml/_ttml.cpython-310-x86_64-linux-gnu.so(_ZNK4ttnn10decorators22registered_operation_tIXtlN7reflect6v1_2_512fixed_stringIcLm30EEEtlA31_cLc116ELc116ELc110ELc110ELc58ELc58ELc116ELc105ELc108ELc105ELc122ELc101ELc95ELc119ELc105ELc116ELc104ELc95ELc122ELc101ELc114ELc111ELc95ELc112ELc97ELc100ELc100ELc105ELc110ELc103EEEENS_10operations13data_movement28ExecuteTilizeWithZeroPaddingEE13traced_invokeIJRN2tt8tt_metal6TensorERNSD_12MemoryConfigERKSt9nullopt_tbEEEDaDpOT_+0x77) [0x7f75401cc8f7]\n --- tt::tt_metal::Tensor ttml::core::from_vector<float, (tt::tt_metal::DataType)0>(std::vector<float, std::allocator<float> > const&, tt::tt_metal::Shape const&, tt::tt_metal::distributed::MeshDevice*, tt::tt_metal::Layout, ttnn::distributed::TensorToMesh const*)\n --- ttml::init::uniform_init(std::shared_ptr<ttml::autograd::Tensor>&, tt::tt_metal::Shape const&, ttml::init::UniformRange)\n --- ttml::modules::LinearLayer::LinearLayer(unsigned int, unsigned int, bool)\n --- ttml::modules::LlamaMLP::LlamaMLP(unsigned int, std::optional<unsigned int>, float)\n --- ttml::modules::LlamaBlock::LlamaBlock(unsigned int, unsigned int, unsigned int, ttml::ops::RotaryEmbeddingParams const&, float, std::optional<unsigned int>)\n --- ttml::models::llama::Llama::Llama(ttml::models::llama::LlamaConfig const&)\n --- ttml::models::llama::create(ttml::models::llama::LlamaConfig const&)\n --- /home/ubuntu/tt-metal/python_env/lib/python3.10/site-packages/ttml/_ttml.cpython-310-x86_64-linux-gnu.so(+0x1003e3) [0x7f754016c3e3]\n --- /home/ubuntu/tt-metal/python_env/lib/python3.10/site-packages/ttml/_ttml.cpython-310-x86_64-linux-gnu.so(+0x13ed13) [0x7f75401aad13]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x5603) [0x556094d39763]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x556094d4a1bc]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x556094d34967]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x556094d4a1bc]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x556094d34967]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x25a566) [0x556094e19566]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(PyEval_EvalCode+0x86) [0x556094e19436]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x25fb4d) [0x556094e1eb4d]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x18b419) [0x556094d4a419]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x6c0) [0x556094d34820]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7270) [0x556094d66270]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x278e) [0x556094d368ee]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7270) [0x556094d66270]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x278e) [0x556094d368ee]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7270) [0x556094d66270]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x277a5f) [0x556094e36a5f]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x195eab) [0x556094d54eab]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x556094d34967]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x556094d4a1bc]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x6c0) [0x556094d34820]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x556094d4a1bc]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x556094d34967]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x198311) [0x556094d57311]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(PyObject_Call+0x122) [0x556094d57fb2]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x2a8e) [0x556094d36bee]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x198311) [0x556094d57311]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x1990) [0x556094d35af0]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7270) [0x556094d66270]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x278e) [0x556094d368ee]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7270) [0x556094d66270]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x278e) [0x556094d368ee]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7270) [0x556094d66270]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x278e) [0x556094d368ee]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7270) [0x556094d66270]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x278e) [0x556094d368ee]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7270) [0x556094d66270]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x278e) [0x556094d368ee]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x1a7270) [0x556094d66270]\n --- /usr/lib/python3.10/lib-dynload/_asyncio.cpython-310-x86_64-linux-gnu.so(+0x928e) [0x7f75cb3a528e]\n --- /usr/lib/python3.10/lib-dynload/_asyncio.cpython-310-x86_64-linux-gnu.so(+0xa49b) [0x7f75cb3a649b]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x18a384) [0x556094d49384]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x25bc65) [0x556094e1ac65]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x2c895a) [0x556094e8795a]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x17e50f) [0x556094d3d50f]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x65c3) [0x556094d3a723]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x556094d4a1bc]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x556094d34967]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x556094d4a1bc]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x556094d34967]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x556094d4a1bc]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x556094d34967]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x556094d4a1bc]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x556094d34967]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x556094d4a1bc]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x807) [0x556094d34967]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x198311) [0x556094d57311]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x5603) [0x556094d39763]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x25a566) [0x556094e19566]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(PyEval_EvalCode+0x86) [0x556094e19436]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x25fb4d) [0x556094e1eb4d]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x18b419) [0x556094d4a419]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x6c0) [0x556094d34820]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x556094d4a1bc]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyEval_EvalFrameDefault+0x6c0) [0x556094d34820]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(_PyFunction_Vectorcall+0x7c) [0x556094d4a1bc]\n --- /home/ubuntu/tt-metal/python_env/bin/python3(+0x27545d) [0x556094e3445d]\n"
     ]
    }
   ],
   "source": [
    "orig_vocab_size = torch_model.vocab_size\n",
    "print(orig_vocab_size)\n",
    "tt_model_factory = TransformerModelFactory(yaml_config)\n",
    "tt_model_factory.transformer_config.vocab_size = orig_vocab_size\n",
    "\n",
    "max_sequence_length = tt_model_factory.transformer_config.max_sequence_length\n",
    "\n",
    "tt_model = tt_model_factory.create_model()\n",
    "tt_model.load_from_safetensors(safetensors_path)\n",
    "\n",
    "padded_vocab_size = round_up_to_tile(orig_vocab_size, 32)\n",
    "\n",
    "if orig_vocab_size != padded_vocab_size:\n",
    "    print(f\"Padding vocab size for tilization: original {orig_vocab_size} -> padded {padded_vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d4f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_causal_mask(T: int) -> ttml.autograd.Tensor:\n",
    "    # [1,1,T,T] float32 with 1s for allowed positions (i >= j), else 0\\n\",\n",
    "    m = np.tril(np.ones((T, T), dtype=np.float32))\n",
    "    return ttml.autograd.Tensor.from_numpy(m.reshape(1, 1, T, T), ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)\n",
    "\n",
    "def build_logits_mask(vocab_size: int, padded_vocab_size: int) -> ttml.autograd.Tensor:\n",
    "    logits_mask = np.zeros((1, 1, 1, padded_vocab_size), dtype=np.float32)\n",
    "    logits_mask[:, :, :, vocab_size:] = 1e4\n",
    "    return ttml.autograd.Tensor.from_numpy(logits_mask, ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)   # [1,1,1,T], bfloat16\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40913e88",
   "metadata": {},
   "source": [
    "`generate_with_tt()` uses TT hardware acceleration to generate output from the chosen LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e6550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_tt(model, prompt_tokens):\n",
    "    import time\n",
    "    \n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(ttml.autograd.GradMode.DISABLED)\n",
    "    model.eval()\n",
    "\n",
    "    logits_mask_tensor = None\n",
    "\n",
    "    if padded_vocab_size != orig_vocab_size:\n",
    "        logits_mask_tensor = build_logits_mask(orig_vocab_size, padded_vocab_size)\n",
    "\n",
    "    causal_mask = build_causal_mask(max_sequence_length)  # [1,1,seq_len,seq_len], float32\n",
    "    padded_prompt_tokens = np.zeros((1, 1, 1, max_sequence_length), \n",
    "                                    dtype=np.uint32)\n",
    "\n",
    "    start_idx = 0\n",
    "\n",
    "    print(\"************************************\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for token_idx in range(OUTPUT_TOKENS):\n",
    "\n",
    "        if len(prompt_tokens) > max_sequence_length:\n",
    "            start_idx = len(prompt_tokens) - max_sequence_length\n",
    "\n",
    "        # padded_prompt_tokens[0, 0, 0, :transformer_cfg[\"max_sequence_length\"]] = 0\n",
    "        padded_prompt_tokens[0, 0, 0, :len(prompt_tokens)] = prompt_tokens[start_idx:]\n",
    "        padded_prompt_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "            padded_prompt_tokens,\n",
    "            ttml.Layout.ROW_MAJOR,\n",
    "            ttml.autograd.DataType.UINT32)  # [1,1,1, max_seq_len], uint32\n",
    "\n",
    "        logits = model(padded_prompt_tensor, causal_mask)  # out=[1,1,seq_len, vocab_size], bf16\n",
    "\n",
    "\n",
    "        next_token_tensor = ttml.ops.sample.sample_op(logits, TEMPERATURE, np.random.randint(low=1e7), logits_mask_tensor)  # out=[1,1,seq_len,1], uint32\n",
    "\n",
    "        next_token_idx = max_sequence_length - 1 if len(prompt_tokens) > max_sequence_length else len(prompt_tokens) - 1\n",
    "        next_token = next_token_tensor.to_numpy().flatten()[next_token_idx]\n",
    "\n",
    "        output = tokenizer.decode(next_token)\n",
    "\n",
    "        prompt_tokens.append(next_token)\n",
    "        print(output, end='', flush=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = OUTPUT_TOKENS / elapsed_time\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {OUTPUT_TOKENS} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af958d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7824c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_pytorch(torch_model, prompt_tokens):\n",
    "    import time\n",
    "    import torch.nn.functional as F\n",
    "    from transformers import DynamicCache\n",
    "    \n",
    "    torch_model.eval()\n",
    "    \n",
    "    print(\"************************************\")\n",
    "    # Convert list to tensor and add batch dimension\n",
    "    if isinstance(prompt_tokens, list):\n",
    "        prompt_tokens = torch.tensor([prompt_tokens])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize KV cache using the new DynamicCache API\n",
    "    past_key_values = DynamicCache()\n",
    "    input_ids = prompt_tokens\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(OUTPUT_TOKENS):\n",
    "            # Get model outputs with KV cache\n",
    "            outputs = torch_model(\n",
    "                input_ids=input_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            past_key_values = outputs.past_key_values\n",
    "            \n",
    "            # Get logits for the last token\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            \n",
    "            # Apply temperature and sample\n",
    "            if WITH_SAMPLING and TEMPERATURE > 0:\n",
    "                next_token_logits = next_token_logits / TEMPERATURE\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                # Greedy sampling\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            \n",
    "            # Decode and print the token\n",
    "            output = tokenizer.decode(next_token[0])\n",
    "            print(output, end='', flush=True)\n",
    "            \n",
    "            # For next iteration, only pass the new token (KV cache handles the rest)\n",
    "            input_ids = next_token\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = OUTPUT_TOKENS / elapsed_time\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {OUTPUT_TOKENS} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17b326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_pytorch_batch(torch_model, prompt_tokens):\n",
    "    \"\"\"Old version: non-streaming batch generation using torch_model.generate()\"\"\"\n",
    "    import time\n",
    "    \n",
    "    torch_model.eval()\n",
    "    \n",
    "    print(\"************************************\")\n",
    "    # Convert list to tensor and add batch dimension\n",
    "    if isinstance(prompt_tokens, list):\n",
    "        prompt_tokens = torch.tensor([prompt_tokens])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = torch_model.generate(\n",
    "            prompt_tokens,\n",
    "            max_new_tokens=OUTPUT_TOKENS,\n",
    "            do_sample=WITH_SAMPLING,  # Enable sampling\n",
    "            temperature=TEMPERATURE,   # Temperature for sampling\n",
    "            num_beams=1  # Use multinomial sampling (standard sampling)\n",
    "        )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = OUTPUT_TOKENS / elapsed_time\n",
    "    \n",
    "    generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    for t in generated_text:\n",
    "        print(t)\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {OUTPUT_TOKENS} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4813faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"Generating with pytorch(CPU, might be slow)\"\n",
    "\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with torch model:\")\n",
    "generate_with_pytorch(torch_model, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88bddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"Generating with pytorch(CPU, might be slow)\"\n",
    "\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with torch model batch:\")\n",
    "generate_with_pytorch_batch(torch_model, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a42273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"Generating with Tenstorrent, (should be fast, even without kv-caching)\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with TT:\")\n",
    "generate_with_tt(tt_model, prompt_tokens.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e38f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_pytorch_no_cache(torch_model, prompt_tokens):\n",
    "    import time\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    torch_model.eval()\n",
    "    \n",
    "    print(\"************************************\")\n",
    "    # Convert list to tensor and add batch dimension\n",
    "    if isinstance(prompt_tokens, list):\n",
    "        input_ids = torch.tensor([prompt_tokens])\n",
    "    else:\n",
    "        input_ids = prompt_tokens\n",
    "    \n",
    "    start_time = time.time()\n",
    "    tokens = 10  # this is very slow\n",
    "    with torch.no_grad():\n",
    "        for i in range(tokens):\n",
    "            # Process the entire sequence every time (no KV cache)\n",
    "            outputs = torch_model(\n",
    "                input_ids=input_ids,\n",
    "                use_cache=False\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Get logits for the last token\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            \n",
    "            # Apply temperature and sample\n",
    "            if WITH_SAMPLING and TEMPERATURE > 0:\n",
    "                next_token_logits = next_token_logits / TEMPERATURE\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                # Greedy sampling\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            \n",
    "            # Decode and print the token\n",
    "            output = tokenizer.decode(next_token[0])\n",
    "            print(output, end='', flush=True)\n",
    "            \n",
    "            # Append the new token to the full sequence for next iteration\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = tokens / elapsed_time\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {tokens} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c930950",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"Generating with PyTorch, (should be slow, without kv-caching)\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with PyTorch:\")\n",
    "generate_with_pytorch_no_cache(torch_model, prompt_tokens.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4744a260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5e4a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b8521eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([128256, 2048])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.0.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.1.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.2.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.3.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.4.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.5.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.6.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.7.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.8.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.9.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.10.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.11.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.12.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.13.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.14.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.15.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.norm.weight torch.Size([2048])\n",
      "lm_head.weight torch.Size([128256, 2048])\n"
     ]
    }
   ],
   "source": [
    "sd = torch_model.state_dict()\n",
    "for s in sd:\n",
    "    print(s, sd[s].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e868214a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama/llama_block_9/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_9/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_1/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_5/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_11/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_10/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_8/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_2/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_10/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_10/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_10/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_2/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_1/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_1/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_1/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_1/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_0/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_12/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_14/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_3/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_0/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_0/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_9/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_10/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_8/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_4/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_3/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_6/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_7/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_1/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_11/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_1/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_0/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_7/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_10/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_12/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_4/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_0/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_12/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_11/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_13/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_4/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_13/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_15/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_5/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_0/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_0/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/fc/weight [1, 1, 128256, 2048]\n",
      "llama/llama_block_13/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_14/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_13/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_12/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_11/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_15/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_6/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_11/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_15/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_2/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_10/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_2/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_8/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_4/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_7/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_12/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_5/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_13/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_9/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_3/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_3/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_11/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_12/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_1/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_11/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_6/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_12/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_12/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_13/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_15/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_14/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_13/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_4/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_14/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_7/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_14/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_14/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_9/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_4/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_15/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_15/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_15/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_14/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_10/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_14/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_15/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_2/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_2/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_4/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_2/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_8/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_3/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_3/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_8/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_5/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_3/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_13/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_3/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_7/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_11/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_4/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_5/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_5/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_5/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_7/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_0/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_6/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_6/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_6/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_6/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_6/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_7/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_2/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_7/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_5/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_8/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_8/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_8/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/ln_fc/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_9/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_9/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_9/attention/q_linear/weight [1, 1, 2048, 2048]\n"
     ]
    }
   ],
   "source": [
    "k = tt_model.parameters()\n",
    "for s in k:\n",
    "    print(s, k[s].shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aa5542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "175e9744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "WEIGHT COMPARISON: PyTorch vs TT-Metal\n",
      "================================================================================\n",
      "Note: Detected num_heads=32 for RoPE permutation\n",
      "Note: Weight tying ENABLED\n",
      "================================================================================\n",
      "\n",
      "✓ model.embed_tokens.weight\n",
      "  PyTorch: (128256, 2048)\n",
      "  TT:      (1, 1, 128256, 2048) -> (128256, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.0.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.0.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.0.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.0.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.0.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.0.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.0.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.1.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.1.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.1.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.1.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.1.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.1.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.1.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.1.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.2.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.2.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.2.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.2.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.2.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.2.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.2.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.2.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.3.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.3.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.3.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.3.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.3.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.3.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.3.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.3.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.4.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.4.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.4.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.4.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.4.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.4.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.4.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.4.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.5.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.5.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.5.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.5.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.5.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.5.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.5.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.5.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.6.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.6.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.6.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.6.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ model.layers.6.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.6.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.6.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.6.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.7.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.7.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.7.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.7.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.7.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.7.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.7.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.7.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.8.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.8.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.8.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.8.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.8.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.8.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.8.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.8.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.9.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.9.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.9.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.9.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.9.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.9.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.9.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.9.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.10.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.10.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.10.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.10.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.10.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.10.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.10.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.10.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.11.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.11.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.11.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.11.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.11.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.11.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.11.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.11.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.12.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.12.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.12.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.12.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.12.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.12.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.12.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.12.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.13.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.13.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.13.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.13.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ model.layers.13.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.13.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.13.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.13.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.14.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.14.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.14.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.14.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.14.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.14.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.14.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.14.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.15.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.15.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.15.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.15.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.15.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.15.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.15.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.15.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.norm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "================================================================================\n",
      "SUMMARY: 114 matches, 0 mismatches\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def apply_rope_permutation(w, num_heads):\n",
    "    \"\"\"\n",
    "    Apply RoPE row permutation to match TT's q_proj weight layout.\n",
    "    TT applies unpermute_proj_rows during loading which interleaves rows within each head.\n",
    "    \"\"\"\n",
    "    rows, cols = w.shape\n",
    "    head_dim = rows // num_heads\n",
    "    \n",
    "    out = np.zeros_like(w)\n",
    "    for h in range(num_heads):\n",
    "        head_start = h * head_dim\n",
    "        half = head_dim // 2\n",
    "        \n",
    "        # Interleave: [0..half-1, half..head_dim-1] → [0, half, 1, half+1, ..., half-1, head_dim-1]\n",
    "        for i in range(half):\n",
    "            out[head_start + 2*i] = w[head_start + i]\n",
    "            out[head_start + 2*i + 1] = w[head_start + half + i]\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def compare_weights(torch_model, tt_model):\n",
    "    \"\"\"\n",
    "    Compare weights between PyTorch model and TT-Metal model.\n",
    "    \n",
    "    Args:\n",
    "        torch_model: PyTorch model (with .state_dict())\n",
    "        tt_model: TT-Metal model (with .parameters())\n",
    "    \"\"\"\n",
    "    \n",
    "    torch_sd = torch_model.state_dict()\n",
    "    tt_params = tt_model.parameters()\n",
    "    \n",
    "    # Get num_heads from torch model config\n",
    "    num_heads = torch_model.config.num_attention_heads\n",
    "    \n",
    "    # Detect weight tying configuration from TT model\n",
    "    has_tok_emb = 'llama/tok_emb/weight' in tt_params\n",
    "    has_fc = 'llama/fc/weight' in tt_params\n",
    "    weight_tying_enabled = not has_tok_emb  # If tok_emb doesn't exist, weight tying is enabled\n",
    "    \n",
    "    # Mapping from PyTorch parameter names to TT-Metal parameter names\n",
    "    pytorch_to_tt_mapping = {\n",
    "        # Final layer norm\n",
    "        'model.norm.weight': 'llama/ln_fc/gamma',\n",
    "    }\n",
    "    \n",
    "    # Add embedding mappings based on weight tying configuration\n",
    "    if weight_tying_enabled:\n",
    "        # Weight tying enabled: both embed_tokens and lm_head use fc/weight\n",
    "        pytorch_to_tt_mapping['model.embed_tokens.weight'] = 'llama/fc/weight'\n",
    "        # lm_head.weight should also map to fc/weight, but we'll skip it to avoid duplicate checks\n",
    "    else:\n",
    "        # Weight tying disabled: separate tok_emb and fc\n",
    "        pytorch_to_tt_mapping['model.embed_tokens.weight'] = 'llama/tok_emb/weight'\n",
    "        pytorch_to_tt_mapping['lm_head.weight'] = 'llama/fc/weight'\n",
    "    \n",
    "    # Add layer-specific mappings\n",
    "    for i in range(50):  # Support up to 50 layers\n",
    "        layer_prefix_pt = f'model.layers.{i}'\n",
    "        layer_prefix_tt = f'llama/llama_block_{i}'\n",
    "        \n",
    "        pytorch_to_tt_mapping.update({\n",
    "            f'{layer_prefix_pt}.input_layernorm.weight': f'{layer_prefix_tt}/attention_norm/gamma',\n",
    "            f'{layer_prefix_pt}.post_attention_layernorm.weight': f'{layer_prefix_tt}/mlp_norm/gamma',\n",
    "            f'{layer_prefix_pt}.self_attn.q_proj.weight': f'{layer_prefix_tt}/attention/q_linear/weight',\n",
    "            f'{layer_prefix_pt}.self_attn.o_proj.weight': f'{layer_prefix_tt}/attention/out_linear/weight',\n",
    "            # k_proj and v_proj are combined into kv_linear in TT\n",
    "            f'{layer_prefix_pt}.mlp.gate_proj.weight': f'{layer_prefix_tt}/mlp/w1/weight',\n",
    "            f'{layer_prefix_pt}.mlp.up_proj.weight': f'{layer_prefix_tt}/mlp/w3/weight',\n",
    "            f'{layer_prefix_pt}.mlp.down_proj.weight': f'{layer_prefix_tt}/mlp/w2/weight',\n",
    "        })\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"WEIGHT COMPARISON: PyTorch vs TT-Metal\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Note: Detected num_heads={num_heads} for RoPE permutation\")\n",
    "    print(f\"Note: Weight tying {'ENABLED' if weight_tying_enabled else 'DISABLED'}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    mismatches = []\n",
    "    matches = []\n",
    "    \n",
    "    for pt_name in torch_sd.keys():\n",
    "        if 'bias' in pt_name:\n",
    "            continue  # Skip bias parameters\n",
    "            \n",
    "        pt_tensor = torch_sd[pt_name]\n",
    "        pt_shape = tuple(pt_tensor.shape)\n",
    "        \n",
    "        # Handle k_proj and v_proj specially (they're combined in TT)\n",
    "        if '.self_attn.k_proj.weight' in pt_name or '.self_attn.v_proj.weight' in pt_name:\n",
    "            layer_idx = pt_name.split('.')[2]\n",
    "            tt_name = f'llama/llama_block_{layer_idx}/attention/kv_linear/weight'\n",
    "            \n",
    "            if tt_name in tt_params:\n",
    "                tt_tensor_np = tt_params[tt_name].to_numpy()\n",
    "                tt_shape = tt_tensor_np.shape\n",
    "                \n",
    "                # k_proj and v_proj are concatenated in kv_linear\n",
    "                # Expected: k_proj [512, 2048] + v_proj [512, 2048] = kv_linear [1, 1, 1024, 2048]\n",
    "                if '.self_attn.k_proj.weight' in pt_name:\n",
    "                    print(f\"\\n{pt_name}\")\n",
    "                    print(f\"  PyTorch: {pt_shape}\")\n",
    "                    print(f\"  TT (kv combined): {tt_shape}\")\n",
    "                    print(f\"  Status: K and V are combined in TT as kv_linear\")\n",
    "            continue\n",
    "        \n",
    "        # Get corresponding TT parameter name\n",
    "        tt_name = pytorch_to_tt_mapping.get(pt_name)\n",
    "        if not tt_name:\n",
    "            continue\n",
    "            \n",
    "        if tt_name not in tt_params:\n",
    "            print(f\"\\n❌ MISSING: {pt_name} -> {tt_name}\")\n",
    "            print(f\"   PyTorch shape: {pt_shape}\")\n",
    "            mismatches.append((pt_name, \"MISSING IN TT\"))\n",
    "            continue\n",
    "        \n",
    "        # Get TT tensor\n",
    "        tt_tensor_np = tt_params[tt_name].to_numpy()\n",
    "        tt_shape = tt_tensor_np.shape\n",
    "        \n",
    "        # Remove batch dimensions [1, 1, ...] from TT tensor\n",
    "        tt_shape_no_batch = tt_shape[2:] if len(tt_shape) == 4 else tt_shape\n",
    "        \n",
    "        # Compare shapes\n",
    "        pt_numpy = pt_tensor.cpu().float().numpy()  # Convert to float32 for numpy compatibility\n",
    "        \n",
    "        # Special handling for q_proj: TT applies RoPE row permutation during loading\n",
    "        is_q_proj = '.self_attn.q_proj.weight' in pt_name\n",
    "        if is_q_proj and len(pt_shape) == 2 and pt_shape[0] % num_heads == 0:\n",
    "            pt_numpy = apply_rope_permutation(pt_numpy, num_heads)\n",
    "        \n",
    "        # For layer norms: PT (N,) vs TT (1, N) - both are fine, just broadcasting\n",
    "        # Check if PT is 1D and TT has leading 1s that can be squeezed\n",
    "        if len(pt_shape) == 1 and len(tt_shape_no_batch) == 2 and tt_shape_no_batch[0] == 1:\n",
    "            tt_shape_no_batch = (tt_shape_no_batch[1],)  # Squeeze leading 1\n",
    "        \n",
    "        # Check if shapes match (with or without transpose)\n",
    "        shape_match = (pt_shape == tt_shape_no_batch) or (pt_shape == tt_shape_no_batch[::-1])\n",
    "        \n",
    "        if shape_match:\n",
    "            # Check actual values\n",
    "            # Reshape TT data to match PT shape (handle batch dims and potential squeezing)\n",
    "            if len(tt_shape) == 4:\n",
    "                tt_data = tt_tensor_np.reshape(tt_shape[2:])  # Remove [1,1,...] batch dims\n",
    "            else:\n",
    "                tt_data = tt_tensor_np.reshape(tt_shape)\n",
    "            \n",
    "            # Squeeze if needed for 1D comparisons\n",
    "            tt_data = tt_data.squeeze()\n",
    "            pt_numpy_squeezed = pt_numpy.squeeze()\n",
    "            \n",
    "            # Handle transpose if needed\n",
    "            if pt_numpy_squeezed.shape != tt_data.shape and len(tt_data.shape) == 2:\n",
    "                tt_data = tt_data.T\n",
    "            \n",
    "            diff = np.abs(pt_numpy_squeezed - tt_data).max()\n",
    "            rel_diff = diff / (np.abs(pt_numpy_squeezed).max() + 1e-8)\n",
    "            \n",
    "            status = \"✓\" if diff < 1e-3 else \"⚠\"\n",
    "            note = \" (after RoPE permutation)\" if is_q_proj else \"\"\n",
    "            print(f\"\\n{status} {pt_name}{note}\")\n",
    "            print(f\"  PyTorch: {pt_shape}\")\n",
    "            print(f\"  TT:      {tt_shape} -> {tt_shape_no_batch}\")\n",
    "            print(f\"  Max diff: {diff:.6f}, Rel diff: {rel_diff:.6f}\")\n",
    "            \n",
    "            if diff < 1e-3:\n",
    "                matches.append(pt_name)\n",
    "            else:\n",
    "                mismatches.append((pt_name, f\"VALUE_DIFF={diff:.6f}\"))\n",
    "        else:\n",
    "            print(f\"\\n❌ SHAPE MISMATCH: {pt_name}\")\n",
    "            print(f\"  PyTorch: {pt_shape}\")\n",
    "            print(f\"  TT:      {tt_shape} -> {tt_shape_no_batch}\")\n",
    "            mismatches.append((pt_name, f\"SHAPE: PT={pt_shape} vs TT={tt_shape_no_batch}\"))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"SUMMARY: {len(matches)} matches, {len(mismatches)} mismatches\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if mismatches:\n",
    "        print(\"\\n❌ MISMATCHES:\")\n",
    "        for name, issue in mismatches:\n",
    "            print(f\"  - {name}: {issue}\")\n",
    "    \n",
    "    return matches, mismatches\n",
    "\n",
    "\n",
    "# Usage example (commented out):\n",
    "matches, mismatches = compare_weights(torch_model, tt_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7af5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
