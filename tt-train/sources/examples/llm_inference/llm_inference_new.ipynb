{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f7bc745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6545c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TOKENS = 100\n",
    "WITH_SAMPLING = True\n",
    "TEMPERATURE = 0.0\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95da3706",
   "metadata": {},
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\" \n",
    "CONFIG = \"training_shakespeare_llama3_2_1B_fixed.yaml\" # now working fine (with proper shape for tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e1948c",
   "metadata": {},
   "source": [
    "model_id =  \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "CONFIG = \"training_shakespeare_tinyllama.yaml\" # working fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a19149b",
   "metadata": {},
   "source": [
    "model_id =  \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "CONFIG = \"training_shakespeare_llama3_8B_tp.yaml\" # OOM on 12 GB, sucesfully loaded weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b615d9",
   "metadata": {},
   "source": [
    "model_id = \"Qwen/Qwen3-0.6B\" \n",
    "CONFIG = \"training_shakespeare_qwen3_0_6B.yaml\" # working, not 1-1 as llama, but speak something nongibberish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b856ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen3-1.7B\" \n",
    "CONFIG = \"training_shakespeare_qwen3_1_7B.yaml\"  # works with keyboard interrupt on weights loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16582320",
   "metadata": {},
   "source": [
    "model_id = \"Qwen/Qwen3-4B\"\n",
    "CONFIG = \"training_shakespeare_qwen3_4B.yaml\" # doesn't work: loads, runs, outputs gibberish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df6d6bf9-a010-4206-b004-807ee500d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "import numpy as np  # For numpy arrays\n",
    "from dataclasses import dataclass # For configuration classes\n",
    "from huggingface_hub import hf_hub_download # To download safetensors from Hugging Face\n",
    "from transformers import AutoTokenizer\n",
    "from yaml import safe_load # To read YAML configs\n",
    "from pathlib import Path\n",
    "\n",
    "import ttml\n",
    "from ttml.common.config import get_config, TransformerConfig\n",
    "from ttml.common.utils import set_seed, round_up_to_tile\n",
    "from ttml.common.model_factory import TransformerModelFactory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55071c7a-74fe-4f56-8f33-e45ffe699c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from Hugging Face and the transformer config from YAML\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "transformer_config = TransformerConfig(get_config(CONFIG).get(\"training_config\", {}).get(\"transformer_config\",{}))\n",
    "yaml_config = get_config(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e7883f6-f8e6-436b-8908-62c8a26e1e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "safetensors_path = hf_hub_download(repo_id=model_id, filename=\"config.json\")\n",
    "safetensors_path = safetensors_path.replace(\"config.json\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f77e69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ded629bbe4c49a6b9f11ce6f789a589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "torch.manual_seed(SEED)\n",
    "torch_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f405407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151643, 151936, 151936)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size, torch_model.state_dict()['model.embed_tokens.weight'].shape[0], torch_model.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50d8ce12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torch_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf74d9ac-7afd-4c6f-887e-b793c6e44c18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151936\n",
      "Qwen3 configuration:\n",
      "    Vocab size: 151936\n",
      "    Max sequence length: 2048\n",
      "    Embedding dim (hidden_size): 2048\n",
      "    Head dim: 128\n",
      "    Attention output dim: 2048\n",
      "    Intermediate dim: 6144\n",
      "    Num heads: 16\n",
      "    Num groups (KV heads): 8\n",
      "    Dropout probability: 0\n",
      "    Num blocks: 28\n",
      "    Positional embedding type: RoPE\n",
      "    Runner type: Memory efficient\n",
      "    Weight tying: Disabled\n",
      "    Theta: 1000000\n",
      "    RMSNorm epsilon: 1e-06\n",
      "2025-11-24 19:31:40.078 | info     |             UMD | Established cluster ETH FW version: 6.14.0 (topology_discovery_wormhole.cpp:359)\n",
      "2025-11-24 19:31:40.082 | info     |          Device | Opening user mode device driver (tt_cluster.cpp:209)\n",
      "2025-11-24 19:31:40.103 | info     |             UMD | Established cluster ETH FW version: 6.14.0 (topology_discovery_wormhole.cpp:359)\n",
      "2025-11-24 19:31:40.142 | info     |             UMD | Established cluster ETH FW version: 6.14.0 (topology_discovery_wormhole.cpp:359)\n",
      "2025-11-24 19:31:40.146 | info     |             UMD | Harvesting mask for chip 0 is 0x1 (NOC0: 0x1, simulated harvesting mask: 0x0). (cluster.cpp:413)\n",
      "2025-11-24 19:31:40.184 | warning  |             UMD | init_detect_tt_device_numanodes(): Could not determine NumaNodeSet for TT device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:564)\n",
      "2025-11-24 19:31:40.184 | warning  |             UMD | Could not find NumaNodeSet for TT Device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:173)\n",
      "2025-11-24 19:31:40.184 | warning  |             UMD | bind_area_memory_nodeset(): Unable to determine TT Device to NumaNode mapping for physical_device_id: 0. Skipping membind. (cpuset_lib.cpp:450)\n",
      "2025-11-24 19:31:40.184 | warning  |             UMD | ---- ttSiliconDevice::init_hugepage: bind_area_to_memory_nodeset() failed (physical_device_id: 0 ch: 0). Hugepage allocation is not on NumaNode matching TT Device. Side-Effect is decreased Device->Host perf (Issue #893). (sysmem_manager.cpp:212)\n",
      "2025-11-24 19:31:40.185 | info     |             UMD | Opening local chip ids/PCIe ids: {0}/[0] and remote chip ids {} (cluster.cpp:257)\n",
      "2025-11-24 19:31:40.185 | info     |             UMD | All devices in cluster running firmware version: 80.17.0 (cluster.cpp:235)\n",
      "2025-11-24 19:31:40.185 | info     |             UMD | IOMMU: disabled (cluster.cpp:177)\n",
      "2025-11-24 19:31:40.185 | info     |             UMD | KMD version: 2.0.0 (cluster.cpp:180)\n",
      "2025-11-24 19:31:40.187 | info     |             UMD | Pinning pages for Hugepage: virtual address 0x7f4dc0000000 and size 0x40000000 pinned to physical address 0x200000000 (pci_device.cpp:536)\n",
      "2025-11-24 19:31:41.333 | info     |           Metal | Profiler started on device 0 (device_pool.cpp:203)\n",
      "Loading model from: /home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-1.7B/snapshots/70d244cc86ccca08cf5af4e1e306ecf908b1ad5e/model-00001-of-00002.safetensors\n",
      "Loading tensor: model.embed_tokens.weight, shape:Shape([151936, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/tok_emb/weight with shape: Shape([1, 1, 151936, 2048])\n",
      "Loading tensor: model.layers.0.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_0/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.0.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_0/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.0.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_0/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.0.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_0/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.0.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_0/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.0.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_0/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.0.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_0/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.0.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_0/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.0.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_0/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.0.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_0/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.0.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_0/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.1.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_1/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.1.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_1/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.1.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_1/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.1.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_1/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.1.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_1/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.1.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_1/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.1.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_1/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.1.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_1/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.1.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_1/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.1.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_1/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.1.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_1/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.10.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_10/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.10.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_10/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.10.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_10/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.10.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_10/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.10.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_10/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.10.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_10/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.10.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_10/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.10.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_10/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.10.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_10/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.10.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_10/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.10.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_10/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.11.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_11/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.11.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_11/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.11.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_11/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.11.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_11/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.11.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_11/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.11.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_11/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.11.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_11/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.11.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_11/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.11.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_11/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.11.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_11/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.11.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_11/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.12.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_12/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.12.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_12/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.12.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_12/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.12.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_12/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.12.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_12/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.12.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_12/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.12.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_12/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.12.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_12/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.12.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_12/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.12.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_12/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.12.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_12/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.13.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_13/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.13.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_13/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.13.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_13/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.13.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_13/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.13.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_13/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.13.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_13/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.13.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_13/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.13.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_13/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.13.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_13/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.13.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_13/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.13.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_13/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.14.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_14/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.14.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_14/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.14.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_14/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.14.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_14/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.14.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_14/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.14.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_14/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.14.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_14/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.14.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_14/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.14.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_14/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.14.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_14/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.14.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_14/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.15.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_15/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.15.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_15/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.15.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_15/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.15.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_15/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.15.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_15/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.15.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_15/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.15.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_15/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.15.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_15/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.15.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_15/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.15.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_15/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.15.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_15/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.16.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_16/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.16.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_16/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.16.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_16/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.16.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_16/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.16.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_16/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.16.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_16/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.16.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_16/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.16.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_16/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.16.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_16/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.16.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_16/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.16.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_16/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.17.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_17/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.17.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_17/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.17.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_17/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.17.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_17/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.17.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_17/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.17.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_17/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.17.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_17/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.17.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_17/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.17.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_17/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.17.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_17/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.17.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_17/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.18.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_18/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.18.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_18/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.18.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_18/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.18.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_18/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.18.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_18/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.18.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_18/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.18.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_18/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.18.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_18/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.18.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_18/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.18.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_18/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.18.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_18/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.19.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_19/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.19.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_19/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.19.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_19/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.19.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_19/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.19.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_19/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.19.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_19/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.19.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_19/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.19.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_19/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.19.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_19/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.19.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_19/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.19.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_19/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.2.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_2/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.2.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_2/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.2.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_2/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.2.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_2/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.2.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_2/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.2.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_2/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.2.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_2/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.2.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_2/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.2.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_2/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.2.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_2/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.2.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_2/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.20.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_20/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.20.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_20/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.20.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_20/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.20.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_20/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.20.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_20/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.20.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_20/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.20.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_20/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.20.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_20/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.20.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_20/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.20.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_20/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.20.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_20/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.21.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_21/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.21.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_21/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.21.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_21/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.21.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_21/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.21.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_21/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.21.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_21/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.21.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_21/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.21.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_21/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.21.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_21/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.21.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_21/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.21.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_21/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.22.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_22/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.22.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_22/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.22.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_22/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.22.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_22/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.22.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_22/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.22.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_22/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.22.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_22/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.22.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_22/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.22.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_22/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.22.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_22/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.22.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_22/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.23.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_23/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.23.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_23/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.23.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_23/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.23.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_23/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.23.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_23/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.23.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_23/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.23.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_23/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.23.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_23/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.23.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_23/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.23.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_23/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.23.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_23/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.24.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_24/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.24.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_24/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.24.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_24/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.24.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_24/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.24.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_24/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.24.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_24/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.24.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_24/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.24.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_24/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.24.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_24/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.24.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_24/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.24.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_24/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.25.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_25/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.25.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_25/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.25.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_25/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.25.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_25/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.25.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_25/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.25.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_25/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.25.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_25/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.25.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_25/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.25.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_25/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.25.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_25/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.25.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_25/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.26.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_26/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.26.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_26/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.26.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_26/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.26.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_26/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.26.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_26/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.26.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_26/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.26.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_26/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.26.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_26/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.26.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_26/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.26.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_26/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.26.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_26/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.27.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_27/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.27.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_27/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.27.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_27/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.27.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_27/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.27.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_27/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.27.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_27/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.27.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_27/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.27.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_27/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.27.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_27/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.27.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_27/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.27.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_27/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.3.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_3/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.3.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_3/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.3.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_3/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.3.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_3/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.3.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_3/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.3.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_3/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.3.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_3/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.3.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_3/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.3.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_3/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.3.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_3/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.3.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_3/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.4.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_4/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.4.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_4/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.4.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_4/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.4.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_4/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.4.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_4/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.4.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_4/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.4.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_4/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.4.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_4/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.4.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_4/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.4.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_4/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.4.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_4/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.5.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_5/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.5.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_5/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.5.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_5/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.5.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_5/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.5.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_5/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.5.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_5/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.5.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_5/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.5.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_5/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.5.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_5/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.5.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_5/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.5.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_5/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.6.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_6/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.6.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_6/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.6.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_6/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.6.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_6/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.6.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_6/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.6.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_6/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.6.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_6/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.6.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_6/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.6.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_6/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.6.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_6/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.6.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_6/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.7.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_7/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.7.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_7/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.7.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_7/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.7.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_7/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.7.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_7/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.7.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_7/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.7.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_7/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.7.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_7/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.7.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_7/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.7.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_7/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.7.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_7/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.8.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_8/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.8.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_8/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.8.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_8/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.8.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_8/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.8.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_8/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.8.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_8/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.8.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_8/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.8.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_8/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.8.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_8/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.8.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_8/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.8.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_8/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.9.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_9/input_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.9.mlp.down_proj.weight, shape:Shape([2048, 6144]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_9/mlp/w2/weight with shape: Shape([1, 1, 2048, 6144])\n",
      "Loading tensor: model.layers.9.mlp.gate_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_9/mlp/w1/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.9.mlp.up_proj.weight, shape:Shape([6144, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_9/mlp/w3/weight with shape: Shape([1, 1, 6144, 2048])\n",
      "Loading tensor: model.layers.9.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_9/post_attention_layernorm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.9.self_attn.k_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_9/self_attn/k_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.9.self_attn.k_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_9/self_attn/k_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.layers.9.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_9/self_attn/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.9.self_attn.q_norm.weight, shape:Shape([128]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_9/self_attn/q_norm/gamma with shape: Shape([1, 1, 1, 128])\n",
      "Loading tensor: model.layers.9.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_9/self_attn/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.9.self_attn.v_proj.weight, shape:Shape([1024, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/qwen3_block_9/self_attn/v_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Loading tensor: model.norm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: qwen3/ln_fc/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Warning: The following parameters were not used during loading:\n",
      "  - qwen3/fc/weight\n",
      "Total unused parameters: 1\n",
      "Loading model from: /home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-1.7B/snapshots/70d244cc86ccca08cf5af4e1e306ecf908b1ad5e/model-00002-of-00002.safetensors\n",
      "Loading tensor: lm_head.weight, shape:Shape([151936, 2048]), dtype:BF16\n",
      "Using parameter: qwen3/fc/weight with shape: Shape([1, 1, 151936, 2048])\n",
      "Warning: The following parameters were not used during loading:\n",
      "  - qwen3/qwen3_block_9/self_attn/v_linear/weight\n",
      "  - qwen3/qwen3_block_9/self_attn/q_norm/gamma\n",
      "  - qwen3/qwen3_block_9/self_attn/q_linear/weight\n",
      "  - qwen3/qwen3_block_9/self_attn/out_linear/weight\n",
      "  - qwen3/qwen3_block_9/self_attn/k_norm/gamma\n",
      "  - qwen3/qwen3_block_9/self_attn/k_linear/weight\n",
      "  - qwen3/qwen3_block_9/mlp/w2/weight\n",
      "  - qwen3/qwen3_block_9/mlp/w1/weight\n",
      "  - qwen3/qwen3_block_8/self_attn/v_linear/weight\n",
      "  - qwen3/qwen3_block_8/self_attn/q_norm/gamma\n",
      "  - qwen3/qwen3_block_8/self_attn/q_linear/weight\n",
      "  - qwen3/qwen3_block_8/self_attn/out_linear/weight\n",
      "  - qwen3/qwen3_block_8/self_attn/k_norm/gamma\n",
      "  - qwen3/qwen3_block_8/mlp/w3/weight\n",
      "  - qwen3/qwen3_block_7/self_attn/q_norm/gamma\n",
      "  - qwen3/qwen3_block_7/self_attn/k_norm/gamma\n",
      "  - qwen3/qwen3_block_7/self_attn/k_linear/weight\n",
      "  - qwen3/qwen3_block_7/mlp/w3/weight\n",
      "  - qwen3/qwen3_block_7/mlp/w2/weight\n",
      "  - qwen3/qwen3_block_7/self_attn/out_linear/weight\n",
      "  - qwen3/qwen3_block_7/mlp/w1/weight\n",
      "  - qwen3/qwen3_block_8/mlp/w2/weight\n",
      "  - qwen3/qwen3_block_6/self_attn/q_norm/gamma\n",
      "  - qwen3/qwen3_block_6/self_attn/out_linear/weight\n",
      "  - qwen3/qwen3_block_6/mlp/w3/weight\n",
      "  - qwen3/qwen3_block_5/self_attn/q_norm/gamma\n",
      "  - qwen3/qwen3_block_5/self_attn/q_linear/weight\n",
      "  - qwen3/qwen3_block_5/self_attn/out_linear/weight\n",
      "  - qwen3/qwen3_block_5/self_attn/k_norm/gamma\n",
      "  - qwen3/qwen3_block_5/self_attn/k_linear/weight\n",
      "  - qwen3/qwen3_block_4/self_attn/v_linear/weight\n",
      "  - qwen3/qwen3_block_4/self_attn/q_norm/gamma\n",
      "  - qwen3/qwen3_block_4/self_attn/out_linear/weight\n",
      "  - qwen3/qwen3_block_4/mlp/w3/weight\n",
      "  - qwen3/qwen3_block_4/mlp/w2/weight\n",
      "  - qwen3/qwen3_block_15/self_attn/k_linear/weight\n",
      "  - qwen3/qwen3_block_18/self_attn/q_norm/gamma\n",
      "  - qwen3/qwen3_block_23/mlp/w2/weight\n",
      "  - qwen3/qwen3_block_14/self_attn/v_linear/weight\n",
      "  - qwen3/qwen3_block_27/self_attn/k_linear/weight\n",
      "  - qwen3/qwen3_block_0/self_attn/k_norm/gamma\n",
      "  - qwen3/qwen3_block_13/self_attn/v_linear/weight\n",
      "  - qwen3/qwen3_block_0/self_attn/k_linear/weight\n",
      "  - qwen3/qwen3_block_1/post_attention_layernorm/gamma\n",
      "  - qwen3/qwen3_block_12/self_attn/q_norm/gamma\n",
      "  - qwen3/qwen3_block_1/mlp/w1/weight\n",
      "  - qwen3/qwen3_block_10/input_layernorm/gamma\n",
      "  - qwen3/qwen3_block_11/post_attention_layernorm/gamma\n",
      "  - qwen3/qwen3_block_4/mlp/w1/weight\n",
      "  - qwen3/qwen3_block_19/mlp/w1/weight\n",
      "  - qwen3/qwen3_block_11/self_attn/v_li"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m max_sequence_length \u001b[38;5;241m=\u001b[39m tt_model_factory\u001b[38;5;241m.\u001b[39mtransformer_config\u001b[38;5;241m.\u001b[39mmax_sequence_length\n\u001b[1;32m      8\u001b[0m tt_model \u001b[38;5;241m=\u001b[39m tt_model_factory\u001b[38;5;241m.\u001b[39mcreate_model()\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtt_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_safetensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43msafetensors_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# for two safetensor files model we have to interrupt cell here\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "orig_vocab_size = torch_model.vocab_size\n",
    "print(orig_vocab_size)\n",
    "tt_model_factory = TransformerModelFactory(yaml_config)\n",
    "tt_model_factory.transformer_config.vocab_size = orig_vocab_size\n",
    "\n",
    "max_sequence_length = tt_model_factory.transformer_config.max_sequence_length\n",
    "\n",
    "tt_model = tt_model_factory.create_model()\n",
    "tt_model.load_from_safetensors(safetensors_path)\n",
    "\n",
    "# for two safetensor files model we have to interrupt cell here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bdc85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_vocab_size = round_up_to_tile(orig_vocab_size, 32)\n",
    "if orig_vocab_size != padded_vocab_size:\n",
    "    print(f\"Padding vocab size for tilization: original {orig_vocab_size} -> padded {padded_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb047274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3cf900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4d4f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_causal_mask(T: int) -> ttml.autograd.Tensor:\n",
    "    # [1,1,T,T] float32 with 1s for allowed positions (i >= j), else 0\\n\",\n",
    "    m = np.tril(np.ones((T, T), dtype=np.float32))\n",
    "    return ttml.autograd.Tensor.from_numpy(m.reshape(1, 1, T, T), ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)\n",
    "\n",
    "def build_logits_mask(vocab_size: int, padded_vocab_size: int) -> ttml.autograd.Tensor:\n",
    "    logits_mask = np.zeros((1, 1, 1, padded_vocab_size), dtype=np.float32)\n",
    "    logits_mask[:, :, :, vocab_size:] = 1e4\n",
    "    return ttml.autograd.Tensor.from_numpy(logits_mask, ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)   # [1,1,1,T], bfloat16\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40913e88",
   "metadata": {},
   "source": [
    "`generate_with_tt()` uses TT hardware acceleration to generate output from the chosen LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18e6550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_tt(model, prompt_tokens):\n",
    "    import time\n",
    "    \n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(ttml.autograd.GradMode.DISABLED)\n",
    "    model.eval()\n",
    "\n",
    "    logits_mask_tensor = None\n",
    "\n",
    "    if padded_vocab_size != orig_vocab_size:\n",
    "        logits_mask_tensor = build_logits_mask(orig_vocab_size, padded_vocab_size)\n",
    "\n",
    "    causal_mask = build_causal_mask(max_sequence_length)  # [1,1,seq_len,seq_len], float32\n",
    "    padded_prompt_tokens = np.zeros((1, 1, 1, max_sequence_length), \n",
    "                                    dtype=np.uint32)\n",
    "\n",
    "    start_idx = 0\n",
    "\n",
    "    print(\"************************************\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for token_idx in range(OUTPUT_TOKENS):\n",
    "\n",
    "        if len(prompt_tokens) > max_sequence_length:\n",
    "            start_idx = len(prompt_tokens) - max_sequence_length\n",
    "\n",
    "        # padded_prompt_tokens[0, 0, 0, :transformer_cfg[\"max_sequence_length\"]] = 0\n",
    "        padded_prompt_tokens[0, 0, 0, :len(prompt_tokens)] = prompt_tokens[start_idx:]\n",
    "        padded_prompt_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "            padded_prompt_tokens,\n",
    "            ttml.Layout.ROW_MAJOR,\n",
    "            ttml.autograd.DataType.UINT32)  # [1,1,1, max_seq_len], uint32\n",
    "\n",
    "        logits = model(padded_prompt_tensor, causal_mask)  # out=[1,1,seq_len, vocab_size], bf16\n",
    "\n",
    "\n",
    "        next_token_tensor = ttml.ops.sample.sample_op(logits, TEMPERATURE, np.random.randint(low=1e7), logits_mask_tensor)  # out=[1,1,seq_len,1], uint32\n",
    "\n",
    "        next_token_idx = max_sequence_length - 1 if len(prompt_tokens) > max_sequence_length else len(prompt_tokens) - 1\n",
    "        next_token = next_token_tensor.to_numpy().flatten()[next_token_idx]\n",
    "\n",
    "        output = tokenizer.decode(next_token)\n",
    "\n",
    "        prompt_tokens.append(next_token)\n",
    "        print(output, end='', flush=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = OUTPUT_TOKENS / elapsed_time\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {OUTPUT_TOKENS} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7824c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_pytorch(torch_model, prompt_tokens):\n",
    "    import time\n",
    "    import torch.nn.functional as F\n",
    "    from transformers import DynamicCache\n",
    "    \n",
    "    torch_model.eval()\n",
    "    \n",
    "    print(\"************************************\")\n",
    "    # Convert list to tensor and add batch dimension\n",
    "    if isinstance(prompt_tokens, list):\n",
    "        prompt_tokens = torch.tensor([prompt_tokens])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize KV cache using the new DynamicCache API\n",
    "    past_key_values = DynamicCache()\n",
    "    input_ids = prompt_tokens\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(OUTPUT_TOKENS):\n",
    "            # Get model outputs with KV cache\n",
    "            outputs = torch_model(\n",
    "                input_ids=input_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            past_key_values = outputs.past_key_values\n",
    "            \n",
    "            # Get logits for the last token\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            \n",
    "            # Apply temperature and sample\n",
    "            if WITH_SAMPLING and TEMPERATURE > 0:\n",
    "                next_token_logits = next_token_logits / TEMPERATURE\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                # Greedy sampling\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            \n",
    "            # Decode and print the token\n",
    "            output = tokenizer.decode(next_token[0])\n",
    "            print(output, end='', flush=True)\n",
    "            \n",
    "            # For next iteration, only pass the new token (KV cache handles the rest)\n",
    "            input_ids = next_token\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = OUTPUT_TOKENS / elapsed_time\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {OUTPUT_TOKENS} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b17b326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_pytorch_batch(torch_model, prompt_tokens):\n",
    "    \"\"\"Old version: non-streaming batch generation using torch_model.generate()\"\"\"\n",
    "    import time\n",
    "    \n",
    "    torch_model.eval()\n",
    "    \n",
    "    print(\"************************************\")\n",
    "    # Convert list to tensor and add batch dimension\n",
    "    if isinstance(prompt_tokens, list):\n",
    "        prompt_tokens = torch.tensor([prompt_tokens])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = torch_model.generate(\n",
    "            prompt_tokens,\n",
    "            max_new_tokens=OUTPUT_TOKENS,\n",
    "            do_sample=WITH_SAMPLING,  # Enable sampling\n",
    "            temperature=TEMPERATURE,   # Temperature for sampling\n",
    "            num_beams=1  # Use multinomial sampling (standard sampling)\n",
    "        )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = OUTPUT_TOKENS / elapsed_time\n",
    "    \n",
    "    generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    for t in generated_text:\n",
    "        print(t)\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {OUTPUT_TOKENS} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4813faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with torch model:\n",
      "************************************\n",
      " - but I need to generate a lot of data, so I need to make it efficient. How to do that?\n",
      "\n",
      "I have a model that I want to generate data with. I have a model that is trained on a dataset, and I want to generate new data using it. But the problem is that the model is on a CPU, and generating data with it is slow. So I need to make it efficient.\n",
      "\n",
      "I know that using the model in a loop and using the model's forward\n",
      "************************************\n",
      "Generated 100 tokens in 19.08 seconds\n",
      "Performance: 5.24 tokens/second\n",
      "************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_str = \"Generating with pytorch(CPU, might be slow)\"\n",
    "\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with torch model:\")\n",
    "generate_with_pytorch(torch_model, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a42273b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with TT:\n",
      "************************************\n",
      " - but I need to generate a lot of data for training. So I need to generate data in a way that is efficient. How to do that?\n",
      "\n",
      "I have a model that is trained on a dataset, and I want to generate new data for training. The model is a transformer-based model, and the data is in the form of a sequence of tokens. I need to generate a lot of data for training, but the current approach of using the model's generate function is too slow on CPU\n",
      "************************************\n",
      "Generated 100 tokens in 97.49 seconds\n",
      "Performance: 1.03 tokens/second\n",
      "************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_str = \"Generating with pytorch(CPU, might be slow)\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with TT:\")\n",
    "generate_with_tt(tt_model, prompt_tokens.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e485d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb0ead4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59adb313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12548c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c891bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706f1484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7d5111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5863214e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ac2db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b811811",
   "metadata": {},
   "source": [
    "def generate_with_pytorch_no_cache(torch_model, prompt_tokens):\n",
    "    import time\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    torch_model.eval()\n",
    "    \n",
    "    print(\"************************************\")\n",
    "    # Convert list to tensor and add batch dimension\n",
    "    if isinstance(prompt_tokens, list):\n",
    "        input_ids = torch.tensor([prompt_tokens])\n",
    "    else:\n",
    "        input_ids = prompt_tokens\n",
    "    \n",
    "    start_time = time.time()\n",
    "    tokens = 10  # this is very slow\n",
    "    with torch.no_grad():\n",
    "        for i in range(tokens):\n",
    "            # Process the entire sequence every time (no KV cache)\n",
    "            outputs = torch_model(\n",
    "                input_ids=input_ids,\n",
    "                use_cache=False\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Get logits for the last token\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            \n",
    "            # Apply temperature and sample\n",
    "            if WITH_SAMPLING and TEMPERATURE > 0:\n",
    "                next_token_logits = next_token_logits / TEMPERATURE\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                # Greedy sampling\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            \n",
    "            # Decode and print the token\n",
    "            output = tokenizer.decode(next_token[0])\n",
    "            print(output, end='', flush=True)\n",
    "            \n",
    "            # Append the new token to the full sequence for next iteration\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = tokens / elapsed_time\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {tokens} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c4b440",
   "metadata": {},
   "source": [
    "prompt_str = \"Generating with PyTorch, (should be slow, without kv-caching)\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with PyTorch:\")\n",
    "generate_with_pytorch_no_cache(torch_model, prompt_tokens.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8521eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = torch_model.state_dict()\n",
    "for s in sd:\n",
    "    print(s, sd[s].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e868214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = tt_model.parameters()\n",
    "for s in k:\n",
    "    print(s, k[s].shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aa5542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e9744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def apply_rope_permutation(w, num_heads):\n",
    "    \"\"\"\n",
    "    Apply RoPE row permutation to match TT's q_proj weight layout.\n",
    "    TT applies unpermute_proj_rows during loading which interleaves rows within each head.\n",
    "    \"\"\"\n",
    "    rows, cols = w.shape\n",
    "    head_dim = rows // num_heads\n",
    "    \n",
    "    out = np.zeros_like(w)\n",
    "    for h in range(num_heads):\n",
    "        head_start = h * head_dim\n",
    "        half = head_dim // 2\n",
    "        \n",
    "        # Interleave: [0..half-1, half..head_dim-1]  [0, half, 1, half+1, ..., half-1, head_dim-1]\n",
    "        for i in range(half):\n",
    "            out[head_start + 2*i] = w[head_start + i]\n",
    "            out[head_start + 2*i + 1] = w[head_start + half + i]\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def compare_weights_llama(torch_model, tt_model):\n",
    "    \"\"\"\n",
    "    Compare weights between PyTorch model and TT-Metal model.\n",
    "    \n",
    "    Args:\n",
    "        torch_model: PyTorch model (with .state_dict())\n",
    "        tt_model: TT-Metal model (with .parameters())\n",
    "    \"\"\"\n",
    "    \n",
    "    torch_sd = torch_model.state_dict()\n",
    "    tt_params = tt_model.parameters()\n",
    "    \n",
    "    # Get num_heads from torch model config\n",
    "    num_heads = torch_model.config.num_attention_heads\n",
    "    \n",
    "    # Detect weight tying configuration from TT model\n",
    "    has_tok_emb = 'llama/tok_emb/weight' in tt_params\n",
    "    has_fc = 'llama/fc/weight' in tt_params\n",
    "    weight_tying_enabled = not has_tok_emb  # If tok_emb doesn't exist, weight tying is enabled\n",
    "    \n",
    "    # Mapping from PyTorch parameter names to TT-Metal parameter names\n",
    "    pytorch_to_tt_mapping = {\n",
    "        # Final layer norm\n",
    "        'model.norm.weight': 'llama/ln_fc/gamma',\n",
    "    }\n",
    "    \n",
    "    # Add embedding mappings based on weight tying configuration\n",
    "    if weight_tying_enabled:\n",
    "        # Weight tying enabled: both embed_tokens and lm_head use fc/weight\n",
    "        pytorch_to_tt_mapping['model.embed_tokens.weight'] = 'llama/fc/weight'\n",
    "        # lm_head.weight should also map to fc/weight, but we'll skip it to avoid duplicate checks\n",
    "    else:\n",
    "        # Weight tying disabled: separate tok_emb and fc\n",
    "        pytorch_to_tt_mapping['model.embed_tokens.weight'] = 'llama/tok_emb/weight'\n",
    "        pytorch_to_tt_mapping['lm_head.weight'] = 'llama/fc/weight'\n",
    "    \n",
    "    # Add layer-specific mappings\n",
    "    for i in range(50):  # Support up to 50 layers\n",
    "        layer_prefix_pt = f'model.layers.{i}'\n",
    "        layer_prefix_tt = f'llama/llama_block_{i}'\n",
    "        \n",
    "        pytorch_to_tt_mapping.update({\n",
    "            f'{layer_prefix_pt}.input_layernorm.weight': f'{layer_prefix_tt}/attention_norm/gamma',\n",
    "            f'{layer_prefix_pt}.post_attention_layernorm.weight': f'{layer_prefix_tt}/mlp_norm/gamma',\n",
    "            f'{layer_prefix_pt}.self_attn.q_proj.weight': f'{layer_prefix_tt}/attention/q_linear/weight',\n",
    "            f'{layer_prefix_pt}.self_attn.o_proj.weight': f'{layer_prefix_tt}/attention/out_linear/weight',\n",
    "            # k_proj and v_proj are combined into kv_linear in TT\n",
    "            f'{layer_prefix_pt}.mlp.gate_proj.weight': f'{layer_prefix_tt}/mlp/w1/weight',\n",
    "            f'{layer_prefix_pt}.mlp.up_proj.weight': f'{layer_prefix_tt}/mlp/w3/weight',\n",
    "            f'{layer_prefix_pt}.mlp.down_proj.weight': f'{layer_prefix_tt}/mlp/w2/weight',\n",
    "        })\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"WEIGHT COMPARISON: PyTorch vs TT-Metal\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Note: Detected num_heads={num_heads} for RoPE permutation\")\n",
    "    print(f\"Note: Weight tying {'ENABLED' if weight_tying_enabled else 'DISABLED'}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    mismatches = []\n",
    "    matches = []\n",
    "    \n",
    "    for pt_name in torch_sd.keys():\n",
    "        if 'bias' in pt_name:\n",
    "            continue  # Skip bias parameters\n",
    "            \n",
    "        pt_tensor = torch_sd[pt_name]\n",
    "        pt_shape = tuple(pt_tensor.shape)\n",
    "        \n",
    "        # Handle k_proj and v_proj specially (they're combined in TT)\n",
    "        if '.self_attn.k_proj.weight' in pt_name or '.self_attn.v_proj.weight' in pt_name:\n",
    "            layer_idx = pt_name.split('.')[2]\n",
    "            tt_name = f'llama/llama_block_{layer_idx}/attention/kv_linear/weight'\n",
    "            \n",
    "            if tt_name in tt_params:\n",
    "                tt_tensor_np = tt_params[tt_name].to_numpy()\n",
    "                tt_shape = tt_tensor_np.shape\n",
    "                \n",
    "                # k_proj and v_proj are concatenated in kv_linear\n",
    "                # Expected: k_proj [512, 2048] + v_proj [512, 2048] = kv_linear [1, 1, 1024, 2048]\n",
    "                if '.self_attn.k_proj.weight' in pt_name:\n",
    "                    print(f\"\\n{pt_name}\")\n",
    "                    print(f\"  PyTorch: {pt_shape}\")\n",
    "                    print(f\"  TT (kv combined): {tt_shape}\")\n",
    "                    print(f\"  Status: K and V are combined in TT as kv_linear\")\n",
    "            continue\n",
    "        \n",
    "        # Get corresponding TT parameter name\n",
    "        tt_name = pytorch_to_tt_mapping.get(pt_name)\n",
    "        if not tt_name:\n",
    "            continue\n",
    "            \n",
    "        if tt_name not in tt_params:\n",
    "            print(f\"\\n MISSING: {pt_name} -> {tt_name}\")\n",
    "            print(f\"   PyTorch shape: {pt_shape}\")\n",
    "            mismatches.append((pt_name, \"MISSING IN TT\"))\n",
    "            continue\n",
    "        \n",
    "        # Get TT tensor\n",
    "        tt_tensor_np = tt_params[tt_name].to_numpy()\n",
    "        tt_shape = tt_tensor_np.shape\n",
    "        \n",
    "        # Remove batch dimensions [1, 1, ...] from TT tensor\n",
    "        tt_shape_no_batch = tt_shape[2:] if len(tt_shape) == 4 else tt_shape\n",
    "        \n",
    "        # Compare shapes\n",
    "        pt_numpy = pt_tensor.cpu().float().numpy()  # Convert to float32 for numpy compatibility\n",
    "        \n",
    "        # Special handling for q_proj: TT applies RoPE row permutation during loading\n",
    "        is_q_proj = '.self_attn.q_proj.weight' in pt_name\n",
    "        if is_q_proj and len(pt_shape) == 2 and pt_shape[0] % num_heads == 0:\n",
    "            pt_numpy = apply_rope_permutation(pt_numpy, num_heads)\n",
    "        \n",
    "        # For layer norms: PT (N,) vs TT (1, N) - both are fine, just broadcasting\n",
    "        # Check if PT is 1D and TT has leading 1s that can be squeezed\n",
    "        if len(pt_shape) == 1 and len(tt_shape_no_batch) == 2 and tt_shape_no_batch[0] == 1:\n",
    "            tt_shape_no_batch = (tt_shape_no_batch[1],)  # Squeeze leading 1\n",
    "        \n",
    "        # Check if shapes match (with or without transpose)\n",
    "        shape_match = (pt_shape == tt_shape_no_batch) or (pt_shape == tt_shape_no_batch[::-1])\n",
    "        \n",
    "        if shape_match:\n",
    "            # Check actual values\n",
    "            # Reshape TT data to match PT shape (handle batch dims and potential squeezing)\n",
    "            if len(tt_shape) == 4:\n",
    "                tt_data = tt_tensor_np.reshape(tt_shape[2:])  # Remove [1,1,...] batch dims\n",
    "            else:\n",
    "                tt_data = tt_tensor_np.reshape(tt_shape)\n",
    "            \n",
    "            # Squeeze if needed for 1D comparisons\n",
    "            tt_data = tt_data.squeeze()\n",
    "            pt_numpy_squeezed = pt_numpy.squeeze()\n",
    "            \n",
    "            # Handle transpose if needed\n",
    "            if pt_numpy_squeezed.shape != tt_data.shape and len(tt_data.shape) == 2:\n",
    "                tt_data = tt_data.T\n",
    "            \n",
    "            diff = np.abs(pt_numpy_squeezed - tt_data).max()\n",
    "            rel_diff = diff / (np.abs(pt_numpy_squeezed).max() + 1e-8)\n",
    "            \n",
    "            status = \"\" if diff < 1e-3 else \"\"\n",
    "            note = \" (after RoPE permutation)\" if is_q_proj else \"\"\n",
    "            print(f\"\\n{status} {pt_name}{note}\")\n",
    "            print(f\"  PyTorch: {pt_shape}\")\n",
    "            print(f\"  TT:      {tt_shape} -> {tt_shape_no_batch}\")\n",
    "            print(f\"  Max diff: {diff:.6f}, Rel diff: {rel_diff:.6f}\")\n",
    "            \n",
    "            if diff < 1e-3:\n",
    "                matches.append(pt_name)\n",
    "            else:\n",
    "                mismatches.append((pt_name, f\"VALUE_DIFF={diff:.6f}\"))\n",
    "        else:\n",
    "            print(f\"\\n SHAPE MISMATCH: {pt_name}\")\n",
    "            print(f\"  PyTorch: {pt_shape}\")\n",
    "            print(f\"  TT:      {tt_shape} -> {tt_shape_no_batch}\")\n",
    "            mismatches.append((pt_name, f\"SHAPE: PT={pt_shape} vs TT={tt_shape_no_batch}\"))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"SUMMARY: {len(matches)} matches, {len(mismatches)} mismatches\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if mismatches:\n",
    "        print(\"\\n MISMATCHES:\")\n",
    "        for name, issue in mismatches:\n",
    "            print(f\"  - {name}: {issue}\")\n",
    "    \n",
    "    return matches, mismatches\n",
    "\n",
    "\n",
    "# Usage example (commented out):\n",
    "matches, mismatches = compare_weights_llama(torch_model, tt_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7af5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Qwen3 Weight Comparison Script\n",
    "\n",
    "Compares weights between PyTorch Qwen3 model and TT-Metal Qwen3 model.\n",
    "\n",
    "Key Qwen3 features:\n",
    "- Explicit head_dim (128 for 0.6B model)\n",
    "- Q/K normalization for numerical stability (CRITICAL!)\n",
    "- Attention dimension != embedding dimension\n",
    "- Weight tying enabled by default\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def apply_rope_permutation(w, num_heads):\n",
    "    \"\"\"\n",
    "    Apply RoPE row permutation to match TT's q_proj/k_proj weight layout.\n",
    "    TT applies unpermute_proj_rows during loading which interleaves rows within each head.\n",
    "    \n",
    "    This reorders: [0..D/2-1, D/2..D-1]  [0, D/2, 1, D/2+1, ..., D/2-1, D-1]\n",
    "    \"\"\"\n",
    "    rows, cols = w.shape\n",
    "    head_dim = rows // num_heads\n",
    "    \n",
    "    if head_dim % 2 != 0:\n",
    "        raise ValueError(f\"Head dimension {head_dim} must be even for RoPE permutation\")\n",
    "    \n",
    "    out = np.zeros_like(w)\n",
    "    for h in range(num_heads):\n",
    "        head_start = h * head_dim\n",
    "        half = head_dim // 2\n",
    "        \n",
    "        # Interleave: first half and second half of each head\n",
    "        for i in range(half):\n",
    "            out[head_start + 2*i] = w[head_start + i]\n",
    "            out[head_start + 2*i + 1] = w[head_start + half + i]\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def compare_qwen3_weights(torch_model, tt_model, verbose=True):\n",
    "    \"\"\"\n",
    "    Compare weights between PyTorch Qwen3 model and TT-Metal Qwen3 model.\n",
    "    \n",
    "    Qwen3-specific features:\n",
    "    - Q projection: [2048, 1024] (projects UP to attention space)\n",
    "    - K projection: [1024, 1024] (num_kv_heads * head_dim)\n",
    "    - V projection: [1024, 1024] (num_kv_heads * head_dim)\n",
    "    - O projection: [1024, 2048] (projects DOWN to embedding space)\n",
    "    - Q/K normalization: RMSNorm on head_dim (128) - CRITICAL!\n",
    "    \n",
    "    Args:\n",
    "        torch_model: PyTorch Qwen3 model (with .state_dict())\n",
    "        tt_model: TT-Metal Qwen3 model (with .parameters())\n",
    "        verbose: Print detailed comparison (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (matches, mismatches, missing_in_tt)\n",
    "    \"\"\"\n",
    "    \n",
    "    torch_sd = torch_model.state_dict()\n",
    "    tt_params = tt_model.parameters()\n",
    "    \n",
    "    # Get configuration from torch model\n",
    "    num_heads = torch_model.config.num_attention_heads\n",
    "    num_kv_heads = torch_model.config.num_key_value_heads\n",
    "    head_dim = getattr(torch_model.config, 'head_dim', 128)\n",
    "    hidden_size = torch_model.config.hidden_size\n",
    "    \n",
    "    # Detect weight tying configuration from TT model\n",
    "    has_tok_emb = 'qwen3/tok_emb/weight' in tt_params\n",
    "    has_fc = 'qwen3/fc/weight' in tt_params\n",
    "    weight_tying_enabled = not has_tok_emb  # If tok_emb doesn't exist, weight tying is enabled\n",
    "    \n",
    "    # Mapping from PyTorch parameter names to TT-Metal parameter names\n",
    "    pytorch_to_tt_mapping = {\n",
    "        # Final layer norm\n",
    "        'model.norm.weight': 'qwen3/ln_fc/gamma',\n",
    "    }\n",
    "    \n",
    "    # Add embedding mappings based on weight tying configuration\n",
    "    if weight_tying_enabled:\n",
    "        pytorch_to_tt_mapping['model.embed_tokens.weight'] = 'qwen3/fc/weight'\n",
    "        pytorch_to_tt_mapping['lm_head.weight'] = 'qwen3/fc/weight'  # Both tied to fc/weight\n",
    "    else:\n",
    "        pytorch_to_tt_mapping['model.embed_tokens.weight'] = 'qwen3/tok_emb/weight'\n",
    "        pytorch_to_tt_mapping['lm_head.weight'] = 'qwen3/fc/weight'\n",
    "    \n",
    "    # Add layer-specific mappings\n",
    "    for i in range(50):  # Support up to 50 layers\n",
    "        layer_prefix_pt = f'model.layers.{i}'\n",
    "        layer_prefix_tt = f'qwen3/qwen3_block_{i}'\n",
    "        \n",
    "        pytorch_to_tt_mapping.update({\n",
    "            # Layer norms\n",
    "            f'{layer_prefix_pt}.input_layernorm.weight': f'{layer_prefix_tt}/attention_norm/gamma',\n",
    "            f'{layer_prefix_pt}.post_attention_layernorm.weight': f'{layer_prefix_tt}/mlp_norm/gamma',\n",
    "            \n",
    "            # Attention projections - SEPARATE Q, K, V (not combined!)\n",
    "            f'{layer_prefix_pt}.self_attn.q_proj.weight': f'{layer_prefix_tt}/attention/q_linear/weight',\n",
    "            f'{layer_prefix_pt}.self_attn.k_proj.weight': f'{layer_prefix_tt}/attention/k_linear/weight',\n",
    "            f'{layer_prefix_pt}.self_attn.v_proj.weight': f'{layer_prefix_tt}/attention/v_linear/weight',\n",
    "            f'{layer_prefix_pt}.self_attn.o_proj.weight': f'{layer_prefix_tt}/attention/out_linear/weight',\n",
    "            \n",
    "            # Q/K norms - CRITICAL for Qwen3 numerical stability!\n",
    "            f'{layer_prefix_pt}.self_attn.q_norm.weight': f'{layer_prefix_tt}/attention/q_norm/gamma',\n",
    "            f'{layer_prefix_pt}.self_attn.k_norm.weight': f'{layer_prefix_tt}/attention/k_norm/gamma',\n",
    "            \n",
    "            # MLP projections\n",
    "            f'{layer_prefix_pt}.mlp.gate_proj.weight': f'{layer_prefix_tt}/mlp/w1/weight',\n",
    "            f'{layer_prefix_pt}.mlp.up_proj.weight': f'{layer_prefix_tt}/mlp/w3/weight',\n",
    "            f'{layer_prefix_pt}.mlp.down_proj.weight': f'{layer_prefix_tt}/mlp/w2/weight',\n",
    "        })\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"WEIGHT COMPARISON: PyTorch Qwen3 vs TT-Metal Qwen3\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Model Configuration:\")\n",
    "        print(f\"  - num_attention_heads: {num_heads}\")\n",
    "        print(f\"  - num_key_value_heads: {num_kv_heads}\")\n",
    "        print(f\"  - head_dim: {head_dim}\")\n",
    "        print(f\"  - hidden_size: {hidden_size}\")\n",
    "        print(f\"  - attention_output_dim: {num_heads * head_dim}\")\n",
    "        print(f\"  - weight_tying: {'ENABLED' if weight_tying_enabled else 'DISABLED'}\")\n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    mismatches = []\n",
    "    matches = []\n",
    "    missing_in_tt = []\n",
    "    \n",
    "    for pt_name in torch_sd.keys():\n",
    "        if 'bias' in pt_name:\n",
    "            continue  # Skip bias parameters (Qwen3 has no biases)\n",
    "            \n",
    "        pt_tensor = torch_sd[pt_name]\n",
    "        pt_shape = tuple(pt_tensor.shape)\n",
    "        \n",
    "        # Special handling for k_proj: apply RoPE permutation\n",
    "        if '.self_attn.k_proj.weight' in pt_name:\n",
    "            layer_idx = pt_name.split('.')[2]\n",
    "            tt_name = f'qwen3/qwen3_block_{layer_idx}/attention/k_linear/weight'\n",
    "            \n",
    "            if tt_name in tt_params:\n",
    "                pt_numpy = pt_tensor.cpu().float().numpy()\n",
    "                \n",
    "                # Apply RoPE permutation to K projection\n",
    "                pt_numpy_permuted = apply_rope_permutation(pt_numpy, num_kv_heads)\n",
    "                \n",
    "                tt_tensor_np = tt_params[tt_name].to_numpy()\n",
    "                tt_shape = tt_tensor_np.shape\n",
    "                tt_data = tt_tensor_np.reshape(tt_shape[2:]) if len(tt_shape) == 4 else tt_tensor_np\n",
    "                \n",
    "                # Handle transpose if needed\n",
    "                if pt_numpy_permuted.shape != tt_data.shape:\n",
    "                    tt_data = tt_data.T\n",
    "                \n",
    "                diff = np.abs(pt_numpy_permuted - tt_data).max()\n",
    "                rel_diff = diff / (np.abs(pt_numpy_permuted).max() + 1e-8)\n",
    "                \n",
    "                status = \"\" if diff < 1e-4 else \"\"\n",
    "                if verbose:\n",
    "                    print(f\"\\n{status} {pt_name} (after RoPE permutation)\")\n",
    "                    print(f\"  PyTorch: {pt_shape}\")\n",
    "                    print(f\"  TT:      {tt_shape}\")\n",
    "                    print(f\"  Max diff: {diff:.6f}, Rel diff: {rel_diff:.6f}\")\n",
    "                \n",
    "                if diff < 1e-3:\n",
    "                    matches.append(pt_name)\n",
    "                else:\n",
    "                    mismatches.append((pt_name, f\"K_DIFF={diff:.6f}\"))\n",
    "            else:\n",
    "                missing_in_tt.append((pt_name, tt_name))\n",
    "            continue\n",
    "        \n",
    "        # Get corresponding TT parameter name\n",
    "        tt_name = pytorch_to_tt_mapping.get(pt_name)\n",
    "        if not tt_name:\n",
    "            if verbose:\n",
    "                print(f\"\\n NOT MAPPED: {pt_name}\")\n",
    "                print(f\"  PyTorch: {pt_shape}\")\n",
    "                print(f\"  Status: No TT equivalent defined in mapping\")\n",
    "            missing_in_tt.append((pt_name, \"NOT_MAPPED\"))\n",
    "            continue\n",
    "            \n",
    "        if tt_name not in tt_params:\n",
    "            if verbose:\n",
    "                print(f\"\\n MISSING IN TT: {pt_name} -> {tt_name}\")\n",
    "                print(f\"   PyTorch shape: {pt_shape}\")\n",
    "            missing_in_tt.append((pt_name, tt_name))\n",
    "            continue\n",
    "        \n",
    "        # Get TT tensor\n",
    "        tt_tensor_np = tt_params[tt_name].to_numpy()\n",
    "        tt_shape = tt_tensor_np.shape\n",
    "        \n",
    "        # Remove batch dimensions [1, 1, ...] from TT tensor\n",
    "        tt_shape_no_batch = tt_shape[2:] if len(tt_shape) == 4 else tt_shape\n",
    "        \n",
    "        # Compare shapes\n",
    "        pt_numpy = pt_tensor.cpu().float().numpy()\n",
    "        \n",
    "        # Check what type of parameter this is\n",
    "        is_q_proj = '.self_attn.q_proj.weight' in pt_name\n",
    "        is_q_norm = '.self_attn.q_norm.weight' in pt_name\n",
    "        is_k_norm = '.self_attn.k_norm.weight' in pt_name\n",
    "        is_v_proj = '.self_attn.v_proj.weight' in pt_name\n",
    "        \n",
    "        # Apply RoPE permutation for Q projection\n",
    "        if is_q_proj:\n",
    "            if len(pt_shape) == 2 and pt_shape[0] == num_heads * head_dim:\n",
    "                pt_numpy = apply_rope_permutation(pt_numpy, num_heads)\n",
    "        \n",
    "        # For layer norms: PT (N,) vs TT (1, N) or (1, 1, 1, N) - handle squeezing\n",
    "        if len(pt_shape) == 1:\n",
    "            if len(tt_shape_no_batch) == 2 and tt_shape_no_batch[0] == 1:\n",
    "                tt_shape_no_batch = (tt_shape_no_batch[1],)\n",
    "            elif len(tt_shape_no_batch) == 1:\n",
    "                pass  # Already 1D\n",
    "        \n",
    "        # Check if shapes match (with or without transpose)\n",
    "        shape_match = (pt_shape == tt_shape_no_batch) or (pt_shape == tt_shape_no_batch[::-1])\n",
    "        \n",
    "        if shape_match:\n",
    "            # Check actual values\n",
    "            if len(tt_shape) == 4:\n",
    "                tt_data = tt_tensor_np.reshape(tt_shape[2:])\n",
    "            else:\n",
    "                tt_data = tt_tensor_np.reshape(tt_shape)\n",
    "            \n",
    "            tt_data = tt_data.squeeze()\n",
    "            pt_numpy_squeezed = pt_numpy.squeeze()\n",
    "            \n",
    "            # Handle transpose if needed\n",
    "            if pt_numpy_squeezed.shape != tt_data.shape and len(tt_data.shape) == 2:\n",
    "                tt_data = tt_data.T\n",
    "            \n",
    "            diff = np.abs(pt_numpy_squeezed - tt_data).max()\n",
    "            rel_diff = diff / (np.abs(pt_numpy_squeezed).max() + 1e-8)\n",
    "            \n",
    "            status = \"\" if diff < 1e-3 else \"\"\n",
    "            note = \"\"\n",
    "            if is_q_proj:\n",
    "                note = \" (after RoPE permutation)\"\n",
    "            elif is_q_norm:\n",
    "                note = \" [CRITICAL Q/K NORM]\"\n",
    "            elif is_k_norm:\n",
    "                note = \" [CRITICAL Q/K NORM]\"\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\n{status} {pt_name}{note}\")\n",
    "                print(f\"  PyTorch: {pt_shape}\")\n",
    "                print(f\"  TT:      {tt_shape} -> {tt_shape_no_batch}\")\n",
    "                print(f\"  Max diff: {diff:.6f}, Rel diff: {rel_diff:.6f}\")\n",
    "            \n",
    "            if diff < 1e-3:\n",
    "                matches.append(pt_name)\n",
    "            else:\n",
    "                mismatches.append((pt_name, f\"VALUE_DIFF={diff:.6f}\"))\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"\\n SHAPE MISMATCH: {pt_name}\")\n",
    "                print(f\"  PyTorch: {pt_shape}\")\n",
    "                print(f\"  TT:      {tt_shape} -> {tt_shape_no_batch}\")\n",
    "            mismatches.append((pt_name, f\"SHAPE: PT={pt_shape} vs TT={tt_shape_no_batch}\"))\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"SUMMARY: {len(matches)} matches, {len(mismatches)} mismatches, {len(missing_in_tt)} missing in TT\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        if missing_in_tt:\n",
    "            print(f\"\\n MISSING IN TT ({len(missing_in_tt)}):\")\n",
    "            for pt_name, tt_name in missing_in_tt:\n",
    "                print(f\"  - {pt_name} -> {tt_name}\")\n",
    "        \n",
    "        if mismatches:\n",
    "            print(\"\\n MISMATCHES:\")\n",
    "            for name, issue in mismatches:\n",
    "                print(f\"  - {name}: {issue}\")\n",
    "        \n",
    "        if len(mismatches) == 0 and len(missing_in_tt) == 0:\n",
    "            print(\"\\n ALL WEIGHTS MATCH PERFECTLY!\")\n",
    "            print(f\" {len(matches)} parameters validated\")\n",
    "            print(\" Q/K normalization layers loaded correctly\")\n",
    "            print(\" Qwen3 model is production-ready!\")\n",
    "    \n",
    "    return matches, mismatches, missing_in_tt\n",
    "\n",
    "\n",
    "matches, mismatches, missing = compare_qwen3_weights(torch_model, tt_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525920ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(torch_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c006c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1274ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d4984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade8881f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ccd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "import sys\n",
    "\n",
    "def convert_notebook_to_py(notebook_path, output_path):\n",
    "    \n",
    "    with open(notebook_path) as ff:\n",
    "        nb_in = nbformat.read(ff, nbformat.NO_CONVERT)\n",
    "    \n",
    "    source = \"\"\n",
    "    for cell in nb_in['cells']:\n",
    "        if cell['cell_type'] == 'code':\n",
    "            # cell['source'] can be a string or a list of strings\n",
    "            cell_source = cell['source']\n",
    "            \n",
    "            # Convert to list of lines if it's a string\n",
    "            if isinstance(cell_source, str):\n",
    "                lines = cell_source.split('\\n')\n",
    "            else:\n",
    "                # It's already a list, but may contain newlines within elements\n",
    "                lines = []\n",
    "                for item in cell_source:\n",
    "                    lines.extend(item.split('\\n'))\n",
    "            \n",
    "            # Filter out lines starting with '%' (magic commands)\n",
    "            filtered_lines = [line for line in lines if not line.strip().startswith('%')]\n",
    "            \n",
    "            # Join the lines back together\n",
    "            cell_code = '\\n'.join(filtered_lines)\n",
    "            \n",
    "            # Add to source with a newline separator\n",
    "            if cell_code.strip():  # Only add non-empty cells\n",
    "                source = source + '\\n' + cell_code\n",
    "    \n",
    "    # Write to output file\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(source)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0893d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_notebook_to_py('llm_inference_new.ipynb', 'llm_inference_new.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e26ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f868e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71842a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
