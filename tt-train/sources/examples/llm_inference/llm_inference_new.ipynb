{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f838d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TT_METAL_HOME=/home/ubuntu/tt-metal\n",
      "env: TT_METAL_RUNTIME_ROOT=/home/ubuntu/tt-metal\n"
     ]
    }
   ],
   "source": [
    "%env TT_METAL_HOME=/home/ubuntu/tt-metal\n",
    "%env TT_METAL_RUNTIME_ROOT=/home/ubuntu/tt-metal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb31718",
   "metadata": {},
   "source": [
    "%env TT_LOGGER_LEVEL=debug\n",
    "%env TT_LOGGER_TYPES=Op\n",
    "%env TTNN_ENABLE_LOGGING=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f7bc745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-18 06:20:20.319 | DEBUG    | ttnn:<module>:77 - Initial ttnn.CONFIG:\n",
      "Config{cache_path=/home/ubuntu/.cache/ttnn,model_cache_path=/home/ubuntu/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=true,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_should_raise_exception=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n"
     ]
    }
   ],
   "source": [
    "import ttml\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c43ffe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttnn import Layout, DataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6545c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TOKENS = 100\n",
    "WITH_SAMPLING = True\n",
    "TEMPERATURE = 0.0\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd831b0",
   "metadata": {},
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\" \n",
    "CONFIG = \"training_shakespeare_llama3_2_1B_fixed.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e1948c",
   "metadata": {},
   "source": [
    "model_id =  \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "CONFIG = \"training_shakespeare_tinyllama.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def0e6e",
   "metadata": {},
   "source": [
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "CONFIG = \"training_shakespeare_llama3_2_3B.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a19149b",
   "metadata": {},
   "source": [
    "model_id =  \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "CONFIG = \"training_shakespeare_llama3_8B_tp.yaml\" # OOM on 12 GB, sucesfully loaded weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abaa683",
   "metadata": {},
   "source": [
    "model_id = \"Qwen/Qwen3-1.7B\" \n",
    "CONFIG = \"training_shakespeare_qwen3_1_7B.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93d1882",
   "metadata": {},
   "source": [
    "model_id = \"Qwen/Qwen3-4B\"\n",
    "CONFIG = \"training_shakespeare_qwen3_4B.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05e81e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen3-0.6B\" \n",
    "CONFIG = \"training_shakespeare_qwen3_0_6B.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df6d6bf9-a010-4206-b004-807ee500d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "import numpy as np  # For numpy arrays\n",
    "from dataclasses import dataclass # For configuration classes\n",
    "from huggingface_hub import hf_hub_download # To download safetensors from Hugging Face\n",
    "from transformers import AutoTokenizer\n",
    "from yaml import safe_load # To read YAML configs\n",
    "from pathlib import Path\n",
    "\n",
    "import ttml\n",
    "from ttml.common.config import get_training_config, load_config, TransformerConfig\n",
    "from ttml.common.utils import set_seed, round_up_to_tile\n",
    "from ttml.common.model_factory import TransformerModelFactory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2372b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6991716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ttml.common.config.TrainingConfig at 0x7f99ac5d9330>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_training_config(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55071c7a-74fe-4f56-8f33-e45ffe699c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from Hugging Face and the transformer config from YAML\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "training_config = get_training_config(CONFIG)\n",
    "model_yaml = load_config(training_config.model_config, configs_root=os.getcwd() + '/../../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e7883f6-f8e6-436b-8908-62c8a26e1e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "safetensors_path = hf_hub_download(repo_id=model_id, filename=\"config.json\")\n",
    "safetensors_path = safetensors_path.replace(\"config.json\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f77e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "torch.manual_seed(SEED)\n",
    "torch_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f405407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151643, 151936, 151936)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size, torch_model.state_dict()['model.embed_tokens.weight'].shape[0], torch_model.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50d8ce12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torch_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf74d9ac-7afd-4c6f-887e-b793c6e44c18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151936\n"
     ]
    }
   ],
   "source": [
    "orig_vocab_size = torch_model.vocab_size\n",
    "print(orig_vocab_size)\n",
    "tt_model_factory = TransformerModelFactory(model_yaml)\n",
    "tt_model_factory.transformer_config.vocab_size = orig_vocab_size\n",
    "\n",
    "max_sequence_length = tt_model_factory.transformer_config.max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56dff343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ad0b618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transformer_config': {'model_type': 'qwen3',\n",
       "  'num_heads': 16,\n",
       "  'num_groups': 8,\n",
       "  'embedding_dim': 1024,\n",
       "  'head_dim': 128,\n",
       "  'intermediate_dim': 3072,\n",
       "  'dropout_prob': 0.0,\n",
       "  'num_blocks': 28,\n",
       "  'weight_tying': 'enabled',\n",
       "  'vocab_size': 151936,\n",
       "  'max_sequence_length': 2048,\n",
       "  'runner_type': 'memory_efficient',\n",
       "  'theta': 1000000.0,\n",
       "  'rms_norm_eps': 1e-06}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "460b4c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff6ac0b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3 configuration:\n",
      "    Vocab size: 151936\n",
      "    Max sequence length: 2048\n",
      "    Embedding dim (hidden_size): 1024\n",
      "    Head dim: 128\n",
      "    Attention output dim: 2048\n",
      "    Intermediate dim: 3072\n",
      "    Num heads: 16\n",
      "    Num groups (KV heads): 8\n",
      "    Dropout probability: 0\n",
      "    Num blocks: 28\n",
      "    Positional embedding type: RoPE\n",
      "    Runner type: Memory efficient\n",
      "    Weight tying: Enabled\n",
      "    Theta: 1000000\n",
      "    RMSNorm epsilon: 1e-06\n",
      "2026-01-18 06:20:28.652 | info     |             UMD | Established firmware bundle version: 19.4.2 (topology_discovery.cpp:368)\n",
      "2026-01-18 06:20:28.652 | info     |             UMD | Firmware bundle version 19.4.2 on the system is newer than the latest fully tested version 19.4.0 for wormhole_b0 architecture. Newest features may not be supported. (topology_discovery.cpp:394)\n",
      "2026-01-18 06:20:28.653 | info     |          Device | Opening user mode device driver (tt_cluster.cpp:223)\n",
      "2026-01-18 06:20:28.656 | info     |             UMD | Established firmware bundle version: 19.4.2 (topology_discovery.cpp:368)\n",
      "2026-01-18 06:20:28.656 | info     |             UMD | Firmware bundle version 19.4.2 on the system is newer than the latest fully tested version 19.4.0 for wormhole_b0 architecture. Newest features may not be supported. (topology_discovery.cpp:394)\n",
      "2026-01-18 06:20:28.677 | info     |             UMD | Established firmware bundle version: 19.4.2 (topology_discovery.cpp:368)\n",
      "2026-01-18 06:20:28.677 | info     |             UMD | Firmware bundle version 19.4.2 on the system is newer than the latest fully tested version 19.4.0 for wormhole_b0 architecture. Newest features may not be supported. (topology_discovery.cpp:394)\n",
      "2026-01-18 06:20:28.677 | info     |             UMD | Harvesting masks for chip 0 tensix: 0x1 dram: 0x0 eth: 0x0 pcie: 0x0 l2cpu: 0x0 (cluster.cpp:338)\n",
      "2026-01-18 06:20:28.707 | warning  |             UMD | init_detect_tt_device_numanodes(): Could not determine NumaNodeSet for TT device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:564)\n",
      "2026-01-18 06:20:28.707 | warning  |             UMD | Could not find NumaNodeSet for TT Device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:173)\n",
      "2026-01-18 06:20:28.708 | warning  |             UMD | bind_area_memory_nodeset(): Unable to determine TT Device to NumaNode mapping for physical_device_id: 0. Skipping membind. (cpuset_lib.cpp:450)\n",
      "2026-01-18 06:20:28.708 | warning  |             UMD | ---- ttSiliconDevice::init_hugepage: bind_area_to_memory_nodeset() failed (physical_device_id: 0 ch: 0). Hugepage allocation is not on NumaNode matching TT Device. Side-Effect is decreased Device->Host perf (Issue #893). (silicon_sysmem_manager.cpp:179)\n",
      "2026-01-18 06:20:28.708 | info     |             UMD | Opening local chip ids/PCIe ids: {0}/[0] and remote chip ids {} (cluster.cpp:185)\n",
      "2026-01-18 06:20:28.709 | info     |             UMD | IOMMU: disabled (cluster.cpp:161)\n",
      "2026-01-18 06:20:28.709 | info     |             UMD | KMD version: 2.6.1 (cluster.cpp:164)\n",
      "2026-01-18 06:20:28.710 | info     |             UMD | Starting devices in cluster (cluster.cpp:963)\n",
      "2026-01-18 06:20:28.711 | info     |             UMD | Mapped hugepage 0x240000000 to NOC address 0x800000000 (silicon_sysmem_manager.cpp:207)\n",
      "2026-01-18 06:20:28.735 | info     |     Distributed | Using auto discovery to generate mesh graph. (metal_context.cpp:859)\n",
      "2026-01-18 06:20:28.735 | info     |     Distributed | Constructing control plane using auto-discovery (no mesh graph descriptor). (metal_context.cpp:836)\n",
      "2026-01-18 06:20:28.735 | info     |          Fabric | TopologyMapper mapping start (mesh=0): n_log=1, n_phys=1, log_deg_hist={0:1}, phys_deg_hist={0:1} (topology_mapper_utils.cpp:171)\n",
      "2026-01-18 06:20:28.735 | info     |          Fabric | TopologyMapper mapping start (mesh=0): n_log=1, n_phys=1, log_deg_hist={0:1}, phys_deg_hist={0:1} (topology_mapper_utils.cpp:171)\n",
      "Model created: 0.20549798011779785\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "tt_model = tt_model_factory.create_model()\n",
    "print(f\"Model created: {time() - start_time}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb62dae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 5.945973634719849\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "tt_model.load_from_safetensors(safetensors_path)\n",
    "print(f\"Model loaded: {time() - start_time}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bdc85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_vocab_size = round_up_to_tile(orig_vocab_size, 32)\n",
    "if orig_vocab_size != padded_vocab_size:\n",
    "    print(f\"Padding vocab size for tilization: original {orig_vocab_size} -> padded {padded_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4d4f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_causal_mask(T: int) -> ttml.autograd.Tensor:\n",
    "    # [1,1,T,T] float32 with 1s for allowed positions (i >= j), else 0\\n\",\n",
    "    m = np.tril(np.ones((T, T), dtype=np.float32))\n",
    "    return ttml.autograd.Tensor.from_numpy(m.reshape(1, 1, T, T), Layout.TILE, DataType.BFLOAT16)\n",
    "\n",
    "def build_logits_mask(vocab_size: int, padded_vocab_size: int) -> ttml.autograd.Tensor:\n",
    "    logits_mask = np.zeros((1, 1, 1, padded_vocab_size), dtype=np.float32)\n",
    "    logits_mask[:, :, :, vocab_size:] = 1e4\n",
    "    return ttml.autograd.Tensor.from_numpy(logits_mask, Layout.TILE, DataType.BFLOAT16)   # [1,1,1,T], bfloat16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77f8e7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_SIZE = 32\n",
    "\n",
    "def round_up(value: int) -> int:\n",
    "    return ((value + TILE_SIZE - 1) // TILE_SIZE) * TILE_SIZE\n",
    "\n",
    "def create_causal_mask_kv(query_len: int, prompt_len: int = 0) -> ttml.autograd.Tensor:\n",
    "    whole_len = prompt_len + query_len\n",
    "    padded_q = round_up(query_len)\n",
    "    padded_w = round_up(whole_len)\n",
    "    mask = np.zeros((padded_q, padded_w), dtype=np.float32)\n",
    "    for i in range(query_len):\n",
    "        for j in range(prompt_len + i + 1):\n",
    "            mask[i, j] = 1.0\n",
    "    return ttml.autograd.Tensor.from_numpy(mask.reshape(1, 1, padded_q, padded_w), Layout.TILE, DataType.BFLOAT16)\n",
    "\n",
    "def tokens_to_tensor_kv(tokens: list) -> ttml.autograd.Tensor:\n",
    "    padded_len = round_up(len(tokens))\n",
    "    padded = np.zeros(padded_len, dtype=np.uint32)\n",
    "    padded[:len(tokens)] = tokens\n",
    "    return ttml.autograd.Tensor.from_numpy(padded.reshape(1, 1, 1, padded_len), Layout.ROW_MAJOR, DataType.UINT32)\n",
    "\n",
    "def generate_with_tt_kv_cache(model, prompt_tokens, transformer_config):\n",
    "    import time\n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(ttml.autograd.GradMode.DISABLED)\n",
    "    model.eval()\n",
    "\n",
    "    logits_mask_tensor = build_logits_mask(orig_vocab_size, padded_vocab_size) if padded_vocab_size != orig_vocab_size else None\n",
    "\n",
    "    head_dim = getattr(transformer_config, 'head_dim', None) or (transformer_config.embedding_dim // transformer_config.num_heads)\n",
    "    kv_cache = ttml.models.KvCache(\n",
    "        transformer_config.num_blocks, 1, transformer_config.num_groups,\n",
    "        transformer_config.max_sequence_length, head_dim\n",
    "    )\n",
    "    kv_cache.reset()\n",
    "\n",
    "    generated = prompt_tokens.copy()\n",
    "    print(\"************************************\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for step in tqdm(range(min(OUTPUT_TOKENS, transformer_config.max_sequence_length - len(prompt_tokens)))):\n",
    "        if kv_cache.get_cache_position() == 0:\n",
    "            input_tokens = generated\n",
    "            processed = 0\n",
    "        else:\n",
    "            input_tokens = [generated[-1]]\n",
    "            processed = len(generated) - 1\n",
    "\n",
    "        token_tensor = tokens_to_tensor_kv(input_tokens)\n",
    "        mask = create_causal_mask_kv(len(input_tokens), processed)\n",
    "        logits = model(token_tensor, mask, kv_cache=kv_cache, new_tokens=len(input_tokens))\n",
    "\n",
    "        next_token_tensor = ttml.ops.sample.sample_op(logits, TEMPERATURE, np.random.randint(low=1e7), logits_mask_tensor)\n",
    "        next_token = int(next_token_tensor.to_numpy().flatten()[len(input_tokens) - 1])\n",
    "        generated.append(next_token)\n",
    "        print(tokenizer.decode([next_token]), end='', flush=True)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    tokens_per_second = OUTPUT_TOKENS / elapsed_time\n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {OUTPUT_TOKENS} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40913e88",
   "metadata": {},
   "source": [
    "`generate_with_tt()` uses TT hardware acceleration to generate output from the chosen LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18e6550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_tt(model, prompt_tokens):\n",
    "    import time\n",
    "    \n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(ttml.autograd.GradMode.DISABLED)\n",
    "    model.eval()\n",
    "\n",
    "    logits_mask_tensor = None\n",
    "\n",
    "    if padded_vocab_size != orig_vocab_size:\n",
    "        logits_mask_tensor = build_logits_mask(orig_vocab_size, padded_vocab_size)\n",
    "\n",
    "    \n",
    "    padded_prompt_tokens = np.zeros((1, 1, 1, max_sequence_length), \n",
    "                                    dtype=np.uint32)\n",
    "\n",
    "    start_idx = 0\n",
    "\n",
    "    print(\"************************************\")\n",
    "    start_time = time.time()\n",
    "    causal_mask = build_causal_mask(max_sequence_length)  # [1,1,seq_len,seq_len], float32\n",
    "    \n",
    "    for token_idx in tqdm(range(OUTPUT_TOKENS)):\n",
    "\n",
    "        if len(prompt_tokens) > max_sequence_length:\n",
    "            start_idx = len(prompt_tokens) - max_sequence_length\n",
    "\n",
    "        padded_prompt_tokens[0, 0, 0, :len(prompt_tokens)] = prompt_tokens[start_idx:]\n",
    "        padded_prompt_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "            padded_prompt_tokens,\n",
    "            Layout.ROW_MAJOR,\n",
    "            DataType.UINT32)  # [1,1,1, max_seq_len], uint32\n",
    "        \n",
    "        #causal_mask = build_causal_mask(len(prompt_tokens))  # [1,1,seq_len,seq_len], float32\n",
    "        \n",
    "        logits = model(padded_prompt_tensor, causal_mask)  # out=[1,1,seq_len, vocab_size], bf16\n",
    "\n",
    "\n",
    "        next_token_tensor = ttml.ops.sample.sample_op(logits, TEMPERATURE, np.random.randint(low=1e7), logits_mask_tensor)  # out=[1,1,seq_len,1], uint32\n",
    "\n",
    "        next_token_idx = max_sequence_length - 1 if len(prompt_tokens) > max_sequence_length else len(prompt_tokens) - 1\n",
    "        next_token = next_token_tensor.to_numpy().flatten()[next_token_idx]\n",
    "\n",
    "        output = tokenizer.decode(next_token)\n",
    "\n",
    "        prompt_tokens.append(next_token)\n",
    "        print(output, end='', flush=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = OUTPUT_TOKENS / elapsed_time\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {OUTPUT_TOKENS} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7824c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_pytorch(torch_model, prompt_tokens):\n",
    "    import time\n",
    "    import torch.nn.functional as F\n",
    "    from transformers import DynamicCache\n",
    "    \n",
    "    torch_model.eval()\n",
    "    \n",
    "    print(\"************************************\")\n",
    "    # Convert list to tensor and add batch dimension\n",
    "    if isinstance(prompt_tokens, list):\n",
    "        prompt_tokens = torch.tensor([prompt_tokens])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize KV cache using the new DynamicCache API\n",
    "    past_key_values = DynamicCache()\n",
    "    input_ids = prompt_tokens\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(OUTPUT_TOKENS)):\n",
    "            # Get model outputs with KV cache\n",
    "            outputs = torch_model(\n",
    "                input_ids=input_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            past_key_values = outputs.past_key_values\n",
    "            \n",
    "            # Get logits for the last token\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            \n",
    "            # Apply temperature and sample\n",
    "            if WITH_SAMPLING and TEMPERATURE > 0:\n",
    "                next_token_logits = next_token_logits / TEMPERATURE\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                # Greedy sampling\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            \n",
    "            # Decode and print the token\n",
    "            output = tokenizer.decode(next_token[0])\n",
    "            print(output, end='', flush=True)\n",
    "            \n",
    "            # For next iteration, only pass the new token (KV cache handles the rest)\n",
    "            input_ids = next_token\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = OUTPUT_TOKENS / elapsed_time\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {OUTPUT_TOKENS} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4813faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with torch:\n",
      "************************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5acd082f3ca3443382c1a455b3949827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Which is better?\n",
      "\n",
      "I'm trying to generate a dataset for a machine learning model. I have a dataset that I can generate with PyTorch, but I'm not sure if I should use PyTorch or ttML. I'm not sure if there are any advantages or disadvantages of each. I'm not sure if I should use PyTorch or ttML for generating the data. I'm not sure if there are any advantages or disadvantages of each. I'm not sure if there are\n",
      "************************************\n",
      "Generated 100 tokens in 8.76 seconds\n",
      "Performance: 11.42 tokens/second\n",
      "************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_str = \"Generating with pyTorch vs ttML:\"\n",
    "\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with torch:\")\n",
    "generate_with_pytorch(torch_model, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59d89257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with TTML (KV Cache):\n",
      "************************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab23c6476e048ebac32589406030b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Which is better?\n",
      "\n",
      "I'm trying to generate a dataset for a machine learning model. I have a dataset that is already generated, but I need to generate more data for training. I'm using PyTorch and I want to generate more data. I'm not sure which approach is better between using PyTorch and ttML. I'm not sure if there are any differences between the two. I'm not sure if there are any differences in the performance of the two approaches. I'm not\n",
      "************************************\n",
      "Generated 100 tokens in 7.89 seconds\n",
      "Performance: 12.68 tokens/second\n",
      "************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_str = \"Generating with pyTorch vs ttML:\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with TTML (KV Cache):\")\n",
    "generate_with_tt_kv_cache(tt_model, prompt_tokens, tt_model_factory.transformer_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a42273b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with TT (No KV Cache, just max_tokens every time):\n",
      "************************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27d784f8f744812b215d6c1bcd6019b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Which is better?\n",
      "\n",
      "I'm trying to generate a dataset for a machine learning model. I have a dataset that I can generate with PyTorch, but I'm not sure if I should use PyTorch or ttML. I'm not sure if there are any advantages or disadvantages of each. I'm not sure if I should use PyTorch or ttML for the same purpose. I'm not sure if there are any advantages or disadvantages of each. I'm not sure if there are\n",
      "************************************\n",
      "Generated 100 tokens in 79.20 seconds\n",
      "Performance: 1.26 tokens/second\n",
      "************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_str = \"Generating with pyTorch vs ttML:\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with TT (No KV Cache, just max_tokens every time):\")\n",
    "generate_with_tt(tt_model, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d334cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
