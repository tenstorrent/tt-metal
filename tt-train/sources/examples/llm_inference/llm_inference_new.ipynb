{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f838d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TT_METAL_HOME=/home/ubuntu/tt-metal\n",
      "env: TT_METAL_RUNTIME_ROOT=/home/ubuntu/tt-metal\n"
     ]
    }
   ],
   "source": [
    "%env TT_METAL_HOME=/home/ubuntu/tt-metal\n",
    "%env TT_METAL_RUNTIME_ROOT=/home/ubuntu/tt-metal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb31718",
   "metadata": {},
   "source": [
    "%env TT_LOGGER_LEVEL=debug\n",
    "%env TT_LOGGER_TYPES=Op\n",
    "%env TTNN_ENABLE_LOGGING=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f7bc745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 05:26:32.635 | DEBUG    | ttnn:<module>:77 - Initial ttnn.CONFIG:\n",
      "Config{cache_path=/home/ubuntu/.cache/ttnn,model_cache_path=/home/ubuntu/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=true,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_should_raise_exception=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n"
     ]
    }
   ],
   "source": [
    "import ttml\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c43ffe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttnn import Layout, DataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6545c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TOKENS = 100\n",
    "WITH_SAMPLING = True\n",
    "TEMPERATURE = 0.0\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a711136",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\" \n",
    "CONFIG = \"training_shakespeare_llama3_2_1B_fixed.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e1948c",
   "metadata": {},
   "source": [
    "model_id =  \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "CONFIG = \"training_shakespeare_tinyllama.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def0e6e",
   "metadata": {},
   "source": [
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "CONFIG = \"training_shakespeare_llama3_2_3B.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a19149b",
   "metadata": {},
   "source": [
    "model_id =  \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "CONFIG = \"training_shakespeare_llama3_8B_tp.yaml\" # OOM on 12 GB, sucesfully loaded weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abaa683",
   "metadata": {},
   "source": [
    "model_id = \"Qwen/Qwen3-1.7B\" \n",
    "CONFIG = \"training_shakespeare_qwen3_1_7B.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93d1882",
   "metadata": {},
   "source": [
    "model_id = \"Qwen/Qwen3-4B\"\n",
    "CONFIG = \"training_shakespeare_qwen3_4B.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a3574",
   "metadata": {},
   "source": [
    "model_id = \"Qwen/Qwen3-0.6B\" \n",
    "CONFIG = \"training_shakespeare_qwen3_0_6B.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df6d6bf9-a010-4206-b004-807ee500d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "import numpy as np  # For numpy arrays\n",
    "from dataclasses import dataclass # For configuration classes\n",
    "from huggingface_hub import hf_hub_download # To download safetensors from Hugging Face\n",
    "from transformers import AutoTokenizer\n",
    "from yaml import safe_load # To read YAML configs\n",
    "from pathlib import Path\n",
    "\n",
    "import ttml\n",
    "from ttml.common.config import get_training_config, load_config, TransformerConfig\n",
    "from ttml.common.utils import set_seed, round_up_to_tile\n",
    "from ttml.common.model_factory import TransformerModelFactory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2372b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6991716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ttml.common.config.TrainingConfig at 0x7fc3b6264430>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_training_config(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55071c7a-74fe-4f56-8f33-e45ffe699c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from Hugging Face and the transformer config from YAML\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "training_config = get_training_config(CONFIG)\n",
    "model_yaml = load_config(training_config.model_config, configs_root=os.getcwd() + '/../../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e7883f6-f8e6-436b-8908-62c8a26e1e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "safetensors_path = hf_hub_download(repo_id=model_id, filename=\"config.json\")\n",
    "safetensors_path = safetensors_path.replace(\"config.json\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f77e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "torch.manual_seed(SEED)\n",
    "torch_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f405407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128000, 128256, 128256)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size, torch_model.state_dict()['model.embed_tokens.weight'].shape[0], torch_model.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50d8ce12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torch_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf74d9ac-7afd-4c6f-887e-b793c6e44c18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128256\n"
     ]
    }
   ],
   "source": [
    "orig_vocab_size = torch_model.vocab_size\n",
    "print(orig_vocab_size)\n",
    "tt_model_factory = TransformerModelFactory(model_yaml)\n",
    "tt_model_factory.transformer_config.vocab_size = orig_vocab_size\n",
    "\n",
    "max_sequence_length = tt_model_factory.transformer_config.max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dff343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ad0b618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transformer_config': {'model_type': 'llama',\n",
       "  'num_heads': 32,\n",
       "  'num_groups': 8,\n",
       "  'embedding_dim': 2048,\n",
       "  'intermediate_dim': 8192,\n",
       "  'dropout_prob': 0.0,\n",
       "  'num_blocks': 16,\n",
       "  'weight_tying': 'enabled',\n",
       "  'vocab_size': 128256,\n",
       "  'max_sequence_length': 2048,\n",
       "  'runner_type': 'memory_efficient',\n",
       "  'theta': 500000.0,\n",
       "  'rope_scaling': {'scaling_factor': 32.0,\n",
       "   'high_freq_factor': 4.0,\n",
       "   'low_freq_factor': 1.0,\n",
       "   'original_context_length': 8192}}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b4c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff6ac0b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama configuration:\n",
      "    Vocab size: 128256\n",
      "    Max sequence length: 2048\n",
      "    Embedding dim: 2048\n",
      "    Intermediate dim: 8192\n",
      "    Num heads: 32\n",
      "    Num groups: 8\n",
      "    Dropout probability: 0\n",
      "    Num blocks: 16\n",
      "    Positional embedding type: RoPE\n",
      "    Runner type: Memory efficient\n",
      "    Weight tying: Enabled\n",
      "    Theta: 500000\n",
      "2026-01-30 05:26:42.726 | info     |             UMD | Established firmware bundle version: 19.4.2 (topology_discovery.cpp:368)\n",
      "2026-01-30 05:26:42.726 | info     |             UMD | Firmware bundle version 19.4.2 on the system is newer than the latest fully tested version 19.4.0 for wormhole_b0 architecture. Newest features may not be supported. (topology_discovery.cpp:394)\n",
      "Model created: 42.800490856170654\n",
      "2026-01-30 05:26:42.727 | info     |          Device | Opening user mode device driver (tt_cluster.cpp:223)\n",
      "2026-01-30 05:26:42.731 | info     |             UMD | Established firmware bundle version: 19.4.2 (topology_discovery.cpp:368)\n",
      "2026-01-30 05:26:42.731 | info     |             UMD | Firmware bundle version 19.4.2 on the system is newer than the latest fully tested version 19.4.0 for wormhole_b0 architecture. Newest features may not be supported. (topology_discovery.cpp:394)\n",
      "2026-01-30 05:26:42.750 | info     |             UMD | Established firmware bundle version: 19.4.2 (topology_discovery.cpp:368)\n",
      "2026-01-30 05:26:42.750 | info     |             UMD | Firmware bundle version 19.4.2 on the system is newer than the latest fully tested version 19.4.0 for wormhole_b0 architecture. Newest features may not be supported. (topology_discovery.cpp:394)\n",
      "2026-01-30 05:26:42.751 | info     |             UMD | Harvesting masks for chip 0 tensix: 0x1 dram: 0x0 eth: 0x0 pcie: 0x0 l2cpu: 0x0 (cluster.cpp:339)\n",
      "2026-01-30 05:26:42.782 | warning  |             UMD | init_detect_tt_device_numanodes(): Could not determine NumaNodeSet for TT device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:564)\n",
      "2026-01-30 05:26:42.782 | warning  |             UMD | Could not find NumaNodeSet for TT Device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:173)\n",
      "2026-01-30 05:26:42.782 | warning  |             UMD | bind_area_memory_nodeset(): Unable to determine TT Device to NumaNode mapping for physical_device_id: 0. Skipping membind. (cpuset_lib.cpp:450)\n",
      "2026-01-30 05:26:42.782 | warning  |             UMD | ---- ttSiliconDevice::init_hugepage: bind_area_to_memory_nodeset() failed (physical_device_id: 0 ch: 0). Hugepage allocation is not on NumaNode matching TT Device. Side-Effect is decreased Device->Host perf (Issue #893). (silicon_sysmem_manager.cpp:179)\n",
      "2026-01-30 05:26:42.783 | info     |             UMD | Opening local chip ids/PCIe ids: {0}/[0] and remote chip ids {} (cluster.cpp:186)\n",
      "2026-01-30 05:26:42.783 | info     |             UMD | IOMMU: disabled (cluster.cpp:161)\n",
      "2026-01-30 05:26:42.783 | info     |             UMD | KMD version: 2.6.1 (cluster.cpp:164)\n",
      "2026-01-30 05:26:42.784 | info     |             UMD | Starting devices in cluster (cluster.cpp:965)\n",
      "2026-01-30 05:26:42.786 | info     |             UMD | Mapped hugepage 0x240000000 to NOC address 0x800000000 (silicon_sysmem_manager.cpp:207)\n",
      "2026-01-30 05:26:42.808 | info     |     Distributed | Using auto discovery to generate mesh graph. (metal_context.cpp:928)\n",
      "2026-01-30 05:26:42.808 | info     |     Distributed | Constructing control plane using auto-discovery (no mesh graph descriptor). (metal_context.cpp:905)\n",
      "2026-01-30 05:26:42.808 | info     |          Fabric | TopologyMapper mapping start (mesh=0): n_log=1, n_phys=1, log_deg_hist={0:1}, phys_deg_hist={0:1} (topology_mapper_utils.cpp:171)\n",
      "2026-01-30 05:26:42.808 | info     |          Fabric | TopologyMapper mapping start (mesh=0): n_log=1, n_phys=1, log_deg_hist={0:1}, phys_deg_hist={0:1} (topology_mapper_utils.cpp:171)\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "tt_model = tt_model_factory.create_model()\n",
    "print(f\"Model created: {time() - start_time}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb62dae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 19.477176189422607    RoPE scaling enabled:\n",
      "        Scaling factor: 32\n",
      "        Original context length: 8192\n",
      "        High freq factor: 4\n",
      "        Low freq factor: 1\n",
      "Loading model from: /home/ubuntu/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/model.safetensors\n",
      "Loading tensor: model.embed_tokens.weight, shape:Shape([128256, 2048]), dtype:BF16\n",
      "Using parameter: llama/fc/weight with shape: Shape([1, 1, 128256, 2048])\n",
      "Loading tensor: model.layers.0.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.0.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.0.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/mlp/w1/weight with shape: Shape([1, 1, 8192\n",
      ", 2048])\n",
      "Loading tensor: model.layers.0.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.0.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.0.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.0.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.0.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.0.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 0\n",
      "Loading tensor: model.layers.1.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.1.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.1.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.1.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.1.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.1.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.1.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.1.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.1.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 1\n",
      "Loading tensor: model.layers.10.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.10.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.10.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.10.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.10.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.10.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.10.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.10.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.10.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 10\n",
      "Loading tensor: model.layers.11.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.11.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.11.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.11.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.11.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.11.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.11.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.11.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.11.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 11\n",
      "Loading tensor: model.layers.12.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.12.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.12.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.12.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.12.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.12.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.12.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.12.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.12.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 12\n",
      "Loading tensor: model.layers.13.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.13.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.13.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.13.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.13.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.13.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.13.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.13.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: l"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "tt_model.load_from_safetensors(safetensors_path)\n",
    "print(f\"Model loaded: {time() - start_time}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bdc85bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lama/llama_block_13/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.13.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 13\n",
      "Loading tensor: model.layers.14.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.14.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.14.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.14.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.14.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.14.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.14.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.14.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.14.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 14\n",
      "Loading tensor: model.layers.15.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.15.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.15.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.15.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.15.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.15.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.15.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.15.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.15.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 15\n",
      "Loading tensor: model.layers.2.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.2.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.2.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.2.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.2.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.2.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.2.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.2.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.2.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 2\n",
      "Loading tensor: model.layers.3.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.3.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.3.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.3.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.3.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.3.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.3.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.3.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.3.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 3\n",
      "Loading tensor: model.layers.4.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.4.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.4.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.4.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.4.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.4.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.4.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.4.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.4.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 4\n",
      "Loading tensor: model.layers.5.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.5.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.5.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.5.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.5.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.5.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.5.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.5.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.5.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 5\n",
      "Loading tensor: model.layers.6.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.6.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.6.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.6.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.6.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.6.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.6.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.6.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.6.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 6\n",
      "Loading tensor: model.layers.7.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.7.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.7.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.7.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.7.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.7.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.7.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.7.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.7.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 7\n",
      "Loading tensor: model.layers.8.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.8.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.8.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.8.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.8.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.8.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.8.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/attention/out_linear/weigh"
     ]
    }
   ],
   "source": [
    "padded_vocab_size = round_up_to_tile(orig_vocab_size, 32)\n",
    "if orig_vocab_size != padded_vocab_size:\n",
    "    print(f\"Padding vocab size for tilization: original {orig_vocab_size} -> padded {padded_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4d4f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_causal_mask(T: int) -> ttml.autograd.Tensor:\n",
    "    # [1,1,T,T] float32 with 1s for allowed positions (i >= j), else 0\\n\",\n",
    "    m = np.tril(np.ones((T, T), dtype=np.float32))\n",
    "    return ttml.autograd.Tensor.from_numpy(m.reshape(1, 1, T, T), Layout.TILE, DataType.BFLOAT16)\n",
    "\n",
    "def build_logits_mask(vocab_size: int, padded_vocab_size: int) -> ttml.autograd.Tensor:\n",
    "    logits_mask = np.zeros((1, 1, 1, padded_vocab_size), dtype=np.float32)\n",
    "    logits_mask[:, :, :, vocab_size:] = 1e4\n",
    "    return ttml.autograd.Tensor.from_numpy(logits_mask, Layout.TILE, DataType.BFLOAT16)   # [1,1,1,T], bfloat16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77f8e7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_SIZE = 32\n",
    "\n",
    "def round_up(value: int) -> int:\n",
    "    return ((value + TILE_SIZE - 1) // TILE_SIZE) * TILE_SIZE\n",
    "\n",
    "def create_causal_mask_kv(query_len: int, prompt_len: int = 0) -> ttml.autograd.Tensor:\n",
    "    whole_len = prompt_len + query_len\n",
    "    padded_q = round_up(query_len)\n",
    "    padded_w = round_up(whole_len)\n",
    "    mask = np.zeros((padded_q, padded_w), dtype=np.float32)\n",
    "    for i in range(query_len):\n",
    "        for j in range(prompt_len + i + 1):\n",
    "            mask[i, j] = 1.0\n",
    "    return ttml.autograd.Tensor.from_numpy(mask.reshape(1, 1, padded_q, padded_w), Layout.TILE, DataType.BFLOAT16)\n",
    "\n",
    "def tokens_to_tensor_kv(tokens: list) -> ttml.autograd.Tensor:\n",
    "    padded_len = round_up(len(tokens))\n",
    "    padded = np.zeros(padded_len, dtype=np.uint32)\n",
    "    padded[:len(tokens)] = tokens\n",
    "    return ttml.autograd.Tensor.from_numpy(padded.reshape(1, 1, 1, padded_len), Layout.ROW_MAJOR, DataType.UINT32)\n",
    "\n",
    "def generate_with_tt_kv_cache(model, prompt_tokens, transformer_config):\n",
    "    import time\n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(ttml.autograd.GradMode.DISABLED)\n",
    "    model.eval()\n",
    "\n",
    "    logits_mask_tensor = build_logits_mask(orig_vocab_size, padded_vocab_size) if padded_vocab_size != orig_vocab_size else None\n",
    "\n",
    "    head_dim = getattr(transformer_config, 'head_dim', None) or (transformer_config.embedding_dim // transformer_config.num_heads)\n",
    "    kv_cache = ttml.models.KvCache(\n",
    "        transformer_config.num_blocks, 1, transformer_config.num_groups,\n",
    "        transformer_config.max_sequence_length, head_dim\n",
    "    )\n",
    "    kv_cache.reset()\n",
    "\n",
    "    generated = prompt_tokens.copy()\n",
    "    print(\"************************************\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for step in tqdm(range(min(OUTPUT_TOKENS, transformer_config.max_sequence_length - len(prompt_tokens)))):\n",
    "        if kv_cache.get_cache_position() == 0:\n",
    "            input_tokens = generated\n",
    "            processed = 0\n",
    "        else:\n",
    "            input_tokens = [generated[-1]]\n",
    "            processed = len(generated) - 1\n",
    "\n",
    "        token_tensor = tokens_to_tensor_kv(input_tokens)\n",
    "        mask = create_causal_mask_kv(len(input_tokens), processed)\n",
    "        logits = model(token_tensor, mask, kv_cache=kv_cache, new_tokens=len(input_tokens))\n",
    "\n",
    "        next_token_tensor = ttml.ops.sample.sample_op(logits, TEMPERATURE, np.random.randint(low=1e7), logits_mask_tensor)\n",
    "        next_token = int(next_token_tensor.to_numpy().flatten()[len(input_tokens) - 1])\n",
    "        generated.append(next_token)\n",
    "        print(tokenizer.decode([next_token]), end='', flush=True)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    tokens_per_second = OUTPUT_TOKENS / elapsed_time\n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {OUTPUT_TOKENS} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40913e88",
   "metadata": {},
   "source": [
    "`generate_with_tt()` uses TT hardware acceleration to generate output from the chosen LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18e6550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_tt(model, prompt_tokens):\n",
    "    import time\n",
    "    \n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(ttml.autograd.GradMode.DISABLED)\n",
    "    model.eval()\n",
    "\n",
    "    logits_mask_tensor = None\n",
    "\n",
    "    if padded_vocab_size != orig_vocab_size:\n",
    "        logits_mask_tensor = build_logits_mask(orig_vocab_size, padded_vocab_size)\n",
    "\n",
    "    \n",
    "    padded_prompt_tokens = np.zeros((1, 1, 1, max_sequence_length), \n",
    "                                    dtype=np.uint32)\n",
    "\n",
    "    start_idx = 0\n",
    "\n",
    "    print(\"************************************\")\n",
    "    start_time = time.time()\n",
    "    causal_mask = build_causal_mask(max_sequence_length)  # [1,1,seq_len,seq_len], float32\n",
    "    \n",
    "    for token_idx in tqdm(range(OUTPUT_TOKENS)):\n",
    "\n",
    "        if len(prompt_tokens) > max_sequence_length:\n",
    "            start_idx = len(prompt_tokens) - max_sequence_length\n",
    "\n",
    "        padded_prompt_tokens[0, 0, 0, :len(prompt_tokens)] = prompt_tokens[start_idx:]\n",
    "        padded_prompt_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "            padded_prompt_tokens,\n",
    "            Layout.ROW_MAJOR,\n",
    "            DataType.UINT32)  # [1,1,1, max_seq_len], uint32\n",
    "        \n",
    "        #causal_mask = build_causal_mask(len(prompt_tokens))  # [1,1,seq_len,seq_len], float32\n",
    "        \n",
    "        logits = model(padded_prompt_tensor, causal_mask)  # out=[1,1,seq_len, vocab_size], bf16\n",
    "\n",
    "\n",
    "        next_token_tensor = ttml.ops.sample.sample_op(logits, TEMPERATURE, np.random.randint(low=1e7), logits_mask_tensor)  # out=[1,1,seq_len,1], uint32\n",
    "\n",
    "        next_token_idx = max_sequence_length - 1 if len(prompt_tokens) > max_sequence_length else len(prompt_tokens) - 1\n",
    "        next_token = next_token_tensor.to_numpy().flatten()[next_token_idx]\n",
    "\n",
    "        output = tokenizer.decode(next_token)\n",
    "\n",
    "        prompt_tokens.append(next_token)\n",
    "        print(output, end='', flush=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = OUTPUT_TOKENS / elapsed_time\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {OUTPUT_TOKENS} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7824c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_pytorch(torch_model, prompt_tokens):\n",
    "    import time\n",
    "    import torch.nn.functional as F\n",
    "    from transformers import DynamicCache\n",
    "    \n",
    "    torch_model.eval()\n",
    "    \n",
    "    print(\"************************************\")\n",
    "    # Convert list to tensor and add batch dimension\n",
    "    if isinstance(prompt_tokens, list):\n",
    "        prompt_tokens = torch.tensor([prompt_tokens])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize KV cache using the new DynamicCache API\n",
    "    past_key_values = DynamicCache()\n",
    "    input_ids = prompt_tokens\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(OUTPUT_TOKENS)):\n",
    "            # Get model outputs with KV cache\n",
    "            outputs = torch_model(\n",
    "                input_ids=input_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            past_key_values = outputs.past_key_values\n",
    "            \n",
    "            # Get logits for the last token\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            \n",
    "            # Apply temperature and sample\n",
    "            if WITH_SAMPLING and TEMPERATURE > 0:\n",
    "                next_token_logits = next_token_logits / TEMPERATURE\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                # Greedy sampling\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            \n",
    "            # Decode and print the token\n",
    "            output = tokenizer.decode(next_token[0])\n",
    "            print(output, end='', flush=True)\n",
    "            \n",
    "            # For next iteration, only pass the new token (KV cache handles the rest)\n",
    "            input_ids = next_token\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = OUTPUT_TOKENS / elapsed_time\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {OUTPUT_TOKENS} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4813faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with torch:\n",
      "************************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc797bfa75a414fb0c362da9a54f29f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A Comparison\n",
      "=====================================\n",
      "\n",
      "In this comparison, we will explore the differences between using pyTorch for generating text and using ttML (TensorFlow's Text-to-Media Library) for generating text.\n",
      "\n",
      "### Introduction\n",
      "\n",
      "Text-to-Media generation is a subfield of natural language processing (NLP) that involves generating images or videos from text descriptions. PyTorch and ttML are two popular libraries used for this task. In this comparison, we will discuss the differences between using pyTorch\n",
      "************************************\n",
      "Generated 100 tokens in 13.56 seconds\n",
      "Performance: 7.37 tokens/second\n",
      "************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_str = \"Generating with pyTorch vs ttML:\"\n",
    "\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with torch:\")\n",
    "generate_with_pytorch(torch_model, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d89257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with TTML (KV Cache):\n",
      "************************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9dc7a873c34807ad43b5334e930ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A Comparison\n",
      "=====================================\n",
      "\n",
      "In this comparison, we will explore the differences between using pyTorch for generating text and using ttML (TensorFlow's Text-to-Media Library) for generating text.\n",
      "\n",
      "### Introduction\n",
      "\n",
      "Text-to-Media (T2M) is a"
     ]
    }
   ],
   "source": [
    "prompt_str = \"Generating with pyTorch vs ttML:\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with TTML (KV Cache):\")\n",
    "generate_with_tt_kv_cache(tt_model, prompt_tokens, tt_model_factory.transformer_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a42273b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prompt_str = \"Generating with pyTorch vs ttML:\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with TT (No KV Cache, just max_tokens every time):\")\n",
    "generate_with_tt(tt_model, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d334cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
