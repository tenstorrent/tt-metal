{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f838d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TT_METAL_HOME=/home/ubuntu/tt-metal\n",
      "env: TT_METAL_RUNTIME_ROOT=/home/ubuntu/tt-metal\n"
     ]
    }
   ],
   "source": [
    "%env TT_METAL_HOME=/home/ubuntu/tt-metal\n",
    "%env TT_METAL_RUNTIME_ROOT=/home/ubuntu/tt-metal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb31718",
   "metadata": {},
   "source": [
    "%env TT_LOGGER_LEVEL=debug\n",
    "%env TT_LOGGER_TYPES=Op\n",
    "%env TTNN_ENABLE_LOGGING=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f7bc745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttml\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6545c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TOKENS = 100\n",
    "WITH_SAMPLING = True\n",
    "TEMPERATURE = 0.0\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41f3ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\" \n",
    "CONFIG = \"training_shakespeare_llama3_2_1B_fixed.yaml\" # now working fine (with proper shape for tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e1948c",
   "metadata": {},
   "source": [
    "model_id =  \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "CONFIG = \"training_shakespeare_tinyllama.yaml\" # working fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def0e6e",
   "metadata": {},
   "source": [
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "CONFIG = \"training_shakespeare_llama3_2_3B.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a19149b",
   "metadata": {},
   "source": [
    "model_id =  \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "CONFIG = \"training_shakespeare_llama3_8B_tp.yaml\" # OOM on 12 GB, sucesfully loaded weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "748b3525",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen3-0.6B\" \n",
    "CONFIG = \"training_shakespeare_qwen3_0_6B.yaml\" # working, not 1-1 as llama, but speak something nongibberish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abaa683",
   "metadata": {},
   "source": [
    "model_id = \"Qwen/Qwen3-1.7B\" \n",
    "CONFIG = \"training_shakespeare_qwen3_1_7B.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5889c7ab",
   "metadata": {},
   "source": [
    "model_id = \"Qwen/Qwen3-4B\"\n",
    "CONFIG = \"training_shakespeare_qwen3_4B.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df6d6bf9-a010-4206-b004-807ee500d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "import numpy as np  # For numpy arrays\n",
    "from dataclasses import dataclass # For configuration classes\n",
    "from huggingface_hub import hf_hub_download # To download safetensors from Hugging Face\n",
    "from transformers import AutoTokenizer\n",
    "from yaml import safe_load # To read YAML configs\n",
    "from pathlib import Path\n",
    "\n",
    "import ttml\n",
    "from ttml.common.config import get_training_config, load_config, TransformerConfig\n",
    "from ttml.common.utils import set_seed, round_up_to_tile\n",
    "from ttml.common.model_factory import TransformerModelFactory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2372b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6991716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ttml.common.config.TrainingConfig at 0x7f8edc03f880>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_training_config(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55071c7a-74fe-4f56-8f33-e45ffe699c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from Hugging Face and the transformer config from YAML\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "training_config = get_training_config(CONFIG)\n",
    "model_yaml = load_config(training_config.model_config, configs_root=os.getcwd() + '/../../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e7883f6-f8e6-436b-8908-62c8a26e1e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "safetensors_path = hf_hub_download(repo_id=model_id, filename=\"config.json\")\n",
    "safetensors_path = safetensors_path.replace(\"config.json\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f77e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "torch.manual_seed(SEED)\n",
    "torch_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f405407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151643, 151936, 151936)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size, torch_model.state_dict()['model.embed_tokens.weight'].shape[0], torch_model.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50d8ce12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torch_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf74d9ac-7afd-4c6f-887e-b793c6e44c18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151936\n"
     ]
    }
   ],
   "source": [
    "orig_vocab_size = torch_model.vocab_size\n",
    "print(orig_vocab_size)\n",
    "tt_model_factory = TransformerModelFactory(model_yaml)\n",
    "tt_model_factory.transformer_config.vocab_size = orig_vocab_size\n",
    "\n",
    "max_sequence_length = tt_model_factory.transformer_config.max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56dff343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ttml.autograd.AutoContext.get_instance().set_init_mode(ttml.autograd.InitMode.DISABLED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ad0b618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transformer_config': {'model_type': 'qwen3',\n",
       "  'num_heads': 16,\n",
       "  'num_groups': 8,\n",
       "  'embedding_dim': 1024,\n",
       "  'head_dim': 128,\n",
       "  'intermediate_dim': 3072,\n",
       "  'dropout_prob': 0.0,\n",
       "  'num_blocks': 28,\n",
       "  'weight_tying': 'enabled',\n",
       "  'vocab_size': 151936,\n",
       "  'max_sequence_length': 2048,\n",
       "  'runner_type': 'memory_efficient',\n",
       "  'theta': 1000000.0,\n",
       "  'rms_norm_eps': 1e-06}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "460b4c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttml.autograd.AutoContext.get_instance().set_init_mode(ttml.autograd.InitMode.DISABLED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff6ac0b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3 configuration:\n",
      "    Vocab size: 151936\n",
      "    Max sequence length: 2048\n",
      "    Embedding dim (hidden_size): 1024\n",
      "    Head dim: 128\n",
      "    Attention output dim: 2048\n",
      "    Intermediate dim: 3072\n",
      "    Num heads: 16\n",
      "    Num groups (KV heads): 8\n",
      "    Dropout probability: 0\n",
      "    Num blocks: 28\n",
      "    Positional embedding type: RoPE\n",
      "    Runner type: Memory efficient\n",
      "    Weight tying: Enabled\n",
      "    Theta: 1000000\n",
      "    RMSNorm epsilon: 1e-06\n",
      "2026-01-07 21:42:18.735 | info     |             UMD | Established firmware bundle version: 19.4.0 (topology_discovery.cpp:369)\n",
      "2026-01-07 21:42:18.735 | info     |             UMD | Firmware bundle version 19.4.0 on the system is newer than the latest fully tested version 19.1.0 for wormhole_b0 architecture. Newest features may not be supported. (topology_discovery.cpp:395)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (9, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (9, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (1, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (1, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (8, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (8, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (2, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (2, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (7, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (7, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (3, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (3, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (6, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (6, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (4, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (4, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (9, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (9, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (1, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (1, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (8, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (8, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (2, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (2, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (7, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (7, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (3, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (3, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (6, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (6, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (4, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.735 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (4, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.736 | info     |          Device | Opening user mode device driver (tt_cluster.cpp:215)\n",
      "2026-01-07 21:42:18.741 | info     |             UMD | Established firmware bundle version: 19.4.0 (topology_discovery.cpp:369)\n",
      "2026-01-07 21:42:18.741 | info     |             UMD | Firmware bundle version 19.4.0 on the system is newer than the latest fully tested version 19.1.0 for wormhole_b0 architecture. Newest features may not be supported. (topology_discovery.cpp:395)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (9, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (9, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (1, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (1, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (8, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (8, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (2, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (2, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (7, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (7, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (3, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (3, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (6, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (6, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (4, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (4, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (9, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (9, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (1, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (1, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (8, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (8, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (2, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (2, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (7, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (7, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (3, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (3, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (6, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (6, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (4, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.741 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (4, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.765 | info     |             UMD | Established firmware bundle version: 19.4.0 (topology_discovery.cpp:369)\n",
      "2026-01-07 21:42:18.765 | info     |             UMD | Firmware bundle version 19.4.0 on the system is newer than the latest fully tested version 19.1.0 for wormhole_b0 architecture. Newest features may not be supported. (topology_discovery.cpp:395)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (9, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (9, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (1, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (1, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (8, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (8, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (2, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (2, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (7, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (7, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (3, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (3, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (6, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (6, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (4, 0, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (4, 0, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (9, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (9, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (1, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (1, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (8, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (8, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (2, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (2, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (7, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (7, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (3, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (3, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (6, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (6, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | ETH FW version mismatch for chip 0 ETH core CoreCoord: (4, 6, ETH, NOC0), found: 7.2.0. (topology_discovery_wormhole.cpp:347)\n",
      "2026-01-07 21:42:18.765 | warning  |             UMD | Skipping discovery from chip 0 ETH core CoreCoord: (4, 6, ETH, NOC0) (topology_discovery.cpp:164)\n",
      "2026-01-07 21:42:18.766 | info     |             UMD | Harvesting masks for chip 0 tensix: 0x1 dram: 0x0 eth: 0x0 pcie: 0x0 l2cpu: 0x0 (cluster.cpp:360)\n",
      "2026-01-07 21:42:18.803 | warning  |             UMD | init_detect_tt_device_numanodes(): Could not determine NumaNodeSet for TT device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:564)\n",
      "2026-01-07 21:42:18.803 | warning  |             UMD | Could not find NumaNodeSet for TT Device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:173)\n",
      "2026-01-07 21:42:18.804 | warning  |             UMD | bind_area_memory_nodeset(): Unable to determine TT Device to NumaNode mapping for physical_device_id: 0. Skipping membind. (cpuset_lib.cpp:450)\n",
      "2026-01-07 21:42:18.804 | warning  |             UMD | ---- ttSiliconDevice::init_hugepage: bind_area_to_memory_nodeset() failed (physical_device_id: 0 ch: 0). Hugepage allocation is not on NumaNode matching TT Device. Side-Effect is decreased Device->Host perf (Issue #893). (sysmem_manager.cpp:217)\n",
      "2026-01-07 21:42:18.804 | info     |             UMD | Opening local chip ids/PCIe ids: {0}/[0] and remote chip ids {} (cluster.cpp:204)\n",
      "2026-01-07 21:42:18.804 | info     |             UMD | IOMMU: disabled (cluster.cpp:180)\n",
      "2026-01-07 21:42:18.804 | info     |             UMD | KMD version: 2.6.1 (cluster.cpp:183)\n",
      "2026-01-07 21:42:18.807 | info     |             UMD | Mapped hugepage 0x200000000 to NOC address 0x800000000 (sysmem_manager.cpp:245)\n",
      "2026-01-07 21:42:18.833 | info     |     Distributed | Using auto discovery to generate mesh graph. (metal_context.cpp:835)\n",
      "2026-01-07 21:42:18.833 | info     |     Distributed | Constructing control plane using auto-discovery (no mesh graph descriptor). (metal_context.cpp:812)\n",
      "2026-01-07 21:42:18.833 | info     |          Fabric | TopologyMapper mapping start (mesh=0): n_log=1, n_phys=1, log_deg_hist={0:1}, phys_deg_hist={0:1} (topology_mapper_utils.cpp:171)\n",
      "2026-01-07 21:42:18.833 | info     |          Fabric | TopologyMapper mapping start (mesh=0): n_log=1, n_phys=1, log_deg_hist={0:1}, phys_deg_hist={0:1} (topology_mapper_utils.cpp:171)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created: 9.044490814208984\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "tt_model = tt_model_factory.create_model()\n",
    "print(f\"Model created: {time() - start_time}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfb5a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b38196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llama 3b\n",
    "#Model created: 238.99020171165466\n",
    "#Model loaded: 47.06614542007446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68b490f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llama 3b\n",
    "#Model created: 7.940993309020996\n",
    "#Model loaded: 58.52051877975464"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb62dae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 14.540028810501099\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "tt_model.load_from_safetensors(safetensors_path)\n",
    "print(f\"Model loaded: {time() - start_time}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bdc85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_vocab_size = round_up_to_tile(orig_vocab_size, 32)\n",
    "if orig_vocab_size != padded_vocab_size:\n",
    "    print(f\"Padding vocab size for tilization: original {orig_vocab_size} -> padded {padded_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb047274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3cf900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4d4f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_causal_mask(T: int) -> ttml.autograd.Tensor:\n",
    "    # [1,1,T,T] float32 with 1s for allowed positions (i >= j), else 0\\n\",\n",
    "    m = np.tril(np.ones((T, T), dtype=np.float32))\n",
    "    return ttml.autograd.Tensor.from_numpy(m.reshape(1, 1, T, T), ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)\n",
    "\n",
    "def build_logits_mask(vocab_size: int, padded_vocab_size: int) -> ttml.autograd.Tensor:\n",
    "    logits_mask = np.zeros((1, 1, 1, padded_vocab_size), dtype=np.float32)\n",
    "    logits_mask[:, :, :, vocab_size:] = 1e4\n",
    "    return ttml.autograd.Tensor.from_numpy(logits_mask, ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)   # [1,1,1,T], bfloat16\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40913e88",
   "metadata": {},
   "source": [
    "`generate_with_tt()` uses TT hardware acceleration to generate output from the chosen LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18e6550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_tt(model, prompt_tokens):\n",
    "    import time\n",
    "    \n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(ttml.autograd.GradMode.DISABLED)\n",
    "    model.eval()\n",
    "\n",
    "    logits_mask_tensor = None\n",
    "\n",
    "    if padded_vocab_size != orig_vocab_size:\n",
    "        logits_mask_tensor = build_logits_mask(orig_vocab_size, padded_vocab_size)\n",
    "\n",
    "    \n",
    "    padded_prompt_tokens = np.zeros((1, 1, 1, max_sequence_length), \n",
    "                                    dtype=np.uint32)\n",
    "\n",
    "    start_idx = 0\n",
    "\n",
    "    print(\"************************************\")\n",
    "    start_time = time.time()\n",
    "    causal_mask = build_causal_mask(max_sequence_length)  # [1,1,seq_len,seq_len], float32\n",
    "    \n",
    "    for token_idx in tqdm(range(OUTPUT_TOKENS)):\n",
    "\n",
    "        if len(prompt_tokens) > max_sequence_length:\n",
    "            start_idx = len(prompt_tokens) - max_sequence_length\n",
    "\n",
    "        padded_prompt_tokens[0, 0, 0, :len(prompt_tokens)] = prompt_tokens[start_idx:]\n",
    "        padded_prompt_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "            padded_prompt_tokens,\n",
    "            ttml.Layout.ROW_MAJOR,\n",
    "            ttml.autograd.DataType.UINT32)  # [1,1,1, max_seq_len], uint32\n",
    "        \n",
    "        #causal_mask = build_causal_mask(len(prompt_tokens))  # [1,1,seq_len,seq_len], float32\n",
    "        \n",
    "        logits = model(padded_prompt_tensor, causal_mask)  # out=[1,1,seq_len, vocab_size], bf16\n",
    "\n",
    "\n",
    "        next_token_tensor = ttml.ops.sample.sample_op(logits, TEMPERATURE, np.random.randint(low=1e7), logits_mask_tensor)  # out=[1,1,seq_len,1], uint32\n",
    "\n",
    "        next_token_idx = max_sequence_length - 1 if len(prompt_tokens) > max_sequence_length else len(prompt_tokens) - 1\n",
    "        next_token = next_token_tensor.to_numpy().flatten()[next_token_idx]\n",
    "\n",
    "        output = tokenizer.decode(next_token)\n",
    "\n",
    "        prompt_tokens.append(next_token)\n",
    "        print(output, end='', flush=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = OUTPUT_TOKENS / elapsed_time\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {OUTPUT_TOKENS} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7824c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_pytorch(torch_model, prompt_tokens):\n",
    "    import time\n",
    "    import torch.nn.functional as F\n",
    "    from transformers import DynamicCache\n",
    "    \n",
    "    torch_model.eval()\n",
    "    \n",
    "    print(\"************************************\")\n",
    "    # Convert list to tensor and add batch dimension\n",
    "    if isinstance(prompt_tokens, list):\n",
    "        prompt_tokens = torch.tensor([prompt_tokens])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize KV cache using the new DynamicCache API\n",
    "    past_key_values = DynamicCache()\n",
    "    input_ids = prompt_tokens\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(OUTPUT_TOKENS)):\n",
    "            # Get model outputs with KV cache\n",
    "            outputs = torch_model(\n",
    "                input_ids=input_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            past_key_values = outputs.past_key_values\n",
    "            \n",
    "            # Get logits for the last token\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            \n",
    "            # Apply temperature and sample\n",
    "            if WITH_SAMPLING and TEMPERATURE > 0:\n",
    "                next_token_logits = next_token_logits / TEMPERATURE\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                # Greedy sampling\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            \n",
    "            # Decode and print the token\n",
    "            output = tokenizer.decode(next_token[0])\n",
    "            print(output, end='', flush=True)\n",
    "            \n",
    "            # For next iteration, only pass the new token (KV cache handles the rest)\n",
    "            input_ids = next_token\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = OUTPUT_TOKENS / elapsed_time\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {OUTPUT_TOKENS} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b17b326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_pytorch_batch(torch_model, prompt_tokens):\n",
    "    \"\"\"Old version: non-streaming batch generation using torch_model.generate()\"\"\"\n",
    "    import time\n",
    "    \n",
    "    torch_model.eval()\n",
    "    \n",
    "    print(\"************************************\")\n",
    "    # Convert list to tensor and add batch dimension\n",
    "    if isinstance(prompt_tokens, list):\n",
    "        prompt_tokens = torch.tensor([prompt_tokens])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = torch_model.generate(\n",
    "            prompt_tokens,\n",
    "            max_new_tokens=OUTPUT_TOKENS,\n",
    "            do_sample=WITH_SAMPLING,  # Enable sampling\n",
    "            temperature=TEMPERATURE,   # Temperature for sampling\n",
    "            num_beams=1  # Use multinomial sampling (standard sampling)\n",
    "        )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = OUTPUT_TOKENS / elapsed_time\n",
    "    \n",
    "    generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    for t in generated_text:\n",
    "        print(t)\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {OUTPUT_TOKENS} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4813faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with torch model:\n",
      "************************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9ca45909e248a5bb1593a3c85350f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - how to optimize it?\n",
      "\n",
      "I have a problem where I need to generate a sequence of numbers using PyTorch, but the process is very slow. The problem is that the code is written in Python, and the code is using a lot of CPU operations. How can I optimize this?\n",
      "\n",
      "I have a code that generates a sequence of numbers using PyTorch, but the process is very slow. The code is written in Python, and the code is using a lot of CPU operations. How\n",
      "************************************\n",
      "Generated 100 tokens in 11.45 seconds\n",
      "Performance: 8.74 tokens/second\n",
      "************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_str = \"Generating with pytorch(CPU, might be slow)\"\n",
    "\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with torch model:\")\n",
    "generate_with_pytorch(torch_model, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a42273b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with TT:\n",
      "************************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023e5ae02ba4406e9212f3a1082fe6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - how to optimize it?\n",
      "\n",
      "I have a problem where I need to generate a sequence of numbers using PyTorch, but the process is very slow. The problem is that the code is written in Python, and the code is run on a CPU. The user wants to optimize this process"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m prompt_tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt_str)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating with TT:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mgenerate_with_tt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtt_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 41\u001b[0m, in \u001b[0;36mgenerate_with_tt\u001b[0;34m(model, prompt_tokens)\u001b[0m\n\u001b[1;32m     38\u001b[0m next_token_tensor \u001b[38;5;241m=\u001b[39m ttml\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39msample\u001b[38;5;241m.\u001b[39msample_op(logits, TEMPERATURE, np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e7\u001b[39m), logits_mask_tensor)  \u001b[38;5;66;03m# out=[1,1,seq_len,1], uint32\u001b[39;00m\n\u001b[1;32m     40\u001b[0m next_token_idx \u001b[38;5;241m=\u001b[39m max_sequence_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt_tokens) \u001b[38;5;241m>\u001b[39m max_sequence_length \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt_tokens) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 41\u001b[0m next_token \u001b[38;5;241m=\u001b[39m \u001b[43mnext_token_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()[next_token_idx]\n\u001b[1;32m     43\u001b[0m output \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(next_token)\n\u001b[1;32m     45\u001b[0m prompt_tokens\u001b[38;5;241m.\u001b[39mappend(next_token)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prompt_str = \"Generating with pytorch(CPU, might be slow)\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with TT:\")\n",
    "generate_with_tt(tt_model, prompt_tokens.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a76d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Generating with torch model:\n",
    "************************************\n",
    " - How to speed it up?\n",
    "\n",
    "PyTorch is a powerful deep learning framework, but it can be slow for generating data, especially when using CPU. Here are some suggestions to speed up the process:\n",
    "\n",
    "1.  **Use a GPU**: PyTorch supports GPU acceleration, which can significantly speed up the process. You can use a GPU with a CUDA-enabled NVIDIA GPU or a Tesla V100 GPU.\n",
    "2.  **Use a faster backend**: PyTorch has a few different backends\n",
    "************************************\n",
    "Generated 100 tokens in 18.55 seconds\n",
    "Performance: 5.39 tokens/second\n",
    "************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ca5d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Generating with TT:\n",
    "************************************\n",
    " - How to speed it up?\n",
    "\n",
    "PyTorch is a powerful deep learning framework, but it can be slow for certain tasks, especially when using CPU. Here are some tips to speed up your PyTorch code:\n",
    "\n",
    "1.  **Use GPU acceleration**: PyTorch supports GPU acceleration through the `torch.device` and `torch.cuda` modules. You can use the `torch.device` to specify the device (GPU or CPU) and the `torch.cuda` module to access the GPU.\n",
    "\n",
    "************************************\n",
    "Generated 100 tokens in 101.75 seconds\n",
    "Performance: 0.98 tokens/second\n",
    "************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c4b440",
   "metadata": {},
   "source": [
    "prompt_str = \"Generating with PyTorch, (should be slow, without kv-caching)\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with PyTorch:\")\n",
    "generate_with_pytorch_no_cache(torch_model, prompt_tokens.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8521eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = torch_model.state_dict()\n",
    "for s in sd:Generating with torch model:\n",
    "************************************\n",
    " - How to speed it up?\n",
    "\n",
    "PyTorch is a powerful deep learning framework, but it can be slow for generating data, especially when using CPU. Here are some suggestions to speed up the process:\n",
    "\n",
    "1.  **Use a GPU**: PyTorch supports GPU acceleration, which can significantly speed up the process. You can use a GPU with a CUDA-enabled NVIDIA GPU or a Tesla V100 GPU.\n",
    "2.  **Use a faster backend**: PyTorch has a few different backends\n",
    "************************************\n",
    "Generated 100 tokens in 18.55 seconds\n",
    "Performance: 5.39 tokens/second\n",
    "************************************\n",
    "    print(s, sd[s].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e868214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = tt_model.parameters()\n",
    "for s in k:\n",
    "    print(s, k[s].shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aa5542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e9744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def apply_rope_permutation(w, num_heads):\n",
    "    \"\"\"\n",
    "    Apply RoPE row permutation to match TT's q_proj weight layout.\n",
    "    TT applies unpermute_proj_rows during loading which interleaves rows within each head.\n",
    "    \"\"\"\n",
    "    rows, cols = w.shape\n",
    "    head_dim = rows // num_heads\n",
    "    \n",
    "    out = np.zeros_like(w)\n",
    "    for h in range(num_heads):\n",
    "        head_start = h * head_dim\n",
    "        half = head_dim // 2\n",
    "        \n",
    "        # Interleave: [0..half-1, half..head_dim-1] â†’ [0, half, 1, half+1, ..., half-1, head_dim-1]\n",
    "        for i in range(half):\n",
    "            out[head_start + 2*i] = w[head_start + i]\n",
    "            out[head_start + 2*i + 1] = w[head_start + half + i]\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def compare_weights_llama(torch_model, tt_model):\n",
    "    \"\"\"\n",
    "    Compare weights between PyTorch model and TT-Metal model.\n",
    "    \n",
    "    Args:\n",
    "        torch_model: PyTorch model (with .state_dict())\n",
    "        tt_model: TT-Metal model (with .parameters())\n",
    "    \"\"\"\n",
    "    \n",
    "    torch_sd = torch_model.state_dict()\n",
    "    tt_params = tt_model.parameters()\n",
    "    \n",
    "    # Get num_heads from torch model config\n",
    "    num_heads = torch_model.config.num_attention_heads\n",
    "    \n",
    "    # Detect weight tying configuration from TT model\n",
    "    has_tok_emb = 'llama/tok_emb/weight' in tt_params\n",
    "    has_fc = 'llama/fc/weight' in tt_params\n",
    "    weight_tying_enabled = not has_tok_emb  # If tok_emb doesn't exist, weight tying is enabled\n",
    "    \n",
    "    # Mapping from PyTorch parameter names to TT-Metal parameter names\n",
    "    pytorch_to_tt_mapping = {\n",
    "        # Final layer norm\n",
    "        'model.norm.weight': 'llama/ln_fc/gamma',\n",
    "    }\n",
    "    \n",
    "    # Add embedding mappings based on weight tying configuration\n",
    "    if weight_tying_enabled:\n",
    "        # Weight tying enabled: both embed_tokens and lm_head use fc/weight\n",
    "        pytorch_to_tt_mapping['model.embed_tokens.weight'] = 'llama/fc/weight'\n",
    "        # lm_head.weight should also map to fc/weight, but we'll skip it to avoid duplicate checks\n",
    "    else:\n",
    "        # Weight tying disabled: separate tok_emb and fc\n",
    "        pytorch_to_tt_mapping['model.embed_tokens.weight'] = 'llama/tok_emb/weight'\n",
    "        pytorch_to_tt_mapping['lm_head.weight'] = 'llama/fc/weight'\n",
    "    \n",
    "    # Add layer-specific mappings\n",
    "    for i in range(50):  # Support up to 50 layers\n",
    "        layer_prefix_pt = f'model.layers.{i}'\n",
    "        layer_prefix_tt = f'llama/llama_block_{i}'\n",
    "        \n",
    "        pytorch_to_tt_mapping.update({\n",
    "            f'{layer_prefix_pt}.input_layernorm.weight': f'{layer_prefix_tt}/attention_norm/gamma',\n",
    "            f'{layer_prefix_pt}.post_attention_layernorm.weight': f'{layer_prefix_tt}/mlp_norm/gamma',\n",
    "            f'{layer_prefix_pt}.self_attn.q_proj.weight': f'{layer_prefix_tt}/attention/q_linear/weight',\n",
    "            f'{layer_prefix_pt}.self_attn.o_proj.weight': f'{layer_prefix_tt}/attention/out_linear/weight',\n",
    "            # k_proj and v_proj are combined into kv_linear in TT\n",
    "            f'{layer_prefix_pt}.mlp.gate_proj.weight': f'{layer_prefix_tt}/mlp/w1/weight',\n",
    "            f'{layer_prefix_pt}.mlp.up_proj.weight': f'{layer_prefix_tt}/mlp/w3/weight',\n",
    "            f'{layer_prefix_pt}.mlp.down_proj.weight': f'{layer_prefix_tt}/mlp/w2/weight',\n",
    "        })\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"WEIGHT COMPARISON: PyTorch vs TT-Metal\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Note: Detected num_heads={num_heads} for RoPE permutation\")\n",
    "    print(f\"Note: Weight tying {'ENABLED' if weight_tying_enabled else 'DISABLED'}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    mismatches = []\n",
    "    matches = []\n",
    "    \n",
    "    for pt_name in torch_sd.keys():\n",
    "        if 'bias' in pt_name:\n",
    "            continue  # Skip bias parameters\n",
    "            \n",
    "        pt_tensor = torch_sd[pt_name]\n",
    "        pt_shape = tuple(pt_tensor.shape)\n",
    "        \n",
    "        # Handle k_proj and v_proj specially (they're combined in TT)\n",
    "        if '.self_attn.k_proj.weight' in pt_name or '.self_attn.v_proj.weight' in pt_name:\n",
    "            layer_idx = pt_name.split('.')[2]\n",
    "            tt_name = f'llama/llama_block_{layer_idx}/attention/kv_linear/weight'\n",
    "            \n",
    "            if tt_name in tt_params:\n",
    "                tt_tensor_np = tt_params[tt_name].to_numpy()\n",
    "                tt_shape = tt_tensor_np.shape\n",
    "                \n",
    "                # k_proj and v_proj are concatenated in kv_linear\n",
    "                # Expected: k_proj [512, 2048] + v_proj [512, 2048] = kv_linear [1, 1, 1024, 2048]\n",
    "                if '.self_attn.k_proj.weight' in pt_name:\n",
    "                    print(f\"\\n{pt_name}\")\n",
    "                    print(f\"  PyTorch: {pt_shape}\")\n",
    "                    print(f\"  TT (kv combined): {tt_shape}\")\n",
    "                    print(f\"  Status: K and V are combined in TT as kv_linear\")\n",
    "            continue\n",
    "        \n",
    "        # Get corresponding TT parameter name\n",
    "        tt_name = pytorch_to_tt_mapping.get(pt_name)\n",
    "        if not tt_name:\n",
    "            continue\n",
    "            \n",
    "        if tt_name not in tt_params:\n",
    "            print(f\"\\nâŒ MISSING: {pt_name} -> {tt_name}\")\n",
    "            print(f\"   PyTorch shape: {pt_shape}\")\n",
    "            mismatches.append((pt_name, \"MISSING IN TT\"))\n",
    "            continue\n",
    "        \n",
    "        # Get TT tensor\n",
    "        tt_tensor_np = tt_params[tt_name].to_numpy()\n",
    "        tt_shape = tt_tensor_np.shape\n",
    "        \n",
    "        # Remove batch dimensions [1, 1, ...] from TT tensor\n",
    "        tt_shape_no_batch = tt_shape[2:] if len(tt_shape) == 4 else tt_shape\n",
    "        \n",
    "        # Compare shapes\n",
    "        pt_numpy = pt_tensor.cpu().float().numpy()  # Convert to float32 for numpy compatibility\n",
    "        \n",
    "        # Special handling for q_proj: TT applies RoPE row permutation during loading\n",
    "        is_q_proj = '.self_attn.q_proj.weight' in pt_name\n",
    "        if is_q_proj and len(pt_shape) == 2 and pt_shape[0] % num_heads == 0:\n",
    "            pt_numpy = apply_rope_permutation(pt_numpy, num_heads)\n",
    "        \n",
    "        # For layer norms: PT (N,) vs TT (1, N) - both are fine, just broadcasting\n",
    "        # Check if PT is 1D and TT has leading 1s that can be squeezed\n",
    "        if len(pt_shape) == 1 and len(tt_shape_no_batch) == 2 and tt_shape_no_batch[0] == 1:\n",
    "            tt_shape_no_batch = (tt_shape_no_batch[1],)  # Squeeze leading 1\n",
    "        \n",
    "        # Check if shapes match (with or without transpose)\n",
    "        shape_match = (pt_shape == tt_shape_no_batch) or (pt_shape == tt_shape_no_batch[::-1])\n",
    "        \n",
    "        if shape_match:\n",
    "            # Check actual values\n",
    "            # Reshape TT data to match PT shape (handle batch dims and potential squeezing)\n",
    "            if len(tt_shape) == 4:\n",
    "                tt_data = tt_tensor_np.reshape(tt_shape[2:])  # Remove [1,1,...] batch dims\n",
    "            else:\n",
    "                tt_data = tt_tensor_np.reshape(tt_shape)\n",
    "            \n",
    "            # Squeeze if needed for 1D comparisons\n",
    "            tt_data = tt_data.squeeze()\n",
    "            pt_numpy_squeezed = pt_numpy.squeeze()\n",
    "            \n",
    "            # Handle transpose if needed\n",
    "            if pt_numpy_squeezed.shape != tt_data.shape and len(tt_data.shape) == 2:\n",
    "                tt_data = tt_data.T\n",
    "            \n",
    "            diff = np.abs(pt_numpy_squeezed - tt_data).max()\n",
    "            rel_diff = diff / (np.abs(pt_numpy_squeezed).max() + 1e-8)\n",
    "            \n",
    "            status = \"âœ“\" if diff < 1e-3 else \"âš \"\n",
    "            note = \" (after RoPE permutation)\" if is_q_proj else \"\"\n",
    "            print(f\"\\n{status} {pt_name}{note}\")\n",
    "            print(f\"  PyTorch: {pt_shape}\")\n",
    "            print(f\"  TT:      {tt_shape} -> {tt_shape_no_batch}\")\n",
    "            print(f\"  Max diff: {diff:.6f}, Rel diff: {rel_diff:.6f}\")\n",
    "            \n",
    "            if diff < 1e-3:\n",
    "                matches.append(pt_name)\n",
    "            else:\n",
    "                mismatches.append((pt_name, f\"VALUE_DIFF={diff:.6f}\"))\n",
    "        else:\n",
    "            print(f\"\\nâŒ SHAPE MISMATCH: {pt_name}\")\n",
    "            print(f\"  PyTorch: {pt_shape}\")\n",
    "            print(f\"  TT:      {tt_shape} -> {tt_shape_no_batch}\")\n",
    "            mismatches.append((pt_name, f\"SHAPE: PT={pt_shape} vs TT={tt_shape_no_batch}\"))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"SUMMARY: {len(matches)} matches, {len(mismatches)} mismatches\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if mismatches:\n",
    "        print(\"\\nâŒ MISMATCHES:\")\n",
    "        for name, issue in mismatches:\n",
    "            print(f\"  - {name}: {issue}\")\n",
    "    \n",
    "    return matches, mismatches\n",
    "\n",
    "\n",
    "# Usage example (commented out):\n",
    "matches, mismatches = compare_weights_llama(torch_model, tt_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7af5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Qwen3 Weight Comparison Script\n",
    "\n",
    "Compares weights between PyTorch Qwen3 model and TT-Metal Qwen3 model.\n",
    "\n",
    "Key Qwen3 features:\n",
    "- Explicit head_dim (128 for 0.6B model)\n",
    "- Q/K normalization for numerical stability (CRITICAL!)\n",
    "- Attention dimension != embedding dimension\n",
    "- Weight tying enabled by default\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def apply_rope_permutation(w, num_heads):\n",
    "    \"\"\"\n",
    "    Apply RoPE row permutation to match TT's q_proj/k_proj weight layout.\n",
    "    TT applies unpermute_proj_rows during loading which interleaves rows within each head.\n",
    "    \n",
    "    This reorders: [0..D/2-1, D/2..D-1] â†’ [0, D/2, 1, D/2+1, ..., D/2-1, D-1]\n",
    "    \"\"\"\n",
    "    rows, cols = w.shape\n",
    "    head_dim = rows // num_heads\n",
    "    \n",
    "    if head_dim % 2 != 0:\n",
    "        raise ValueError(f\"Head dimension {head_dim} must be even for RoPE permutation\")\n",
    "    \n",
    "    out = np.zeros_like(w)\n",
    "    for h in range(num_heads):\n",
    "        head_start = h * head_dim\n",
    "        half = head_dim // 2\n",
    "        \n",
    "        # Interleave: first half and second half of each head\n",
    "        for i in range(half):\n",
    "            out[head_start + 2*i] = w[head_start + i]\n",
    "            out[head_start + 2*i + 1] = w[head_start + half + i]\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def compare_qwen3_weights(torch_model, tt_model, verbose=True):\n",
    "    \"\"\"\n",
    "    Compare weights between PyTorch Qwen3 model and TT-Metal Qwen3 model.\n",
    "    \n",
    "    Qwen3-specific features:\n",
    "    - Q projection: [2048, 1024] (projects UP to attention space)\n",
    "    - K projection: [1024, 1024] (num_kv_heads * head_dim)\n",
    "    - V projection: [1024, 1024] (num_kv_heads * head_dim)\n",
    "    - O projection: [1024, 2048] (projects DOWN to embedding space)\n",
    "    - Q/K normalization: RMSNorm on head_dim (128) - CRITICAL!\n",
    "    \n",
    "    Args:\n",
    "        torch_model: PyTorch Qwen3 model (with .state_dict())\n",
    "        tt_model: TT-Metal Qwen3 model (with .parameters())\n",
    "        verbose: Print detailed comparison (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (matches, mismatches, missing_in_tt)\n",
    "    \"\"\"\n",
    "    \n",
    "    torch_sd = torch_model.state_dict()\n",
    "    tt_params = tt_model.parameters()\n",
    "    \n",
    "    # Get configuration from torch model\n",
    "    num_heads = torch_model.config.num_attention_heads\n",
    "    num_kv_heads = torch_model.config.num_key_value_heads\n",
    "    head_dim = getattr(torch_model.config, 'head_dim', 128)\n",
    "    hidden_size = torch_model.config.hidden_size\n",
    "    \n",
    "    # Detect weight tying configuration from TT model\n",
    "    has_tok_emb = 'qwen3/tok_emb/weight' in tt_params\n",
    "    has_fc = 'qwen3/fc/weight' in tt_params\n",
    "    weight_tying_enabled = not has_tok_emb  # If tok_emb doesn't exist, weight tying is enabled\n",
    "    \n",
    "    # Mapping from PyTorch parameter names to TT-Metal parameter names\n",
    "    pytorch_to_tt_mapping = {\n",
    "        # Final layer norm\n",
    "        'model.norm.weight': 'qwen3/ln_fc/gamma',\n",
    "    }\n",
    "    \n",
    "    # Add embedding mappings based on weight tying configuration\n",
    "    if weight_tying_enabled:\n",
    "        pytorch_to_tt_mapping['model.embed_tokens.weight'] = 'qwen3/fc/weight'\n",
    "        pytorch_to_tt_mapping['lm_head.weight'] = 'qwen3/fc/weight'  # Both tied to fc/weight\n",
    "    else:\n",
    "        pytorch_to_tt_mapping['model.embed_tokens.weight'] = 'qwen3/tok_emb/weight'\n",
    "        pytorch_to_tt_mapping['lm_head.weight'] = 'qwen3/fc/weight'\n",
    "    \n",
    "    # Add layer-specific mappings\n",
    "    for i in range(50):  # Support up to 50 layers\n",
    "        layer_prefix_pt = f'model.layers.{i}'\n",
    "        layer_prefix_tt = f'qwen3/qwen3_block_{i}'\n",
    "        \n",
    "        pytorch_to_tt_mapping.update({\n",
    "            # Layer norms\n",
    "            f'{layer_prefix_pt}.input_layernorm.weight': f'{layer_prefix_tt}/attention_norm/gamma',\n",
    "            f'{layer_prefix_pt}.post_attention_layernorm.weight': f'{layer_prefix_tt}/mlp_norm/gamma',\n",
    "            \n",
    "            # Attention projections - SEPARATE Q, K, V (not combined!)\n",
    "            f'{layer_prefix_pt}.self_attn.q_proj.weight': f'{layer_prefix_tt}/attention/q_linear/weight',\n",
    "            f'{layer_prefix_pt}.self_attn.k_proj.weight': f'{layer_prefix_tt}/attention/k_linear/weight',\n",
    "            f'{layer_prefix_pt}.self_attn.v_proj.weight': f'{layer_prefix_tt}/attention/v_linear/weight',\n",
    "            f'{layer_prefix_pt}.self_attn.o_proj.weight': f'{layer_prefix_tt}/attention/out_linear/weight',\n",
    "            \n",
    "            # Q/K norms - CRITICAL for Qwen3 numerical stability!\n",
    "            f'{layer_prefix_pt}.self_attn.q_norm.weight': f'{layer_prefix_tt}/attention/q_norm/gamma',\n",
    "            f'{layer_prefix_pt}.self_attn.k_norm.weight': f'{layer_prefix_tt}/attention/k_norm/gamma',\n",
    "            \n",
    "            # MLP projections\n",
    "            f'{layer_prefix_pt}.mlp.gate_proj.weight': f'{layer_prefix_tt}/mlp/w1/weight',\n",
    "            f'{layer_prefix_pt}.mlp.up_proj.weight': f'{layer_prefix_tt}/mlp/w3/weight',\n",
    "            f'{layer_prefix_pt}.mlp.down_proj.weight': f'{layer_prefix_tt}/mlp/w2/weight',\n",
    "        })\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"WEIGHT COMPARISON: PyTorch Qwen3 vs TT-Metal Qwen3\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Model Configuration:\")\n",
    "        print(f\"  - num_attention_heads: {num_heads}\")\n",
    "        print(f\"  - num_key_value_heads: {num_kv_heads}\")\n",
    "        print(f\"  - head_dim: {head_dim}\")\n",
    "        print(f\"  - hidden_size: {hidden_size}\")\n",
    "        print(f\"  - attention_output_dim: {num_heads * head_dim}\")\n",
    "        print(f\"  - weight_tying: {'ENABLED' if weight_tying_enabled else 'DISABLED'}\")\n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    mismatches = []\n",
    "    matches = []\n",
    "    missing_in_tt = []\n",
    "    \n",
    "    for pt_name in torch_sd.keys():\n",
    "        if 'bias' in pt_name:\n",
    "            continue  # Skip bias parameters (Qwen3 has no biases)\n",
    "            \n",
    "        pt_tensor = torch_sd[pt_name]\n",
    "        pt_shape = tuple(pt_tensor.shape)\n",
    "        \n",
    "        # Special handling for k_proj: apply RoPE permutation\n",
    "        if '.self_attn.k_proj.weight' in pt_name:\n",
    "            layer_idx = pt_name.split('.')[2]\n",
    "            tt_name = f'qwen3/qwen3_block_{layer_idx}/attention/k_linear/weight'\n",
    "            \n",
    "            if tt_name in tt_params:\n",
    "                pt_numpy = pt_tensor.cpu().float().numpy()\n",
    "                \n",
    "                # Apply RoPE permutation to K projection\n",
    "                pt_numpy_permuted = apply_rope_permutation(pt_numpy, num_kv_heads)\n",
    "                \n",
    "                tt_tensor_np = tt_params[tt_name].to_numpy()\n",
    "                tt_shape = tt_tensor_np.shape\n",
    "                tt_data = tt_tensor_np.reshape(tt_shape[2:]) if len(tt_shape) == 4 else tt_tensor_np\n",
    "                \n",
    "                # Handle transpose if needed\n",
    "                if pt_numpy_permuted.shape != tt_data.shape:\n",
    "                    tt_data = tt_data.T\n",
    "                \n",
    "                diff = np.abs(pt_numpy_permuted - tt_data).max()\n",
    "                rel_diff = diff / (np.abs(pt_numpy_permuted).max() + 1e-8)\n",
    "                \n",
    "                status = \"âœ“\" if diff < 1e-4 else \"âš \"\n",
    "                if verbose:\n",
    "                    print(f\"\\n{status} {pt_name} (after RoPE permutation)\")\n",
    "                    print(f\"  PyTorch: {pt_shape}\")\n",
    "                    print(f\"  TT:      {tt_shape}\")\n",
    "                    print(f\"  Max diff: {diff:.6f}, Rel diff: {rel_diff:.6f}\")\n",
    "                \n",
    "                if diff < 1e-3:\n",
    "                    matches.append(pt_name)\n",
    "                else:\n",
    "                    mismatches.append((pt_name, f\"K_DIFF={diff:.6f}\"))\n",
    "            else:\n",
    "                missing_in_tt.append((pt_name, tt_name))\n",
    "            continue\n",
    "        \n",
    "        # Get corresponding TT parameter name\n",
    "        tt_name = pytorch_to_tt_mapping.get(pt_name)\n",
    "        if not tt_name:\n",
    "            if verbose:\n",
    "                print(f\"\\nâŠ— NOT MAPPED: {pt_name}\")\n",
    "                print(f\"  PyTorch: {pt_shape}\")\n",
    "                print(f\"  Status: No TT equivalent defined in mapping\")\n",
    "            missing_in_tt.append((pt_name, \"NOT_MAPPED\"))\n",
    "            continue\n",
    "            \n",
    "        if tt_name not in tt_params:\n",
    "            if verbose:\n",
    "                print(f\"\\nâŒ MISSING IN TT: {pt_name} -> {tt_name}\")\n",
    "                print(f\"   PyTorch shape: {pt_shape}\")\n",
    "            missing_in_tt.append((pt_name, tt_name))\n",
    "            continue\n",
    "        \n",
    "        # Get TT tensor\n",
    "        tt_tensor_np = tt_params[tt_name].to_numpy()\n",
    "        tt_shape = tt_tensor_np.shape\n",
    "        \n",
    "        # Remove batch dimensions [1, 1, ...] from TT tensor\n",
    "        tt_shape_no_batch = tt_shape[2:] if len(tt_shape) == 4 else tt_shape\n",
    "        \n",
    "        # Compare shapes\n",
    "        pt_numpy = pt_tensor.cpu().float().numpy()\n",
    "        \n",
    "        # Check what type of parameter this is\n",
    "        is_q_proj = '.self_attn.q_proj.weight' in pt_name\n",
    "        is_q_norm = '.self_attn.q_norm.weight' in pt_name\n",
    "        is_k_norm = '.self_attn.k_norm.weight' in pt_name\n",
    "        is_v_proj = '.self_attn.v_proj.weight' in pt_name\n",
    "        \n",
    "        # Apply RoPE permutation for Q projection\n",
    "        if is_q_proj:\n",
    "            if len(pt_shape) == 2 and pt_shape[0] == num_heads * head_dim:\n",
    "                pt_numpy = apply_rope_permutation(pt_numpy, num_heads)\n",
    "        \n",
    "        # For layer norms: PT (N,) vs TT (1, N) or (1, 1, 1, N) - handle squeezing\n",
    "        if len(pt_shape) == 1:\n",
    "            if len(tt_shape_no_batch) == 2 and tt_shape_no_batch[0] == 1:\n",
    "                tt_shape_no_batch = (tt_shape_no_batch[1],)\n",
    "            elif len(tt_shape_no_batch) == 1:\n",
    "                pass  # Already 1D\n",
    "        \n",
    "        # Check if shapes match (with or without transpose)\n",
    "        shape_match = (pt_shape == tt_shape_no_batch) or (pt_shape == tt_shape_no_batch[::-1])\n",
    "        \n",
    "        if shape_match:\n",
    "            # Check actual values\n",
    "            if len(tt_shape) == 4:\n",
    "                tt_data = tt_tensor_np.reshape(tt_shape[2:])\n",
    "            else:\n",
    "                tt_data = tt_tensor_np.reshape(tt_shape)\n",
    "            \n",
    "            tt_data = tt_data.squeeze()\n",
    "            pt_numpy_squeezed = pt_numpy.squeeze()\n",
    "            \n",
    "            # Handle transpose if needed\n",
    "            if pt_numpy_squeezed.shape != tt_data.shape and len(tt_data.shape) == 2:\n",
    "                tt_data = tt_data.T\n",
    "            \n",
    "            diff = np.abs(pt_numpy_squeezed - tt_data).max()\n",
    "            rel_diff = diff / (np.abs(pt_numpy_squeezed).max() + 1e-8)\n",
    "            \n",
    "            status = \"âœ“\" if diff < 1e-3 else \"âš \"\n",
    "            note = \"\"\n",
    "            if is_q_proj:\n",
    "                note = \" (after RoPE permutation)\"\n",
    "            elif is_q_norm:\n",
    "                note = \" [CRITICAL Q/K NORM]\"\n",
    "            elif is_k_norm:\n",
    "                note = \" [CRITICAL Q/K NORM]\"\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\n{status} {pt_name}{note}\")\n",
    "                print(f\"  PyTorch: {pt_shape}\")\n",
    "                print(f\"  TT:      {tt_shape} -> {tt_shape_no_batch}\")\n",
    "                print(f\"  Max diff: {diff:.6f}, Rel diff: {rel_diff:.6f}\")\n",
    "            \n",
    "            if diff < 1e-3:\n",
    "                matches.append(pt_name)\n",
    "            else:\n",
    "                mismatches.append((pt_name, f\"VALUE_DIFF={diff:.6f}\"))\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"\\nâŒ SHAPE MISMATCH: {pt_name}\")\n",
    "                print(f\"  PyTorch: {pt_shape}\")\n",
    "                print(f\"  TT:      {tt_shape} -> {tt_shape_no_batch}\")\n",
    "            mismatches.append((pt_name, f\"SHAPE: PT={pt_shape} vs TT={tt_shape_no_batch}\"))\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"SUMMARY: {len(matches)} matches, {len(mismatches)} mismatches, {len(missing_in_tt)} missing in TT\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        if missing_in_tt:\n",
    "            print(f\"\\nâŒ MISSING IN TT ({len(missing_in_tt)}):\")\n",
    "            for pt_name, tt_name in missing_in_tt:\n",
    "                print(f\"  - {pt_name} -> {tt_name}\")\n",
    "        \n",
    "        if mismatches:\n",
    "            print(\"\\nâŒ MISMATCHES:\")\n",
    "            for name, issue in mismatches:\n",
    "                print(f\"  - {name}: {issue}\")\n",
    "        \n",
    "        if len(mismatches) == 0 and len(missing_in_tt) == 0:\n",
    "            print(\"\\nðŸŽ‰ ALL WEIGHTS MATCH PERFECTLY!\")\n",
    "            print(f\"âœ… {len(matches)} parameters validated\")\n",
    "            print(\"âœ… Q/K normalization layers loaded correctly\")\n",
    "            print(\"âœ… Qwen3 model is production-ready!\")\n",
    "    \n",
    "    return matches, mismatches, missing_in_tt\n",
    "\n",
    "\n",
    "matches, mismatches, missing = compare_qwen3_weights(torch_model, tt_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525920ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(torch_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c006c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1274ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d4984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade8881f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ccd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "import sys\n",
    "\n",
    "def convert_notebook_to_py(notebook_path, output_path):\n",
    "    \n",
    "    with open(notebook_path) as ff:\n",
    "        nb_in = nbformat.read(ff, nbformat.NO_CONVERT)\n",
    "    \n",
    "    source = \"\"\n",
    "    for cell in nb_in['cells']:\n",
    "        if cell['cell_type'] == 'code':\n",
    "            # cell['source'] can be a string or a list of strings\n",
    "            cell_source = cell['source']\n",
    "            \n",
    "            # Convert to list of lines if it's a string\n",
    "            if isinstance(cell_source, str):\n",
    "                lines = cell_source.split('\\n')\n",
    "            else:\n",
    "                # It's already a list, but may contain newlines within elements\n",
    "                lines = []\n",
    "                for item in cell_source:\n",
    "                    lines.extend(item.split('\\n'))\n",
    "            \n",
    "            # Filter out lines starting with '%' (magic commands)\n",
    "            filtered_lines = [line for line in lines if not line.strip().startswith('%')]\n",
    "            \n",
    "            # Join the lines back together\n",
    "            cell_code = '\\n'.join(filtered_lines)\n",
    "            \n",
    "            # Add to source with a newline separator\n",
    "            if cell_code.strip():  # Only add non-empty cells\n",
    "                source = source + '\\n' + cell_code\n",
    "    \n",
    "    # Write to output file\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(source)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0893d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_notebook_to_py('llm_inference_new.ipynb', 'llm_inference_new.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e26ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f868e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71842a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
