{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0725ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\" \n",
    "CONFIG = \"training_shakespeare_llama3_2_1B_fixed.yaml\" # now working fine (with proper shape for tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e1948c",
   "metadata": {},
   "source": [
    "model_id =  \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "CONFIG = \"training_shakespeare_tinyllama.yaml\" # working fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582d5c9f",
   "metadata": {},
   "source": [
    "model_id = \"Qwen/Qwen3-0.6B\" \n",
    "CONFIG = \"training_shakespeare_qwen3_0_6B.yaml\" #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a19149b",
   "metadata": {},
   "source": [
    "model_id =  \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "CONFIG = \"training_shakespeare_llama3_8B_tp.yaml\" # OOM on 12 GB, sucesfully loaded weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6545c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TOKENS = 100\n",
    "WITH_SAMPLING = True\n",
    "TEMPERATURE = 0.8\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a090295",
   "metadata": {},
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df6d6bf9-a010-4206-b004-807ee500d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "import numpy as np  # For numpy arrays\n",
    "from dataclasses import dataclass # For configuration classes\n",
    "from huggingface_hub import hf_hub_download # To download safetensors from Hugging Face\n",
    "from transformers import AutoTokenizer\n",
    "from yaml import safe_load # To read YAML configs\n",
    "from pathlib import Path\n",
    "\n",
    "import ttml\n",
    "from ttml.common.config import get_config, TransformerConfig\n",
    "from ttml.common.utils import set_seed, round_up_to_tile\n",
    "from ttml.common.model_factory import TransformerModelFactory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55071c7a-74fe-4f56-8f33-e45ffe699c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from Hugging Face and the transformer config from YAML\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "transformer_config = TransformerConfig(get_config(CONFIG).get(\"training_config\", {}).get(\"transformer_config\",{}))\n",
    "yaml_config = get_config(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af21d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e7883f6-f8e6-436b-8908-62c8a26e1e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "safetensors_path = hf_hub_download(repo_id=model_id, filename=\"config.json\")\n",
    "safetensors_path = safetensors_path.replace(\"config.json\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f77e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "torch.manual_seed(SEED)\n",
    "torch_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f405407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128000, 128256, 128256)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size, torch_model.state_dict()['model.embed_tokens.weight'].shape[0], torch_model.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50d8ce12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torch_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18d627a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_size = tokenizer.vocab_size + len(tokenizer.added_tokens_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf74d9ac-7afd-4c6f-887e-b793c6e44c18",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128256\n",
      "Llama configuration:\n",
      "    Vocab size: 128256\n",
      "    Max sequence length: 2048\n",
      "    Embedding dim: 2048\n",
      "    Intermediate dim: 8192\n",
      "    Num heads: 32\n",
      "    Num groups: 8\n",
      "    Dropout probability: 0\n",
      "    Num blocks: 16\n",
      "    Positional embedding type: RoPE\n",
      "    Runner type: Memory efficient\n",
      "    Weight tying: Enabled\n",
      "    Theta: 500000\n",
      "2025-11-07 05:49:59.513 | info     |             UMD | Established cluster ETH FW version: 6.14.0 (topology_discovery_wormhole.cpp:359)\n",
      "2025-11-07 05:49:59.517 | info     |          Device | Opening user mode device driver (tt_cluster.cpp:209)\n",
      "2025-11-07 05:49:59.538 | info     |             UMD | Established cluster ETH FW version: 6.14.0 (topology_discovery_wormhole.cpp:359)\n",
      "2025-11-07 05:49:59.578 | info     |             UMD | Established cluster ETH FW version: 6.14.0 (topology_discovery_wormhole.cpp:359)\n",
      "2025-11-07 05:49:59.581 | info     |             UMD | Harvesting mask for chip 0 is 0x1 (NOC0: 0x1, simulated harvesting mask: 0x0). (cluster.cpp:413)\n",
      "2025-11-07 05:49:59.615 | warning  |             UMD | init_detect_tt_device_numanodes(): Could not determine NumaNodeSet for TT device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:564)\n",
      "2025-11-07 05:49:59.615 | warning  |             UMD | Could not find NumaNodeSet for TT Device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:173)\n",
      "2025-11-07 05:49:59.615 | warning  |             UMD | bind_area_memory_nodeset(): Unable to determine TT Device to NumaNode mapping for physical_device_id: 0. Skipping membind. (cpuset_lib.cpp:450)\n",
      "2025-11-07 05:49:59.615 | warning  |             UMD | ---- ttSiliconDevice::init_hugepage: bind_area_to_memory_nodeset() failed (physical_device_id: 0 ch: 0). Hugepage allocation is not on NumaNode matching TT Device. Side-Effect is decreased Device->Host perf (Issue #893). (sysmem_manager.cpp:212)\n",
      "2025-11-07 05:49:59.616 | info     |             UMD | Opening local chip ids/PCIe ids: {0}/[0] and remote chip ids {} (cluster.cpp:257)\n",
      "2025-11-07 05:49:59.616 | info     |             UMD | All devices in cluster running firmware version: 80.17.0 (cluster.cpp:235)\n",
      "2025-11-07 05:49:59.616 | info     |             UMD | IOMMU: disabled (cluster.cpp:177)\n",
      "2025-11-07 05:49:59.616 | info     |             UMD | KMD version: 2.0.0 (cluster.cpp:180)\n",
      "2025-11-07 05:49:59.618 | info     |             UMD | Pinning pages for Hugepage: virtual address 0x7f4780000000 and size 0x40000000 pinned to physical address 0x200000000 (pci_device.cpp:536)\n",
      "2025-11-07 05:50:00.865 | info     |           Metal | Profiler started on device 0 (device_pool.cpp:203)\n",
      "    RoPE scaling enabled:\n",
      "        Scaling factor: 32\n",
      "        Original context length: 8192\n",
      "        High freq factor: 4\n",
      "        Low freq factor: 1\n",
      "Loading model from: /home/ubuntu/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/model.safetensors\n",
      "Loading tensor: model.embed_tokens.weight, shape:Shape([128256, 2048]), dtype:BF16\n",
      "Using parameter: llama/fc/weight with shape: Shape([1, 1, 128256, 2048])\n",
      "Loading tensor: model.layers.0.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.0.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.0.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.0.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.0.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.0.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.0.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.0.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.0.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_0/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 0\n",
      "Loading tensor: model.layers.1.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.1.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.1.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.1.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.1.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.1.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.1.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.1.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.1.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_1/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 1\n",
      "Loading tensor: model.layers.10.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.10.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.10.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.10.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.10.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.10.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.10.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.10.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.10.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_10/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 10\n",
      "Loading tensor: model.layers.11.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.11.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.11.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.11.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.11.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.11.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.11.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.11.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.11.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_11/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 11\n",
      "Loading tensor: model.layers.12.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.12.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.12.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.12.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.12.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.12.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.12.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.12.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.12.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_12/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 12\n",
      "Loading tensor: model.layers.13.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.13.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.13.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.13.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.13.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.13.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.13.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using parameter: llama/llama_block_13/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.13.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.13.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_13/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 13\n",
      "Loading tensor: model.layers.14.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.14.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.14.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.14.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.14.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.14.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.14.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.14.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_14/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.14.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: lla"
     ]
    }
   ],
   "source": [
    "orig_vocab_size = torch_model.vocab_size\n",
    "print(orig_vocab_size)\n",
    "tt_model_factory = TransformerModelFactory(yaml_config)\n",
    "tt_model_factory.transformer_config.vocab_size = orig_vocab_size\n",
    "\n",
    "max_sequence_length = tt_model_factory.transformer_config.max_sequence_length\n",
    "\n",
    "tt_model = tt_model_factory.create_model()\n",
    "tt_model.load_from_safetensors(safetensors_path)\n",
    "\n",
    "padded_vocab_size = round_up_to_tile(orig_vocab_size, 32)\n",
    "\n",
    "if orig_vocab_size != padded_vocab_size:\n",
    "    print(f\"Padding vocab size for tilization: original {orig_vocab_size} -> padded {padded_vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4d4f108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ma/llama_block_14/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 14\n",
      "Loading tensor: model.layers.15.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.15.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.15.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.15.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.15.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.15.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.15.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.15.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.15.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_15/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 15\n",
      "Loading tensor: model.layers.2.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.2.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.2.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.2.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.2.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.2.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.2.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.2.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.2.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_2/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 2\n",
      "Loading tensor: model.layers.3.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.3.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.3.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.3.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.3.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.3.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.3.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.3.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.3.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_3/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 3\n",
      "Loading tensor: model.layers.4.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.4.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.4.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.4.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.4.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.4.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.4.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.4.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.4.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_4/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 4\n",
      "Loading tensor: model.layers.5.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.5.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.5.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.5.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.5.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.5.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.5.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.5.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.5.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_5/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 5\n",
      "Loading tensor: model.layers.6.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.6.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.6.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.6.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.6.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.6.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.6.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.6.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.6.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_6/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 6\n",
      "Loading tensor: model.layers.7.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.7.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.7.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.7.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.7.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.7.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.7.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.7.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.7.self_attn.v_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_7/attention/kv_linear/weight with shape: Shape([1, 1, 1024, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 7\n",
      "Loading tensor: model.layers.8.input_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.8.mlp.down_proj.weight, shape:Shape([2048, 8192]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/mlp/w2/weight with shape: Shape([1, 1, 2048, 8192])\n",
      "Loading tensor: model.layers.8.mlp.gate_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/mlp/w1/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.8.mlp.up_proj.weight, shape:Shape([8192, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/mlp/w3/weight with shape: Shape([1, 1, 8192, 2048])\n",
      "Loading tensor: model.layers.8.post_attention_layernorm.weight, shape:Shape([2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.8.self_attn.k_proj.weight, shape:Shape([512, 2048]), dtype:BF16\n",
      "Loading tensor: model.layers.8.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:BF16\n",
      "Using parameter: llama/llama_block_8/attention/out_linear/weigh"
     ]
    }
   ],
   "source": [
    "def build_causal_mask(T: int) -> ttml.autograd.Tensor:\n",
    "    # [1,1,T,T] float32 with 1s for allowed positions (i >= j), else 0\\n\",\n",
    "    m = np.tril(np.ones((T, T), dtype=np.float32))\n",
    "    return ttml.autograd.Tensor.from_numpy(m.reshape(1, 1, T, T), ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)\n",
    "\n",
    "def build_logits_mask(vocab_size: int, padded_vocab_size: int) -> ttml.autograd.Tensor:\n",
    "    logits_mask = np.zeros((1, 1, 1, padded_vocab_size), dtype=np.float32)\n",
    "    logits_mask[:, :, :, vocab_size:] = 1e4\n",
    "    return ttml.autograd.Tensor.from_numpy(logits_mask, ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16)   # [1,1,1,T], bfloat16\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40913e88",
   "metadata": {},
   "source": [
    "`generate_with_tt()` uses TT hardware acceleration to generate output from the chosen LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18e6550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_tt(model, prompt_tokens):\n",
    "    import time\n",
    "    \n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(ttml.autograd.GradMode.DISABLED)\n",
    "    model.eval()\n",
    "\n",
    "    logits_mask_tensor = None\n",
    "\n",
    "    if padded_vocab_size != orig_vocab_size:\n",
    "        logits_mask_tensor = build_logits_mask(orig_vocab_size, padded_vocab_size)\n",
    "\n",
    "    causal_mask = build_causal_mask(max_sequence_length)  # [1,1,seq_len,seq_len], float32\n",
    "    padded_prompt_tokens = np.zeros((1, 1, 1, max_sequence_length), \n",
    "                                    dtype=np.uint32)\n",
    "\n",
    "    start_idx = 0\n",
    "\n",
    "    print(\"************************************\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for token_idx in range(OUTPUT_TOKENS):\n",
    "\n",
    "        if len(prompt_tokens) > max_sequence_length:\n",
    "            start_idx = len(prompt_tokens) - max_sequence_length\n",
    "\n",
    "        # padded_prompt_tokens[0, 0, 0, :transformer_cfg[\"max_sequence_length\"]] = 0\n",
    "        padded_prompt_tokens[0, 0, 0, :len(prompt_tokens)] = prompt_tokens[start_idx:]\n",
    "        padded_prompt_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "            padded_prompt_tokens,\n",
    "            ttml.Layout.ROW_MAJOR,\n",
    "            ttml.autograd.DataType.UINT32)  # [1,1,1, max_seq_len], uint32\n",
    "\n",
    "        logits = model(padded_prompt_tensor, causal_mask)  # out=[1,1,seq_len, vocab_size], bf16\n",
    "\n",
    "\n",
    "        next_token_tensor = ttml.ops.sample.sample_op(logits, TEMPERATURE, np.random.randint(low=1e7), logits_mask_tensor)  # out=[1,1,seq_len,1], uint32\n",
    "\n",
    "        next_token_idx = max_sequence_length - 1 if len(prompt_tokens) > max_sequence_length else len(prompt_tokens) - 1\n",
    "        next_token = next_token_tensor.to_numpy().flatten()[next_token_idx]\n",
    "\n",
    "        output = tokenizer.decode(next_token)\n",
    "\n",
    "        prompt_tokens.append(next_token)\n",
    "        print(output, end='', flush=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = OUTPUT_TOKENS / elapsed_time\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {OUTPUT_TOKENS} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af958d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7824c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_pytorch(torch_model, prompt_tokens):\n",
    "    import time\n",
    "    import torch.nn.functional as F\n",
    "    from transformers import DynamicCache\n",
    "    \n",
    "    torch_model.eval()\n",
    "    \n",
    "    print(\"************************************\")\n",
    "    # Convert list to tensor and add batch dimension\n",
    "    if isinstance(prompt_tokens, list):\n",
    "        prompt_tokens = torch.tensor([prompt_tokens])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize KV cache using the new DynamicCache API\n",
    "    past_key_values = DynamicCache()\n",
    "    input_ids = prompt_tokens\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(OUTPUT_TOKENS):\n",
    "            # Get model outputs with KV cache\n",
    "            outputs = torch_model(\n",
    "                input_ids=input_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            past_key_values = outputs.past_key_values\n",
    "            \n",
    "            # Get logits for the last token\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            \n",
    "            # Apply temperature and sample\n",
    "            if WITH_SAMPLING and TEMPERATURE > 0:\n",
    "                next_token_logits = next_token_logits / TEMPERATURE\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                # Greedy sampling\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            \n",
    "            # Decode and print the token\n",
    "            output = tokenizer.decode(next_token[0])\n",
    "            print(output, end='', flush=True)\n",
    "            \n",
    "            # For next iteration, only pass the new token (KV cache handles the rest)\n",
    "            input_ids = next_token\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = OUTPUT_TOKENS / elapsed_time\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {OUTPUT_TOKENS} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b17b326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_pytorch_batch(torch_model, prompt_tokens):\n",
    "    \"\"\"Old version: non-streaming batch generation using torch_model.generate()\"\"\"\n",
    "    import time\n",
    "    \n",
    "    torch_model.eval()\n",
    "    \n",
    "    print(\"************************************\")\n",
    "    # Convert list to tensor and add batch dimension\n",
    "    if isinstance(prompt_tokens, list):\n",
    "        prompt_tokens = torch.tensor([prompt_tokens])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = torch_model.generate(\n",
    "            prompt_tokens,\n",
    "            max_new_tokens=OUTPUT_TOKENS,\n",
    "            do_sample=WITH_SAMPLING,  # Enable sampling\n",
    "            temperature=TEMPERATURE,   # Temperature for sampling\n",
    "            num_beams=1  # Use multinomial sampling (standard sampling)\n",
    "        )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = OUTPUT_TOKENS / elapsed_time\n",
    "    \n",
    "    generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    for t in generated_text:\n",
    "        print(t)\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {OUTPUT_TOKENS} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4813faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with torch model:\n",
      "************************************\n",
      " with Cuda support\n",
      "\n",
      "You've got a good understanding of PyTorch and CUDA, you're looking to leverage their great performance capabilities. Here is an example that demonstrates generating a random number within the range [0, 1) using PyTorch and CUDA.\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "from torchvision import datasets, datasets_cifar10\n",
      "from torchvision import transforms\n",
      "import numpy as np\n",
      "\n",
      "def get_device():\n",
      "    if torch.cuda\n",
      "************************************\n",
      "Generated 100 tokens in 18.37 seconds\n",
      "Performance: 5.45 tokens/second\n",
      "************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_str = \"Generating with pytorch(CPU, might be slow)\"\n",
    "\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with torch model:\")\n",
    "generate_with_pytorch(torch_model, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d88bddde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with torch model batch:\n",
      "************************************\n",
      "Generating with pytorch(CPU, might be slow) - Alternatives to CPU and PyTorch\n",
      "\n",
      "If you're looking for ways to speed up your code, you can consider using a GPU instead of the CPU. PyTorch can be used on both CPU and GPU, but it has some differences in terms of speed.\n",
      "\n",
      "Here are some alternatives to using the CPU and PyTorch:\n",
      "\n",
      "### 1. Use a GPU\n",
      "\n",
      "If you have a GPU available, you can use it instead of the CPU. Here's an example of how to use\n",
      "\n",
      "************************************\n",
      "Generated 100 tokens in 19.03 seconds\n",
      "Performance: 5.25 tokens/second\n",
      "************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_str = \"Generating with pytorch(CPU, might be slow)\"\n",
    "\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with torch model batch:\")\n",
    "generate_with_pytorch_batch(torch_model, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a42273b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with TT:\n",
      "************************************\n",
      " and (optional) using kv-caching\n",
      "=====================================\n",
      "\n",
      "In this tutorial, we will guide you through the process of generating data with Tenstorrent and explore the benefits of using kv-caching.\n",
      "\n",
      "Table of Contents\n",
      "----------------\n",
      "\n",
      "1. [Introduction](#introduction)\n",
      "2. [Installing Tenstorrent](#installing-tenstorrent)\n",
      "3. [Generating Data with Tenstorrent](#generating-data-with-tenstorrent)\n",
      "4. [Using kv-caching](#\n",
      "************************************\n",
      "Generated 100 tokens in 124.67 seconds\n",
      "Performance: 0.80 tokens/second\n",
      "************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_str = \"Generating with Tenstorrent, (should be fast, even without kv-caching)\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with TT:\")\n",
    "generate_with_tt(tt_model, prompt_tokens.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62e38f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_pytorch_no_cache(torch_model, prompt_tokens):\n",
    "    import time\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    torch_model.eval()\n",
    "    \n",
    "    print(\"************************************\")\n",
    "    # Convert list to tensor and add batch dimension\n",
    "    if isinstance(prompt_tokens, list):\n",
    "        input_ids = torch.tensor([prompt_tokens])\n",
    "    else:\n",
    "        input_ids = prompt_tokens\n",
    "    \n",
    "    start_time = time.time()\n",
    "    tokens = 10  # this is very slow\n",
    "    with torch.no_grad():\n",
    "        for i in range(tokens):\n",
    "            # Process the entire sequence every time (no KV cache)\n",
    "            outputs = torch_model(\n",
    "                input_ids=input_ids,\n",
    "                use_cache=False\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Get logits for the last token\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            \n",
    "            # Apply temperature and sample\n",
    "            if WITH_SAMPLING and TEMPERATURE > 0:\n",
    "                next_token_logits = next_token_logits / TEMPERATURE\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                # Greedy sampling\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            \n",
    "            # Decode and print the token\n",
    "            output = tokenizer.decode(next_token[0])\n",
    "            print(output, end='', flush=True)\n",
    "            \n",
    "            # Append the new token to the full sequence for next iteration\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = tokens / elapsed_time\n",
    "    \n",
    "    print(f\"\\n************************************\")\n",
    "    print(f\"Generated {tokens} tokens in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Performance: {tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"************************************\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c930950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with PyTorch:\n",
      "************************************\n",
      " \n",
      "=====================================\n",
      "\n",
      "This example demonstrates how to generate\n",
      "************************************\n",
      "Generated 10 tokens in 20.46 seconds\n",
      "Performance: 0.49 tokens/second\n",
      "************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_str = \"Generating with PyTorch, (should be slow, without kv-caching)\"\n",
    "prompt_tokens = tokenizer.encode(prompt_str)\n",
    "print(\"Generating with PyTorch:\")\n",
    "generate_with_pytorch_no_cache(torch_model, prompt_tokens.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4744a260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5e4a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8521eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([128256, 2048])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.0.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.1.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.2.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.3.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.4.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.5.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.6.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.7.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.8.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.9.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.10.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.11.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.12.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.13.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.14.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.15.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.norm.weight torch.Size([2048])\n",
      "lm_head.weight torch.Size([128256, 2048])\n"
     ]
    }
   ],
   "source": [
    "sd = torch_model.state_dict()\n",
    "for s in sd:\n",
    "    print(s, sd[s].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e868214a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama/llama_block_9/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_9/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_1/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_5/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_11/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_10/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_8/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_2/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_10/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_10/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_10/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_2/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_1/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_1/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_1/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_1/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_0/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_12/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_14/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_3/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_0/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_0/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_9/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_10/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_8/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_4/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_3/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_6/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_7/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_1/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_11/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_1/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_0/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_7/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_10/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_12/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_4/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_0/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_12/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_11/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_13/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_4/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_13/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_15/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_5/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_0/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_0/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/fc/weight [1, 1, 128256, 2048]\n",
      "llama/llama_block_13/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_14/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_13/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_12/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_11/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_15/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_6/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_11/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_15/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_2/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_10/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_2/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_8/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_4/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_7/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_12/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_5/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_13/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_9/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_3/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_3/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_11/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_12/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_1/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_11/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_6/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_12/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_12/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_13/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_15/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_14/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_13/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_4/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_14/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_7/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_14/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_14/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_9/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_4/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_15/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_15/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_15/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_14/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_10/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_14/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_15/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_2/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_2/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_4/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_2/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_8/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_3/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_3/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_8/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_5/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_3/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_13/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_3/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_7/mlp_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_11/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_4/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_5/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_5/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_5/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_7/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/llama_block_0/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_6/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_6/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_6/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_6/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_6/mlp/w3/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_7/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_2/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_7/mlp/w1/weight [1, 1, 8192, 2048]\n",
      "llama/llama_block_5/attention_norm/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_8/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_8/attention/q_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_8/mlp/w2/weight [1, 1, 2048, 8192]\n",
      "llama/ln_fc/gamma [1, 1, 1, 2048]\n",
      "llama/llama_block_9/attention/kv_linear/weight [1, 1, 1024, 2048]\n",
      "llama/llama_block_9/attention/out_linear/weight [1, 1, 2048, 2048]\n",
      "llama/llama_block_9/attention/q_linear/weight [1, 1, 2048, 2048]\n"
     ]
    }
   ],
   "source": [
    "k = tt_model.parameters()\n",
    "for s in k:\n",
    "    print(s, k[s].shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aa5542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "175e9744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "WEIGHT COMPARISON: PyTorch vs TT-Metal\n",
      "================================================================================\n",
      "Note: Detected num_heads=32 for RoPE permutation\n",
      "Note: Weight tying ENABLED\n",
      "================================================================================\n",
      "\n",
      "✓ model.embed_tokens.weight\n",
      "  PyTorch: (128256, 2048)\n",
      "  TT:      (1, 1, 128256, 2048) -> (128256, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.0.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.0.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.0.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.0.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.0.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.0.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.0.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.1.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.1.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.1.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.1.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.1.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.1.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.1.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.1.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.2.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.2.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.2.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.2.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.2.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.2.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.2.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.2.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.3.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.3.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.3.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.3.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.3.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.3.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.3.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.3.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.4.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.4.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.4.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.4.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.4.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.4.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.4.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.4.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.5.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.5.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.5.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.5.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.5.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.5.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.5.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.5.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.6.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.6.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.6.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ model.layers.6.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.6.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.6.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.6.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.6.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.7.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.7.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.7.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.7.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.7.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.7.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.7.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.7.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.8.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.8.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.8.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.8.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.8.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.8.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.8.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.8.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.9.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.9.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.9.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.9.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.9.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.9.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.9.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.9.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.10.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.10.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.10.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.10.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.10.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.10.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.10.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.10.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.11.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.11.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.11.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.11.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.11.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.11.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.11.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.11.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.12.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.12.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.12.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.12.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.12.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.12.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.12.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.12.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.13.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.13.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.13.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ model.layers.13.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.13.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.13.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.13.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.13.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.14.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.14.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.14.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.14.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.14.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.14.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.14.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.14.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.15.self_attn.q_proj.weight (after RoPE permutation)\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "model.layers.15.self_attn.k_proj.weight\n",
      "  PyTorch: (512, 2048)\n",
      "  TT (kv combined): (1, 1, 1024, 2048)\n",
      "  Status: K and V are combined in TT as kv_linear\n",
      "\n",
      "✓ model.layers.15.self_attn.o_proj.weight\n",
      "  PyTorch: (2048, 2048)\n",
      "  TT:      (1, 1, 2048, 2048) -> (2048, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.15.mlp.gate_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.15.mlp.up_proj.weight\n",
      "  PyTorch: (8192, 2048)\n",
      "  TT:      (1, 1, 8192, 2048) -> (8192, 2048)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.15.mlp.down_proj.weight\n",
      "  PyTorch: (2048, 8192)\n",
      "  TT:      (1, 1, 2048, 8192) -> (2048, 8192)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.15.input_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.layers.15.post_attention_layernorm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "✓ model.norm.weight\n",
      "  PyTorch: (2048,)\n",
      "  TT:      (1, 1, 1, 2048) -> (2048,)\n",
      "  Max diff: 0.000000, Rel diff: 0.000000\n",
      "\n",
      "================================================================================\n",
      "SUMMARY: 114 matches, 0 mismatches\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def apply_rope_permutation(w, num_heads):\n",
    "    \"\"\"\n",
    "    Apply RoPE row permutation to match TT's q_proj weight layout.\n",
    "    TT applies unpermute_proj_rows during loading which interleaves rows within each head.\n",
    "    \"\"\"\n",
    "    rows, cols = w.shape\n",
    "    head_dim = rows // num_heads\n",
    "    \n",
    "    out = np.zeros_like(w)\n",
    "    for h in range(num_heads):\n",
    "        head_start = h * head_dim\n",
    "        half = head_dim // 2\n",
    "        \n",
    "        # Interleave: [0..half-1, half..head_dim-1] → [0, half, 1, half+1, ..., half-1, head_dim-1]\n",
    "        for i in range(half):\n",
    "            out[head_start + 2*i] = w[head_start + i]\n",
    "            out[head_start + 2*i + 1] = w[head_start + half + i]\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def compare_weights_llama(torch_model, tt_model):\n",
    "    \"\"\"\n",
    "    Compare weights between PyTorch model and TT-Metal model.\n",
    "    \n",
    "    Args:\n",
    "        torch_model: PyTorch model (with .state_dict())\n",
    "        tt_model: TT-Metal model (with .parameters())\n",
    "    \"\"\"\n",
    "    \n",
    "    torch_sd = torch_model.state_dict()\n",
    "    tt_params = tt_model.parameters()\n",
    "    \n",
    "    # Get num_heads from torch model config\n",
    "    num_heads = torch_model.config.num_attention_heads\n",
    "    \n",
    "    # Detect weight tying configuration from TT model\n",
    "    has_tok_emb = 'llama/tok_emb/weight' in tt_params\n",
    "    has_fc = 'llama/fc/weight' in tt_params\n",
    "    weight_tying_enabled = not has_tok_emb  # If tok_emb doesn't exist, weight tying is enabled\n",
    "    \n",
    "    # Mapping from PyTorch parameter names to TT-Metal parameter names\n",
    "    pytorch_to_tt_mapping = {\n",
    "        # Final layer norm\n",
    "        'model.norm.weight': 'llama/ln_fc/gamma',\n",
    "    }\n",
    "    \n",
    "    # Add embedding mappings based on weight tying configuration\n",
    "    if weight_tying_enabled:\n",
    "        # Weight tying enabled: both embed_tokens and lm_head use fc/weight\n",
    "        pytorch_to_tt_mapping['model.embed_tokens.weight'] = 'llama/fc/weight'\n",
    "        # lm_head.weight should also map to fc/weight, but we'll skip it to avoid duplicate checks\n",
    "    else:\n",
    "        # Weight tying disabled: separate tok_emb and fc\n",
    "        pytorch_to_tt_mapping['model.embed_tokens.weight'] = 'llama/tok_emb/weight'\n",
    "        pytorch_to_tt_mapping['lm_head.weight'] = 'llama/fc/weight'\n",
    "    \n",
    "    # Add layer-specific mappings\n",
    "    for i in range(50):  # Support up to 50 layers\n",
    "        layer_prefix_pt = f'model.layers.{i}'\n",
    "        layer_prefix_tt = f'llama/llama_block_{i}'\n",
    "        \n",
    "        pytorch_to_tt_mapping.update({\n",
    "            f'{layer_prefix_pt}.input_layernorm.weight': f'{layer_prefix_tt}/attention_norm/gamma',\n",
    "            f'{layer_prefix_pt}.post_attention_layernorm.weight': f'{layer_prefix_tt}/mlp_norm/gamma',\n",
    "            f'{layer_prefix_pt}.self_attn.q_proj.weight': f'{layer_prefix_tt}/attention/q_linear/weight',\n",
    "            f'{layer_prefix_pt}.self_attn.o_proj.weight': f'{layer_prefix_tt}/attention/out_linear/weight',\n",
    "            # k_proj and v_proj are combined into kv_linear in TT\n",
    "            f'{layer_prefix_pt}.mlp.gate_proj.weight': f'{layer_prefix_tt}/mlp/w1/weight',\n",
    "            f'{layer_prefix_pt}.mlp.up_proj.weight': f'{layer_prefix_tt}/mlp/w3/weight',\n",
    "            f'{layer_prefix_pt}.mlp.down_proj.weight': f'{layer_prefix_tt}/mlp/w2/weight',\n",
    "        })\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"WEIGHT COMPARISON: PyTorch vs TT-Metal\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Note: Detected num_heads={num_heads} for RoPE permutation\")\n",
    "    print(f\"Note: Weight tying {'ENABLED' if weight_tying_enabled else 'DISABLED'}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    mismatches = []\n",
    "    matches = []\n",
    "    \n",
    "    for pt_name in torch_sd.keys():\n",
    "        if 'bias' in pt_name:\n",
    "            continue  # Skip bias parameters\n",
    "            \n",
    "        pt_tensor = torch_sd[pt_name]\n",
    "        pt_shape = tuple(pt_tensor.shape)\n",
    "        \n",
    "        # Handle k_proj and v_proj specially (they're combined in TT)\n",
    "        if '.self_attn.k_proj.weight' in pt_name or '.self_attn.v_proj.weight' in pt_name:\n",
    "            layer_idx = pt_name.split('.')[2]\n",
    "            tt_name = f'llama/llama_block_{layer_idx}/attention/kv_linear/weight'\n",
    "            \n",
    "            if tt_name in tt_params:\n",
    "                tt_tensor_np = tt_params[tt_name].to_numpy()\n",
    "                tt_shape = tt_tensor_np.shape\n",
    "                \n",
    "                # k_proj and v_proj are concatenated in kv_linear\n",
    "                # Expected: k_proj [512, 2048] + v_proj [512, 2048] = kv_linear [1, 1, 1024, 2048]\n",
    "                if '.self_attn.k_proj.weight' in pt_name:\n",
    "                    print(f\"\\n{pt_name}\")\n",
    "                    print(f\"  PyTorch: {pt_shape}\")\n",
    "                    print(f\"  TT (kv combined): {tt_shape}\")\n",
    "                    print(f\"  Status: K and V are combined in TT as kv_linear\")\n",
    "            continue\n",
    "        \n",
    "        # Get corresponding TT parameter name\n",
    "        tt_name = pytorch_to_tt_mapping.get(pt_name)\n",
    "        if not tt_name:\n",
    "            continue\n",
    "            \n",
    "        if tt_name not in tt_params:\n",
    "            print(f\"\\n❌ MISSING: {pt_name} -> {tt_name}\")\n",
    "            print(f\"   PyTorch shape: {pt_shape}\")\n",
    "            mismatches.append((pt_name, \"MISSING IN TT\"))\n",
    "            continue\n",
    "        \n",
    "        # Get TT tensor\n",
    "        tt_tensor_np = tt_params[tt_name].to_numpy()\n",
    "        tt_shape = tt_tensor_np.shape\n",
    "        \n",
    "        # Remove batch dimensions [1, 1, ...] from TT tensor\n",
    "        tt_shape_no_batch = tt_shape[2:] if len(tt_shape) == 4 else tt_shape\n",
    "        \n",
    "        # Compare shapes\n",
    "        pt_numpy = pt_tensor.cpu().float().numpy()  # Convert to float32 for numpy compatibility\n",
    "        \n",
    "        # Special handling for q_proj: TT applies RoPE row permutation during loading\n",
    "        is_q_proj = '.self_attn.q_proj.weight' in pt_name\n",
    "        if is_q_proj and len(pt_shape) == 2 and pt_shape[0] % num_heads == 0:\n",
    "            pt_numpy = apply_rope_permutation(pt_numpy, num_heads)\n",
    "        \n",
    "        # For layer norms: PT (N,) vs TT (1, N) - both are fine, just broadcasting\n",
    "        # Check if PT is 1D and TT has leading 1s that can be squeezed\n",
    "        if len(pt_shape) == 1 and len(tt_shape_no_batch) == 2 and tt_shape_no_batch[0] == 1:\n",
    "            tt_shape_no_batch = (tt_shape_no_batch[1],)  # Squeeze leading 1\n",
    "        \n",
    "        # Check if shapes match (with or without transpose)\n",
    "        shape_match = (pt_shape == tt_shape_no_batch) or (pt_shape == tt_shape_no_batch[::-1])\n",
    "        \n",
    "        if shape_match:\n",
    "            # Check actual values\n",
    "            # Reshape TT data to match PT shape (handle batch dims and potential squeezing)\n",
    "            if len(tt_shape) == 4:\n",
    "                tt_data = tt_tensor_np.reshape(tt_shape[2:])  # Remove [1,1,...] batch dims\n",
    "            else:\n",
    "                tt_data = tt_tensor_np.reshape(tt_shape)\n",
    "            \n",
    "            # Squeeze if needed for 1D comparisons\n",
    "            tt_data = tt_data.squeeze()\n",
    "            pt_numpy_squeezed = pt_numpy.squeeze()\n",
    "            \n",
    "            # Handle transpose if needed\n",
    "            if pt_numpy_squeezed.shape != tt_data.shape and len(tt_data.shape) == 2:\n",
    "                tt_data = tt_data.T\n",
    "            \n",
    "            diff = np.abs(pt_numpy_squeezed - tt_data).max()\n",
    "            rel_diff = diff / (np.abs(pt_numpy_squeezed).max() + 1e-8)\n",
    "            \n",
    "            status = \"✓\" if diff < 1e-3 else \"⚠\"\n",
    "            note = \" (after RoPE permutation)\" if is_q_proj else \"\"\n",
    "            print(f\"\\n{status} {pt_name}{note}\")\n",
    "            print(f\"  PyTorch: {pt_shape}\")\n",
    "            print(f\"  TT:      {tt_shape} -> {tt_shape_no_batch}\")\n",
    "            print(f\"  Max diff: {diff:.6f}, Rel diff: {rel_diff:.6f}\")\n",
    "            \n",
    "            if diff < 1e-3:\n",
    "                matches.append(pt_name)\n",
    "            else:\n",
    "                mismatches.append((pt_name, f\"VALUE_DIFF={diff:.6f}\"))\n",
    "        else:\n",
    "            print(f\"\\n❌ SHAPE MISMATCH: {pt_name}\")\n",
    "            print(f\"  PyTorch: {pt_shape}\")\n",
    "            print(f\"  TT:      {tt_shape} -> {tt_shape_no_batch}\")\n",
    "            mismatches.append((pt_name, f\"SHAPE: PT={pt_shape} vs TT={tt_shape_no_batch}\"))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"SUMMARY: {len(matches)} matches, {len(mismatches)} mismatches\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if mismatches:\n",
    "        print(\"\\n❌ MISMATCHES:\")\n",
    "        for name, issue in mismatches:\n",
    "            print(f\"  - {name}: {issue}\")\n",
    "    \n",
    "    return matches, mismatches\n",
    "\n",
    "\n",
    "# Usage example (commented out):\n",
    "matches, mismatches = compare_weights_llama(torch_model, tt_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b7af5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "WEIGHT COMPARISON: PyTorch Qwen3 vs TT-Metal Qwen3\n",
      "================================================================================\n",
      "Model Configuration:\n",
      "  - num_attention_heads: 32\n",
      "  - num_key_value_heads: 8\n",
      "  - head_dim: 64\n",
      "  - hidden_size: 2048\n",
      "  - attention_output_dim: 2048\n",
      "  - weight_tying: ENABLED\n",
      "================================================================================\n",
      "\n",
      "❌ MISSING IN TT: model.embed_tokens.weight -> qwen3/fc/weight\n",
      "   PyTorch shape: (128256, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.0.self_attn.q_proj.weight -> qwen3/qwen3_block_0/attention/q_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.0.self_attn.o_proj.weight -> qwen3/qwen3_block_0/attention/out_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.0.mlp.gate_proj.weight -> qwen3/qwen3_block_0/mlp/w1/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.0.mlp.up_proj.weight -> qwen3/qwen3_block_0/mlp/w3/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.0.mlp.down_proj.weight -> qwen3/qwen3_block_0/mlp/w2/weight\n",
      "   PyTorch shape: (2048, 8192)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.0.input_layernorm.weight -> qwen3/qwen3_block_0/attention_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.0.post_attention_layernorm.weight -> qwen3/qwen3_block_0/mlp_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.1.self_attn.q_proj.weight -> qwen3/qwen3_block_1/attention/q_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.1.self_attn.o_proj.weight -> qwen3/qwen3_block_1/attention/out_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.1.mlp.gate_proj.weight -> qwen3/qwen3_block_1/mlp/w1/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.1.mlp.up_proj.weight -> qwen3/qwen3_block_1/mlp/w3/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.1.mlp.down_proj.weight -> qwen3/qwen3_block_1/mlp/w2/weight\n",
      "   PyTorch shape: (2048, 8192)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.1.input_layernorm.weight -> qwen3/qwen3_block_1/attention_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.1.post_attention_layernorm.weight -> qwen3/qwen3_block_1/mlp_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.2.self_attn.q_proj.weight -> qwen3/qwen3_block_2/attention/q_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.2.self_attn.o_proj.weight -> qwen3/qwen3_block_2/attention/out_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.2.mlp.gate_proj.weight -> qwen3/qwen3_block_2/mlp/w1/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.2.mlp.up_proj.weight -> qwen3/qwen3_block_2/mlp/w3/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.2.mlp.down_proj.weight -> qwen3/qwen3_block_2/mlp/w2/weight\n",
      "   PyTorch shape: (2048, 8192)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.2.input_layernorm.weight -> qwen3/qwen3_block_2/attention_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.2.post_attention_layernorm.weight -> qwen3/qwen3_block_2/mlp_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.3.self_attn.q_proj.weight -> qwen3/qwen3_block_3/attention/q_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.3.self_attn.o_proj.weight -> qwen3/qwen3_block_3/attention/out_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.3.mlp.gate_proj.weight -> qwen3/qwen3_block_3/mlp/w1/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.3.mlp.up_proj.weight -> qwen3/qwen3_block_3/mlp/w3/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.3.mlp.down_proj.weight -> qwen3/qwen3_block_3/mlp/w2/weight\n",
      "   PyTorch shape: (2048, 8192)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.3.input_layernorm.weight -> qwen3/qwen3_block_3/attention_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.3.post_attention_layernorm.weight -> qwen3/qwen3_block_3/mlp_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.4.self_attn.q_proj.weight -> qwen3/qwen3_block_4/attention/q_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.4.self_attn.o_proj.weight -> qwen3/qwen3_block_4/attention/out_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.4.mlp.gate_proj.weight -> qwen3/qwen3_block_4/mlp/w1/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.4.mlp.up_proj.weight -> qwen3/qwen3_block_4/mlp/w3/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.4.mlp.down_proj.weight -> qwen3/qwen3_block_4/mlp/w2/weight\n",
      "   PyTorch shape: (2048, 8192)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.4.input_layernorm.weight -> qwen3/qwen3_block_4/attention_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.4.post_attention_layernorm.weight -> qwen3/qwen3_block_4/mlp_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.5.self_attn.q_proj.weight -> qwen3/qwen3_block_5/attention/q_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.5.self_attn.o_proj.weight -> qwen3/qwen3_block_5/attention/out_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.5.mlp.gate_proj.weight -> qwen3/qwen3_block_5/mlp/w1/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.5.mlp.up_proj.weight -> qwen3/qwen3_block_5/mlp/w3/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.5.mlp.down_proj.weight -> qwen3/qwen3_block_5/mlp/w2/weight\n",
      "   PyTorch shape: (2048, 8192)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.5.input_layernorm.weight -> qwen3/qwen3_block_5/attention_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.5.post_attention_layernorm.weight -> qwen3/qwen3_block_5/mlp_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.6.self_attn.q_proj.weight -> qwen3/qwen3_block_6/attention/q_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.6.self_attn.o_proj.weight -> qwen3/qwen3_block_6/attention/out_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.6.mlp.gate_proj.weight -> qwen3/qwen3_block_6/mlp/w1/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.6.mlp.up_proj.weight -> qwen3/qwen3_block_6/mlp/w3/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.6.mlp.down_proj.weight -> qwen3/qwen3_block_6/mlp/w2/weight\n",
      "   PyTorch shape: (2048, 8192)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.6.input_layernorm.weight -> qwen3/qwen3_block_6/attention_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.6.post_attention_layernorm.weight -> qwen3/qwen3_block_6/mlp_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.7.self_attn.q_proj.weight -> qwen3/qwen3_block_7/attention/q_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.7.self_attn.o_proj.weight -> qwen3/qwen3_block_7/attention/out_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.7.mlp.gate_proj.weight -> qwen3/qwen3_block_7/mlp/w1/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.7.mlp.up_proj.weight -> qwen3/qwen3_block_7/mlp/w3/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.7.mlp.down_proj.weight -> qwen3/qwen3_block_7/mlp/w2/weight\n",
      "   PyTorch shape: (2048, 8192)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.7.input_layernorm.weight -> qwen3/qwen3_block_7/attention_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.7.post_attention_layernorm.weight -> qwen3/qwen3_block_7/mlp_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.8.self_attn.q_proj.weight -> qwen3/qwen3_block_8/attention/q_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.8.self_attn.o_proj.weight -> qwen3/qwen3_block_8/attention/out_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.8.mlp.gate_proj.weight -> qwen3/qwen3_block_8/mlp/w1/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.8.mlp.up_proj.weight -> qwen3/qwen3_block_8/mlp/w3/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.8.mlp.down_proj.weight -> qwen3/qwen3_block_8/mlp/w2/weight\n",
      "   PyTorch shape: (2048, 8192)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.8.input_layernorm.weight -> qwen3/qwen3_block_8/attention_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.8.post_attention_layernorm.weight -> qwen3/qwen3_block_8/mlp_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.9.self_attn.q_proj.weight -> qwen3/qwen3_block_9/attention/q_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.9.self_attn.o_proj.weight -> qwen3/qwen3_block_9/attention/out_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.9.mlp.gate_proj.weight -> qwen3/qwen3_block_9/mlp/w1/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.9.mlp.up_proj.weight -> qwen3/qwen3_block_9/mlp/w3/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.9.mlp.down_proj.weight -> qwen3/qwen3_block_9/mlp/w2/weight\n",
      "   PyTorch shape: (2048, 8192)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.9.input_layernorm.weight -> qwen3/qwen3_block_9/attention_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.9.post_attention_layernorm.weight -> qwen3/qwen3_block_9/mlp_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.10.self_attn.q_proj.weight -> qwen3/qwen3_block_10/attention/q_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.10.self_attn.o_proj.weight -> qwen3/qwen3_block_10/attention/out_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.10.mlp.gate_proj.weight -> qwen3/qwen3_block_10/mlp/w1/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.10.mlp.up_proj.weight -> qwen3/qwen3_block_10/mlp/w3/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.10.mlp.down_proj.weight -> qwen3/qwen3_block_10/mlp/w2/weight\n",
      "   PyTorch shape: (2048, 8192)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.10.input_layernorm.weight -> qwen3/qwen3_block_10/attention_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.10.post_attention_layernorm.weight -> qwen3/qwen3_block_10/mlp_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.11.self_attn.q_proj.weight -> qwen3/qwen3_block_11/attention/q_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.11.self_attn.o_proj.weight -> qwen3/qwen3_block_11/attention/out_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.11.mlp.gate_proj.weight -> qwen3/qwen3_block_11/mlp/w1/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.11.mlp.up_proj.weight -> qwen3/qwen3_block_11/mlp/w3/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.11.mlp.down_proj.weight -> qwen3/qwen3_block_11/mlp/w2/weight\n",
      "   PyTorch shape: (2048, 8192)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.11.input_layernorm.weight -> qwen3/qwen3_block_11/attention_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.11.post_attention_layernorm.weight -> qwen3/qwen3_block_11/mlp_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.12.self_attn.q_proj.weight -> qwen3/qwen3_block_12/attention/q_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.12.self_attn.o_proj.weight -> qwen3/qwen3_block_12/attention/out_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.12.mlp.gate_proj.weight -> qwen3/qwen3_block_12/mlp/w1/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.12.mlp.up_proj.weight -> qwen3/qwen3_block_12/mlp/w3/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.12.mlp.down_proj.weight -> qwen3/qwen3_block_12/mlp/w2/weight\n",
      "   PyTorch shape: (2048, 8192)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.12.input_layernorm.weight -> qwen3/qwen3_block_12/attention_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.12.post_attention_layernorm.weight -> qwen3/qwen3_block_12/mlp_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.13.self_attn.q_proj.weight -> qwen3/qwen3_block_13/attention/q_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.13.self_attn.o_proj.weight -> qwen3/qwen3_block_13/attention/out_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.13.mlp.gate_proj.weight -> qwen3/qwen3_block_13/mlp/w1/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.13.mlp.up_proj.weight -> qwen3/qwen3_block_13/mlp/w3/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.13.mlp.down_proj.weight -> qwen3/qwen3_block_13/mlp/w2/weight\n",
      "   PyTorch shape: (2048, 8192)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.13.input_layernorm.weight -> qwen3/qwen3_block_13/attention_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.13.post_attention_layernorm.weight -> qwen3/qwen3_block_13/mlp_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.14.self_attn.q_proj.weight -> qwen3/qwen3_block_14/attention/q_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.14.self_attn.o_proj.weight -> qwen3/qwen3_block_14/attention/out_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.14.mlp.gate_proj.weight -> qwen3/qwen3_block_14/mlp/w1/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.14.mlp.up_proj.weight -> qwen3/qwen3_block_14/mlp/w3/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.14.mlp.down_proj.weight -> qwen3/qwen3_block_14/mlp/w2/weight\n",
      "   PyTorch shape: (2048, 8192)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.14.input_layernorm.weight -> qwen3/qwen3_block_14/attention_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.14.post_attention_layernorm.weight -> qwen3/qwen3_block_14/mlp_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.15.self_attn.q_proj.weight -> qwen3/qwen3_block_15/attention/q_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.15.self_attn.o_proj.weight -> qwen3/qwen3_block_15/attention/out_linear/weight\n",
      "   PyTorch shape: (2048, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.15.mlp.gate_proj.weight -> qwen3/qwen3_block_15/mlp/w1/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.15.mlp.up_proj.weight -> qwen3/qwen3_block_15/mlp/w3/weight\n",
      "   PyTorch shape: (8192, 2048)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.15.mlp.down_proj.weight -> qwen3/qwen3_block_15/mlp/w2/weight\n",
      "   PyTorch shape: (2048, 8192)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.15.input_layernorm.weight -> qwen3/qwen3_block_15/attention_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.layers.15.post_attention_layernorm.weight -> qwen3/qwen3_block_15/mlp_norm/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "❌ MISSING IN TT: model.norm.weight -> qwen3/ln_fc/gamma\n",
      "   PyTorch shape: (2048,)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY: 0 matches, 0 mismatches, 114 missing in TT\n",
      "================================================================================\n",
      "\n",
      "❌ MISSING IN TT (114):\n",
      "  - model.embed_tokens.weight -> qwen3/fc/weight\n",
      "  - model.layers.0.self_attn.q_proj.weight -> qwen3/qwen3_block_0/attention/q_linear/weight\n",
      "  - model.layers.0.self_attn.o_proj.weight -> qwen3/qwen3_block_0/attention/out_linear/weight\n",
      "  - model.layers.0.mlp.gate_proj.weight -> qwen3/qwen3_block_0/mlp/w1/weight\n",
      "  - model.layers.0.mlp.up_proj.weight -> qwen3/qwen3_block_0/mlp/w3/weight\n",
      "  - model.layers.0.mlp.down_proj.weight -> qwen3/qwen3_block_0/mlp/w2/weight\n",
      "  - model.layers.0.input_layernorm.weight -> qwen3/qwen3_block_0/attention_norm/gamma\n",
      "  - model.layers.0.post_attention_layernorm.weight -> qwen3/qwen3_block_0/mlp_norm/gamma\n",
      "  - model.layers.1.self_attn.q_proj.weight -> qwen3/qwen3_block_1/attention/q_linear/weight\n",
      "  - model.layers.1.self_attn.o_proj.weight -> qwen3/qwen3_block_1/attention/out_linear/weight\n",
      "  - model.layers.1.mlp.gate_proj.weight -> qwen3/qwen3_block_1/mlp/w1/weight\n",
      "  - model.layers.1.mlp.up_proj.weight -> qwen3/qwen3_block_1/mlp/w3/weight\n",
      "  - model.layers.1.mlp.down_proj.weight -> qwen3/qwen3_block_1/mlp/w2/weight\n",
      "  - model.layers.1.input_layernorm.weight -> qwen3/qwen3_block_1/attention_norm/gamma\n",
      "  - model.layers.1.post_attention_layernorm.weight -> qwen3/qwen3_block_1/mlp_norm/gamma\n",
      "  - model.layers.2.self_attn.q_proj.weight -> qwen3/qwen3_block_2/attention/q_linear/weight\n",
      "  - model.layers.2.self_attn.o_proj.weight -> qwen3/qwen3_block_2/attention/out_linear/weight\n",
      "  - model.layers.2.mlp.gate_proj.weight -> qwen3/qwen3_block_2/mlp/w1/weight\n",
      "  - model.layers.2.mlp.up_proj.weight -> qwen3/qwen3_block_2/mlp/w3/weight\n",
      "  - model.layers.2.mlp.down_proj.weight -> qwen3/qwen3_block_2/mlp/w2/weight\n",
      "  - model.layers.2.input_layernorm.weight -> qwen3/qwen3_block_2/attention_norm/gamma\n",
      "  - model.layers.2.post_attention_layernorm.weight -> qwen3/qwen3_block_2/mlp_norm/gamma\n",
      "  - model.layers.3.self_attn.q_proj.weight -> qwen3/qwen3_block_3/attention/q_linear/weight\n",
      "  - model.layers.3.self_attn.o_proj.weight -> qwen3/qwen3_block_3/attention/out_linear/weight\n",
      "  - model.layers.3.mlp.gate_proj.weight -> qwen3/qwen3_block_3/mlp/w1/weight\n",
      "  - model.layers.3.mlp.up_proj.weight -> qwen3/qwen3_block_3/mlp/w3/weight\n",
      "  - model.layers.3.mlp.down_proj.weight -> qwen3/qwen3_block_3/mlp/w2/weight\n",
      "  - model.layers.3.input_layernorm.weight -> qwen3/qwen3_block_3/attention_norm/gamma\n",
      "  - model.layers.3.post_attention_layernorm.weight -> qwen3/qwen3_block_3/mlp_norm/gamma\n",
      "  - model.layers.4.self_attn.q_proj.weight -> qwen3/qwen3_block_4/attention/q_linear/weight\n",
      "  - model.layers.4.self_attn.o_proj.weight -> qwen3/qwen3_block_4/attention/out_linear/weight\n",
      "  - model.layers.4.mlp.gate_proj.weight -> qwen3/qwen3_block_4/mlp/w1/weight\n",
      "  - model.layers.4.mlp.up_proj.weight -> qwen3/qwen3_block_4/mlp/w3/weight\n",
      "  - model.layers.4.mlp.down_proj.weight -> qwen3/qwen3_block_4/mlp/w2/weight\n",
      "  - model.layers.4.input_layernorm.weight -> qwen3/qwen3_block_4/attention_norm/gamma\n",
      "  - model.layers.4.post_attention_layernorm.weight -> qwen3/qwen3_block_4/mlp_norm/gamma\n",
      "  - model.layers.5.self_attn.q_proj.weight -> qwen3/qwen3_block_5/attention/q_linear/weight\n",
      "  - model.layers.5.self_attn.o_proj.weight -> qwen3/qwen3_block_5/attention/out_linear/weight\n",
      "  - model.layers.5.mlp.gate_proj.weight -> qwen3/qwen3_block_5/mlp/w1/weight\n",
      "  - model.layers.5.mlp.up_proj.weight -> qwen3/qwen3_block_5/mlp/w3/weight\n",
      "  - model.layers.5.mlp.down_proj.weight -> qwen3/qwen3_block_5/mlp/w2/weight\n",
      "  - model.layers.5.input_layernorm.weight -> qwen3/qwen3_block_5/attention_norm/gamma\n",
      "  - model.layers.5.post_attention_layernorm.weight -> qwen3/qwen3_block_5/mlp_norm/gamma\n",
      "  - model.layers.6.self_attn.q_proj.weight -> qwen3/qwen3_block_6/attention/q_linear/weight\n",
      "  - model.layers.6.self_attn.o_proj.weight -> qwen3/qwen3_block_6/attention/out_linear/weight\n",
      "  - model.layers.6.mlp.gate_proj.weight -> qwen3/qwen3_block_6/mlp/w1/weight\n",
      "  - model.layers.6.mlp.up_proj.weight -> qwen3/qwen3_block_6/mlp/w3/weight\n",
      "  - model.layers.6.mlp.down_proj.weight -> qwen3/qwen3_block_6/mlp/w2/weight\n",
      "  - model.layers.6.input_layernorm.weight -> qwen3/qwen3_block_6/attention_norm/gamma\n",
      "  - model.layers.6.post_attention_layernorm.weight -> qwen3/qwen3_block_6/mlp_norm/gamma\n",
      "  - model.layers.7.self_attn.q_proj.weight -> qwen3/qwen3_block_7/attention/q_linear/weight\n",
      "  - model.layers.7.self_attn.o_proj.weight -> qwen3/qwen3_block_7/attention/out_linear/weight\n",
      "  - model.layers.7.mlp.gate_proj.weight -> qwen3/qwen3_block_7/mlp/w1/weight\n",
      "  - model.layers.7.mlp.up_proj.weight -> qwen3/qwen3_block_7/mlp/w3/weight\n",
      "  - model.layers.7.mlp.down_proj.weight -> qwen3/qwen3_block_7/mlp/w2/weight\n",
      "  - model.layers.7.input_layernorm.weight -> qwen3/qwen3_block_7/attention_norm/gamma\n",
      "  - model.layers.7.post_attention_layernorm.weight -> qwen3/qwen3_block_7/mlp_norm/gamma\n",
      "  - model.layers.8.self_attn.q_proj.weight -> qwen3/qwen3_block_8/attention/q_linear/weight\n",
      "  - model.layers.8.self_attn.o_proj.weight -> qwen3/qwen3_block_8/attention/out_linear/weight\n",
      "  - model.layers.8.mlp.gate_proj.weight -> qwen3/qwen3_block_8/mlp/w1/weight\n",
      "  - model.layers.8.mlp.up_proj.weight -> qwen3/qwen3_block_8/mlp/w3/weight\n",
      "  - model.layers.8.mlp.down_proj.weight -> qwen3/qwen3_block_8/mlp/w2/weight\n",
      "  - model.layers.8.input_layernorm.weight -> qwen3/qwen3_block_8/attention_norm/gamma\n",
      "  - model.layers.8.post_attention_layernorm.weight -> qwen3/qwen3_block_8/mlp_norm/gamma\n",
      "  - model.layers.9.self_attn.q_proj.weight -> qwen3/qwen3_block_9/attention/q_linear/weight\n",
      "  - model.layers.9.self_attn.o_proj.weight -> qwen3/qwen3_block_9/attention/out_linear/weight\n",
      "  - model.layers.9.mlp.gate_proj.weight -> qwen3/qwen3_block_9/mlp/w1/weight\n",
      "  - model.layers.9.mlp.up_proj.weight -> qwen3/qwen3_block_9/mlp/w3/weight\n",
      "  - model.layers.9.mlp.down_proj.weight -> qwen3/qwen3_block_9/mlp/w2/weight\n",
      "  - model.layers.9.input_layernorm.weight -> qwen3/qwen3_block_9/attention_norm/gamma\n",
      "  - model.layers.9.post_attention_layernorm.weight -> qwen3/qwen3_block_9/mlp_norm/gamma\n",
      "  - model.layers.10.self_attn.q_proj.weight -> qwen3/qwen3_block_10/attention/q_linear/weight\n",
      "  - model.layers.10.self_attn.o_proj.weight -> qwen3/qwen3_block_10/attention/out_linear/weight\n",
      "  - model.layers.10.mlp.gate_proj.weight -> qwen3/qwen3_block_10/mlp/w1/weight\n",
      "  - model.layers.10.mlp.up_proj.weight -> qwen3/qwen3_block_10/mlp/w3/weight\n",
      "  - model.layers.10.mlp.down_proj.weight -> qwen3/qwen3_block_10/mlp/w2/weight\n",
      "  - model.layers.10.input_layernorm.weight -> qwen3/qwen3_block_10/attention_norm/gamma\n",
      "  - model.layers.10.post_attention_layernorm.weight -> qwen3/qwen3_block_10/mlp_norm/gamma\n",
      "  - model.layers.11.self_attn.q_proj.weight -> qwen3/qwen3_block_11/attention/q_linear/weight\n",
      "  - model.layers.11.self_attn.o_proj.weight -> qwen3/qwen3_block_11/attention/out_linear/weight\n",
      "  - model.layers.11.mlp.gate_proj.weight -> qwen3/qwen3_block_11/mlp/w1/weight\n",
      "  - model.layers.11.mlp.up_proj.weight -> qwen3/qwen3_block_11/mlp/w3/weight\n",
      "  - model.layers.11.mlp.down_proj.weight -> qwen3/qwen3_block_11/mlp/w2/weight\n",
      "  - model.layers.11.input_layernorm.weight -> qwen3/qwen3_block_11/attention_norm/gamma\n",
      "  - model.layers.11.post_attention_layernorm.weight -> qwen3/qwen3_block_11/mlp_norm/gamma\n",
      "  - model.layers.12.self_attn.q_proj.weight -> qwen3/qwen3_block_12/attention/q_linear/weight\n",
      "  - model.layers.12.self_attn.o_proj.weight -> qwen3/qwen3_block_12/attention/out_linear/weight\n",
      "  - model.layers.12.mlp.gate_proj.weight -> qwen3/qwen3_block_12/mlp/w1/weight\n",
      "  - model.layers.12.mlp.up_proj.weight -> qwen3/qwen3_block_12/mlp/w3/weight\n",
      "  - model.layers.12.mlp.down_proj.weight -> qwen3/qwen3_block_12/mlp/w2/weight\n",
      "  - model.layers.12.input_layernorm.weight -> qwen3/qwen3_block_12/attention_norm/gamma\n",
      "  - model.layers.12.post_attention_layernorm.weight -> qwen3/qwen3_block_12/mlp_norm/gamma\n",
      "  - model.layers.13.self_attn.q_proj.weight -> qwen3/qwen3_block_13/attention/q_linear/weight\n",
      "  - model.layers.13.self_attn.o_proj.weight -> qwen3/qwen3_block_13/attention/out_linear/weight\n",
      "  - model.layers.13.mlp.gate_proj.weight -> qwen3/qwen3_block_13/mlp/w1/weight\n",
      "  - model.layers.13.mlp.up_proj.weight -> qwen3/qwen3_block_13/mlp/w3/weight\n",
      "  - model.layers.13.mlp.down_proj.weight -> qwen3/qwen3_block_13/mlp/w2/weight\n",
      "  - model.layers.13.input_layernorm.weight -> qwen3/qwen3_block_13/attention_norm/gamma\n",
      "  - model.layers.13.post_attention_layernorm.weight -> qwen3/qwen3_block_13/mlp_norm/gamma\n",
      "  - model.layers.14.self_attn.q_proj.weight -> qwen3/qwen3_block_14/attention/q_linear/weight\n",
      "  - model.layers.14.self_attn.o_proj.weight -> qwen3/qwen3_block_14/attention/out_linear/weight\n",
      "  - model.layers.14.mlp.gate_proj.weight -> qwen3/qwen3_block_14/mlp/w1/weight\n",
      "  - model.layers.14.mlp.up_proj.weight -> qwen3/qwen3_block_14/mlp/w3/weight\n",
      "  - model.layers.14.mlp.down_proj.weight -> qwen3/qwen3_block_14/mlp/w2/weight\n",
      "  - model.layers.14.input_layernorm.weight -> qwen3/qwen3_block_14/attention_norm/gamma\n",
      "  - model.layers.14.post_attention_layernorm.weight -> qwen3/qwen3_block_14/mlp_norm/gamma\n",
      "  - model.layers.15.self_attn.q_proj.weight -> qwen3/qwen3_block_15/attention/q_linear/weight\n",
      "  - model.layers.15.self_attn.o_proj.weight -> qwen3/qwen3_block_15/attention/out_linear/weight\n",
      "  - model.layers.15.mlp.gate_proj.weight -> qwen3/qwen3_block_15/mlp/w1/weight\n",
      "  - model.layers.15.mlp.up_proj.weight -> qwen3/qwen3_block_15/mlp/w3/weight\n",
      "  - model.layers.15.mlp.down_proj.weight -> qwen3/qwen3_block_15/mlp/w2/weight\n",
      "  - model.layers.15.input_layernorm.weight -> qwen3/qwen3_block_15/attention_norm/gamma\n",
      "  - model.layers.15.post_attention_layernorm.weight -> qwen3/qwen3_block_15/mlp_norm/gamma\n",
      "  - model.norm.weight -> qwen3/ln_fc/gamma\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Qwen3 Weight Comparison Script\n",
    "\n",
    "Compares weights between PyTorch Qwen3 model and TT-Metal Qwen3 model.\n",
    "\n",
    "Key Qwen3 features:\n",
    "- Explicit head_dim (128 for 0.6B model)\n",
    "- Q/K normalization for numerical stability (CRITICAL!)\n",
    "- Attention dimension != embedding dimension\n",
    "- Weight tying enabled by default\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def apply_rope_permutation(w, num_heads):\n",
    "    \"\"\"\n",
    "    Apply RoPE row permutation to match TT's q_proj/k_proj weight layout.\n",
    "    TT applies unpermute_proj_rows during loading which interleaves rows within each head.\n",
    "    \n",
    "    This reorders: [0..D/2-1, D/2..D-1] → [0, D/2, 1, D/2+1, ..., D/2-1, D-1]\n",
    "    \"\"\"\n",
    "    rows, cols = w.shape\n",
    "    head_dim = rows // num_heads\n",
    "    \n",
    "    if head_dim % 2 != 0:\n",
    "        raise ValueError(f\"Head dimension {head_dim} must be even for RoPE permutation\")\n",
    "    \n",
    "    out = np.zeros_like(w)\n",
    "    for h in range(num_heads):\n",
    "        head_start = h * head_dim\n",
    "        half = head_dim // 2\n",
    "        \n",
    "        # Interleave: first half and second half of each head\n",
    "        for i in range(half):\n",
    "            out[head_start + 2*i] = w[head_start + i]\n",
    "            out[head_start + 2*i + 1] = w[head_start + half + i]\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def compare_qwen3_weights(torch_model, tt_model, verbose=True):\n",
    "    \"\"\"\n",
    "    Compare weights between PyTorch Qwen3 model and TT-Metal Qwen3 model.\n",
    "    \n",
    "    Qwen3-specific features:\n",
    "    - Q projection: [2048, 1024] (projects UP to attention space)\n",
    "    - K projection: [1024, 1024] (num_kv_heads * head_dim)\n",
    "    - V projection: [1024, 1024] (num_kv_heads * head_dim)\n",
    "    - O projection: [1024, 2048] (projects DOWN to embedding space)\n",
    "    - Q/K normalization: RMSNorm on head_dim (128) - CRITICAL!\n",
    "    \n",
    "    Args:\n",
    "        torch_model: PyTorch Qwen3 model (with .state_dict())\n",
    "        tt_model: TT-Metal Qwen3 model (with .parameters())\n",
    "        verbose: Print detailed comparison (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (matches, mismatches, missing_in_tt)\n",
    "    \"\"\"\n",
    "    \n",
    "    torch_sd = torch_model.state_dict()\n",
    "    tt_params = tt_model.parameters()\n",
    "    \n",
    "    # Get configuration from torch model\n",
    "    num_heads = torch_model.config.num_attention_heads\n",
    "    num_kv_heads = torch_model.config.num_key_value_heads\n",
    "    head_dim = getattr(torch_model.config, 'head_dim', 128)\n",
    "    hidden_size = torch_model.config.hidden_size\n",
    "    \n",
    "    # Detect weight tying configuration from TT model\n",
    "    has_tok_emb = 'qwen3/tok_emb/weight' in tt_params\n",
    "    has_fc = 'qwen3/fc/weight' in tt_params\n",
    "    weight_tying_enabled = not has_tok_emb  # If tok_emb doesn't exist, weight tying is enabled\n",
    "    \n",
    "    # Mapping from PyTorch parameter names to TT-Metal parameter names\n",
    "    pytorch_to_tt_mapping = {\n",
    "        # Final layer norm\n",
    "        'model.norm.weight': 'qwen3/ln_fc/gamma',\n",
    "    }\n",
    "    \n",
    "    # Add embedding mappings based on weight tying configuration\n",
    "    if weight_tying_enabled:\n",
    "        pytorch_to_tt_mapping['model.embed_tokens.weight'] = 'qwen3/fc/weight'\n",
    "        # Skip lm_head.weight to avoid duplicate checks\n",
    "    else:\n",
    "        pytorch_to_tt_mapping['model.embed_tokens.weight'] = 'qwen3/tok_emb/weight'\n",
    "        pytorch_to_tt_mapping['lm_head.weight'] = 'qwen3/fc/weight'\n",
    "    \n",
    "    # Add layer-specific mappings\n",
    "    for i in range(50):  # Support up to 50 layers\n",
    "        layer_prefix_pt = f'model.layers.{i}'\n",
    "        layer_prefix_tt = f'qwen3/qwen3_block_{i}'\n",
    "        \n",
    "        pytorch_to_tt_mapping.update({\n",
    "            # Layer norms\n",
    "            f'{layer_prefix_pt}.input_layernorm.weight': f'{layer_prefix_tt}/attention_norm/gamma',\n",
    "            f'{layer_prefix_pt}.post_attention_layernorm.weight': f'{layer_prefix_tt}/mlp_norm/gamma',\n",
    "            \n",
    "            # Attention projections\n",
    "            f'{layer_prefix_pt}.self_attn.q_proj.weight': f'{layer_prefix_tt}/attention/q_linear/weight',\n",
    "            f'{layer_prefix_pt}.self_attn.o_proj.weight': f'{layer_prefix_tt}/attention/out_linear/weight',\n",
    "            # k_proj and v_proj are combined into kv_linear in TT\n",
    "            \n",
    "            # Q/K norms - CRITICAL for Qwen3 numerical stability!\n",
    "            f'{layer_prefix_pt}.self_attn.q_norm.weight': f'{layer_prefix_tt}/attention/q_norm/gamma',\n",
    "            f'{layer_prefix_pt}.self_attn.k_norm.weight': f'{layer_prefix_tt}/attention/k_norm/gamma',\n",
    "            \n",
    "            # MLP projections\n",
    "            f'{layer_prefix_pt}.mlp.gate_proj.weight': f'{layer_prefix_tt}/mlp/w1/weight',\n",
    "            f'{layer_prefix_pt}.mlp.up_proj.weight': f'{layer_prefix_tt}/mlp/w3/weight',\n",
    "            f'{layer_prefix_pt}.mlp.down_proj.weight': f'{layer_prefix_tt}/mlp/w2/weight',\n",
    "        })\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"WEIGHT COMPARISON: PyTorch Qwen3 vs TT-Metal Qwen3\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Model Configuration:\")\n",
    "        print(f\"  - num_attention_heads: {num_heads}\")\n",
    "        print(f\"  - num_key_value_heads: {num_kv_heads}\")\n",
    "        print(f\"  - head_dim: {head_dim}\")\n",
    "        print(f\"  - hidden_size: {hidden_size}\")\n",
    "        print(f\"  - attention_output_dim: {num_heads * head_dim}\")\n",
    "        print(f\"  - weight_tying: {'ENABLED' if weight_tying_enabled else 'DISABLED'}\")\n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    mismatches = []\n",
    "    matches = []\n",
    "    missing_in_tt = []\n",
    "    \n",
    "    for pt_name in torch_sd.keys():\n",
    "        if 'bias' in pt_name:\n",
    "            continue  # Skip bias parameters (Qwen3 has no biases)\n",
    "            \n",
    "        pt_tensor = torch_sd[pt_name]\n",
    "        pt_shape = tuple(pt_tensor.shape)\n",
    "        \n",
    "        # Handle k_proj and v_proj specially (they're combined in TT)\n",
    "        if '.self_attn.k_proj.weight' in pt_name or '.self_attn.v_proj.weight' in pt_name:\n",
    "            layer_idx = pt_name.split('.')[2]\n",
    "            tt_name = f'qwen3/qwen3_block_{layer_idx}/attention/kv_linear/weight'\n",
    "            \n",
    "            if tt_name in tt_params:\n",
    "                tt_tensor_np = tt_params[tt_name].to_numpy()\n",
    "                tt_shape = tt_tensor_np.shape\n",
    "                \n",
    "                if '.self_attn.k_proj.weight' in pt_name:\n",
    "                    if verbose:\n",
    "                        print(f\"\\n{pt_name}\")\n",
    "                        print(f\"  PyTorch K: {pt_shape}\")\n",
    "                        print(f\"  TT (kv combined): {tt_shape}\")\n",
    "                        print(f\"  Status: K and V are combined in TT as kv_linear\")\n",
    "                    \n",
    "                    # Verify K part\n",
    "                    pt_numpy = pt_tensor.cpu().float().numpy()\n",
    "                    tt_data = tt_tensor_np.reshape(tt_shape[2:])  # Remove batch dims [1,1]\n",
    "                    \n",
    "                    # Apply RoPE permutation to K projection\n",
    "                    pt_numpy_permuted = apply_rope_permutation(pt_numpy, num_kv_heads)\n",
    "                    \n",
    "                    # K is the first half of kv_linear: [1024, 1024] in kv_linear[:1024, :]\n",
    "                    tt_k_data = tt_data[:pt_shape[0], :]\n",
    "                    \n",
    "                    # Handle transpose if needed\n",
    "                    if pt_numpy_permuted.shape != tt_k_data.shape:\n",
    "                        tt_k_data = tt_k_data.T\n",
    "                    \n",
    "                    diff = np.abs(pt_numpy_permuted - tt_k_data).max()\n",
    "                    if verbose:\n",
    "                        print(f\"  K weight max diff: {diff:.6f} (after RoPE permutation)\")\n",
    "                    \n",
    "                    if diff < 1e-3:\n",
    "                        matches.append(f\"{pt_name} (K part)\")\n",
    "                    else:\n",
    "                        mismatches.append((pt_name, f\"K_DIFF={diff:.6f}\"))\n",
    "                    \n",
    "                elif '.self_attn.v_proj.weight' in pt_name:\n",
    "                    if verbose:\n",
    "                        print(f\"\\n{pt_name}\")\n",
    "                        print(f\"  PyTorch V: {pt_shape}\")\n",
    "                        print(f\"  Status: V is second half of kv_linear\")\n",
    "                    \n",
    "                    # Verify V part\n",
    "                    pt_numpy = pt_tensor.cpu().float().numpy()\n",
    "                    tt_data = tt_tensor_np.reshape(tt_shape[2:])\n",
    "                    \n",
    "                    # V is the second half of kv_linear: [1024, 1024] in kv_linear[1024:, :]\n",
    "                    tt_v_data = tt_data[pt_shape[0]:2*pt_shape[0], :]\n",
    "                    \n",
    "                    # Handle transpose if needed\n",
    "                    if pt_numpy.shape != tt_v_data.shape:\n",
    "                        tt_v_data = tt_v_data.T\n",
    "                    \n",
    "                    diff = np.abs(pt_numpy - tt_v_data).max()\n",
    "                    if verbose:\n",
    "                        print(f\"  V weight max diff: {diff:.6f}\")\n",
    "                    \n",
    "                    if diff < 1e-3:\n",
    "                        matches.append(f\"{pt_name} (V part)\")\n",
    "                    else:\n",
    "                        mismatches.append((pt_name, f\"V_DIFF={diff:.6f}\"))\n",
    "            continue\n",
    "        \n",
    "        # Get corresponding TT parameter name\n",
    "        tt_name = pytorch_to_tt_mapping.get(pt_name)\n",
    "        if not tt_name:\n",
    "            continue\n",
    "            \n",
    "        if tt_name not in tt_params:\n",
    "            if verbose:\n",
    "                print(f\"\\n❌ MISSING IN TT: {pt_name} -> {tt_name}\")\n",
    "                print(f\"   PyTorch shape: {pt_shape}\")\n",
    "            missing_in_tt.append((pt_name, tt_name))\n",
    "            continue\n",
    "        \n",
    "        # Get TT tensor\n",
    "        tt_tensor_np = tt_params[tt_name].to_numpy()\n",
    "        tt_shape = tt_tensor_np.shape\n",
    "        \n",
    "        # Remove batch dimensions [1, 1, ...] from TT tensor\n",
    "        tt_shape_no_batch = tt_shape[2:] if len(tt_shape) == 4 else tt_shape\n",
    "        \n",
    "        # Compare shapes\n",
    "        pt_numpy = pt_tensor.cpu().float().numpy()\n",
    "        \n",
    "        # Special handling for q_proj: TT applies RoPE row permutation during loading\n",
    "        is_q_proj = '.self_attn.q_proj.weight' in pt_name\n",
    "        is_q_norm = '.self_attn.q_norm.weight' in pt_name\n",
    "        is_k_norm = '.self_attn.k_norm.weight' in pt_name\n",
    "        \n",
    "        if is_q_proj:\n",
    "            # Qwen3 Q projection: [2048, 1024] (16 heads × 128 dim each)\n",
    "            if len(pt_shape) == 2 and pt_shape[0] == num_heads * head_dim:\n",
    "                pt_numpy = apply_rope_permutation(pt_numpy, num_heads)\n",
    "        \n",
    "        # For layer norms: PT (N,) vs TT (1, N) or (1, 1, 1, N) - handle squeezing\n",
    "        if len(pt_shape) == 1:\n",
    "            if len(tt_shape_no_batch) == 2 and tt_shape_no_batch[0] == 1:\n",
    "                tt_shape_no_batch = (tt_shape_no_batch[1],)\n",
    "            elif len(tt_shape_no_batch) == 1:\n",
    "                pass  # Already 1D\n",
    "        \n",
    "        # Check if shapes match (with or without transpose)\n",
    "        shape_match = (pt_shape == tt_shape_no_batch) or (pt_shape == tt_shape_no_batch[::-1])\n",
    "        \n",
    "        if shape_match:\n",
    "            # Check actual values\n",
    "            if len(tt_shape) == 4:\n",
    "                tt_data = tt_tensor_np.reshape(tt_shape[2:])\n",
    "            else:\n",
    "                tt_data = tt_tensor_np.reshape(tt_shape)\n",
    "            \n",
    "            tt_data = tt_data.squeeze()\n",
    "            pt_numpy_squeezed = pt_numpy.squeeze()\n",
    "            \n",
    "            # Handle transpose if needed\n",
    "            if pt_numpy_squeezed.shape != tt_data.shape and len(tt_data.shape) == 2:\n",
    "                tt_data = tt_data.T\n",
    "            \n",
    "            diff = np.abs(pt_numpy_squeezed - tt_data).max()\n",
    "            rel_diff = diff / (np.abs(pt_numpy_squeezed).max() + 1e-8)\n",
    "            \n",
    "            status = \"✓\" if diff < 1e-3 else \"⚠\"\n",
    "            note = \"\"\n",
    "            if is_q_proj:\n",
    "                note = \" (after RoPE permutation)\"\n",
    "            elif is_q_norm:\n",
    "                note = \" [CRITICAL Q/K NORM]\"\n",
    "            elif is_k_norm:\n",
    "                note = \" [CRITICAL Q/K NORM]\"\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\n{status} {pt_name}{note}\")\n",
    "                print(f\"  PyTorch: {pt_shape}\")\n",
    "                print(f\"  TT:      {tt_shape} -> {tt_shape_no_batch}\")\n",
    "                print(f\"  Max diff: {diff:.6f}, Rel diff: {rel_diff:.6f}\")\n",
    "            \n",
    "            if diff < 1e-3:\n",
    "                matches.append(pt_name)\n",
    "            else:\n",
    "                mismatches.append((pt_name, f\"VALUE_DIFF={diff:.6f}\"))\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"\\n❌ SHAPE MISMATCH: {pt_name}\")\n",
    "                print(f\"  PyTorch: {pt_shape}\")\n",
    "                print(f\"  TT:      {tt_shape} -> {tt_shape_no_batch}\")\n",
    "            mismatches.append((pt_name, f\"SHAPE: PT={pt_shape} vs TT={tt_shape_no_batch}\"))\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"SUMMARY: {len(matches)} matches, {len(mismatches)} mismatches, {len(missing_in_tt)} missing in TT\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        if missing_in_tt:\n",
    "            print(f\"\\n❌ MISSING IN TT ({len(missing_in_tt)}):\")\n",
    "            for pt_name, tt_name in missing_in_tt:\n",
    "                print(f\"  - {pt_name} -> {tt_name}\")\n",
    "        \n",
    "        if mismatches:\n",
    "            print(\"\\n❌ MISMATCHES:\")\n",
    "            for name, issue in mismatches:\n",
    "                print(f\"  - {name}: {issue}\")\n",
    "        \n",
    "        if len(mismatches) == 0 and len(missing_in_tt) == 0:\n",
    "            print(\"\\n🎉 ALL WEIGHTS MATCH PERFECTLY!\")\n",
    "            print(f\"✅ {len(matches)} parameters validated\")\n",
    "            print(\"✅ Q/K normalization layers loaded correctly\")\n",
    "            print(\"✅ Qwen3 model is production-ready!\")\n",
    "    \n",
    "    return matches, mismatches, missing_in_tt\n",
    "\n",
    "\n",
    "matches, mismatches, missing = compare_qwen3_weights(torch_model, tt_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525920ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "265ccd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "import sys\n",
    "\n",
    "def convert_notebook_to_py(notebook_path, output_path):\n",
    "    \n",
    "    with open(notebook_path) as ff:\n",
    "        nb_in = nbformat.read(ff, nbformat.NO_CONVERT)\n",
    "    \n",
    "    source = \"\"\n",
    "    for cell in nb_in['cells']:\n",
    "        if cell['cell_type'] == 'code':\n",
    "            # cell['source'] can be a string or a list of strings\n",
    "            cell_source = cell['source']\n",
    "            \n",
    "            # Convert to list of lines if it's a string\n",
    "            if isinstance(cell_source, str):\n",
    "                lines = cell_source.split('\\n')\n",
    "            else:\n",
    "                # It's already a list, but may contain newlines within elements\n",
    "                lines = []\n",
    "                for item in cell_source:\n",
    "                    lines.extend(item.split('\\n'))\n",
    "            \n",
    "            # Filter out lines starting with '%' (magic commands)\n",
    "            filtered_lines = [line for line in lines if not line.strip().startswith('%')]\n",
    "            \n",
    "            # Join the lines back together\n",
    "            cell_code = '\\n'.join(filtered_lines)\n",
    "            \n",
    "            # Add to source with a newline separator\n",
    "            if cell_code.strip():  # Only add non-empty cells\n",
    "                source = source + '\\n' + cell_code\n",
    "    \n",
    "    # Write to output file\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(source)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0893d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_notebook_to_py('llm_inference_new.ipynb', 'llm_inference_new.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e26ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f868e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71842a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
