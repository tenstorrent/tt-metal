{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85286ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bb2d98f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TT_METAL_HOME=/home/ubuntu/tt-metal\n",
      "env: TT_METAL_RUNTIME_ROOT=/home/ubuntu/tt-metal\n"
     ]
    }
   ],
   "source": [
    "%env TT_METAL_HOME=/home/ubuntu/tt-metal\n",
    "%env TT_METAL_RUNTIME_ROOT=/home/ubuntu/tt-metal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb55490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\" \n",
    "CONFIG = \"training_shakespeare_llama3_2_1B_fixed.yaml\"\n",
    "#max lenght 704\n",
    "#accum 3 (max, 4 oom on n150) \n",
    "# broken model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098b3d66",
   "metadata": {},
   "source": [
    "model_id = \"Qwen/Qwen3-1.7B\" \n",
    "CONFIG = \"training_shakespeare_qwen3_1_7B.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7957aba",
   "metadata": {},
   "source": [
    "model_id = \"Qwen/Qwen3-0.6B\" \n",
    "CONFIG = \"training_shakespeare_qwen3_0_6B.yaml\"\n",
    "#max context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d629d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id =  \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "CONFIG = \"training_shakespeare_tinyllama.yaml\" # must be working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb6b171",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_id = \"Qwen/Qwen3-4B\"\n",
    "#CONFIG = \"training_shakespeare_qwen3_4B.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f7bc745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ac9556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import hf_hub_download\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Optional\n",
    "\n",
    "import ttml\n",
    "from ttml.common.config import DeviceConfig, get_training_config, load_config\n",
    "from ttml.common.model_factory import TransformerModelFactory\n",
    "from ttml.common.utils import round_up_to_tile, create_optimizer, initialize_device\n",
    "#from ttml import build_causal_mask\n",
    "\n",
    "#import tt_serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d7add4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_causal_mask(T: int):\n",
    "    m = np.tril(np.ones((T, T), dtype=np.float32))\n",
    "    return m.reshape(1, 1, T, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3fc2b7",
   "metadata": {},
   "source": [
    "class SpeedrunScheduler:\n",
    "    \"\"\"Linear warmup -> optional hold -> linear decay; optional beta1 warmup.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: SchedulerConfig):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def lr_at(self, step: int) -> float:\n",
    "        s = step\n",
    "        w = max(0, self.cfg.warmup_steps)\n",
    "        h = max(0, self.cfg.hold_steps)\n",
    "        T = max(1, self.cfg.total_steps)\n",
    "        peak = self.cfg.max_lr\n",
    "        min_lr = self.cfg.min_lr\n",
    "\n",
    "        if s <= w:\n",
    "            # linear warmup 0 -> lr_max\n",
    "            return peak * (s / max(1, w))\n",
    "        elif s <= w + h:\n",
    "            # hold at lr_max\n",
    "            return peak\n",
    "        else:\n",
    "            # linear decay from lr_max at (w+h) to min_lr at T\n",
    "            s2 = min(s, T)\n",
    "            frac = (s2 - (w + h)) / max(1, (T - (w + h)))\n",
    "            return peak + (min_lr - peak) * frac\n",
    "\n",
    "    def beta1_at(self, step: int) -> Optional[float]:\n",
    "        if (\n",
    "            self.cfg.beta1_start is None\n",
    "            or self.cfg.beta1_end is None\n",
    "            or self.cfg.beta1_warmup_steps <= 0\n",
    "        ):\n",
    "            return None\n",
    "        s = min(step, self.cfg.beta1_warmup_steps)\n",
    "        t = s / float(self.cfg.beta1_warmup_steps)\n",
    "        return (1.0 - t) * self.cfg.beta1_start + t * self.cfg.beta1_end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "566cfa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimParamSetter:\n",
    "    def __init__(self, optim):\n",
    "        self.optim = optim\n",
    "        self._warned_lr = False\n",
    "        self._warned_beta1 = False\n",
    "\n",
    "    def set_lr(self, lr: float):\n",
    "        self.optim.set_lr(float(lr))\n",
    "\n",
    "    def set_beta1(self, beta1: float):\n",
    "        raise NotImplementedError(\n",
    "            \"set_beta1 is not implemented in TTML AdamW optimizer.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def build_logits_mask(vocab_size: int, padded_vocab_size: int) -> ttml.autograd.Tensor:\n",
    "    logits_mask = np.zeros((1, 1, 1, padded_vocab_size), dtype=np.float32)\n",
    "    logits_mask[:, :, :, vocab_size:] = 1e4\n",
    "    return ttml.autograd.Tensor.from_numpy(\n",
    "        logits_mask, ttml.Layout.TILE, ttml.autograd.DataType.BFLOAT16\n",
    "    )  # [1,1,1,T], bfloat16\"\n",
    "\n",
    "\n",
    "class CollateFn:\n",
    "    def __init__(self, eos_token_id, max_sequence_length, padded_vocab_size):\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.padded_vocab_size = padded_vocab_size\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        X = [sample[0] for sample in batch]\n",
    "        Y = [sample[1] for sample in batch]\n",
    "\n",
    "        batch_size = len(X)\n",
    "\n",
    "        data_np = np.full(\n",
    "            (batch_size, self.max_sequence_length), self.eos_token_id, dtype=np.uint32\n",
    "        )\n",
    "        mask_lens = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            x_tokens = X[i]\n",
    "            y_tokens = Y[i]\n",
    "\n",
    "            # Concatenate question + answer\n",
    "            combined_length = len(x_tokens) + len(y_tokens)\n",
    "            if combined_length > self.max_sequence_length:\n",
    "                # Truncate if too long, prioritizing keeping the answer\n",
    "                available_space = self.max_sequence_length - len(y_tokens)\n",
    "                if available_space > 0:\n",
    "                    x_tokens = x_tokens[:available_space]\n",
    "                    data_np[i, : len(x_tokens)] = x_tokens\n",
    "                    data_np[i, len(x_tokens) : len(x_tokens) + len(y_tokens)] = y_tokens\n",
    "\n",
    "                else:\n",
    "                    # If answer is too long, just use the answer\n",
    "                    data_np[i, : self.max_sequence_length] = y_tokens[\n",
    "                        : self.max_sequence_length\n",
    "                    ]\n",
    "                    x_tokens = []\n",
    "\n",
    "            else:\n",
    "                # Normal case: concatenate question + answer\n",
    "\n",
    "                data_np[i, : len(x_tokens)] = x_tokens\n",
    "                data_np[i, len(x_tokens) : len(x_tokens) + len(y_tokens)] = y_tokens\n",
    "\n",
    "            mask_lens.append(len(x_tokens))\n",
    "\n",
    "        # Shape: [batch_size, 1, 1, max_sequence_length]\n",
    "        X_np = np.expand_dims(data_np, axis=(1, 2))\n",
    "\n",
    "        y_np = np.full(\n",
    "            (batch_size, self.max_sequence_length), self.eos_token_id, dtype=np.uint32\n",
    "        )  # Shape: [batch, seq_len]\n",
    "        y_np[:, 0:-1] = X_np[:, 0, 0, 1:]  # Shift left by 1\n",
    "\n",
    "        loss_scaler_np = np.full(\n",
    "            (batch_size, 1, self.max_sequence_length, 1), 1.0, dtype=np.float32\n",
    "        )\n",
    "        for i, mask_len in enumerate(mask_lens):\n",
    "            loss_scaler_np[i, :, :mask_len, :] = 0.0\n",
    "            pad_positions = X_np[i, 0, 0, :] == self.eos_token_id\n",
    "            loss_scaler_np[i, :, pad_positions, :] = 0.0\n",
    "        loss_scaler_ratio = (\n",
    "            self.max_sequence_length * batch_size / np.sum(loss_scaler_np)\n",
    "        )\n",
    "        loss_scaler_np = loss_scaler_np * loss_scaler_ratio\n",
    "\n",
    "        return X_np, y_np, loss_scaler_np\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return self.collate_fn(batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17e7629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_generator(\n",
    "    dataloader,\n",
    "    batch_size,\n",
    "    max_sequence_length,\n",
    "    padded_vocab_size,\n",
    "    tokenizer,\n",
    "    device_config=None,\n",
    "):\n",
    "    \"\"\"Custom data generator for GSM8K dataset.\"\"\"\n",
    "    mapper = None\n",
    "    if device_config is not None:\n",
    "        device = ttml.autograd.AutoContext.get_instance().get_device()\n",
    "        mapper = ttml.core.distributed.shard_tensor_to_mesh_mapper(device, 0)\n",
    "\n",
    "    while True:\n",
    "        for batch in dataloader:\n",
    "            X_np, y_np, loss_scaler_np = batch\n",
    "\n",
    "            X = ttml.autograd.Tensor.from_numpy(\n",
    "                X_np, ttml.Layout.ROW_MAJOR, ttml.autograd.DataType.UINT32, mapper\n",
    "            )\n",
    "            y = ttml.autograd.Tensor.from_numpy(\n",
    "                y_np, ttml.Layout.ROW_MAJOR, ttml.autograd.DataType.UINT32, mapper\n",
    "            )\n",
    "            loss_scaler = ttml.autograd.Tensor.from_numpy(\n",
    "                loss_scaler_np,\n",
    "                ttml.Layout.TILE,\n",
    "                ttml.autograd.DataType.BFLOAT16,\n",
    "                mapper,\n",
    "            )\n",
    "\n",
    "            yield (X, y, loss_scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dc81f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_tt(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    question,\n",
    "    max_sequence_length,\n",
    "    causal_mask,\n",
    "    temperature,\n",
    "    logits_mask_tensor,\n",
    "    max_gen_tokens,\n",
    "    pad_token_id=None,\n",
    "    return_with_prompt=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Greedy/temperature=0 generation that prints the *full* text once at the end.\n",
    "    Uses a sliding window if prompt exceeds max_sequence_length.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(\n",
    "        ttml.autograd.GradMode.DISABLED\n",
    "    )\n",
    "\n",
    "    # --- Tokenize once ---\n",
    "    prompt_tokens = tokenizer.encode(question)\n",
    "    if pad_token_id is None:\n",
    "        # Try tokenizer.pad_token_id, else fall back to 0\n",
    "        pad_token_id = getattr(tokenizer, \"pad_token_id\", None)\n",
    "        if pad_token_id is None:\n",
    "            pad_token_id = 0\n",
    "\n",
    "    generated_tokens = []\n",
    "\n",
    "    device = ttml.autograd.AutoContext.get_instance().get_device()\n",
    "    composer = ttml.core.distributed.concat_mesh_to_tensor_composer(device, 0)\n",
    "\n",
    "    # Preallocate once\n",
    "    padded_prompt_tokens = np.full(\n",
    "        (1, 1, 1, max_sequence_length), pad_token_id, dtype=np.uint32\n",
    "    )\n",
    "    for _ in tqdm(range(max_gen_tokens)):\n",
    "        # Sliding window for long prompts\n",
    "        if len(prompt_tokens) > max_sequence_length:\n",
    "            start_idx = len(prompt_tokens) - max_sequence_length\n",
    "            window = prompt_tokens[start_idx:]\n",
    "        else:\n",
    "            start_idx = 0\n",
    "            window = prompt_tokens\n",
    "\n",
    "        # Refill buffer (fully) to avoid stale ids\n",
    "        padded_prompt_tokens[...] = pad_token_id\n",
    "        padded_prompt_tokens[0, 0, 0, : len(window)] = np.asarray(\n",
    "            window, dtype=np.uint32\n",
    "        )\n",
    "\n",
    "        # [1,1,1,T] -> TT tensor\n",
    "        padded_prompt_tensor = ttml.autograd.Tensor.from_numpy(\n",
    "            padded_prompt_tokens, ttml.Layout.ROW_MAJOR, ttml.autograd.DataType.UINT32\n",
    "        )\n",
    "\n",
    "        # Forward: logits [1,1,T,V]\n",
    "        logits = model(padded_prompt_tensor, causal_mask)\n",
    "\n",
    "        # Sample: next tokens for all positions [1,1,T,1]\n",
    "        # With temperature=0.0 this behaves like argmax/greedy.\n",
    "        next_token_tensor = ttml.ops.sample.sample_op(\n",
    "            logits, 0.0, np.random.randint(low=1e7), logits_mask_tensor\n",
    "        )\n",
    "\n",
    "        # Take the token at the last active position in the current window\n",
    "        next_token_idx = (\n",
    "            max_sequence_length - 1\n",
    "            if len(prompt_tokens) > max_sequence_length\n",
    "            else len(window) - 1\n",
    "        )\n",
    "        next_token = int(\n",
    "            next_token_tensor.to_numpy(composer=composer).reshape(-1, 1)[\n",
    "                next_token_idx\n",
    "            ][0]\n",
    "        )\n",
    "\n",
    "        if next_token == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        generated_tokens.append(next_token)\n",
    "        prompt_tokens.append(next_token)\n",
    "\n",
    "    # Decode once at the end\n",
    "    out = tokenizer.decode(generated_tokens)\n",
    "    if return_with_prompt:\n",
    "        out = tokenizer.decode(prompt_tokens)\n",
    "\n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(\n",
    "        ttml.autograd.GradMode.ENABLED\n",
    "    )\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b741c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "    tt_model,\n",
    "    tokenizer,\n",
    "    val_batch_generator,\n",
    "    testing_data,\n",
    "    loss_fn,\n",
    "    causal_mask,\n",
    "    logits_mask_tensor,\n",
    "    max_sequence_length,\n",
    "    max_gen_tokens,\n",
    "    current_step,\n",
    "):\n",
    "    reduce = ttml.ops.ReduceType.NONE\n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(\n",
    "        ttml.autograd.GradMode.DISABLED\n",
    "    )\n",
    "    tt_model.eval()\n",
    "    eval_batch_count = 4\n",
    "    cur_val_losses = []\n",
    "    for _ in range(eval_batch_count):\n",
    "        val_X, val_y, val_loss_scaler = next(val_batch_generator)\n",
    "        val_logits = tt_model(val_X, causal_mask)\n",
    "\n",
    "        # Compute validation loss\n",
    "        val_loss = loss_fn(val_logits, val_y, reduce)\n",
    "        val_loss = val_loss * val_loss_scaler\n",
    "        val_loss = ttml.ops.unary.mean(val_loss)\n",
    "        cur_val_losses.append(get_loss_over_devices(val_loss))\n",
    "\n",
    "    checks_count = 4\n",
    "\n",
    "    with open(\"validation.txt\", \"a+\") as val_file:\n",
    "        val_file.write(f\"Validation at step {current_step}\\n\")\n",
    "        for check in range(checks_count):\n",
    "            val_file.write(f\"Validation check: {check}\\n\")\n",
    "            val_file.write(\"====================================\\n\")\n",
    "\n",
    "            tokenized_question, tokenized_answer = testing_data[check]\n",
    "            question = tokenizer.decode(tokenized_question, skip_special_tokens=True)\n",
    "\n",
    "            val_file.write(f\"Question: {question}\\n\")\n",
    "            val_file.write(\"====================================\\n\")\n",
    "\n",
    "            gen_text = generate_text_tt(\n",
    "                tt_model,\n",
    "                tokenizer,\n",
    "                question,\n",
    "                max_sequence_length,\n",
    "                causal_mask,\n",
    "                0.0,\n",
    "                logits_mask_tensor,\n",
    "                max_gen_tokens\n",
    "            )\n",
    "\n",
    "            val_file.write(f\"Generated Answer: {gen_text}\\n\")\n",
    "            val_file.write(\"\\n====================================\\n\")\n",
    "        \n",
    "        mean_loss = float(np.mean(cur_val_losses))\n",
    "        val_file.write(f\"Last validation loss: {mean_loss:.4f}\\n\\n\\n\")\n",
    "\n",
    "    ttml.autograd.AutoContext.get_instance().set_gradient_mode(\n",
    "        ttml.autograd.GradMode.ENABLED\n",
    "    )\n",
    "    tt_model.train()\n",
    "    return np.mean(cur_val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb572805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_logits(logits, binary_mask, add_mask):\n",
    "    masked_logits = binary_mask * logits\n",
    "    masked_logits = masked_logits + add_mask\n",
    "\n",
    "    return masked_logits\n",
    "\n",
    "\n",
    "def get_loss_over_devices(loss):\n",
    "    device = ttml.autograd.AutoContext.get_instance().get_device()\n",
    "    composer = ttml.core.distributed.concat_mesh_to_tensor_composer(device, 0)\n",
    "    loss_numpy = loss.to_numpy(composer=composer)\n",
    "    return loss_numpy.mean()\n",
    "\n",
    "\n",
    "def tokenize_dataset(data, tokenizer):\n",
    "    X = [sample[\"question\"] for sample in data]\n",
    "    y = [sample[\"answer\"] for sample in data]\n",
    "\n",
    "    X = tokenizer(X, return_tensors=\"np\", add_special_tokens=False)[\"input_ids\"]\n",
    "    y = tokenizer(y, return_tensors=\"np\", add_special_tokens=False)[\"input_ids\"]\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1086c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.len = len(X)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d635ba82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_shakespeare_tinyllama.yaml'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49df22b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and config...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer and config...\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "# Disable tokenizer parallelism to avoid conflicts with DataLoader multiprocessing\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "yaml_config = get_training_config(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd00c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c10a4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GSM8K dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration main-2ef46c85c6cfc797\n",
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/parquet/main-2ef46c85c6cfc797/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Using custom data configuration main-2ef46c85c6cfc797\n",
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/parquet/main-2ef46c85c6cfc797/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "864\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print(\"Loading GSM8K dataset...\")\n",
    "training_data = datasets.load_dataset(\"gsm8k\", \"main\", split=\"train\", ignore_verifications=True)\n",
    "testing_data = datasets.load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
    "\n",
    "training_data_x, training_data_y = tokenize_dataset(training_data, tokenizer)\n",
    "testing_data_x, testing_data_y = tokenize_dataset(testing_data, tokenizer)\n",
    "training_data = TokenizedDataset(training_data_x, training_data_y)\n",
    "testing_data = TokenizedDataset(testing_data_x, testing_data_y)\n",
    "\n",
    "max_gen_tokens = max(max(s.shape[0] for s in training_data_y),\n",
    "                     max(s.shape[0] for s in testing_data_y))\n",
    "max_seq_lenght = max(max(s.shape[0] for s in training_data_x),\n",
    "                     max(s.shape[0] for s in testing_data_x)) + max_gen_tokens\n",
    "\n",
    "max_seq_lenght = round_up_to_tile(max_seq_lenght)\n",
    "print(max_seq_lenght)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de86cb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_config.use_moreh_adamw = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5f895e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_config.gradient_accumulation_steps = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "676e743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_config.steps = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5434b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('overrride max sequence length:', yaml_config['training_config']['transformer_config']['max_sequence_length'], max_seq_lenght)\n",
    "#yaml_config['training_config']['transformer_config']['max_sequence_length'] = max_seq_lenght\n",
    "#yaml_config['training_config']['gradient_accumulation_steps'] = 128\n",
    "#yaml_config['training_config']['max_steps'] = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02262214",
   "metadata": {},
   "outputs": [],
   "source": [
    "#yaml_config['training_config']['transformer_config']['max_sequence_length'] = max_seq_lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7c50c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = yaml_config.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa1f185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_config = DeviceConfig(yaml_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "473de333",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device_config.total_devices() > 1:\n",
    "    initialize_device(yaml_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07ae022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_config = TrainingConfig(yaml_config)\n",
    "#scheduler_config = SchedulerConfig(yaml_config)\n",
    "\n",
    "\n",
    "# Download safetensors\n",
    "safetensors_path = hf_hub_download(repo_id=model_id, filename=\"config.json\")\n",
    "safetensors_path = safetensors_path.replace(\"config.json\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9efdd80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aad36cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.53.0'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eacb0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5b850b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "torch.manual_seed(42)\n",
    "torch_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f570acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_vocab_size = torch_model.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "460c060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = get_training_config(CONFIG)\n",
    "model_yaml = load_config(training_config.model_config, configs_root=os.getcwd() + '/../../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52004343",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_model_factory = TransformerModelFactory(model_yaml)\n",
    "tt_model_factory.transformer_config.vocab_size = orig_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2faff2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_model_factory.transformer_config.max_sequence_length = max_seq_lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "285ed883",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = tt_model_factory.transformer_config.max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a572fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3104282e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_model_factory.transformer_config.embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01235c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_model_factory.transformer_config.num_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35476558",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttml.autograd.AutoContext.get_instance().set_init_mode(ttml.autograd.InitMode.DISABLED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d52e8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama configuration:\n",
      "    Vocab size: 32000\n",
      "    Max sequence length: 864\n",
      "    Embedding dim: 2048\n",
      "    Intermediate dim: None\n",
      "    Num heads: 32\n",
      "    Num groups: 4\n",
      "    Dropout probability: 0\n",
      "    Num blocks: 22\n",
      "    Positional embedding type: RoPE\n",
      "    Runner type: Memory efficient\n",
      "    Weight tying: Disabled\n",
      "    Theta: 10000\n",
      "2025-12-23 22:40:27.465 | info     |             UMD | Starting topology discovery. (topology_discovery.cpp:69)\n",
      "2025-12-23 22:40:27.532 | info     |             UMD | Established firmware bundle version: 80.17.0 (topology_discovery.cpp:363)\n",
      "2025-12-23 22:40:27.532 | info     |             UMD | Established ETH FW version: 6.14.0 (topology_discovery_wormhole.cpp:324)\n",
      "2025-12-23 22:40:27.532 | info     |             UMD | Completed topology discovery. (topology_discovery.cpp:73)\n",
      "2025-12-23 22:40:27.541 | info     |          Device | Opening user mode device driver (tt_cluster.cpp:214)\n",
      "2025-12-23 22:40:27.541 | info     |             UMD | Starting topology discovery. (topology_discovery.cpp:69)\n",
      "2025-12-23 22:40:27.547 | info     |             UMD | Established firmware bundle version: 80.17.0 (topology_discovery.cpp:363)\n",
      "2025-12-23 22:40:27.547 | info     |             UMD | Established ETH FW version: 6.14.0 (topology_discovery_wormhole.cpp:324)\n",
      "2025-12-23 22:40:27.547 | info     |             UMD | Completed topology discovery. (topology_discovery.cpp:73)\n",
      "2025-12-23 22:40:27.548 | info     |             UMD | Starting topology discovery. (topology_discovery.cpp:69)\n",
      "2025-12-23 22:40:27.659 | info     |             UMD | Established firmware bundle version: 80.17.0 (topology_discovery.cpp:363)\n",
      "2025-12-23 22:40:27.659 | info     |             UMD | Established ETH FW version: 6.14.0 (topology_discovery_wormhole.cpp:324)\n",
      "2025-12-23 22:40:27.660 | info     |             UMD | Completed topology discovery. (topology_discovery.cpp:73)\n",
      "2025-12-23 22:40:27.660 | info     |             UMD | Harvesting masks for chip 0 tensix: 0x1 dram: 0x0 eth: 0x0 pcie: 0x0 l2cpu: 0x0 (cluster.cpp:358)\n",
      "2025-12-23 22:40:27.794 | warning  |             UMD | init_detect_tt_device_numanodes(): Could not determine NumaNodeSet for TT device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:564)\n",
      "2025-12-23 22:40:27.794 | warning  |             UMD | Could not find NumaNodeSet for TT Device (physical_device_id: 0 pci_bus_id: 0000:04:00.0) (cpuset_lib.cpp:173)\n",
      "2025-12-23 22:40:27.795 | warning  |             UMD | bind_area_memory_nodeset(): Unable to determine TT Device to NumaNode mapping for physical_device_id: 0. Skipping membind. (cpuset_lib.cpp:450)\n",
      "2025-12-23 22:40:27.795 | warning  |             UMD | ---- ttSiliconDevice::init_hugepage: bind_area_to_memory_nodeset() failed (physical_device_id: 0 ch: 0). Hugepage allocation is not on NumaNode matching TT Device. Side-Effect is decreased Device->Host perf (Issue #893). (sysmem_manager.cpp:219)\n",
      "2025-12-23 22:40:27.796 | info     |             UMD | Opening local chip ids/PCIe ids: {0}/[0] and remote chip ids {} (cluster.cpp:202)\n",
      "2025-12-23 22:40:27.796 | info     |             UMD | IOMMU: disabled (cluster.cpp:178)\n",
      "2025-12-23 22:40:27.796 | info     |             UMD | KMD version: 2.0.0 (cluster.cpp:181)\n",
      "2025-12-23 22:40:27.809 | info     |             UMD | Mapped hugepage 0x200000000 to NOC address 0x800000000 (sysmem_manager.cpp:247)\n",
      "2025-12-23 22:40:27.860 | info     |     Distributed | Using auto discovery to generate mesh graph. (metal_context.cpp:765)\n",
      "2025-12-23 22:40:27.860 | info     |     Distributed | Constructing control plane using auto-discovery (no mesh graph descriptor). (metal_context.cpp:742)\n",
      "2025-12-23 22:40:27.861 | info     |          Fabric | TopologyMapper mapping start (mesh=0): n_log=1, n_phys=1, log_deg_hist={0:1}, phys_deg_hist={0:1} (topology_mapper_utils.cpp:171)\n",
      "2025-12-23 22:40:27.861 | info     |          Fabric | TopologyMapper mapping start (mesh=0): n_log=1, n_phys=1, log_deg_hist={0:1}, phys_deg_hist={0:1} (topology_mapper_utils.cpp:171)\n",
      "Model created: 1.4868171215057373\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "tt_model = tt_model_factory.create_model()\n",
    "print(f\"Model created: {time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e07f3886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model created: 133.076\n",
    "# Model loaded: 27.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af03e747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created: 1.498899221420288\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model created: {time() - start_time}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9e3ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model created: 68.56\n",
    "# Model loaded: 26.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c92c3570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model created: 8.84\n",
    "# Model loaded: 32.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "475e5c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/ubuntu/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1431k-3T/snapshots/59f6f375b26bde864a6ca194a9a3044570490064/model.safetensors\n",
      "Loading tensor: lm_head.weight, shape:Shape([32000, 2048]), dtype:F32\n",
      "Using parameter: llama/fc/weight with shape: Shape([1, 1, 32000, 2048])\n",
      "Loading tensor: model.embed_tokens.weight, shape:Shape([32000, 2048]), dtype:F32\n",
      "Using parameter: llama/tok_emb/weight with shape: Shape([1, 1, 32000, 2048])\n",
      "Loading tensor: model.layers.0.input_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_0/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.0.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:F32\n",
      "Using parameter: llama/llama_block_0/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.0.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_0/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.0.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_0/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.0.post_attention_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_0/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.0.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Loading tensor: model.layers.0.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_0/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.0.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_0/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.0.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_0/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 0\n",
      "Loading tensor: model.layers.1.input_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_1/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.1.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:F32\n",
      "Using parameter: llama/llama_block_1/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.1.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_1/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.1.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_1/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.1.post_attention_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_1/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.1.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Loading tensor: model.layers.1.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_1/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.1.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_1/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.1.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_1/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 1\n",
      "Loading tensor: model.layers.10.input_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_10/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.10.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:F32\n",
      "Using parameter: llama/llama_block_10/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.10.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_10/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.10.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_10/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.10.post_attention_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_10/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.10.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Loading tensor: model.layers.10.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_10/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.10.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_10/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.10.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_10/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 10\n",
      "Loading tensor: model.layers.11.input_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_11/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.11.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:F32\n",
      "Using parameter: llama/llama_block_11/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.11.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_11/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.11.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_11/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.11.post_attention_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_11/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.11.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Loading tensor: model.layers.11.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_11/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.11.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_11/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.11.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_11/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 11\n",
      "Loading tensor: model.layers.12.input_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_12/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.12.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:F32\n",
      "Using parameter: llama/llama_block_12/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.12.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_12/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.12.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_12/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.12.post_attention_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_12/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.12.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Loading tensor: model.layers.12.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_12/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.12.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_12/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.12.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_12/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 12\n",
      "Loading tensor: model.layers.13.input_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_13/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.13.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:F32\n",
      "Using parameter: llama/llama_block_13/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.13.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_13/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.13.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_13/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.13.post_attention_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_13/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.13.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Loading tensor: model.layers.13.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_13/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.13.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_13/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.13.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_13/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 13\n",
      "Loading tensor: model.layers.14.input_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_14/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.14.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:F32\n",
      "Using parameter: llama/llama_block_14/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.14.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_14/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.14.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_14/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.14.post_attention_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_14/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.14.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Loading tensor: model.layers.14.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_14/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.14.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_14/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.14.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_14/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 14\n",
      "Loading tensor: model.layers.15.input_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_15/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.15.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:F32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using parameter: llama/llama_block_15/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.15.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_15/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.15.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_15/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.15.post_attention_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_15/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.15.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Loading tensor: model.layers.15.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_15/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.15.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_15/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.15.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_15/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 15\n",
      "Loading tensor: model.layers.16.input_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_16/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.16.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:F32\n",
      "Using parameter: llama/llama_block_16/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.16.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_16/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.16.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_16/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.16.post_attention_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_16/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.16.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Loading tensor: model.layers.16.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_16/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.16.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_16/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.16.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_16/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 16\n",
      "Loading tensor: model.layers.17.input_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_17/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.17.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:F32\n",
      "Using parameter: llama/llama_block_17/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.17.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_17/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.17.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_17/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.17.post_attention_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_17/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.17.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Loading tensor: model.layers.17.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_17/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.17.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_17/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.17.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_17/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 17\n",
      "Loading tensor: model.layers.18.input_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_18/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.18.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:F32\n",
      "Using parameter: llama/llama_block_18/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.18.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_18/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.18.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_18/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.18.post_attention_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_18/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.18.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Loading tensor: model.layers.18.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_18/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.18.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_18/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.18.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_18/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 18\n",
      "Loading tensor: model.layers.19.input_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_19/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.19.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:F32\n",
      "Using parameter: llama/llama_block_19/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.19.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_19/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.19.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_19/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.19.post_attention_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_19/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.19.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Loading tensor: model.layers.19.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_19/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.19.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_19/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.19.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_19/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 19\n",
      "Loading tensor: model.layers.2.input_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_2/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.2.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:F32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using parameter: llama/llama_block_2/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.2.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_2/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.2.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_2/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.2.post_attention_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_2/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.2.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Loading tensor: model.layers.2.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_2/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.2.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_2/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.2.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_2/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 2\n",
      "Loading tensor: model.layers.20.input_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_20/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.20.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:F32\n",
      "Using parameter: llama/llama_block_20/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.20.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_20/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.20.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_20/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.20.post_attention_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_20/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.20.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Loading tensor: model.layers.20.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_20/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.20.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_20/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.20.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_20/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 20\n",
      "Loading tensor: model.layers.21.input_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_21/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.21.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:F32\n",
      "Using parameter: llama/llama_block_21/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.21.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_21/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.21.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_21/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.21.post_attention_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_21/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.21.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Loading tensor: model.layers.21.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_21/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.21.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_21/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.21.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_21/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 21\n",
      "Loading tensor: model.layers.3.input_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_3/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.3.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:F32\n",
      "Using parameter: llama/llama_block_3/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.3.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_3/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.3.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_3/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.3.post_attention_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_3/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.3.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Loading tensor: model.layers.3.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_3/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.3.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_3/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.3.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_3/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 3\n",
      "Loading tensor: model.layers.4.input_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_4/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.4.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:F32\n",
      "Using parameter: llama/llama_block_4/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.4.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_4/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.4.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_4/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.4.post_attention_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_4/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.4.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Loading tensor: model.layers.4.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_4/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.4.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_4/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.4.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_4/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 4\n",
      "Loading tensor: model.layers.5.input_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_5/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.5.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:F32\n",
      "Using parameter: llama/llama_block_5/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tensor: model.layers.5.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_5/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.5.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_5/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.5.post_attention_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_5/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.5.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Loading tensor: model.layers.5.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_5/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.5.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_5/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.5.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_5/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 5\n",
      "Loading tensor: model.layers.6.input_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_6/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.6.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:F32\n",
      "Using parameter: llama/llama_block_6/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.6.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_6/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.6.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_6/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.6.post_attention_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_6/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.6.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Loading tensor: model.layers.6.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_6/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.6.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_6/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.6.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_6/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 6\n",
      "Loading tensor: model.layers.7.input_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_7/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.7.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:F32\n",
      "Using parameter: llama/llama_block_7/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.7.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_7/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.7.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_7/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.7.post_attention_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_7/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.7.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Loading tensor: model.layers.7.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_7/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.7.self_attn.q_proj.weight, shape:ShModel loaded: 56.90805435180664\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "tt_model.load_from_safetensors(safetensors_path)\n",
    "print(f\"Model loaded: {time() - start_time}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58a1eeb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 56.91741180419922\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model loaded: {time() - start_time}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dcc96817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 56.9238657951355\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model loaded: {time() - start_time}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8775f715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 56.92977499961853\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model loaded: {time() - start_time}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "63c9e88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_vocab_size = round_up_to_tile(orig_vocab_size, 32)\n",
    "if orig_vocab_size != padded_vocab_size:\n",
    "    print(f\"Padding vocab size for tilization: original {orig_vocab_size} -> padded {padded_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "44b7e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#yaml_config.validation_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ebf14ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_config.validation_batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e2a04d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ttml.common.config.TrainingConfig at 0x7f703234e380>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fbf50e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_dataloader = DataLoader(\n",
    "    training_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,  # Shuffle the dataset for each epoch\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=CollateFn(\n",
    "        tokenizer.eos_token_id, max_seq_lenght, padded_vocab_size\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "num_devices = device_config.total_devices()\n",
    "testing_dataloader = DataLoader(\n",
    "    testing_data,\n",
    "    batch_size=yaml_config.validation_batch_size * num_devices,\n",
    "    shuffle=False,  # Disable shuffling for validation\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=CollateFn(\n",
    "        tokenizer.eos_token_id, max_sequence_length, padded_vocab_size\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3fdeb7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "41c4b4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml_config.beta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8935f25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0003, 0.9)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml_config.lr, yaml_config.beta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "642a01b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "08ffbbb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function ttml.common.utils.create_optimizer(model, config)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff6c1b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training\n",
    "\n",
    "optim = create_optimizer(tt_model, yaml_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "91ebbee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask = build_causal_mask(max_sequence_length)\n",
    "causal_mask = ttml.autograd.Tensor.from_numpy(\n",
    "    causal_mask, ttml.Layout.ROW_MAJOR, ttml.autograd.DataType.BFLOAT16\n",
    ")\n",
    "\n",
    "logits_mask_tensor = build_logits_mask(orig_vocab_size, padded_vocab_size)\n",
    "\n",
    "loss_fn = ttml.ops.loss.cross_entropy_loss\n",
    "reduce = ttml.ops.ReduceType.NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "03b9e44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training setup\n",
    "tt_model.train()\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_batch_generator = get_batch_generator(\n",
    "    training_dataloader,\n",
    "    batch_size,\n",
    "    max_sequence_length,\n",
    "    padded_vocab_size,\n",
    "    tokenizer,\n",
    "    device_config,\n",
    ")\n",
    "\n",
    "val_batch_generator = get_batch_generator(\n",
    "    testing_dataloader,\n",
    "    yaml_config.validation_batch_size * num_devices,\n",
    "    max_sequence_length,\n",
    "    padded_vocab_size,\n",
    "    tokenizer,\n",
    "    device_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "88efd8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens per micro-batch: 864\n",
      "Tokens per accumulated batch: 27648\n"
     ]
    }
   ],
   "source": [
    "tokens_per_batch = batch_size * max_sequence_length\n",
    "print(\"Tokens per micro-batch:\", tokens_per_batch)\n",
    "print(\n",
    "    \"Tokens per accumulated batch:\",\n",
    "    tokens_per_batch * yaml_config.gradient_accumulation_steps,\n",
    ")\n",
    "\n",
    "#sched = SpeedrunScheduler(scheduler_config)\n",
    "setter = OptimParamSetter(optim)\n",
    "\n",
    "f = open(\"validation.txt\", \"w\")\n",
    "f.write(\"Validation log\\n\")\n",
    "f.write(\"===============\\n\")\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "183c45cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml_config.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b007a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e1125d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_config.eval_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa45590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for max 500 steps...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e0f9e2cde849c887a48ae753b578c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6964c6e2f9d1452b89e27cd9056e7e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_7/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.7.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_7/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 7\n",
      "Loading tensor: model.layers.8.input_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_8/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.8.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:F32\n",
      "Using parameter: llama/llama_block_8/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.8.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_8/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.8.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_8/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.8.post_attention_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_8/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.8.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Loading tensor: model.layers.8.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_8/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.8.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_8/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.8.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_8/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 8\n",
      "Loading tensor: model.layers.9.input_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_9/attention_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.9.mlp.down_proj.weight, shape:Shape([2048, 5632]), dtype:F32\n",
      "Using parameter: llama/llama_block_9/mlp/w2/weight with shape: Shape([1, 1, 2048, 5632])\n",
      "Loading tensor: model.layers.9.mlp.gate_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_9/mlp/w1/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.9.mlp.up_proj.weight, shape:Shape([5632, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_9/mlp/w3/weight with shape: Shape([1, 1, 5632, 2048])\n",
      "Loading tensor: model.layers.9.post_attention_layernorm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_9/mlp_norm/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "Loading tensor: model.layers.9.self_attn.k_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Loading tensor: model.layers.9.self_attn.o_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_9/attention/out_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.9.self_attn.q_proj.weight, shape:Shape([2048, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_9/attention/q_linear/weight with shape: Shape([1, 1, 2048, 2048])\n",
      "Loading tensor: model.layers.9.self_attn.v_proj.weight, shape:Shape([256, 2048]), dtype:F32\n",
      "Using parameter: llama/llama_block_9/attention/kv_linear/weight with shape: Shape([1, 1, 512, 2048])\n",
      "Combined k_proj + v_proj → kv_linear for layer 9\n",
      "Loading tensor: model.norm.weight, shape:Shape([2048]), dtype:F32\n",
      "Using parameter: llama/ln_fc/gamma with shape: Shape([1, 1, 1, 2048])\n",
      "All 179 parameters were successfully loaded and used.\n",
      "2025-12-23 22:41:47.312 | info     |            Test | Small tensor algorithm selected (softmax_backward_w_small.cpp:18)\n",
      "1.4752197265625\n",
      "False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4596c2c28d324c08b163773fcebb5878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\n",
    "    f\"Starting training for max {yaml_config.steps} steps...\"\n",
    ")\n",
    "bar = tqdm(range(1, yaml_config.steps + 1))\n",
    "\n",
    "total_steps = 0\n",
    "last_val_loss = 0\n",
    "accum_steps = yaml_config.gradient_accumulation_steps\n",
    "\n",
    "\n",
    "# ========== Training Loop ===========\n",
    "for opt_step in bar:\n",
    "    # LR (and optional beta1) updated once per optimizer step\n",
    "    optim.zero_grad()\n",
    "    #lr_now = sched.lr_at(opt_step - 1)  # zero-based inside scheduler\n",
    "    #setter.set_lr(lr_now)\n",
    "\n",
    "    # ---- internal micro-steps ----\n",
    "    # Aggregate the true (unscaled) mean losses across micro-steps to report per optimizer step.\n",
    "    micro_losses = []\n",
    "    loss_acc = float(0)\n",
    "\n",
    "    for micro in tqdm(range(accum_steps)):\n",
    "        X, y, loss_scaler = next(train_batch_generator)\n",
    "\n",
    "        # Forward\n",
    "        logits = tt_model(X, causal_mask)  # [B,1,T,V]\n",
    "\n",
    "        # CE on masked logits\n",
    "        loss = loss_fn(logits, y, reduce)  # [B,1,T,1] shape reduced later\n",
    "        loss = loss * loss_scaler\n",
    "        loss = ttml.ops.unary.mean(loss)  # scalar\n",
    "        loss_acc = loss_acc + loss.to_numpy()[0]\n",
    "\n",
    "        # Track true loss for reporting\n",
    "        # micro_losses.append(float(loss.to_numpy()))\n",
    "        micro_losses.append(get_loss_over_devices(loss))\n",
    "\n",
    "        # Scale for accumulation and backward\n",
    "        scaled_loss = ttml.ops.binary.mul(\n",
    "            loss, 1 / accum_steps\n",
    "        ) \n",
    "        scaled_loss.backward(False)\n",
    "        ttml.autograd.AutoContext.get_instance().reset_graph()\n",
    "    print(loss_acc[0,0,0] / accum_steps)\n",
    "    # Synchronize gradients if DDP is enabled\n",
    "    if device_config.enable_ddp:\n",
    "        ttml.core.distributed.synchronize_parameters(tt_model.parameters())\n",
    "\n",
    "    # Optimizer step after micro-steps\n",
    "    optim.step()\n",
    "\n",
    "    # Average loss across micro-steps (this corresponds to the optimizer step)\n",
    "    step_loss = float(np.mean(micro_losses)) if len(micro_losses) > 0 else 0.0\n",
    "    train_losses.append(step_loss)\n",
    "\n",
    "    # tqdm postfix\n",
    "    postfix = {\"train_loss\": f\"{step_loss:.4f}\"}\n",
    "    if last_val_loss is not None:\n",
    "        postfix[\"val_loss\"] = f\"{last_val_loss:.4f}\"\n",
    "    bar.set_postfix(postfix, refresh=False)\n",
    "    print((total_steps + 1) % yaml_config.eval_every == 0\n",
    "        or total_steps + 1 == yaml_config.steps)\n",
    "    total_steps += 1\n",
    "    \n",
    "    # Validation every eval_every steps\n",
    "    if (\n",
    "        total_steps % yaml_config.eval_every == 0\n",
    "        or total_steps + 1 == yaml_config.steps\n",
    "    ):\n",
    "        last_val_loss = validate(\n",
    "            tt_model,\n",
    "            tokenizer,\n",
    "            val_batch_generator,\n",
    "            testing_data,\n",
    "            loss_fn,\n",
    "            causal_mask,\n",
    "            logits_mask_tensor,\n",
    "            max_sequence_length=max_seq_lenght,\n",
    "            max_gen_tokens=max_gen_tokens,\n",
    "            current_step=total_steps,\n",
    "        )\n",
    "        \n",
    "        val_losses.append(last_val_loss)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1708d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "525336576 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093c1c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "525336576 / 2 / 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20489929",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98469aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_config.eval_every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ed08d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aab6140",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91948c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0a938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training completed!\")\n",
    "\n",
    "# Plot training curves\n",
    "print(\"Plotting training curves...\")\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 5))\n",
    "axs.plot(train_losses, color=\"blue\", label=\"Train Loss\")\n",
    "axs.plot(\n",
    "    np.arange(0, len(val_losses)) * yaml_config.eval_every,\n",
    "    val_losses,\n",
    "    color=\"orange\",\n",
    "    label=\"Val Loss\",\n",
    ")\n",
    "axs.set_title(\"Training Loss\")\n",
    "axs.set_xlabel(\"Steps\")\n",
    "axs.set_ylabel(\"Loss\")\n",
    "axs.legend()\n",
    "plt.savefig(\"training_curves.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71842a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e728cb06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
